Current timestep = 0. State = [[-0.23950107  0.11468221]]. Action = [[ 0.06125146 -0.07920149  0.10081658  0.69067574]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is None
Current timestep = 1. State = [[-0.23930737  0.11471249]]. Action = [[-0.23240227  0.1631751  -0.17369232 -0.90625656]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False]
Scene graph at timestep 1 is [True, False, False, False, True, False]
State prediction error at timestep 1 is tensor(0.0341, grad_fn=<MseLossBackward0>)
Current timestep = 2. State = [[-0.24033454  0.1162285 ]]. Action = [[ 0.11020851 -0.17398492  0.23851222 -0.82345456]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False]
Current timestep = 3. State = [[-0.24009721  0.11571652]]. Action = [[-0.15591268 -0.21682884 -0.08905159 -0.9415077 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False]
Scene graph at timestep 3 is [True, False, False, False, True, False]
State prediction error at timestep 3 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Current timestep = 4. State = [[-0.23989096  0.11365449]]. Action = [[-0.22425121 -0.07965478 -0.01384744 -0.9665873 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False]
Current timestep = 5. State = [[-0.2405932   0.11189709]]. Action = [[-0.15721938  0.22521341 -0.17863904 -0.8161903 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, False, True, False]
Current timestep = 6. State = [[-0.24287851  0.11333573]]. Action = [[-0.22898799 -0.23605625 -0.01535991 -0.9413063 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False]
Current timestep = 7. State = [[-0.24561985  0.11131049]]. Action = [[ 0.24853906 -0.16049099  0.10509264 -0.44437706]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7 is [True, False, False, False, True, False]
Scene graph at timestep 7 is [True, False, False, False, True, False]
State prediction error at timestep 7 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Current timestep = 8. State = [[-0.24582529  0.10755327]]. Action = [[-0.12100756 -0.00739892  0.2298272   0.68757486]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8 is [True, False, False, False, True, False]
Current timestep = 9. State = [[-0.24684624  0.10552902]]. Action = [[0.11523247 0.222624   0.17902136 0.39586926]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9 is [True, False, False, False, True, False]
Scene graph at timestep 9 is [True, False, False, False, True, False]
State prediction error at timestep 9 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Current timestep = 10. State = [[-0.2473767   0.10628031]]. Action = [[-0.11354211 -0.11628427 -0.12555505 -0.8633428 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 10 is [True, False, False, False, True, False]
Scene graph at timestep 10 is [True, False, False, False, True, False]
State prediction error at timestep 10 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 11. State = [[-0.24778229  0.10557562]]. Action = [[ 0.09525976  0.01120397 -0.1876632  -0.26554167]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 11 is [True, False, False, False, True, False]
Current timestep = 12. State = [[-0.24791723  0.10523903]]. Action = [[ 0.1296584  -0.05235423  0.24371058 -0.15083975]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 12 is [True, False, False, False, True, False]
Scene graph at timestep 12 is [True, False, False, False, True, False]
State prediction error at timestep 12 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 13. State = [[-0.24777661  0.10419567]]. Action = [[-0.15797791 -0.07763529 -0.11209282 -0.10451931]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 13 is [True, False, False, False, True, False]
Scene graph at timestep 13 is [True, False, False, False, True, False]
State prediction error at timestep 13 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 14. State = [[-0.24767488  0.10255185]]. Action = [[-0.19274928 -0.09004012  0.19842708 -0.15036672]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 14 is [True, False, False, False, True, False]
Current timestep = 15. State = [[-0.24889967  0.10066217]]. Action = [[-0.22927104 -0.15082522 -0.22894144 -0.09126753]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 15 is [True, False, False, False, True, False]
Current timestep = 16. State = [[-0.251948    0.09777514]]. Action = [[ 0.03064281  0.05021557 -0.16024068 -0.69185215]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 16 is [True, False, False, False, True, False]
Scene graph at timestep 16 is [True, False, False, False, True, False]
State prediction error at timestep 16 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 17. State = [[-0.25409088  0.09625612]]. Action = [[ 0.17463517  0.15604433  0.24448806 -0.95077765]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 17 is [True, False, False, False, True, False]
Current timestep = 18. State = [[-0.25434792  0.09686727]]. Action = [[ 0.09556437 -0.07789388  0.18662673  0.56470084]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 18 is [True, False, False, False, True, False]
Human Feedback received at timestep 18 of -1
Current timestep = 19. State = [[-0.25426295  0.09647927]]. Action = [[ 0.1945337  -0.04518774 -0.21434964  0.79148483]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 19 is [True, False, False, False, True, False]
Scene graph at timestep 19 is [True, False, False, False, True, False]
State prediction error at timestep 19 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 20. State = [[-0.2539828   0.09570526]]. Action = [[ 0.19571912 -0.02177945 -0.20282015 -0.00666219]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 20 is [True, False, False, False, True, False]
Current timestep = 21. State = [[-0.2529851   0.09507071]]. Action = [[0.17142382 0.0700101  0.24049526 0.69910705]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 21 is [True, False, False, False, True, False]
Current timestep = 22. State = [[-0.25088817  0.09514232]]. Action = [[-0.10362384 -0.20784561  0.09729922  0.8456893 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 22 is [True, False, False, False, True, False]
Current timestep = 23. State = [[-0.2501252   0.09306768]]. Action = [[ 0.0236645   0.05033991 -0.02818383 -0.9057755 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 23 is [True, False, False, False, True, False]
Scene graph at timestep 23 is [True, False, False, False, True, False]
State prediction error at timestep 23 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 24. State = [[-0.24946961  0.09178628]]. Action = [[-0.14729021 -0.14237218 -0.01522212  0.38929212]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 24 is [True, False, False, False, True, False]
Current timestep = 25. State = [[-0.2491941   0.08908658]]. Action = [[-0.11353922 -0.18248971  0.16365397  0.67170453]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 25 is [True, False, False, False, True, False]
Current timestep = 26. State = [[-0.24894492  0.08559828]]. Action = [[-0.18300065 -0.17774987 -0.03361453 -0.28961003]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 26 is [True, False, False, False, True, False]
Scene graph at timestep 26 is [True, False, False, False, True, False]
State prediction error at timestep 26 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 27. State = [[-0.24936995  0.08110351]]. Action = [[-0.11199    -0.13817774 -0.17463104  0.6919198 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 27 is [True, False, False, False, True, False]
Scene graph at timestep 27 is [True, False, False, False, True, False]
State prediction error at timestep 27 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 28. State = [[-0.2508144   0.07587057]]. Action = [[-0.1920211   0.08383453  0.10100484  0.96648264]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 28 is [True, False, False, False, True, False]
Current timestep = 29. State = [[-0.25345796  0.07404263]]. Action = [[-0.01731634 -0.07067281 -0.20136437 -0.19058585]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 29 is [True, False, False, False, True, False]
Current timestep = 30. State = [[-0.25518298  0.07212715]]. Action = [[-0.13271551  0.0255155   0.20439184  0.45229352]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 30 is [True, False, False, False, True, False]
Current timestep = 31. State = [[-0.25756973  0.07095957]]. Action = [[ 0.00549838 -0.16863191 -0.10852988 -0.96869713]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 31 is [True, False, False, False, True, False]
Current timestep = 32. State = [[-0.25871322  0.06878799]]. Action = [[0.14654538 0.20455241 0.20483431 0.6679505 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 32 is [True, False, False, False, True, False]
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.25908807  0.06900312]]. Action = [[-0.06236637 -0.17380977 -0.078408   -0.51843   ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 33 is [True, False, False, False, True, False]
Current timestep = 34. State = [[-0.259112    0.06732683]]. Action = [[-0.05200478 -0.19007333  0.01057526 -0.17875588]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 34 is [True, False, False, False, True, False]
Current timestep = 35. State = [[-0.2597445   0.06410354]]. Action = [[0.06994635 0.20842853 0.18416706 0.6360973 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 35 is [True, False, False, False, True, False]
Current timestep = 36. State = [[-0.26006815  0.06392101]]. Action = [[ 0.18429822 -0.16816425  0.02209798 -0.79408544]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 36 is [True, False, False, False, True, False]
Current timestep = 37. State = [[-0.25980017  0.06216895]]. Action = [[ 0.23065042  0.17128944 -0.10643074 -0.44098306]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 37 is [True, False, False, False, True, False]
Current timestep = 38. State = [[-0.25875065  0.06227333]]. Action = [[-0.0432487  -0.21703185 -0.12132403 -0.6984433 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 38 is [True, False, False, False, True, False]
Current timestep = 39. State = [[-0.25818482  0.06078113]]. Action = [[ 0.13756603 -0.00868264 -0.10311551  0.29481697]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 39 is [True, False, False, False, True, False]
Current timestep = 40. State = [[-0.25707892  0.0595547 ]]. Action = [[ 0.0899449  -0.17090644 -0.12754029  0.4766431 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 40 is [True, False, False, False, True, False]
Current timestep = 41. State = [[-0.25540745  0.05687967]]. Action = [[-0.04686414 -0.14174327  0.2457833  -0.2435869 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 41 is [True, False, False, False, True, False]
Scene graph at timestep 41 is [True, False, False, False, True, False]
State prediction error at timestep 41 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 42. State = [[-0.25463668  0.05324598]]. Action = [[-0.13059838 -0.00561941 -0.10372397 -0.666057  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 42 is [True, False, False, False, True, False]
Scene graph at timestep 42 is [True, False, False, False, True, False]
State prediction error at timestep 42 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 43. State = [[-0.25455454  0.05025208]]. Action = [[ 0.09928268  0.15801567 -0.15900153 -0.7904552 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 43 is [True, False, False, False, True, False]
Current timestep = 44. State = [[-0.25460798  0.0504329 ]]. Action = [[-0.00336602  0.18661505  0.04539928 -0.8666584 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 44 is [True, False, False, False, True, False]
Scene graph at timestep 44 is [True, False, False, False, True, False]
State prediction error at timestep 44 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 45. State = [[-0.25474137  0.05159245]]. Action = [[-0.14856608  0.05456081 -0.24320772 -0.50153685]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 45 is [True, False, False, False, True, False]
Scene graph at timestep 45 is [True, False, False, False, True, False]
State prediction error at timestep 45 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 46. State = [[-0.25544834  0.05319486]]. Action = [[-0.15558264  0.14129701  0.14648622  0.22833812]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 46 is [True, False, False, False, True, False]
Scene graph at timestep 46 is [True, False, False, False, True, False]
State prediction error at timestep 46 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 47. State = [[-0.2570126   0.05641825]]. Action = [[-0.13268562  0.03682837 -0.07889481  0.68273294]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 47 is [True, False, False, False, True, False]
Current timestep = 48. State = [[-0.25838062  0.05916412]]. Action = [[ 0.15395015 -0.13780013 -0.20114571  0.59022784]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 48 is [True, False, False, False, True, False]
Current timestep = 49. State = [[-0.25854307  0.0592498 ]]. Action = [[-0.19978777 -0.04375301 -0.24073125 -0.7305892 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 49 is [True, False, False, False, True, False]
Current timestep = 50. State = [[-0.25916356  0.05906601]]. Action = [[-0.04320748 -0.01604342  0.16759813 -0.62704104]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 50 is [True, False, False, False, True, False]
Current timestep = 51. State = [[-0.2596305   0.05881653]]. Action = [[-0.23342137 -0.19401217  0.08487657  0.49414492]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 51 is [True, False, False, False, True, False]
Scene graph at timestep 51 is [True, False, False, False, True, False]
State prediction error at timestep 51 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 52. State = [[-0.26146305  0.05617957]]. Action = [[ 0.16247874 -0.21334147 -0.22457044 -0.31741846]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 52 is [True, False, False, False, True, False]
Current timestep = 53. State = [[-0.26168752  0.05271477]]. Action = [[0.15722871 0.14402896 0.18897983 0.7888596 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 53 is [True, False, False, False, True, False]
Current timestep = 54. State = [[-0.26161277  0.05130397]]. Action = [[ 0.06860775 -0.21541448  0.14107335 -0.05355585]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 54 is [True, False, False, False, True, False]
Current timestep = 55. State = [[-0.26141548  0.04884514]]. Action = [[-0.06585839  0.23764744  0.00338116 -0.00323081]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 55 is [True, False, False, False, True, False]
Current timestep = 56. State = [[-0.2617656   0.04910067]]. Action = [[0.23156911 0.14898217 0.22592264 0.57030773]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 56 is [True, False, False, False, True, False]
Scene graph at timestep 56 is [True, False, False, False, True, False]
State prediction error at timestep 56 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 57. State = [[-0.2613811   0.04996916]]. Action = [[-0.05281274 -0.12621558  0.08010852 -0.9121257 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 57 is [True, False, False, False, True, False]
Current timestep = 58. State = [[-0.261317    0.04988119]]. Action = [[ 0.1199666  -0.18853293 -0.17270833  0.9168744 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 58 is [True, False, False, False, True, False]
Scene graph at timestep 58 is [True, False, False, False, True, False]
State prediction error at timestep 58 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 59. State = [[-0.26005358  0.04796289]]. Action = [[ 0.02078465 -0.03104053 -0.03146581  0.90825343]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 59 is [True, False, False, False, True, False]
Current timestep = 60. State = [[-0.25941464  0.04660763]]. Action = [[0.21374488 0.07077202 0.00422707 0.274042  ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 60 is [True, False, False, False, True, False]
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.25750378  0.04643974]]. Action = [[-0.16751912 -0.12476635  0.08278424 -0.10604596]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 61 is [True, False, False, False, True, False]
Current timestep = 62. State = [[-0.25718546  0.0445866 ]]. Action = [[ 0.2307871  -0.14633259 -0.01901098  0.39002895]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 62 is [True, False, False, False, True, False]
Current timestep = 63. State = [[-0.25529265  0.04181242]]. Action = [[-0.12280011 -0.05511093 -0.20343304  0.8707857 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 63 is [True, False, False, False, True, False]
Current timestep = 64. State = [[-0.2546975   0.03980984]]. Action = [[-0.04315822 -0.05802825  0.2044794  -0.55210614]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 64 is [True, False, False, False, True, False]
Current timestep = 65. State = [[-0.2543804   0.03818738]]. Action = [[ 0.14795089  0.01178226 -0.23825745 -0.63092285]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 65 is [True, False, False, False, True, False]
Current timestep = 66. State = [[-0.25342175  0.03720488]]. Action = [[-0.1673308  -0.07853842 -0.23443875 -0.8157449 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 66 is [True, False, False, False, True, False]
Scene graph at timestep 66 is [True, False, False, False, True, False]
State prediction error at timestep 66 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 67. State = [[-0.25320917  0.03566191]]. Action = [[-2.0687659e-01  2.5928020e-06 -2.2151917e-01 -3.7325656e-01]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 67 is [True, False, False, False, True, False]
Scene graph at timestep 67 is [True, False, False, False, True, False]
State prediction error at timestep 67 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 68. State = [[-0.25364828  0.03479723]]. Action = [[-0.1287593   0.23604012  0.11214674  0.8493867 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 68 is [True, False, False, False, True, False]
Current timestep = 69. State = [[-0.2548246   0.03581484]]. Action = [[ 0.19770879 -0.03321095  0.17434072  0.16791916]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 69 is [True, False, False, False, True, False]
Current timestep = 70. State = [[-0.2547677   0.03592315]]. Action = [[ 0.03738934  0.10749632 -0.13603474 -0.20307899]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 70 is [True, False, False, False, True, False]
Scene graph at timestep 70 is [True, False, False, False, True, False]
State prediction error at timestep 70 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 71. State = [[-0.2550386   0.03649952]]. Action = [[-0.22664323  0.19597489 -0.23881157  0.48552942]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 71 is [True, False, False, False, True, False]
Current timestep = 72. State = [[-0.25649807  0.03982482]]. Action = [[ 0.19648004 -0.02525993 -0.13229069  0.62389255]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 72 is [True, False, False, False, True, False]
Current timestep = 73. State = [[-0.2567883   0.04068105]]. Action = [[-0.16679129 -0.05872965 -0.06860828  0.07385314]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 73 is [True, False, False, False, True, False]
Current timestep = 74. State = [[-0.2569794  0.0411281]]. Action = [[ 0.15723604 -0.02975847 -0.13552251  0.19698775]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 74 is [True, False, False, False, True, False]
Current timestep = 75. State = [[-0.25694102  0.04091143]]. Action = [[0.11413053 0.01973748 0.03406483 0.98758864]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 75 is [True, False, False, False, True, False]
Current timestep = 76. State = [[-0.2569071   0.04083588]]. Action = [[-0.12930232 -0.2381919  -0.05386369  0.56703186]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 76 is [True, False, False, False, True, False]
Current timestep = 77. State = [[-0.25648823  0.0397436 ]]. Action = [[ 0.12810615  0.0360164  -0.10267082 -0.03367728]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 77 is [True, False, False, False, True, False]
Current timestep = 78. State = [[-0.25612733  0.03911709]]. Action = [[ 0.11180994 -0.06242886 -0.09522502  0.33479953]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 78 is [True, False, False, False, True, False]
Current timestep = 79. State = [[-0.255319    0.03805559]]. Action = [[-0.20123133  0.11847395  0.22405213 -0.35045552]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 79 is [True, False, False, False, True, False]
Scene graph at timestep 79 is [True, False, False, False, True, False]
State prediction error at timestep 79 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 80. State = [[-0.25533357  0.03805031]]. Action = [[ 0.1097911  -0.21713008  0.23135513 -0.8077356 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 80 is [True, False, False, False, True, False]
Current timestep = 81. State = [[-0.25490242  0.03659536]]. Action = [[ 0.06318972  0.00196645  0.15744773 -0.51288575]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 81 is [True, False, False, False, True, False]
Current timestep = 82. State = [[-0.2545569   0.03554285]]. Action = [[0.07634807 0.09969711 0.23429975 0.02487266]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 82 is [True, False, False, False, True, False]
Current timestep = 83. State = [[-0.25415516  0.03562381]]. Action = [[-0.11067255  0.03159094  0.24273866 -0.6567034 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 83 is [True, False, False, False, True, False]
Current timestep = 84. State = [[-0.254255   0.0358379]]. Action = [[-0.22006048  0.16603208 -0.05301428 -0.6679596 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 84 is [True, False, False, False, True, False]
Current timestep = 85. State = [[-0.25497213  0.03723664]]. Action = [[ 0.05401188 -0.05698958  0.16255668 -0.5387961 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 85 is [True, False, False, False, True, False]
Scene graph at timestep 85 is [True, False, False, False, True, False]
State prediction error at timestep 85 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 86. State = [[-0.2550378   0.03749238]]. Action = [[-0.05241899  0.17783833 -0.17364295  0.16418171]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 86 is [True, False, False, False, True, False]
Current timestep = 87. State = [[-0.25584134  0.03939805]]. Action = [[-0.17662214  0.1867772  -0.22340529  0.19332123]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 87 is [True, False, False, False, True, False]
Current timestep = 88. State = [[-0.25757658  0.04359065]]. Action = [[ 0.05668694 -0.00423293  0.20787919 -0.3706807 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 88 is [True, False, False, False, True, False]
Scene graph at timestep 88 is [True, False, False, False, True, False]
State prediction error at timestep 88 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 89. State = [[-0.25865695  0.04608561]]. Action = [[-0.0114226   0.14270777  0.08523464 -0.6912026 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 89 is [True, False, False, False, True, False]
Scene graph at timestep 89 is [True, False, False, False, True, False]
State prediction error at timestep 89 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 90. State = [[-0.26000625  0.04918411]]. Action = [[ 0.1776979  -0.08190247 -0.04971351 -0.9396821 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 90 is [True, False, False, False, True, False]
Scene graph at timestep 90 is [True, False, False, False, True, False]
State prediction error at timestep 90 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 91. State = [[-0.26003572  0.04940961]]. Action = [[-0.19844855 -0.12348884  0.08305144 -0.23069519]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 91 is [True, False, False, False, True, False]
Current timestep = 92. State = [[-0.26003572  0.04940961]]. Action = [[-0.01569369 -0.00223018  0.11867496  0.88777304]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 92 is [True, False, False, False, True, False]
Current timestep = 93. State = [[-0.26001444  0.04930313]]. Action = [[ 0.08684766 -0.1451396   0.22086316  0.18342304]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 93 is [True, False, False, False, True, False]
Current timestep = 94. State = [[-0.2598467   0.04851401]]. Action = [[0.09562814 0.1356917  0.2131603  0.22521925]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 94 is [True, False, False, False, True, False]
Scene graph at timestep 94 is [True, False, False, False, True, False]
State prediction error at timestep 94 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 95. State = [[-0.25986665  0.04850062]]. Action = [[-0.17675127 -0.07125725 -0.12197796  0.82217944]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 95 is [True, False, False, False, True, False]
Current timestep = 96. State = [[-0.25995064  0.04833829]]. Action = [[-0.21514165 -0.1985593  -0.07933287 -0.7316852 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 96 is [True, False, False, False, True, False]
Current timestep = 97. State = [[-0.2612602   0.04622122]]. Action = [[ 0.08449465  0.09364069 -0.17390908 -0.66230464]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 97 is [True, False, False, False, True, False]
Current timestep = 98. State = [[-0.2614994   0.04602703]]. Action = [[-0.21094707  0.21921161 -0.21898118 -0.18680727]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 98 is [True, False, False, False, True, False]
Current timestep = 99. State = [[-0.26359296  0.04817234]]. Action = [[-0.09146652  0.15070003  0.16536742 -0.53180027]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 99 is [True, False, False, False, True, False]
Current timestep = 100. State = [[-0.26597783  0.05181139]]. Action = [[-0.17630666 -0.1075283  -0.03087144 -0.4813224 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 100 is [True, False, False, False, True, False]
Scene graph at timestep 100 is [True, False, False, False, True, False]
State prediction error at timestep 100 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 101. State = [[-0.26840252  0.05271995]]. Action = [[ 0.23738956 -0.1698909  -0.17201817  0.60935545]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 101 is [True, False, False, False, True, False]
Current timestep = 102. State = [[-0.26823622  0.05131114]]. Action = [[-0.11464077  0.02383807  0.14109457 -0.8488544 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 102 is [True, False, False, False, True, False]
Current timestep = 103. State = [[-0.26905227  0.05056026]]. Action = [[-0.07697181 -0.12669507 -0.19476081  0.01672924]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 103 is [True, False, False, False, True, False]
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.26998922  0.04871479]]. Action = [[ 0.23897582  0.04145184 -0.2065824  -0.80017847]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 104 is [True, False, False, False, True, False]
Current timestep = 105. State = [[-0.26996234  0.04831661]]. Action = [[ 0.10929978  0.08252189 -0.22101071 -0.81709766]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 105 is [True, False, False, False, True, False]
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.27009606  0.04852154]]. Action = [[-0.16457854  0.21888274 -0.05434933 -0.03807652]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 106 is [True, False, False, False, True, False]
Scene graph at timestep 106 is [True, False, False, False, True, False]
State prediction error at timestep 106 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 107. State = [[-0.2707949   0.05010505]]. Action = [[ 0.03097072 -0.20623338 -0.22967306  0.22577715]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 107 is [True, False, False, False, True, False]
Current timestep = 108. State = [[-0.2706171   0.04975665]]. Action = [[-0.0093962   0.05963713  0.19589797  0.96149516]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 108 is [True, False, False, False, True, False]
Current timestep = 109. State = [[-0.2706787   0.04989149]]. Action = [[ 0.05723774 -0.07426454  0.11424217 -0.83314764]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 109 is [True, False, False, False, True, False]
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.270636    0.04968256]]. Action = [[-0.1525751  -0.18846071 -0.19538608 -0.07901096]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 110 is [True, False, False, False, True, False]
Current timestep = 111. State = [[-0.270636    0.04968256]]. Action = [[ 0.19030571 -0.01054215  0.17087743 -0.9277152 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 111 is [True, False, False, False, True, False]
Current timestep = 112. State = [[-0.27046573  0.04939721]]. Action = [[-0.14753917 -0.04689696  0.06225491  0.13051105]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 112 is [True, False, False, False, True, False]
Current timestep = 113. State = [[-0.27038425  0.0490594 ]]. Action = [[ 0.24365464 -0.08245514 -0.21475296  0.5318234 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 113 is [True, False, False, False, True, False]
Scene graph at timestep 113 is [True, False, False, False, True, False]
State prediction error at timestep 113 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 114. State = [[-0.26917672  0.04745142]]. Action = [[ 0.12855196 -0.07097587 -0.11549389  0.536165  ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 114 is [True, False, False, False, True, False]
Current timestep = 115. State = [[-0.26744962  0.0457886 ]]. Action = [[-0.09311023 -0.10441136 -0.21564664  0.92819154]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 115 is [True, False, False, False, True, False]
Current timestep = 116. State = [[-0.26619586  0.04369131]]. Action = [[ 0.17186278  0.08610034 -0.24139957 -0.09331912]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 116 is [True, False, False, False, True, False]
Current timestep = 117. State = [[-0.2652181   0.04313023]]. Action = [[-0.13974889  0.14797777  0.04792199  0.16590011]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 117 is [True, False, False, False, True, False]
Current timestep = 118. State = [[-0.26517227  0.04348721]]. Action = [[-0.03299814  0.06696945  0.16183412 -0.13340718]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 118 is [True, False, False, False, True, False]
Current timestep = 119. State = [[-0.26525313  0.04396737]]. Action = [[ 0.23369837 -0.15044306 -0.11467794  0.00220358]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 119 is [True, False, False, False, True, False]
Scene graph at timestep 119 is [True, False, False, False, True, False]
State prediction error at timestep 119 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 120. State = [[-0.2640279   0.04375683]]. Action = [[0.01854494 0.1862998  0.12054768 0.8476317 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 120 is [True, False, False, False, True, False]
Current timestep = 121. State = [[-0.2625589   0.04431567]]. Action = [[ 0.18914801 -0.21862203  0.17021734  0.9582648 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 121 is [True, False, False, False, True, False]
Current timestep = 122. State = [[-0.2603017   0.04335817]]. Action = [[ 0.07292596 -0.06844229 -0.21585004  0.8700621 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 122 is [True, False, False, False, True, False]
Current timestep = 123. State = [[-0.25805634  0.04177293]]. Action = [[-0.13870427 -0.04956079  0.1244258  -0.50535756]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 123 is [True, False, False, False, True, False]
Current timestep = 124. State = [[-0.25746968  0.04084802]]. Action = [[ 0.12215579  0.21304598 -0.14641573  0.54448104]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 124 is [True, False, False, False, True, False]
Scene graph at timestep 124 is [True, False, False, False, True, False]
State prediction error at timestep 124 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 125. State = [[-0.25670794  0.04156825]]. Action = [[ 0.19668847  0.21633762 -0.12415472 -0.927535  ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 125 is [True, False, False, False, True, False]
Current timestep = 126. State = [[-0.25489134  0.04406355]]. Action = [[-0.24293609 -0.01735044  0.09706753  0.23817861]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 126 is [True, False, False, False, True, False]
Scene graph at timestep 126 is [True, False, False, False, True, False]
State prediction error at timestep 126 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 127. State = [[-0.25495392  0.04519681]]. Action = [[ 0.16892418  0.13252378  0.22332299 -0.75395435]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 127 is [True, False, False, False, True, False]
Current timestep = 128. State = [[-0.25429863  0.04673719]]. Action = [[0.05685568 0.10731477 0.12473717 0.27722096]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 128 is [True, False, False, False, True, False]
Current timestep = 129. State = [[-0.25335184  0.04905885]]. Action = [[-0.05844899  0.1749267   0.0182938   0.64816415]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 129 is [True, False, False, False, True, False]
Current timestep = 130. State = [[-0.25354627  0.05232448]]. Action = [[ 0.19427156 -0.16293728 -0.1384525  -0.0610103 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 130 is [True, False, False, False, True, False]
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.25175008  0.05257824]]. Action = [[-0.17543183 -0.13134806 -0.02624011 -0.8003752 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 131 is [True, False, False, False, True, False]
Scene graph at timestep 131 is [True, False, False, False, True, False]
State prediction error at timestep 131 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 132. State = [[-0.25125092  0.05256256]]. Action = [[-0.04784974  0.02409324  0.23382473 -0.8420028 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 132 is [True, False, False, False, True, False]
Current timestep = 133. State = [[-0.25144118  0.05234992]]. Action = [[ 0.05276981 -0.15166716  0.19889694 -0.93644774]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 133 is [True, False, False, False, True, False]
Current timestep = 134. State = [[-0.25085333  0.05145471]]. Action = [[ 0.12002569 -0.19343728  0.05394027 -0.6760245 ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 134 is [True, False, False, False, True, False]
Scene graph at timestep 134 is [True, False, False, False, True, False]
State prediction error at timestep 134 is tensor(1.0391e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.24957022  0.04851603]]. Action = [[-0.0450701   0.03560591  0.00758779 -0.5850479 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 135 is [True, False, False, False, True, False]
Current timestep = 136. State = [[-0.24920267  0.04779748]]. Action = [[0.11234552 0.16187742 0.1642254  0.1946764 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 136 is [True, False, False, False, True, False]
Scene graph at timestep 136 is [True, False, False, False, True, False]
State prediction error at timestep 136 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 137. State = [[-0.24784288  0.04804206]]. Action = [[ 0.01358783 -0.05131666 -0.06316176 -0.5472408 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 137 is [True, False, False, False, True, False]
Current timestep = 138. State = [[-0.24633728  0.04820848]]. Action = [[-0.01655325 -0.10119456 -0.11347631  0.7778828 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 138 is [True, False, False, False, True, False]
Current timestep = 139. State = [[-0.24531645  0.04770018]]. Action = [[-0.16302712  0.19256103 -0.18910389 -0.74795264]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 139 is [True, False, False, False, True, False]
Scene graph at timestep 139 is [True, False, False, False, True, False]
State prediction error at timestep 139 is tensor(7.6647e-05, grad_fn=<MseLossBackward0>)
Current timestep = 140. State = [[-0.24559672  0.04853469]]. Action = [[-0.13629341  0.20954913 -0.12031102 -0.5884417 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 140 is [True, False, False, False, True, False]
Current timestep = 141. State = [[-0.24670163  0.05128341]]. Action = [[ 0.09850052 -0.04911341  0.12579155 -0.5116478 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 141 is [True, False, False, False, True, False]
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.24693987  0.05194245]]. Action = [[-0.06908545 -0.1017594  -0.13691337 -0.46473807]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 142 is [True, False, False, False, True, False]
Current timestep = 143. State = [[-0.24690898  0.05194471]]. Action = [[-0.23587598 -0.12045994  0.02976394  0.2626866 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 143 is [True, False, False, False, True, False]
Current timestep = 144. State = [[-0.24697687  0.05140727]]. Action = [[-0.04261106 -0.13136359 -0.01715079  0.37510347]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 144 is [True, False, False, False, True, False]
Current timestep = 145. State = [[-0.24704008  0.04998386]]. Action = [[0.16733706 0.18685222 0.20293629 0.55614436]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 145 is [True, False, False, False, True, False]
Scene graph at timestep 145 is [True, False, False, False, True, False]
State prediction error at timestep 145 is tensor(7.2006e-05, grad_fn=<MseLossBackward0>)
Current timestep = 146. State = [[-0.24721165  0.05016751]]. Action = [[-0.19010782 -0.21936129  0.18017489  0.58077   ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 146 is [True, False, False, False, True, False]
Current timestep = 147. State = [[-0.24767277  0.0487082 ]]. Action = [[-0.07642365 -0.09220144 -0.16844654  0.16214323]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 147 is [True, False, False, False, True, False]
Current timestep = 148. State = [[-0.24878164  0.04662547]]. Action = [[-0.15493686 -0.141364    0.11217183  0.7831247 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 148 is [True, False, False, False, True, False]
Scene graph at timestep 148 is [True, False, False, False, True, False]
State prediction error at timestep 148 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 149. State = [[-0.25042182  0.04378312]]. Action = [[-0.02786554 -0.1424245  -0.0806075   0.6290796 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 149 is [True, False, False, False, True, False]
Current timestep = 150. State = [[-0.2522298   0.04007609]]. Action = [[0.08164233 0.22817123 0.18804961 0.47902763]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 150 is [True, False, False, False, True, False]
Current timestep = 151. State = [[-0.25355327  0.04033213]]. Action = [[-0.22641326  0.23447469  0.15504774  0.7618673 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 151 is [True, False, False, False, True, False]
Current timestep = 152. State = [[-0.2561734   0.04303372]]. Action = [[-1.4439046e-01  1.8795663e-01 -7.4115396e-04 -7.9461843e-01]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 152 is [True, False, False, False, True, False]
Current timestep = 153. State = [[-0.25952953  0.04744628]]. Action = [[0.12484473 0.13797063 0.01142782 0.8558793 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 153 is [True, False, False, False, True, False]
Current timestep = 154. State = [[-0.26128116  0.05121484]]. Action = [[-0.17673808 -0.1480004  -0.20519128  0.7718239 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 154 is [True, False, False, False, True, False]
Scene graph at timestep 154 is [True, False, False, False, True, False]
State prediction error at timestep 154 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.26366565  0.05225309]]. Action = [[ 0.04398564 -0.1492795   0.05071622  0.78663254]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 155 is [True, False, False, False, True, False]
Scene graph at timestep 155 is [True, False, False, False, True, False]
State prediction error at timestep 155 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 156. State = [[-0.2646629   0.05135867]]. Action = [[ 0.24487454  0.20461598  0.23168701 -0.99664587]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 156 is [True, False, False, False, True, False]
Current timestep = 157. State = [[-0.26470175  0.05159592]]. Action = [[-0.17565294 -0.06737947  0.10186264  0.6184442 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 157 is [True, False, False, False, True, False]
Current timestep = 158. State = [[-0.26488963  0.05206617]]. Action = [[ 0.11403704  0.14818895  0.05636773 -0.8512148 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 158 is [True, False, False, False, True, False]
Scene graph at timestep 158 is [True, False, False, False, True, False]
State prediction error at timestep 158 is tensor(1.1627e-05, grad_fn=<MseLossBackward0>)
Current timestep = 159. State = [[-0.2652952  0.0532733]]. Action = [[ 0.22480765  0.05787992  0.00337535 -0.4508596 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 159 is [True, False, False, False, True, False]
Scene graph at timestep 159 is [True, False, False, False, True, False]
State prediction error at timestep 159 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 160. State = [[-0.2647716  0.0542481]]. Action = [[-0.02726942  0.05784139 -0.17237012 -0.77868444]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 160 is [True, False, False, False, True, False]
Scene graph at timestep 160 is [True, False, False, False, True, False]
State prediction error at timestep 160 is tensor(1.2463e-05, grad_fn=<MseLossBackward0>)
Current timestep = 161. State = [[-0.26483357  0.05566707]]. Action = [[-0.03350534  0.2051264   0.12989464 -0.76221836]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 161 is [True, False, False, False, True, False]
Current timestep = 162. State = [[-0.26594675  0.05916924]]. Action = [[-0.20697603 -0.07396774 -0.24306709  0.6270163 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 162 is [True, False, False, False, True, False]
Scene graph at timestep 162 is [True, False, False, False, True, False]
State prediction error at timestep 162 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 163. State = [[-0.2671155   0.06155761]]. Action = [[ 0.1123063   0.15129626 -0.16850007  0.08542919]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 163 is [True, False, False, False, True, False]
Current timestep = 164. State = [[-0.2682779   0.06418423]]. Action = [[-0.09029059  0.09066498  0.05877474 -0.66467446]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 164 is [True, False, False, False, True, False]
Current timestep = 165. State = [[-0.26967987  0.06734222]]. Action = [[ 0.08011782  0.21795988 -0.2092611  -0.49874467]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 165 is [True, False, False, False, True, False]
Scene graph at timestep 165 is [True, False, False, False, True, False]
State prediction error at timestep 165 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 166. State = [[-0.27066192  0.07170382]]. Action = [[-0.18619595 -0.0927352  -0.192797   -0.42172724]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 166 is [True, False, False, False, True, False]
Scene graph at timestep 166 is [True, False, False, False, True, False]
State prediction error at timestep 166 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 167. State = [[-0.2716683   0.07452363]]. Action = [[ 0.14738542  0.03940356  0.05426887 -0.06126583]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 167 is [True, False, False, False, True, False]
Current timestep = 168. State = [[-0.270608    0.07627849]]. Action = [[-0.11580303 -0.00524281 -0.0040984   0.62814164]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 168 is [True, False, False, False, True, False]
Scene graph at timestep 168 is [True, False, False, False, True, False]
State prediction error at timestep 168 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 169. State = [[-0.27060032  0.07769283]]. Action = [[ 0.03924924  0.21944952 -0.15070274 -0.8339478 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 169 is [True, False, False, False, True, False]
Scene graph at timestep 169 is [True, False, False, False, True, False]
State prediction error at timestep 169 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.27070644  0.08187674]]. Action = [[ 0.22527197 -0.11124888 -0.04097322  0.90576386]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 170 is [True, False, False, False, True, False]
Current timestep = 171. State = [[-0.26859942  0.0832925 ]]. Action = [[-0.05366781  0.15251222  0.06727391  0.89166594]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 171 is [True, False, False, False, True, False]
Current timestep = 172. State = [[-0.26757777  0.08544621]]. Action = [[ 0.03277481  0.12475404  0.13162443 -0.94833094]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 172 is [True, False, False, False, True, False]
Current timestep = 173. State = [[-0.2666319   0.08816522]]. Action = [[ 0.12072209 -0.20529242 -0.10957584  0.6632123 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 173 is [True, False, False, False, True, False]
Current timestep = 174. State = [[-0.26507524  0.08828298]]. Action = [[ 0.04729715 -0.14918335  0.16329432 -0.9560348 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 174 is [True, False, False, False, True, False]
Current timestep = 175. State = [[-0.2634739   0.08772124]]. Action = [[-0.22890934 -0.10902321 -0.11537126 -0.91132486]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 175 is [True, False, False, False, True, False]
Scene graph at timestep 175 is [True, False, False, False, True, False]
State prediction error at timestep 175 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 176. State = [[-0.26326033  0.0864325 ]]. Action = [[ 0.12898916  0.14629042  0.24468118 -0.6762471 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 176 is [True, False, False, False, True, False]
Scene graph at timestep 176 is [True, False, False, False, True, False]
State prediction error at timestep 176 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 177. State = [[-0.2631056   0.08654521]]. Action = [[ 0.18135208 -0.05108687  0.19044751 -0.15645945]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 177 is [True, False, False, False, True, False]
Current timestep = 178. State = [[-0.26179177  0.08641288]]. Action = [[-0.09368278 -0.1708933   0.01249143 -0.6425633 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 178 is [True, False, False, False, True, False]
Current timestep = 179. State = [[-0.2610829   0.08470208]]. Action = [[-0.1375424  -0.0333678  -0.21495433  0.60447216]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 179 is [True, False, False, False, True, False]
Current timestep = 180. State = [[-0.26092654  0.08373389]]. Action = [[-0.21206224  0.12781918  0.01283205  0.04768157]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 180 is [True, False, False, False, True, False]
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.26117283  0.08390006]]. Action = [[-0.03973004 -0.17178482  0.03999132  0.11165011]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 181 is [True, False, False, False, True, False]
Current timestep = 182. State = [[-0.26125637  0.08302045]]. Action = [[-0.06229734  0.15095311 -0.08231673  0.11074138]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 182 is [True, False, False, False, True, False]
Current timestep = 183. State = [[-0.26154795  0.08337981]]. Action = [[-0.17905293 -0.09038377 -0.00405522 -0.65646565]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 183 is [True, False, False, False, True, False]
Scene graph at timestep 183 is [True, False, False, False, True, False]
State prediction error at timestep 183 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 184. State = [[-0.26334766  0.0830306 ]]. Action = [[-0.23090151 -0.01725408  0.2030968   0.765259  ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 184 is [True, False, False, False, True, False]
Current timestep = 185. State = [[-0.26632655  0.08295073]]. Action = [[ 0.08215907  0.23023257 -0.09360296  0.835485  ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 185 is [True, False, False, False, True, False]
Current timestep = 186. State = [[-0.2686612   0.08439091]]. Action = [[ 0.01734442 -0.14791873  0.04738     0.41318345]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 186 is [True, False, False, False, True, False]
Current timestep = 187. State = [[-0.2697968   0.08388358]]. Action = [[ 0.13765669 -0.15015736  0.15752193  0.9905119 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 187 is [True, False, False, False, True, False]
Scene graph at timestep 187 is [True, False, False, False, True, False]
State prediction error at timestep 187 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 188. State = [[-0.26955217  0.08223706]]. Action = [[ 0.14513493 -0.15306255 -0.10636534 -0.32369518]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 188 is [True, False, False, False, True, False]
Scene graph at timestep 188 is [True, False, False, False, True, False]
State prediction error at timestep 188 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 189. State = [[-0.26890093  0.07905837]]. Action = [[ 0.15296513  0.19629222 -0.22953762  0.14285016]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 189 is [True, False, False, False, True, False]
Current timestep = 190. State = [[-0.26880863  0.07889607]]. Action = [[ 0.00329298 -0.13950604  0.0356957   0.89710236]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 190 is [True, False, False, False, True, False]
Current timestep = 191. State = [[-0.26827255  0.07731955]]. Action = [[ 0.24437433 -0.08261731  0.05874419  0.6012579 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 191 is [True, False, False, False, True, False]
Current timestep = 192. State = [[-0.26645526  0.07514537]]. Action = [[ 0.10681266  0.04738647  0.01362509 -0.9518421 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 192 is [True, False, False, False, True, False]
Scene graph at timestep 192 is [True, False, False, False, True, False]
State prediction error at timestep 192 is tensor(5.9114e-05, grad_fn=<MseLossBackward0>)
Current timestep = 193. State = [[-0.26416188  0.0743988 ]]. Action = [[ 0.06811193 -0.07079825  0.05308312  0.67035365]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 193 is [True, False, False, False, True, False]
Current timestep = 194. State = [[-0.26187804  0.07285722]]. Action = [[ 0.17802072  0.16924298  0.19771922 -0.7348145 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 194 is [True, False, False, False, True, False]
Current timestep = 195. State = [[-0.2590178   0.07320982]]. Action = [[-0.10618235 -0.1181903  -0.1934851  -0.9217885 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 195 is [True, False, False, False, True, False]
Current timestep = 196. State = [[-0.25777322  0.07260571]]. Action = [[-0.14748248 -0.1137307   0.18912101  0.6828742 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 196 is [True, False, False, False, True, False]
Current timestep = 197. State = [[-0.2572616   0.07116085]]. Action = [[ 0.22651058 -0.16096567 -0.14434622 -0.07432556]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 197 is [True, False, False, False, True, False]
Scene graph at timestep 197 is [True, False, False, False, True, False]
State prediction error at timestep 197 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.25554052  0.06786479]]. Action = [[-0.03593731  0.21588832 -0.09001288 -0.67717975]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 198 is [True, False, False, False, True, False]
Current timestep = 199. State = [[-0.2552772   0.06782405]]. Action = [[-0.24324071  0.0635393  -0.21851791 -0.3587126 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 199 is [True, False, False, False, True, False]
Current timestep = 200. State = [[-0.25551572  0.06836975]]. Action = [[ 0.11720362 -0.18306786  0.07756206  0.405648  ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 200 is [True, False, False, False, True, False]
Current timestep = 201. State = [[-0.25518283  0.06731725]]. Action = [[ 0.00616714 -0.20199993  0.19579011 -0.4998954 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 201 is [True, False, False, False, True, False]
Current timestep = 202. State = [[-0.2542413   0.06421833]]. Action = [[-0.10374975 -0.19406103 -0.1844265   0.32909453]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 202 is [True, False, False, False, True, False]
Current timestep = 203. State = [[-0.25349948  0.06084436]]. Action = [[-0.04585026  0.19630831  0.04693797  0.29672658]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 203 is [True, False, False, False, True, False]
Scene graph at timestep 203 is [True, False, False, False, True, False]
State prediction error at timestep 203 is tensor(8.6191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 204. State = [[-0.2534612   0.06056179]]. Action = [[ 0.19449866  0.21184361 -0.2007973   0.862386  ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 204 is [True, False, False, False, True, False]
Current timestep = 205. State = [[-0.25366357  0.06167471]]. Action = [[-0.05421001 -0.03334306  0.01001635  0.08384442]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 205 is [True, False, False, False, True, False]
Scene graph at timestep 205 is [True, False, False, False, True, False]
State prediction error at timestep 205 is tensor(9.6366e-05, grad_fn=<MseLossBackward0>)
Current timestep = 206. State = [[-0.25378412  0.06189619]]. Action = [[-0.0441675  -0.00357759 -0.00183436 -0.78255147]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 206 is [True, False, False, False, True, False]
Current timestep = 207. State = [[-0.25372836  0.06209556]]. Action = [[ 0.12433785  0.07387909 -0.2166552   0.7349732 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 207 is [True, False, False, False, True, False]
Scene graph at timestep 207 is [True, False, False, False, True, False]
State prediction error at timestep 207 is tensor(3.1945e-05, grad_fn=<MseLossBackward0>)
Current timestep = 208. State = [[-0.25368938  0.06289157]]. Action = [[ 0.04207113  0.15571404 -0.11846316 -0.9450764 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 208 is [True, False, False, False, True, False]
Current timestep = 209. State = [[-0.25357133  0.06468125]]. Action = [[-0.21025296 -0.04119682 -0.19297193 -0.96888816]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 209 is [True, False, False, False, True, False]
Current timestep = 210. State = [[-0.25368914  0.06548874]]. Action = [[-0.1235113  -0.08262236 -0.00109519 -0.3647021 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 210 is [True, False, False, False, True, False]
Current timestep = 211. State = [[-0.25391668  0.06581977]]. Action = [[ 0.08559182 -0.12208259 -0.06647494  0.49835205]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 211 is [True, False, False, False, True, False]
Current timestep = 212. State = [[-0.25374404  0.0649888 ]]. Action = [[ 0.13004267 -0.23148073 -0.17252007 -0.39713103]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 212 is [True, False, False, False, True, False]
Current timestep = 213. State = [[-0.25287557  0.06208248]]. Action = [[ 0.18932897 -0.05161308  0.18046832 -0.02012742]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 213 is [True, False, False, False, True, False]
Current timestep = 214. State = [[-0.25174022  0.05930123]]. Action = [[ 0.20961553  0.02380475 -0.02322373 -0.3738051 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 214 is [True, False, False, False, True, False]
Current timestep = 215. State = [[-0.24948293  0.05731816]]. Action = [[-0.16685794  0.2238828   0.1975852  -0.80862933]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 215 is [True, False, False, False, True, False]
Current timestep = 216. State = [[-0.24949045  0.05842115]]. Action = [[-0.12574673  0.13860947  0.11579269  0.0727886 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 216 is [True, False, False, False, True, False]
Current timestep = 217. State = [[-0.2499857   0.06020448]]. Action = [[-0.19097926 -0.2000411  -0.11015046  0.09608853]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 217 is [True, False, False, False, True, False]
Current timestep = 218. State = [[-0.25010818  0.06040826]]. Action = [[-0.08230916  0.17867988  0.12966728 -0.20613796]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 218 is [True, False, False, False, True, False]
Scene graph at timestep 218 is [True, False, False, False, True, False]
State prediction error at timestep 218 is tensor(6.1071e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.2506635   0.06180773]]. Action = [[-0.05959609  0.03117216 -0.10422701 -0.8245441 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 219 is [True, False, False, False, True, False]
Current timestep = 220. State = [[-0.25119996  0.06283311]]. Action = [[ 0.05329257 -0.2299622   0.09514153  0.1851623 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 220 is [True, False, False, False, True, False]
Current timestep = 221. State = [[-0.25132102  0.06179106]]. Action = [[-0.19122073 -0.08899269  0.09235299 -0.73758966]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 221 is [True, False, False, False, True, False]
Current timestep = 222. State = [[-0.25272954  0.06017575]]. Action = [[-0.10847251 -0.23349002 -0.09071547  0.80235076]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 222 is [True, False, False, False, True, False]
Current timestep = 223. State = [[-0.2544278   0.05706712]]. Action = [[-0.14806321  0.2115801  -0.00292063 -0.5354651 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 223 is [True, False, False, False, True, False]
Current timestep = 224. State = [[-0.2567069   0.05726102]]. Action = [[-0.0434981   0.10866487 -0.14801848 -0.87763715]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 224 is [True, False, False, False, True, False]
Scene graph at timestep 224 is [True, False, False, False, True, False]
State prediction error at timestep 224 is tensor(7.9793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 225. State = [[-0.25942764  0.05799836]]. Action = [[-0.09227753  0.10109344  0.2184956   0.10188091]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 225 is [True, False, False, False, True, False]
Current timestep = 226. State = [[-0.2622754   0.05943539]]. Action = [[-0.13158363 -0.1330083  -0.02651885  0.02047741]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 226 is [True, False, False, False, True, False]
Current timestep = 227. State = [[-0.26478773  0.05923444]]. Action = [[ 0.10269564 -0.21714056  0.1844579  -0.26328313]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 227 is [True, False, False, False, True, False]
Current timestep = 228. State = [[-0.2658588   0.05686933]]. Action = [[-0.01148191 -0.08143046 -0.07381888 -0.8732506 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 228 is [True, False, False, False, True, False]
Scene graph at timestep 228 is [True, False, False, False, True, False]
State prediction error at timestep 228 is tensor(6.7611e-05, grad_fn=<MseLossBackward0>)
Current timestep = 229. State = [[-0.26663658  0.0541425 ]]. Action = [[-0.19747727  0.06600985 -0.23535068  0.20037162]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 229 is [True, False, False, False, True, False]
Current timestep = 230. State = [[-0.26830292  0.05273178]]. Action = [[ 0.18360469 -0.2021757  -0.07750919 -0.6412281 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 230 is [True, False, False, False, True, False]
Scene graph at timestep 230 is [True, False, False, False, True, False]
State prediction error at timestep 230 is tensor(8.8099e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.26833922  0.04945226]]. Action = [[ 0.00761697  0.00355878  0.00149319 -0.08653837]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 231 is [True, False, False, False, True, False]
Scene graph at timestep 231 is [True, False, False, False, True, False]
State prediction error at timestep 231 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 232. State = [[-0.26831344  0.0474009 ]]. Action = [[-0.22707027  0.13327995  0.13196033 -0.16705519]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 232 is [True, False, False, False, True, False]
Current timestep = 233. State = [[-0.26836288  0.04671385]]. Action = [[-0.07211646  0.20308858  0.2213105   0.59236956]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 233 is [True, False, False, False, True, False]
Current timestep = 234. State = [[-0.26884565  0.04753924]]. Action = [[ 0.04612482 -0.13972345  0.1054047  -0.21312732]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 234 is [True, False, False, False, True, False]
Scene graph at timestep 234 is [True, False, False, False, True, False]
State prediction error at timestep 234 is tensor(9.6526e-05, grad_fn=<MseLossBackward0>)
Current timestep = 235. State = [[-0.26891574  0.04730539]]. Action = [[ 0.03729552 -0.1619582  -0.09470439 -0.7754714 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 235 is [True, False, False, False, True, False]
Scene graph at timestep 235 is [True, False, False, False, True, False]
State prediction error at timestep 235 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 236. State = [[-0.26863396  0.04561626]]. Action = [[-0.2310562  -0.21759795 -0.00121628 -0.21874541]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 236 is [True, False, False, False, True, False]
Current timestep = 237. State = [[-0.2685592   0.04455857]]. Action = [[-0.18092558  0.1574477   0.05600598 -0.32934564]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 237 is [True, False, False, False, True, False]
Scene graph at timestep 237 is [True, False, False, False, True, False]
State prediction error at timestep 237 is tensor(7.2301e-05, grad_fn=<MseLossBackward0>)
Current timestep = 238. State = [[-0.26967755  0.04526993]]. Action = [[ 0.17011449  0.22168523 -0.19456162 -0.32351536]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 238 is [True, False, False, False, True, False]
Current timestep = 239. State = [[-0.27052826  0.04716202]]. Action = [[ 0.05608812  0.04890406 -0.01830631 -0.42090523]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 239 is [True, False, False, False, True, False]
Current timestep = 240. State = [[-0.27107793  0.04890757]]. Action = [[ 0.07599741  0.12746814 -0.14525369  0.9713023 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 240 is [True, False, False, False, True, False]
Current timestep = 241. State = [[-0.2716073   0.05097254]]. Action = [[ 0.02432743 -0.00709204 -0.02343139  0.22693324]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 241 is [True, False, False, False, True, False]
Current timestep = 242. State = [[-0.271585    0.05237702]]. Action = [[ 0.23809087  0.12023288 -0.14173368 -0.08073157]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 242 is [True, False, False, False, True, False]
Current timestep = 243. State = [[-0.27004996  0.05465756]]. Action = [[-0.02706714  0.12168306 -0.08663729 -0.5050815 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 243 is [True, False, False, False, True, False]
Current timestep = 244. State = [[-0.26932168  0.05717853]]. Action = [[ 0.14628643  0.07708225  0.13827181 -0.62358713]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 244 is [True, False, False, False, True, False]
Scene graph at timestep 244 is [True, False, False, False, True, False]
State prediction error at timestep 244 is tensor(3.5423e-05, grad_fn=<MseLossBackward0>)
Current timestep = 245. State = [[-0.26800767  0.06003897]]. Action = [[-0.1715703   0.22779813 -0.19358842 -0.9176501 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 245 is [True, False, False, False, True, False]
Current timestep = 246. State = [[-0.26901418  0.06405201]]. Action = [[-0.18804045 -0.00961344 -0.03538585 -0.27116066]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 246 is [True, False, False, False, True, False]
Scene graph at timestep 246 is [True, False, False, False, True, False]
State prediction error at timestep 246 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 247. State = [[-0.27081636  0.06784413]]. Action = [[-0.1338514   0.13536268 -0.04713431  0.28898406]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 247 is [True, False, False, False, True, False]
Current timestep = 248. State = [[-0.27278543  0.07186606]]. Action = [[ 0.0205718  -0.21760562  0.20837116 -0.96484435]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 248 is [True, False, False, False, True, False]
Current timestep = 249. State = [[-0.27301893  0.07209775]]. Action = [[-0.16060022  0.21264517 -0.13402636 -0.4098904 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 249 is [True, False, False, False, True, False]
Current timestep = 250. State = [[-0.27306563  0.07214311]]. Action = [[ 0.11214328 -0.18574914 -0.06927951 -0.41456866]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 250 is [True, False, False, False, True, False]
Current timestep = 251. State = [[-0.27284104  0.07096425]]. Action = [[-0.24069041 -0.1139341   0.21710056  0.893132  ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 251 is [True, False, False, False, True, False]
Human Feedback received at timestep 251 of -1
Current timestep = 252. State = [[-0.27255356  0.06995278]]. Action = [[-0.18979682 -0.14838023  0.15366518 -0.48046267]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 252 is [True, False, False, False, True, False]
Scene graph at timestep 252 is [True, False, False, False, True, False]
State prediction error at timestep 252 is tensor(3.8726e-05, grad_fn=<MseLossBackward0>)
Current timestep = 253. State = [[-0.27249694  0.0694219 ]]. Action = [[-0.19754866 -0.24133487  0.11570185 -0.21841031]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 253 is [True, False, False, False, True, False]
Current timestep = 254. State = [[-0.27247846  0.06911904]]. Action = [[-0.21382345 -0.07317172  0.2155484   0.6836287 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 254 is [True, False, False, False, True, False]
Current timestep = 255. State = [[-0.27248994  0.06898234]]. Action = [[ 0.23718077  0.10967478 -0.13818052  0.6244054 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 255 is [True, False, False, False, True, False]
Current timestep = 256. State = [[-0.27205923  0.06919002]]. Action = [[-0.22911032 -0.20211941  0.07653445 -0.51246864]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 256 is [True, False, False, False, True, False]
Scene graph at timestep 256 is [True, False, False, False, True, False]
State prediction error at timestep 256 is tensor(3.2949e-05, grad_fn=<MseLossBackward0>)
Current timestep = 257. State = [[-0.2715619   0.06924986]]. Action = [[-0.15508072  0.19782329  0.19063562 -0.6470772 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 257 is [True, False, False, False, True, False]
Scene graph at timestep 257 is [True, False, False, False, True, False]
State prediction error at timestep 257 is tensor(4.7945e-05, grad_fn=<MseLossBackward0>)
Current timestep = 258. State = [[-0.2713685   0.06912743]]. Action = [[ 0.18160552 -0.15233356 -0.07379547 -0.7399207 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 258 is [True, False, False, False, True, False]
Current timestep = 259. State = [[-0.26969945  0.06802746]]. Action = [[ 0.03850695  0.02651653 -0.16453816  0.4271567 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 259 is [True, False, False, False, True, False]
Current timestep = 260. State = [[-0.26841003  0.06801867]]. Action = [[-0.00226589  0.21830374  0.0736776   0.3146851 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 260 is [True, False, False, False, True, False]
Current timestep = 261. State = [[-0.267388   0.0688548]]. Action = [[0.00196698 0.05705899 0.09383407 0.09378922]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 261 is [True, False, False, False, True, False]
Current timestep = 262. State = [[-0.26678625  0.06970761]]. Action = [[ 0.19473565 -0.17735295  0.05940521  0.68994   ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 262 is [True, False, False, False, True, False]
Current timestep = 263. State = [[-0.2648065   0.06971648]]. Action = [[-0.20178206  0.07915759 -0.1569568   0.01812243]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 263 is [True, False, False, False, True, False]
Scene graph at timestep 263 is [True, False, False, False, True, False]
State prediction error at timestep 263 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 264. State = [[-0.26472226  0.06974595]]. Action = [[-0.24487372 -0.08988556  0.17629308 -0.8171254 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 264 is [True, False, False, False, True, False]
Current timestep = 265. State = [[-0.26480004  0.06988236]]. Action = [[-0.02936378 -0.11872721  0.16599917  0.9457201 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 265 is [True, False, False, False, True, False]
Current timestep = 266. State = [[-0.264858    0.06913085]]. Action = [[-0.17168379  0.15854293  0.12446681  0.73068094]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 266 is [True, False, False, False, True, False]
Human Feedback received at timestep 266 of 1
Current timestep = 267. State = [[-0.2655357   0.07022206]]. Action = [[ 0.15709996  0.20530838 -0.20163067 -0.824959  ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 267 is [True, False, False, False, True, False]
Current timestep = 268. State = [[-0.2662798   0.07218005]]. Action = [[0.14413148 0.1032562  0.09494448 0.19165552]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 268 is [True, False, False, False, True, False]
Current timestep = 269. State = [[-0.26673648  0.07386963]]. Action = [[-0.17749932 -0.022935   -0.1692727  -0.51184523]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 269 is [True, False, False, False, True, False]
Scene graph at timestep 269 is [True, False, False, False, True, False]
State prediction error at timestep 269 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 270. State = [[-0.26731262  0.07515661]]. Action = [[ 0.20121065 -0.13905714  0.1627121  -0.23729855]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 270 is [True, False, False, False, True, False]
Current timestep = 271. State = [[-0.26714307  0.07484204]]. Action = [[-0.07173455  0.00369242  0.10617891 -0.02100003]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 271 is [True, False, False, False, True, False]
Current timestep = 272. State = [[-0.26714307  0.07484204]]. Action = [[-0.06902558 -0.13303849 -0.02225977 -0.9620396 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 272 is [True, False, False, False, True, False]
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.26692066  0.07418609]]. Action = [[-0.01242691 -0.10362846  0.18097821  0.8329861 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 273 is [True, False, False, False, True, False]
Current timestep = 274. State = [[-0.2666032   0.07231323]]. Action = [[-0.05369887 -0.2121755   0.23354697 -0.5221353 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 274 is [True, False, False, False, True, False]
Scene graph at timestep 274 is [True, False, False, False, True, False]
State prediction error at timestep 274 is tensor(3.8165e-06, grad_fn=<MseLossBackward0>)
Current timestep = 275. State = [[-0.26612905  0.06927511]]. Action = [[-1.0086596e-04  1.3308373e-01  1.4666957e-01 -6.5038520e-01]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 275 is [True, False, False, False, True, False]
Scene graph at timestep 275 is [True, False, False, False, True, False]
State prediction error at timestep 275 is tensor(2.5306e-05, grad_fn=<MseLossBackward0>)
Current timestep = 276. State = [[-0.26610228  0.06895407]]. Action = [[-0.20244302  0.1937111   0.06060213 -0.725733  ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 276 is [True, False, False, False, True, False]
Scene graph at timestep 276 is [True, False, False, False, True, False]
State prediction error at timestep 276 is tensor(4.0718e-05, grad_fn=<MseLossBackward0>)
Current timestep = 277. State = [[-0.2674193   0.07060535]]. Action = [[ 0.13954744  0.09324199  0.12520495 -0.7990027 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 277 is [True, False, False, False, True, False]
Current timestep = 278. State = [[-0.26786423  0.07190745]]. Action = [[ 0.21802136  0.13549238 -0.21919425  0.3512367 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 278 is [True, False, False, False, True, False]
Current timestep = 279. State = [[-0.26817754  0.07353535]]. Action = [[-0.09178267 -0.10895252 -0.1388646  -0.96691835]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 279 is [True, False, False, False, True, False]
Current timestep = 280. State = [[-0.26818603  0.07365713]]. Action = [[ 0.12765974 -0.05765444 -0.24707267 -0.3264953 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 280 is [True, False, False, False, True, False]
Current timestep = 281. State = [[-0.26799875  0.07349044]]. Action = [[-0.10263027  0.1276046  -0.20419274  0.34989214]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 281 is [True, False, False, False, True, False]
Scene graph at timestep 281 is [True, False, False, False, True, False]
State prediction error at timestep 281 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 282. State = [[-0.26811105  0.07417077]]. Action = [[ 0.14759737 -0.16790228  0.00472435  0.05962908]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 282 is [True, False, False, False, True, False]
Current timestep = 283. State = [[-0.26783633  0.07377761]]. Action = [[ 0.03085291  0.01773828 -0.09889722 -0.31282616]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 283 is [True, False, False, False, True, False]
Current timestep = 284. State = [[-0.26773143  0.07345038]]. Action = [[-0.18404882 -0.23227756  0.12749085  0.6651778 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 284 is [True, False, False, False, True, False]
Current timestep = 285. State = [[-0.2670635   0.07148793]]. Action = [[-0.15897484  0.19372231 -0.13706748  0.73230565]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 285 is [True, False, False, False, True, False]
Current timestep = 286. State = [[-0.26722676  0.07180411]]. Action = [[ 0.1442526   0.08835894 -0.08580115 -0.04496014]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 286 is [True, False, False, False, True, False]
Current timestep = 287. State = [[-0.26754     0.07263717]]. Action = [[-0.02625471  0.22375461 -0.2315567   0.8590801 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 287 is [True, False, False, False, True, False]
Scene graph at timestep 287 is [True, False, False, False, True, False]
State prediction error at timestep 287 is tensor(8.9288e-05, grad_fn=<MseLossBackward0>)
Current timestep = 288. State = [[-0.2684744   0.07547516]]. Action = [[-0.02456145  0.09576347  0.23904616 -0.8023964 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 288 is [True, False, False, False, True, False]
Current timestep = 289. State = [[-0.26930243  0.07762598]]. Action = [[ 0.06692886 -0.17188527  0.01683557  0.9158492 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 289 is [True, False, False, False, True, False]
Human Feedback received at timestep 289 of 1
Current timestep = 290. State = [[-0.26929766  0.0775653 ]]. Action = [[-0.06689423 -0.18422829 -0.05691656  0.31397426]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 290 is [True, False, False, False, True, False]
Scene graph at timestep 290 is [True, False, False, False, True, False]
State prediction error at timestep 290 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 291. State = [[-0.26912647  0.07706181]]. Action = [[ 0.04284763  0.24685329  0.09722993 -0.28360736]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 291 is [True, False, False, False, True, False]
Current timestep = 292. State = [[-0.2693513   0.07795504]]. Action = [[ 0.02027881  0.22056031 -0.18168198  0.6687008 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 292 is [True, False, False, False, True, False]
Current timestep = 293. State = [[-0.27010763  0.08056949]]. Action = [[ 0.21559659  0.09092301 -0.23311909  0.0241245 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 293 is [True, False, False, False, True, False]
Current timestep = 294. State = [[-0.26901075  0.08267722]]. Action = [[ 0.06444865  0.05424467  0.02583832 -0.11706722]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 294 is [True, False, False, False, True, False]
Current timestep = 295. State = [[-0.2673445   0.08491166]]. Action = [[-0.2297636   0.20427966 -0.20326047 -0.4859048 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 295 is [True, False, False, False, True, False]
Current timestep = 296. State = [[-0.2660216   0.08640482]]. Action = [[-0.20289712 -0.04836771 -0.14825901 -0.00219375]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 296 is [True, False, False, False, True, False]
Current timestep = 297. State = [[-0.2661335   0.08770765]]. Action = [[ 0.16049826  0.01605135 -0.19832827  0.34737325]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 297 is [True, False, False, False, True, False]
Scene graph at timestep 297 is [True, False, False, False, True, False]
State prediction error at timestep 297 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 298. State = [[-0.26595777  0.08884761]]. Action = [[-0.07233529  0.13190031 -0.14346586 -0.75879365]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 298 is [True, False, False, False, True, False]
Scene graph at timestep 298 is [True, False, False, False, True, False]
State prediction error at timestep 298 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 299. State = [[-0.2661288   0.09143286]]. Action = [[ 0.00630772 -0.23020738 -0.18604542 -0.18910432]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 299 is [True, False, False, False, True, False]
Current timestep = 300. State = [[-0.26585162  0.09147748]]. Action = [[-0.13389607  0.14390022  0.02325937 -0.7779493 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 300 is [True, False, False, False, True, False]
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.265991    0.09224828]]. Action = [[ 0.22126317 -0.21895769  0.22479516 -0.86805236]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 301 is [True, False, False, False, True, False]
Current timestep = 302. State = [[-0.2652791   0.09132767]]. Action = [[ 0.14912045 -0.10691315 -0.22670126  0.3364787 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 302 is [True, False, False, False, True, False]
Current timestep = 303. State = [[-0.26362866  0.08970272]]. Action = [[-0.13647734  0.02103952  0.21427456 -0.832279  ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 303 is [True, False, False, False, True, False]
Current timestep = 304. State = [[-0.26335773  0.08933238]]. Action = [[-0.22182715  0.11657804 -0.02479036  0.5740197 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 304 is [True, False, False, False, True, False]
Scene graph at timestep 304 is [True, False, False, False, True, False]
State prediction error at timestep 304 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 305. State = [[-0.26349473  0.08957358]]. Action = [[ 0.04858375 -0.16884516 -0.17564586 -0.53632504]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 305 is [True, False, False, False, True, False]
Scene graph at timestep 305 is [True, False, False, False, True, False]
State prediction error at timestep 305 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 306. State = [[-0.26339903  0.08898799]]. Action = [[ 0.24390268  0.23438531 -0.20994098  0.553035  ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 306 is [True, False, False, False, True, False]
Current timestep = 307. State = [[-0.26286566  0.08971092]]. Action = [[ 0.1578592  -0.03432624  0.03702554  0.35886967]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 307 is [True, False, False, False, True, False]
Current timestep = 308. State = [[-0.2613606   0.09015744]]. Action = [[-0.07138997  0.08998144 -0.09106159  0.28414774]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 308 is [True, False, False, False, True, False]
Current timestep = 309. State = [[-0.26004145  0.09079472]]. Action = [[ 0.1391666  -0.03927271  0.19847882 -0.69174683]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 309 is [True, False, False, False, True, False]
Current timestep = 310. State = [[-0.25768095  0.09168839]]. Action = [[-0.16963361  0.18922836  0.23257524  0.86274457]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 310 is [True, False, False, False, True, False]
Current timestep = 311. State = [[-0.25753105  0.0935729 ]]. Action = [[-0.08692187 -0.10337022  0.22448105 -0.32926106]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 311 is [True, False, False, False, True, False]
Scene graph at timestep 311 is [True, False, False, False, True, False]
State prediction error at timestep 311 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 312. State = [[-0.25755873  0.09378691]]. Action = [[ 0.21298316  0.08827922 -0.05366112 -0.8558116 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 312 is [True, False, False, False, True, False]
Scene graph at timestep 312 is [True, False, False, False, True, False]
State prediction error at timestep 312 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.25696048  0.09462257]]. Action = [[-0.20896292 -0.00855058 -0.24729751 -0.57853657]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 313 is [True, False, False, False, True, False]
Scene graph at timestep 313 is [True, False, False, False, True, False]
State prediction error at timestep 313 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 314. State = [[-0.25704592  0.09575842]]. Action = [[-0.19367431  0.15279898 -0.09542127  0.6806102 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 314 is [True, False, False, False, True, False]
Current timestep = 315. State = [[-0.25825495  0.09826638]]. Action = [[ 0.20765454  0.13424605 -0.1173141   0.004686  ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 315 is [True, False, False, False, True, False]
Current timestep = 316. State = [[-0.2588948   0.10063026]]. Action = [[ 0.12516093  0.03200155 -0.24228473  0.31155396]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 316 is [True, False, False, False, True, False]
Current timestep = 317. State = [[-0.25806037  0.10223201]]. Action = [[-0.03936969  0.03226513 -0.06555238  0.54549813]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 317 is [True, False, False, False, True, False]
Current timestep = 318. State = [[-0.25823867  0.10397843]]. Action = [[-0.1550302   0.05341902 -0.18512592  0.3761717 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 318 is [True, False, False, False, True, False]
Scene graph at timestep 318 is [True, False, False, False, True, False]
State prediction error at timestep 318 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 319. State = [[-0.25889707  0.10583422]]. Action = [[-0.0528048   0.06659234 -0.05360876 -0.5228499 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 319 is [True, False, False, False, True, False]
Current timestep = 320. State = [[-0.2598827   0.10803314]]. Action = [[-0.16466144  0.00384241 -0.05438595  0.27490282]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 320 is [True, False, False, False, True, False]
Current timestep = 321. State = [[-0.2612641   0.11050247]]. Action = [[ 0.18121749  0.21921933 -0.1300453  -0.96480757]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 321 is [True, False, False, False, True, False]
Current timestep = 322. State = [[-0.26230282  0.11359131]]. Action = [[ 0.205306   -0.12076297  0.10986465  0.79621124]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 322 is [True, False, False, False, True, False]
Current timestep = 323. State = [[-0.2613466   0.11430447]]. Action = [[-0.17445378 -0.10864259  0.04758167 -0.7882807 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 323 is [True, False, False, False, True, False]
Current timestep = 324. State = [[-0.26114386  0.11402736]]. Action = [[ 0.19105273 -0.08478434 -0.08680993  0.4973495 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 324 is [True, False, False, False, True, False]
Scene graph at timestep 324 is [True, False, False, False, True, False]
State prediction error at timestep 324 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 325. State = [[-0.2604167   0.11363607]]. Action = [[ 0.00533825  0.18548232 -0.16427952  0.39401245]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 325 is [True, False, False, False, True, False]
Current timestep = 326. State = [[-0.259789    0.11434413]]. Action = [[-0.23691136 -0.22257218 -0.1732369  -0.53226405]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 326 is [True, False, False, False, True, False]
Current timestep = 327. State = [[-0.25979114  0.11420164]]. Action = [[-0.12995179  0.04083726  0.17034411  0.95134866]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 327 is [True, False, False, False, True, False]
Current timestep = 328. State = [[-0.2597332   0.11411115]]. Action = [[ 0.10702613 -0.17388545 -0.22199883  0.5846101 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 328 is [True, False, False, False, True, False]
Current timestep = 329. State = [[-0.25936404  0.11263202]]. Action = [[ 0.06180859  0.05527398 -0.01969901 -0.93122447]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 329 is [True, False, False, False, True, False]
Human Feedback received at timestep 329 of 1
Current timestep = 330. State = [[-0.2593368   0.11201667]]. Action = [[-0.21413931 -0.24189784  0.07702065 -0.8471863 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 330 is [True, False, False, False, True, False]
Current timestep = 331. State = [[-0.25924847  0.10901201]]. Action = [[ 0.16816485 -0.15120795 -0.22735305 -0.70326513]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 331 is [True, False, False, False, True, False]
Current timestep = 332. State = [[-0.2583749   0.10483165]]. Action = [[-0.08181658 -0.19996662 -0.13106349 -0.38106203]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 332 is [True, False, False, False, True, False]
Current timestep = 333. State = [[-0.25769493  0.10011696]]. Action = [[ 0.08220953 -0.04871677  0.03407076 -0.2265771 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 333 is [True, False, False, False, True, False]
Scene graph at timestep 333 is [True, False, False, False, True, False]
State prediction error at timestep 333 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 334. State = [[-0.25707093  0.09675936]]. Action = [[-0.07399738 -0.129154   -0.18229577  0.82299376]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 334 is [True, False, False, False, True, False]
Current timestep = 335. State = [[-0.25684714  0.09253067]]. Action = [[ 0.07933652 -0.19315125  0.18823066  0.7383187 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 335 is [True, False, False, False, True, False]
Current timestep = 336. State = [[-0.25607952  0.08731572]]. Action = [[ 0.20743006 -0.07636112 -0.12087762  0.47792482]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 336 is [True, False, False, False, True, False]
Scene graph at timestep 336 is [True, False, False, False, True, False]
State prediction error at timestep 336 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 337. State = [[-0.25480494  0.08302888]]. Action = [[-0.1110369   0.1934093   0.20128497 -0.23006684]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 337 is [True, False, False, False, True, False]
Current timestep = 338. State = [[-0.25467554  0.08276954]]. Action = [[-0.2352191  -0.19958767  0.10009956  0.36291516]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 338 is [True, False, False, False, True, False]
Current timestep = 339. State = [[-0.25550127  0.08084296]]. Action = [[-0.08489683  0.04315999 -0.10787269 -0.95474416]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 339 is [True, False, False, False, True, False]
Human Feedback received at timestep 339 of 1
Current timestep = 340. State = [[-0.25651944  0.07989841]]. Action = [[-0.16033696  0.1810331  -0.03372678 -0.46236116]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 340 is [True, False, False, False, True, False]
Current timestep = 341. State = [[-0.25788575  0.08096408]]. Action = [[ 0.10490704 -0.0464967  -0.09917974  0.8389509 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 341 is [True, False, False, False, True, False]
Scene graph at timestep 341 is [True, False, False, False, True, False]
State prediction error at timestep 341 is tensor(7.9823e-05, grad_fn=<MseLossBackward0>)
Current timestep = 342. State = [[-0.2583457   0.08078489]]. Action = [[ 0.10701159 -0.045783    0.02517033 -0.4997071 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 342 is [True, False, False, False, True, False]
Scene graph at timestep 342 is [True, False, False, False, True, False]
State prediction error at timestep 342 is tensor(6.3262e-06, grad_fn=<MseLossBackward0>)
Current timestep = 343. State = [[-0.25830603  0.08055196]]. Action = [[-0.10728481 -0.00245562  0.18921524 -0.8318868 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 343 is [True, False, False, False, True, False]
Scene graph at timestep 343 is [True, False, False, False, True, False]
State prediction error at timestep 343 is tensor(4.1571e-05, grad_fn=<MseLossBackward0>)
Current timestep = 344. State = [[-0.25859103  0.08061823]]. Action = [[-0.11190309  0.2191984  -0.13896315 -0.2398389 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 344 is [True, False, False, False, True, False]
Current timestep = 345. State = [[-0.25986174  0.082163  ]]. Action = [[ 0.21179694  0.06529325 -0.15580304  0.19668448]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 345 is [True, False, False, False, True, False]
Current timestep = 346. State = [[-0.26018587  0.08304217]]. Action = [[-0.06866974  0.17162937 -0.03313009 -0.5774679 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 346 is [True, False, False, False, True, False]
Current timestep = 347. State = [[-0.26107863  0.08560066]]. Action = [[-0.07582888 -0.00385177 -0.09539786  0.1627667 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 347 is [True, False, False, False, True, False]
Current timestep = 348. State = [[-0.26203397  0.08771711]]. Action = [[ 0.1011757   0.07133415 -0.03429706 -0.89954203]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 348 is [True, False, False, False, True, False]
Current timestep = 349. State = [[-0.2626135   0.08946131]]. Action = [[ 0.23117733  0.06941062 -0.22646818  0.37374318]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 349 is [True, False, False, False, True, False]
Current timestep = 350. State = [[-0.2626081   0.09119681]]. Action = [[0.17122996 0.17314395 0.00487319 0.8664911 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 350 is [True, False, False, False, True, False]
Current timestep = 351. State = [[-0.26132727  0.09410201]]. Action = [[ 0.12238014  0.24591458  0.02826777 -0.43485773]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 351 is [True, False, False, False, True, False]
Current timestep = 352. State = [[-0.25936693  0.09845836]]. Action = [[-0.00027303  0.09973019 -0.23676021  0.0943501 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 352 is [True, False, False, False, True, False]
Current timestep = 353. State = [[-0.25797957  0.1020914 ]]. Action = [[-0.09560242 -0.00967637  0.08305371  0.91700125]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 353 is [True, False, False, False, True, False]
Scene graph at timestep 353 is [True, False, False, False, True, False]
State prediction error at timestep 353 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 354. State = [[-0.2578029   0.10518209]]. Action = [[-0.19919251  0.06418547  0.13887525 -0.21472001]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 354 is [True, False, False, False, True, False]
Current timestep = 355. State = [[-0.25857604  0.1082113 ]]. Action = [[-0.22676574 -0.1352104   0.03126362  0.94228363]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 355 is [True, False, False, False, True, False]
Scene graph at timestep 355 is [True, False, False, False, True, False]
State prediction error at timestep 355 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 356. State = [[-0.2596241   0.10957463]]. Action = [[0.21668032 0.21830314 0.03419378 0.00198793]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 356 is [True, False, False, False, True, False]
Current timestep = 357. State = [[-0.2606185   0.11197014]]. Action = [[0.18584526 0.04919469 0.17537457 0.8262348 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 357 is [True, False, False, False, True, False]
Human Feedback received at timestep 357 of 1
Current timestep = 358. State = [[-0.26021826  0.11354642]]. Action = [[-0.02640362 -0.13202992 -0.0142843  -0.3309222 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 358 is [True, False, False, False, True, False]
Scene graph at timestep 358 is [True, False, False, False, True, False]
State prediction error at timestep 358 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 359. State = [[-0.26008967  0.11398781]]. Action = [[-0.01944268  0.12568662  0.14781249 -0.18436229]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 359 is [True, False, False, False, True, False]
Current timestep = 360. State = [[-0.2604398   0.11505726]]. Action = [[-0.1689649   0.06507236 -0.12333697  0.2679342 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 360 is [True, False, False, False, True, False]
Current timestep = 361. State = [[-0.26122493  0.11667702]]. Action = [[ 0.00209022 -0.15566567  0.04687271 -0.9088693 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 361 is [True, False, False, False, True, False]
Current timestep = 362. State = [[-0.26114988  0.11645984]]. Action = [[-0.00333703 -0.23957734 -0.06933799 -0.4533826 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 362 is [True, False, False, False, True, False]
Current timestep = 363. State = [[-0.26065406  0.1151303 ]]. Action = [[-0.05196062 -0.09058148 -0.05437431  0.49244225]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 363 is [True, False, False, False, True, False]
Current timestep = 364. State = [[-0.26026514  0.11340702]]. Action = [[ 0.00794759 -0.04680467 -0.09297776  0.76519585]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 364 is [True, False, False, False, True, False]
Current timestep = 365. State = [[-0.25995037  0.11189289]]. Action = [[ 0.01090187  0.04830939  0.11538431 -0.4891839 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 365 is [True, False, False, False, True, False]
Current timestep = 366. State = [[-0.25993004  0.11162594]]. Action = [[-0.04002956  0.05515701 -0.02648616  0.72982335]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 366 is [True, False, False, False, True, False]
Current timestep = 367. State = [[-0.26015604  0.11195765]]. Action = [[-0.22899799  0.23065329 -0.17421724  0.2972939 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 367 is [True, False, False, False, True, False]
Current timestep = 368. State = [[-0.26172736  0.11412942]]. Action = [[ 0.21126765 -0.16506362  0.14943236 -0.26414943]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 368 is [True, False, False, False, True, False]
Scene graph at timestep 368 is [True, False, False, False, True, False]
State prediction error at timestep 368 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 369. State = [[-0.26164374  0.11384983]]. Action = [[-0.16379806  0.08576161  0.05552304  0.8431797 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 369 is [True, False, False, False, True, False]
Current timestep = 370. State = [[-0.26215628  0.11418552]]. Action = [[ 0.06104946  0.10251784 -0.17906003  0.04224145]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 370 is [True, False, False, False, True, False]
Current timestep = 371. State = [[-0.2625279   0.11525144]]. Action = [[ 0.24252596  0.14593029  0.088368   -0.77043104]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 371 is [True, False, False, False, True, False]
Current timestep = 372. State = [[-0.26271415  0.11697301]]. Action = [[ 0.15580922  0.19763428 -0.10807836  0.17878735]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 372 is [True, False, False, False, True, False]
Current timestep = 373. State = [[-0.26170576  0.11978299]]. Action = [[-0.10004893 -0.20013438  0.24607092 -0.24717963]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 373 is [True, False, False, False, True, False]
Current timestep = 374. State = [[-0.26156792  0.11991961]]. Action = [[-0.20644806 -0.05708294 -0.19856189 -0.794959  ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 374 is [True, False, False, False, True, False]
Current timestep = 375. State = [[-0.261794    0.12018977]]. Action = [[ 0.04614723  0.21145815 -0.05529322  0.04957771]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 375 is [True, False, False, False, True, False]
Scene graph at timestep 375 is [True, False, False, False, True, False]
State prediction error at timestep 375 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 376. State = [[-0.2626643   0.12187524]]. Action = [[-0.01160845 -0.15545042  0.05698109  0.48676562]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 376 is [True, False, False, False, True, False]
Current timestep = 377. State = [[-0.26273054  0.12193397]]. Action = [[-0.08829808  0.13461474 -0.08033957 -0.6590942 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 377 is [True, False, False, False, True, False]
Current timestep = 378. State = [[-0.2635659   0.12324568]]. Action = [[-0.13966076 -0.23297966 -0.17320669 -0.7225231 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 378 is [True, False, False, False, True, False]
Current timestep = 379. State = [[-0.26400375  0.12295728]]. Action = [[-0.05176625 -0.20729832  0.07805198  0.7105191 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 379 is [True, False, False, False, True, False]
Current timestep = 380. State = [[-0.2648155   0.12055536]]. Action = [[0.17886692 0.14811713 0.20192811 0.5029142 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 380 is [True, False, False, False, True, False]
Current timestep = 381. State = [[-0.264645    0.12017092]]. Action = [[0.21279913 0.20179886 0.24075523 0.6134304 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 381 is [True, False, False, False, True, False]
Current timestep = 382. State = [[-0.2645765  0.1208373]]. Action = [[-0.18537238 -0.07942906  0.10041165  0.7395719 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 382 is [True, False, False, False, True, False]
Current timestep = 383. State = [[-0.26463228  0.12092858]]. Action = [[ 0.07395035  0.05588841  0.20021945 -0.36833316]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 383 is [True, False, False, False, True, False]
Current timestep = 384. State = [[-0.26464832  0.12121075]]. Action = [[-0.13428725  0.02139398 -0.14549893 -0.04425007]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 384 is [True, False, False, False, True, False]
Current timestep = 385. State = [[-0.26491168  0.12183024]]. Action = [[-0.21462834 -0.08290207 -0.02292734  0.8768369 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 385 is [True, False, False, False, True, False]
Current timestep = 386. State = [[-0.26559055  0.1223252 ]]. Action = [[ 0.20513004  0.1666429  -0.16413005  0.10196602]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 386 is [True, False, False, False, True, False]
Scene graph at timestep 386 is [True, False, False, False, True, False]
State prediction error at timestep 386 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Current timestep = 387. State = [[-0.2659595   0.12328224]]. Action = [[-0.10839315 -0.03161623  0.22888112 -0.8469759 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 387 is [True, False, False, False, True, False]
Current timestep = 388. State = [[-0.26643804  0.12405468]]. Action = [[-0.15412925  0.01448908  0.2112585   0.9405308 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 388 is [True, False, False, False, True, False]
Current timestep = 389. State = [[-0.26735038  0.12525548]]. Action = [[ 0.20830205  0.09396291 -0.00969698  0.05638289]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 389 is [True, False, False, False, True, False]
Current timestep = 390. State = [[-0.2679271  0.1262205]]. Action = [[-0.03607135 -0.03498229 -0.16883332 -0.70689255]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 390 is [True, False, False, False, False, True]
Current timestep = 391. State = [[-0.2681831   0.12662785]]. Action = [[ 0.07329476  0.08438778 -0.24194491 -0.82514113]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 391 is [True, False, False, False, False, True]
Current timestep = 392. State = [[-0.26852566  0.1273906 ]]. Action = [[-0.13395624 -0.13389513  0.11158666  0.38366044]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 392 is [True, False, False, False, False, True]
Current timestep = 393. State = [[-0.26859877  0.12741606]]. Action = [[ 0.05225331 -0.0970611   0.03432336 -0.91149867]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 393 is [True, False, False, False, False, True]
Current timestep = 394. State = [[-0.26853687  0.12712403]]. Action = [[-0.18748233 -0.07626429  0.187516   -0.850633  ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 394 is [True, False, False, False, False, True]
Scene graph at timestep 394 is [True, False, False, False, False, True]
State prediction error at timestep 394 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 395. State = [[-0.2691437  0.1263103]]. Action = [[-1.11311838e-01 -8.68752599e-04 -1.24820665e-01 -9.09889281e-01]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 395 is [True, False, False, False, False, True]
Current timestep = 396. State = [[-0.2701062   0.12575229]]. Action = [[-0.08127508 -0.06452441 -0.05702135 -0.8436233 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 396 is [True, False, False, False, False, True]
Current timestep = 397. State = [[-0.27115816  0.12487198]]. Action = [[-0.20032385 -0.21970838 -0.21400917  0.01798952]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 397 is [True, False, False, False, False, True]
Current timestep = 398. State = [[-0.27168676  0.12441314]]. Action = [[ 0.10214084  0.19066966 -0.16148558  0.5265975 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 398 is [True, False, False, False, True, False]
Current timestep = 399. State = [[-0.2722027   0.12512994]]. Action = [[ 0.01361072 -0.16423193 -0.13040182  0.11777377]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 399 is [True, False, False, False, True, False]
Current timestep = 400. State = [[-0.27214563  0.12481996]]. Action = [[-0.21811436  0.05334452  0.08453196 -0.39180744]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 400 is [True, False, False, False, False, True]
Current timestep = 401. State = [[-0.27203977  0.12448438]]. Action = [[ 0.1698012  -0.17744792  0.17819971 -0.71899354]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 401 is [True, False, False, False, True, False]
Scene graph at timestep 401 is [True, False, False, False, True, False]
State prediction error at timestep 401 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 402. State = [[-0.27136967  0.12223569]]. Action = [[ 0.0722543   0.24436516  0.17899463 -0.30740678]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 402 is [True, False, False, False, True, False]
Current timestep = 403. State = [[-0.27148333  0.12262777]]. Action = [[ 0.0607053   0.22836205  0.13719082 -0.49336803]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 403 is [True, False, False, False, True, False]
Current timestep = 404. State = [[-0.2717137   0.12453899]]. Action = [[-0.17200564 -0.01006308 -0.20011573  0.97349024]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 404 is [True, False, False, False, True, False]
Current timestep = 405. State = [[-0.27195224  0.12553811]]. Action = [[ 0.11909789  0.11064139  0.23855644 -0.7064814 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 405 is [True, False, False, False, True, False]
Scene graph at timestep 405 is [True, False, False, False, False, True]
State prediction error at timestep 405 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 406. State = [[-0.27111113  0.12713337]]. Action = [[-0.10081899 -0.01945439  0.2033292   0.37837553]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 406 is [True, False, False, False, False, True]
Scene graph at timestep 406 is [True, False, False, False, False, True]
State prediction error at timestep 406 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 407. State = [[-0.2712978   0.12793589]]. Action = [[-0.16639194 -0.03578629 -0.17130281 -0.3147136 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 407 is [True, False, False, False, False, True]
Current timestep = 408. State = [[-0.2714628   0.12859184]]. Action = [[-0.22624207  0.15303749 -0.06761786  0.8594191 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 408 is [True, False, False, False, False, True]
Current timestep = 409. State = [[-0.27153403  0.128964  ]]. Action = [[-0.22477539  0.08300987 -0.12990718  0.28369093]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 409 is [True, False, False, False, False, True]
Human Feedback received at timestep 409 of 1
Current timestep = 410. State = [[-0.2716153   0.12946998]]. Action = [[ 0.05960673  0.15945163 -0.09279004  0.0623759 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 410 is [True, False, False, False, False, True]
Current timestep = 411. State = [[-0.27146792  0.13096663]]. Action = [[ 0.18383342  0.05622888 -0.1105092   0.30154347]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 411 is [True, False, False, False, False, True]
Current timestep = 412. State = [[-0.26986477  0.13238436]]. Action = [[ 0.2450304  -0.06030279  0.13541174 -0.53444207]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 412 is [True, False, False, False, False, True]
Current timestep = 413. State = [[-0.26624298  0.13365395]]. Action = [[ 0.21554872 -0.1985548   0.06756401 -0.81310236]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 413 is [True, False, False, False, False, True]
Scene graph at timestep 413 is [True, False, False, False, False, True]
State prediction error at timestep 413 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 414. State = [[-0.26187268  0.1332575 ]]. Action = [[-0.22856967 -0.03553723 -0.17738284 -0.0776419 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 414 is [True, False, False, False, False, True]
Scene graph at timestep 414 is [True, False, False, False, False, True]
State prediction error at timestep 414 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 415. State = [[-0.26123294  0.1330344 ]]. Action = [[-0.1745595   0.19457614 -0.24037853 -0.6512589 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 415 is [True, False, False, False, False, True]
Scene graph at timestep 415 is [True, False, False, False, False, True]
State prediction error at timestep 415 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 416. State = [[-0.26170817  0.13396382]]. Action = [[ 0.14455873 -0.22824764 -0.07441835 -0.82780945]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 416 is [True, False, False, False, False, True]
Scene graph at timestep 416 is [True, False, False, False, False, True]
State prediction error at timestep 416 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 417. State = [[-0.2610698   0.13289116]]. Action = [[ 0.16658989  0.15096092  0.01387316 -0.923739  ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 417 is [True, False, False, False, False, True]
Current timestep = 418. State = [[-0.25987172  0.13318546]]. Action = [[-0.118646   -0.23086448 -0.14391729 -0.79012275]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 418 is [True, False, False, False, False, True]
Current timestep = 419. State = [[-0.25923744  0.13181974]]. Action = [[ 0.19482225  0.14600846 -0.02501649 -0.45477843]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 419 is [True, False, False, False, False, True]
Scene graph at timestep 419 is [True, False, False, False, False, True]
State prediction error at timestep 419 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 420. State = [[-0.25761035  0.1321693 ]]. Action = [[-0.11734752 -0.08106016 -0.05062011 -0.5109858 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 420 is [True, False, False, False, False, True]
Human Feedback received at timestep 420 of 1
Current timestep = 421. State = [[-0.25716934  0.13171285]]. Action = [[-0.05280571 -0.23286776  0.13288164 -0.7293309 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 421 is [True, False, False, False, False, True]
Current timestep = 422. State = [[-0.25604385  0.12890011]]. Action = [[ 0.20361418 -0.05183616  0.05495369  0.6633282 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 422 is [True, False, False, False, False, True]
Scene graph at timestep 422 is [True, False, False, False, False, True]
State prediction error at timestep 422 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 423. State = [[-0.2544635   0.12610264]]. Action = [[ 0.01095769 -0.20820957 -0.19150616 -0.2597704 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 423 is [True, False, False, False, False, True]
Current timestep = 424. State = [[-0.25246897  0.12238999]]. Action = [[ 0.22664857  0.06664297  0.02099207 -0.50013864]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 424 is [True, False, False, False, False, True]
Current timestep = 425. State = [[-0.24963917  0.12033119]]. Action = [[-0.18112339 -0.01263897 -0.05382416 -0.84178996]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 425 is [True, False, False, False, True, False]
Scene graph at timestep 425 is [True, False, False, False, True, False]
State prediction error at timestep 425 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 426. State = [[-0.24895121  0.11906479]]. Action = [[-0.18488532  0.18333232 -0.19353919  0.8284557 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 426 is [True, False, False, False, True, False]
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.24928601  0.11959546]]. Action = [[-0.23820725 -0.0255959   0.15812686  0.05968869]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 427 is [True, False, False, False, True, False]
Scene graph at timestep 427 is [True, False, False, False, True, False]
State prediction error at timestep 427 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 428. State = [[-0.25003943  0.12006136]]. Action = [[0.07551634 0.01393139 0.06433952 0.39435053]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 428 is [True, False, False, False, True, False]
Current timestep = 429. State = [[-0.25028688  0.12082251]]. Action = [[ 0.24178231  0.10141057 -0.1281616   0.08431125]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 429 is [True, False, False, False, True, False]
Current timestep = 430. State = [[-0.25023654  0.12085656]]. Action = [[ 0.09645408 -0.06392187  0.1330269   0.22118902]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 430 is [True, False, False, False, True, False]
Scene graph at timestep 430 is [True, False, False, False, True, False]
State prediction error at timestep 430 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 431. State = [[-0.2499443   0.12071048]]. Action = [[-0.14508477 -0.13033879  0.01123941 -0.7039537 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 431 is [True, False, False, False, True, False]
Current timestep = 432. State = [[-0.24961272  0.120267  ]]. Action = [[0.04233274 0.2107208  0.21451962 0.00869882]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 432 is [True, False, False, False, True, False]
Scene graph at timestep 432 is [True, False, False, False, True, False]
State prediction error at timestep 432 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 433. State = [[-0.24989375  0.12098048]]. Action = [[ 0.20971918  0.23248202 -0.1472606  -0.22308302]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 433 is [True, False, False, False, True, False]
Current timestep = 434. State = [[-0.24856925  0.12348334]]. Action = [[ 0.1096091   0.16235942 -0.03158098 -0.15410489]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 434 is [True, False, False, False, True, False]
Current timestep = 435. State = [[-0.2463402   0.12715612]]. Action = [[-0.21309067 -0.04990524 -0.1851091   0.7601559 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 435 is [True, False, False, False, True, False]
Scene graph at timestep 435 is [True, False, False, False, False, True]
State prediction error at timestep 435 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 436. State = [[-0.24559103  0.1290644 ]]. Action = [[0.17310476 0.07664254 0.17838529 0.31766033]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 436 is [True, False, False, False, False, True]
Scene graph at timestep 436 is [True, False, False, False, False, True]
State prediction error at timestep 436 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 437. State = [[-0.24411261  0.13114533]]. Action = [[ 0.21956015  0.14180067 -0.22750069  0.1894095 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 437 is [True, False, False, False, False, True]
Current timestep = 438. State = [[-0.24191672  0.1338669 ]]. Action = [[-0.15408143  0.23490441 -0.02600504 -0.9149604 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 438 is [True, False, False, False, False, True]
Human Feedback received at timestep 438 of 1
Current timestep = 439. State = [[-0.24144493  0.13954282]]. Action = [[-0.21516676  0.19332969 -0.03775047  0.01626611]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 439 is [True, False, False, False, False, True]
Current timestep = 440. State = [[-0.24337098  0.14596169]]. Action = [[-0.18104391  0.07146668 -0.0711996   0.33436453]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 440 is [True, False, False, False, False, True]
Current timestep = 441. State = [[-0.246685    0.15144159]]. Action = [[ 0.07185087  0.2228558   0.18894178 -0.31918538]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 441 is [True, False, False, False, False, True]
Current timestep = 442. State = [[-0.24933358  0.15659803]]. Action = [[-0.1726668   0.10394734 -0.10426301 -0.02325636]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 442 is [True, False, False, False, False, True]
Current timestep = 443. State = [[-0.25234884  0.16140282]]. Action = [[-0.0581256  -0.03683816 -0.01576193  0.4133191 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 443 is [True, False, False, False, False, True]
Current timestep = 444. State = [[-0.25445947  0.16458006]]. Action = [[ 0.0623934   0.03719547 -0.09710522 -0.32606912]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 444 is [True, False, False, False, False, True]
Current timestep = 445. State = [[-0.25563788  0.16631928]]. Action = [[-0.19299397  0.20735943  0.10774362  0.90660644]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 445 is [True, False, False, False, False, True]
Human Feedback received at timestep 445 of 1
Current timestep = 446. State = [[-0.25847635  0.17077626]]. Action = [[ 0.04452884 -0.12504317 -0.03687085 -0.47753692]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 446 is [True, False, False, False, False, True]
Current timestep = 447. State = [[-0.25940406  0.17230445]]. Action = [[0.04953167 0.11333781 0.14445013 0.6910398 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 447 is [True, False, False, False, False, True]
Current timestep = 448. State = [[-0.26022586  0.17375468]]. Action = [[ 0.14029968  0.19967508  0.07715568 -0.16326481]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 448 is [True, False, False, False, False, True]
Current timestep = 449. State = [[-0.25962088  0.17677234]]. Action = [[0.18865764 0.07503781 0.06469539 0.6843146 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 449 is [True, False, False, False, False, True]
Current timestep = 450. State = [[-0.25715002  0.17951116]]. Action = [[-0.08908242 -0.07823625 -0.20494942 -0.9295348 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 450 is [True, False, False, False, False, True]
Current timestep = 451. State = [[-0.2550256   0.18114847]]. Action = [[ 0.07296044 -0.07010268 -0.07458338 -0.2950213 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 451 is [True, False, False, False, False, True]
Current timestep = 452. State = [[-0.2537081   0.18163347]]. Action = [[ 0.15082455  0.06940064 -0.19296013 -0.2820027 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 452 is [True, False, False, False, False, True]
Current timestep = 453. State = [[-0.25182194  0.18295011]]. Action = [[-0.22984083  0.19529027 -0.04266323  0.36973298]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 453 is [True, False, False, False, False, True]
Current timestep = 454. State = [[-0.25164106  0.18584882]]. Action = [[-0.05558726 -0.22662923 -0.15625218  0.53536344]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 454 is [True, False, False, False, False, True]
Scene graph at timestep 454 is [True, False, False, False, False, True]
State prediction error at timestep 454 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Current timestep = 455. State = [[-0.2515133   0.18597724]]. Action = [[ 0.14394897  0.04978666 -0.08955461  0.5401757 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 455 is [True, False, False, False, False, True]
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.2509565   0.18623033]]. Action = [[-0.04384714  0.06218842  0.05522478 -0.26731825]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 456 is [True, False, False, False, False, True]
Current timestep = 457. State = [[-0.2505738   0.18687907]]. Action = [[ 0.09114915  0.12885633 -0.18291758  0.834429  ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 457 is [True, False, False, False, False, True]
Current timestep = 458. State = [[-0.24996327  0.18847644]]. Action = [[-0.0099497   0.03421199 -0.13379584  0.72809064]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 458 is [True, False, False, False, False, True]
Current timestep = 459. State = [[-0.24907818  0.18989989]]. Action = [[ 0.19952449  0.21049905 -0.15551573 -0.15005893]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 459 is [True, False, False, False, False, True]
Current timestep = 460. State = [[-0.24690206  0.19286755]]. Action = [[-0.14944997 -0.16495316  0.02581596  0.4327737 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 460 is [True, False, False, False, False, True]
Scene graph at timestep 460 is [True, False, False, False, False, True]
State prediction error at timestep 460 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Current timestep = 461. State = [[-0.24635635  0.19351634]]. Action = [[-0.22646472 -0.08308955  0.00987667 -0.6485947 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 461 is [True, False, False, False, False, True]
Current timestep = 462. State = [[-0.24668276  0.19362769]]. Action = [[-0.15422082 -0.23249716 -0.10076126  0.5752933 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 462 is [True, False, False, False, False, True]
Current timestep = 463. State = [[-0.24713142  0.19244573]]. Action = [[-0.10951124  0.10085425  0.18590146  0.2910285 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 463 is [True, False, False, False, False, True]
Human Feedback received at timestep 463 of 1
Current timestep = 464. State = [[-0.24806829  0.19300026]]. Action = [[ 0.19167167  0.10808355 -0.21037841 -0.40085232]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 464 is [True, False, False, False, False, True]
Current timestep = 465. State = [[-0.2483628   0.19339076]]. Action = [[ 0.23797646 -0.02717274  0.18621242 -0.30631435]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 465 is [True, False, False, False, False, True]
Scene graph at timestep 465 is [True, False, False, False, False, True]
State prediction error at timestep 465 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 466. State = [[-0.24818915  0.193378  ]]. Action = [[ 0.24578354 -0.06591269 -0.08341855  0.84459305]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 466 is [True, False, False, False, False, True]
Scene graph at timestep 466 is [True, False, False, False, False, True]
State prediction error at timestep 466 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Current timestep = 467. State = [[-0.24659851  0.19241561]]. Action = [[-0.08779338  0.10284311 -0.04220197 -0.09411335]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 467 is [True, False, False, False, False, True]
Current timestep = 468. State = [[-0.24647465  0.19275029]]. Action = [[-0.02171615 -0.15265901  0.03507447 -0.87583876]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 468 is [True, False, False, False, False, True]
Current timestep = 469. State = [[-0.24612734  0.192111  ]]. Action = [[ 0.13819677  0.11383581  0.20942715 -0.00645959]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 469 is [True, False, False, False, False, True]
Scene graph at timestep 469 is [True, False, False, False, False, True]
State prediction error at timestep 469 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Current timestep = 470. State = [[-0.2452686   0.19223039]]. Action = [[ 0.1850475   0.11615163 -0.01973958 -0.39264327]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 470 is [True, False, False, False, False, True]
Current timestep = 471. State = [[-0.24298881  0.19326338]]. Action = [[ 0.00750029 -0.14493696 -0.12106815 -0.3416434 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 471 is [True, False, False, False, False, True]
Current timestep = 472. State = [[-0.24054353  0.19338766]]. Action = [[ 0.16911256  0.02426419 -0.17463744  0.9403794 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 472 is [True, False, False, False, False, True]
Current timestep = 473. State = [[-0.23681515  0.19402184]]. Action = [[-0.1985706  -0.01882066 -0.06353819 -0.7325801 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 473 is [True, False, False, False, False, True]
Scene graph at timestep 473 is [True, False, False, False, False, True]
State prediction error at timestep 473 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Current timestep = 474. State = [[-0.23518938  0.19458285]]. Action = [[ 0.1950528  -0.02391064 -0.06025213  0.52133226]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 474 is [True, False, False, False, False, True]
Current timestep = 475. State = [[-0.23274569  0.19441448]]. Action = [[ 0.22407228  0.22160494 -0.01131165  0.92555356]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 475 is [True, False, False, False, False, True]
Current timestep = 476. State = [[-0.22954766  0.19594629]]. Action = [[-0.05411075 -0.19687274 -0.22544277 -0.3655914 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 476 is [True, False, False, False, False, True]
Current timestep = 477. State = [[-0.22641777  0.19566016]]. Action = [[-0.02280225 -0.04340661 -0.07860573 -0.5980329 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 477 is [True, False, False, False, False, True]
Current timestep = 478. State = [[-0.22470357  0.19501242]]. Action = [[ 0.04005745 -0.07011443 -0.14109337 -0.26063442]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 478 is [True, False, False, False, False, True]
Current timestep = 479. State = [[-0.22359104  0.19365369]]. Action = [[-0.01026343  0.00792697  0.10020757  0.2025162 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 479 is [True, False, False, False, False, True]
Current timestep = 480. State = [[-0.22343436  0.19264625]]. Action = [[-0.00802176  0.11467534 -0.11224738  0.5141903 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 480 is [True, False, False, False, False, True]
Current timestep = 481. State = [[-0.22369404  0.19273442]]. Action = [[-0.17338732  0.08353856 -0.07898235  0.94406724]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 481 is [True, False, False, False, False, True]
Scene graph at timestep 481 is [True, False, False, False, False, True]
State prediction error at timestep 481 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 482. State = [[-0.22429432  0.19377969]]. Action = [[ 0.20336819  0.10386792 -0.06704351 -0.5990872 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 482 is [True, False, False, False, False, True]
Current timestep = 483. State = [[-0.22352186  0.19481921]]. Action = [[ 0.2343865   0.22750098 -0.2270661  -0.89434385]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 483 is [True, False, False, False, False, True]
Current timestep = 484. State = [[-0.22055638  0.19733804]]. Action = [[-0.18388444 -0.08319721 -0.1972667   0.12803245]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 484 is [True, False, False, False, False, True]
Human Feedback received at timestep 484 of 1
Current timestep = 485. State = [[-0.21811976  0.1993382 ]]. Action = [[-0.05710091 -0.00193374 -0.14855325  0.9592509 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 485 is [True, False, False, False, False, True]
Current timestep = 486. State = [[-0.21695909  0.2005577 ]]. Action = [[ 0.09130871 -0.05116335  0.10524648  0.46396792]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 486 is [True, False, False, False, False, True]
Current timestep = 487. State = [[-0.21587084  0.20105554]]. Action = [[-0.22255377 -0.1578498   0.00263584  0.9622556 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 487 is [True, False, False, False, False, True]
Current timestep = 488. State = [[-0.21598783  0.20041585]]. Action = [[ 0.21757877 -0.11355327  0.01904252  0.6947751 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 488 is [True, False, False, False, False, True]
Current timestep = 489. State = [[-0.2149257   0.19847532]]. Action = [[ 0.01560214 -0.11648241  0.11032662  0.5964894 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 489 is [True, False, False, False, False, True]
Current timestep = 490. State = [[-0.21413763  0.19603533]]. Action = [[ 0.05906686  0.17016816 -0.14172883 -0.67571723]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 490 is [True, False, False, False, False, True]
Scene graph at timestep 490 is [True, False, False, False, False, True]
State prediction error at timestep 490 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 491. State = [[-0.21354721  0.1960796 ]]. Action = [[ 0.19310194  0.17127806 -0.07780312  0.8788748 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 491 is [True, False, False, False, False, True]
Scene graph at timestep 491 is [True, False, False, False, False, True]
State prediction error at timestep 491 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 492. State = [[-0.2109211   0.19793953]]. Action = [[-0.18015528 -0.20325375 -0.22758281  0.5217463 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 492 is [True, False, False, False, False, True]
Current timestep = 493. State = [[-0.20973589  0.19756949]]. Action = [[-0.21002094 -0.12024412  0.06445277  0.56529355]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 493 is [True, False, False, False, False, True]
Scene graph at timestep 493 is [True, False, False, False, False, True]
State prediction error at timestep 493 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Current timestep = 494. State = [[-0.20984182  0.19616003]]. Action = [[ 0.04425937 -0.08981635  0.05006677  0.23786259]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 494 is [True, False, False, False, False, True]
Current timestep = 495. State = [[-0.20949157  0.19437073]]. Action = [[ 0.13381219  0.17582074  0.11397657 -0.96389115]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 495 is [True, False, False, False, False, True]
Current timestep = 496. State = [[-0.20964159  0.19464792]]. Action = [[-0.15646423 -0.02971026 -0.15681195  0.05700397]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 496 is [True, False, False, False, False, True]
Current timestep = 497. State = [[-0.2099899   0.19506602]]. Action = [[-0.19025205 -0.1552424   0.20217341 -0.63160145]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 497 is [True, False, False, False, False, True]
Scene graph at timestep 497 is [True, False, False, False, False, True]
State prediction error at timestep 497 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 498. State = [[-0.21021648  0.1943318 ]]. Action = [[0.11772206 0.18476069 0.00730982 0.29370022]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 498 is [True, False, False, False, False, True]
Current timestep = 499. State = [[-0.21066476  0.19472879]]. Action = [[-0.04698679  0.2175461  -0.16923995  0.6635759 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 499 is [True, False, False, False, False, True]
Scene graph at timestep 499 is [True, False, False, False, False, True]
State prediction error at timestep 499 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.21244995  0.19729504]]. Action = [[-0.23390086 -0.1504787  -0.03039818 -0.030442  ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 500 is [True, False, False, False, False, True]
Current timestep = 501. State = [[-0.26278654  0.06004168]]. Action = [[-0.23081619  0.1604057   0.24349129  0.28117263]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 501 is [True, False, False, False, False, True]
Scene graph at timestep 501 is [True, False, False, False, True, False]
State prediction error at timestep 501 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 501 of 1
Current timestep = 502. State = [[-0.2616809   0.06449397]]. Action = [[-0.09800956 -0.12334576 -0.02469061  0.96456754]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 502 is [True, False, False, False, True, False]
Current timestep = 503. State = [[-0.2616809   0.06449397]]. Action = [[ 0.01036152  0.1053693  -0.09163041 -0.07855558]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 503 is [True, False, False, False, True, False]
Current timestep = 504. State = [[-0.2616809   0.06449397]]. Action = [[ 0.19218141  0.00625077 -0.06829235  0.8989785 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 504 is [True, False, False, False, True, False]
Scene graph at timestep 504 is [True, False, False, False, True, False]
State prediction error at timestep 504 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 505. State = [[-0.2610333   0.06470593]]. Action = [[-0.05669731  0.16001147  0.18492511  0.18030334]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 505 is [True, False, False, False, True, False]
Current timestep = 506. State = [[-0.26101544  0.06626376]]. Action = [[-0.20100199  0.18931216  0.2146877   0.8260131 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 506 is [True, False, False, False, True, False]
Scene graph at timestep 506 is [True, False, False, False, True, False]
State prediction error at timestep 506 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 507. State = [[-0.26283607  0.07083933]]. Action = [[ 0.21095866  0.20304015 -0.18253097 -0.01550126]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 507 is [True, False, False, False, True, False]
Current timestep = 508. State = [[-0.26272032  0.07559703]]. Action = [[ 0.23674256  0.08355001 -0.10169578  0.7358966 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 508 is [True, False, False, False, True, False]
Scene graph at timestep 508 is [True, False, False, False, True, False]
State prediction error at timestep 508 is tensor(2.9258e-05, grad_fn=<MseLossBackward0>)
Current timestep = 509. State = [[-0.25954524  0.07988949]]. Action = [[0.16385177 0.02421337 0.10154232 0.00981009]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 509 is [True, False, False, False, True, False]
Current timestep = 510. State = [[-0.2557022   0.08290588]]. Action = [[0.177692   0.19471866 0.18403918 0.1702559 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 510 is [True, False, False, False, True, False]
Scene graph at timestep 510 is [True, False, False, False, True, False]
State prediction error at timestep 510 is tensor(3.7191e-05, grad_fn=<MseLossBackward0>)
Current timestep = 511. State = [[-0.25081688  0.0869232 ]]. Action = [[ 0.24674493 -0.21804702 -0.14301433  0.15573263]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 511 is [True, False, False, False, True, False]
Current timestep = 512. State = [[-0.24431042  0.08757173]]. Action = [[-0.2251984  -0.19240782 -0.19994952 -0.37993443]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 512 is [True, False, False, False, True, False]
Current timestep = 513. State = [[-0.24089973  0.08711004]]. Action = [[ 0.17067483 -0.09431931  0.07865021 -0.14212239]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 513 is [True, False, False, False, True, False]
Current timestep = 514. State = [[-0.23766242  0.08603323]]. Action = [[ 0.00917634  0.11797285  0.09253567 -0.08231872]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 514 is [True, False, False, False, True, False]
Current timestep = 515. State = [[-0.2366425   0.08617114]]. Action = [[-0.23850779 -0.162536   -0.1689759  -0.35087204]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 515 is [True, False, False, False, True, False]
Current timestep = 516. State = [[-0.23675068  0.08476912]]. Action = [[-0.17554633  0.04500321  0.23757666  0.05339468]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 516 is [True, False, False, False, True, False]
Current timestep = 517. State = [[-0.23704258  0.08433368]]. Action = [[-0.21600467 -0.24246332 -0.1236819   0.6593814 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 517 is [True, False, False, False, True, False]
Scene graph at timestep 517 is [True, False, False, False, True, False]
State prediction error at timestep 517 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 518. State = [[-0.23878825  0.08100741]]. Action = [[-0.18048167 -0.20961821  0.01134908  0.00065577]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 518 is [True, False, False, False, True, False]
Scene graph at timestep 518 is [True, False, False, False, True, False]
State prediction error at timestep 518 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 519. State = [[-0.2410584   0.07613382]]. Action = [[ 0.22089213  0.1432223  -0.20050374 -0.20181793]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 519 is [True, False, False, False, True, False]
Current timestep = 520. State = [[-0.24099608  0.07462837]]. Action = [[ 0.20828223  0.09813496  0.01316988 -0.35284907]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 520 is [True, False, False, False, True, False]
Human Feedback received at timestep 520 of -1
Current timestep = 521. State = [[-0.24108541  0.07465748]]. Action = [[ 0.14164099  0.04401249 -0.00885659  0.20851648]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 521 is [True, False, False, False, True, False]
Current timestep = 522. State = [[-0.24089585  0.07462373]]. Action = [[-0.17013237 -0.14137147  0.0977506   0.5532825 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 522 is [True, False, False, False, True, False]
Current timestep = 523. State = [[-0.24060656  0.07356141]]. Action = [[ 0.03856504 -0.17232093 -0.04929259 -0.508806  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 523 is [True, False, False, False, True, False]
Current timestep = 524. State = [[-0.24009193  0.07106072]]. Action = [[0.16973892 0.13267308 0.03781658 0.8955294 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 524 is [True, False, False, False, True, False]
Current timestep = 525. State = [[-0.23941176  0.07026273]]. Action = [[-0.16495703 -0.2317335   0.19948679  0.27717257]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 525 is [True, False, False, False, True, False]
Current timestep = 526. State = [[-0.23932813  0.06755109]]. Action = [[ 0.21067047 -0.13606131  0.20555604 -0.54765457]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 526 is [True, False, False, False, True, False]
Current timestep = 527. State = [[-0.23858865  0.0640163 ]]. Action = [[ 0.23160574  0.16703993  0.1436409  -0.16143572]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 527 is [True, False, False, False, True, False]
Scene graph at timestep 527 is [True, False, False, False, True, False]
State prediction error at timestep 527 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 528. State = [[-0.23617516  0.06373056]]. Action = [[ 0.19639575 -0.07628292 -0.24558324 -0.944287  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 528 is [True, False, False, False, True, False]
Scene graph at timestep 528 is [True, False, False, False, True, False]
State prediction error at timestep 528 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 529. State = [[-0.23231196  0.06264002]]. Action = [[-0.02717981 -0.17331521  0.17000258  0.99132097]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 529 is [True, False, False, False, True, False]
Scene graph at timestep 529 is [True, False, False, False, True, False]
State prediction error at timestep 529 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 530. State = [[-0.22984411  0.05993783]]. Action = [[-0.19748683  0.02156508  0.08822045  0.55022025]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 530 is [True, False, False, False, True, False]
Scene graph at timestep 530 is [True, False, False, False, True, False]
State prediction error at timestep 530 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 531. State = [[-0.22984686  0.05847099]]. Action = [[-0.15988898 -0.03773785  0.02896687  0.84849024]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 531 is [True, False, False, False, True, False]
Human Feedback received at timestep 531 of 1
Current timestep = 532. State = [[-0.22986418  0.05662334]]. Action = [[-0.10687424 -0.20329538  0.04726136  0.7187598 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 532 is [True, False, False, False, True, False]
Current timestep = 533. State = [[-0.23001468  0.05340246]]. Action = [[-0.0833163   0.11021036 -0.0656392  -0.77590525]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 533 is [True, False, False, False, True, False]
Scene graph at timestep 533 is [True, False, False, False, True, False]
State prediction error at timestep 533 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 534. State = [[-0.23058434  0.05219893]]. Action = [[ 0.1942822  -0.22725886 -0.23314947  0.7396004 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 534 is [True, False, False, False, True, False]
Current timestep = 535. State = [[-0.23032452  0.04884655]]. Action = [[ 0.09433755 -0.08337535  0.2461389  -0.8055797 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 535 is [True, False, False, False, True, False]
Current timestep = 536. State = [[-0.2300123   0.04574903]]. Action = [[ 0.00456727 -0.11136198 -0.18461439 -0.31465024]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 536 is [True, False, False, False, True, False]
Scene graph at timestep 536 is [True, False, False, False, True, False]
State prediction error at timestep 536 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 536 of 1
Current timestep = 537. State = [[-0.22983243  0.04214735]]. Action = [[ 0.20449927 -0.22498609 -0.11964205 -0.07137448]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 537 is [True, False, False, False, True, False]
Current timestep = 538. State = [[-0.2283111   0.03640161]]. Action = [[ 0.24273023 -0.17339873  0.13283801 -0.7662999 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 538 is [True, False, False, False, True, False]
Current timestep = 539. State = [[-0.2249442   0.03174717]]. Action = [[ 0.19961506  0.20675305  0.10303757 -0.6436122 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 539 is [True, False, False, False, True, False]
Scene graph at timestep 539 is [True, False, False, False, True, False]
State prediction error at timestep 539 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 540. State = [[-0.21993026  0.03135135]]. Action = [[ 0.21912503  0.24205661  0.01134205 -0.61917984]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 540 is [True, False, False, False, True, False]
Current timestep = 541. State = [[-0.21413302  0.03285268]]. Action = [[-0.23440428  0.04017499  0.00715196 -0.4803847 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 541 is [True, False, False, False, True, False]
Current timestep = 542. State = [[-0.21123843  0.03384456]]. Action = [[-0.06827338  0.02772266  0.08370042  0.88657844]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 542 is [True, False, False, False, True, False]
Scene graph at timestep 542 is [True, False, False, False, True, False]
State prediction error at timestep 542 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 542 of 1
Current timestep = 543. State = [[-0.21086553  0.03475133]]. Action = [[-0.24571797  0.08395785  0.1753211  -0.01581299]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 543 is [True, False, False, False, True, False]
Scene graph at timestep 543 is [True, False, False, False, True, False]
State prediction error at timestep 543 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 544. State = [[-0.21177524  0.03661358]]. Action = [[ 0.07388946 -0.02456881 -0.02500252  0.689811  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 544 is [True, False, False, False, True, False]
Current timestep = 545. State = [[-0.21190651  0.03688029]]. Action = [[-0.06150904 -0.21642616 -0.08570001  0.8547399 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 545 is [True, False, False, False, True, False]
Current timestep = 546. State = [[-0.2119506   0.03592231]]. Action = [[-0.24078757 -0.09086853  0.08982468  0.41529155]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 546 is [True, False, False, False, True, False]
Current timestep = 547. State = [[-0.2124591   0.03409267]]. Action = [[ 0.16146964 -0.19924103 -0.22710459 -0.5257579 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 547 is [True, False, False, False, True, False]
Human Feedback received at timestep 547 of 1
Current timestep = 548. State = [[-0.21236642  0.03075813]]. Action = [[ 0.05680016 -0.17657878  0.14831549 -0.82574296]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 548 is [True, False, False, False, True, False]
Current timestep = 549. State = [[-0.21223336  0.02627275]]. Action = [[-0.09031412 -0.01801586  0.03660312  0.51152945]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 549 is [True, False, False, False, True, False]
Scene graph at timestep 549 is [True, False, False, False, True, False]
State prediction error at timestep 549 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Current timestep = 550. State = [[-0.21230614  0.0229776 ]]. Action = [[-0.18435955 -0.02004224 -0.14245304 -0.6378026 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 550 is [True, False, False, False, True, False]
Current timestep = 551. State = [[-0.21325476  0.02050986]]. Action = [[-0.06666474 -0.23949586  0.13595569  0.4460708 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 551 is [True, False, False, False, True, False]
Current timestep = 552. State = [[-0.2144119   0.01652338]]. Action = [[-0.1498282  -0.06522484  0.17784226  0.7255682 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 552 is [True, False, False, False, True, False]
Scene graph at timestep 552 is [True, False, False, False, True, False]
State prediction error at timestep 552 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Current timestep = 553. State = [[-0.21643196  0.0123639 ]]. Action = [[-0.22982658  0.08878231 -0.23260143  0.43255663]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 553 is [True, False, False, False, True, False]
Current timestep = 554. State = [[-0.21954721  0.01050051]]. Action = [[ 0.09674469 -0.22350457  0.04953399  0.2951256 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 554 is [True, False, False, False, True, False]
Current timestep = 555. State = [[-0.22126208  0.00730352]]. Action = [[ 0.01773503  0.22488892 -0.04405877 -0.23601347]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 555 is [True, False, False, False, True, False]
Current timestep = 556. State = [[-0.22256705  0.00757203]]. Action = [[-0.21000187  0.08866575  0.21661448 -0.85224855]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 556 is [True, False, False, False, True, False]
Scene graph at timestep 556 is [True, False, False, False, True, False]
State prediction error at timestep 556 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Current timestep = 557. State = [[-0.225266   0.0084309]]. Action = [[ 0.01326814 -0.14007285 -0.04062207  0.5504067 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 557 is [True, False, False, False, True, False]
Current timestep = 558. State = [[-0.22680636  0.00765205]]. Action = [[-0.0892745  -0.03616446 -0.14976387  0.5219486 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 558 is [True, False, False, False, True, False]
Current timestep = 559. State = [[-0.22828631  0.00678355]]. Action = [[ 0.10628721 -0.07799219  0.1649259  -0.86414295]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 559 is [True, False, False, False, True, False]
Current timestep = 560. State = [[-0.22899204  0.00523115]]. Action = [[ 0.18710321 -0.10988754 -0.13608806  0.817425  ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 560 is [True, False, False, False, True, False]
Current timestep = 561. State = [[-0.22903466  0.00305533]]. Action = [[-0.14549962  0.06685534 -0.15384786  0.14058101]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 561 is [True, False, False, False, True, False]
Current timestep = 562. State = [[-0.22912303  0.00201883]]. Action = [[ 0.15675592 -0.09832358  0.00099498  0.18454981]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 562 is [True, False, False, False, True, False]
Current timestep = 563. State = [[-0.22919014  0.00041207]]. Action = [[0.04743743 0.02132714 0.12740877 0.01453269]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 563 is [True, False, False, False, True, False]
Current timestep = 564. State = [[-0.22932373 -0.00043221]]. Action = [[ 0.1454508   0.02823547 -0.15075845 -0.8398055 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 564 is [True, False, False, False, True, False]
Current timestep = 565. State = [[-0.22908215 -0.00066156]]. Action = [[ 0.0942367  -0.23110871 -0.04691923 -0.34928942]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 565 is [True, False, False, False, True, False]
Current timestep = 566. State = [[-0.22832152 -0.00322156]]. Action = [[ 0.24047461  0.1958428   0.02949005 -0.07016224]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 566 is [True, False, False, False, True, False]
Scene graph at timestep 566 is [True, False, False, False, True, False]
State prediction error at timestep 566 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Current timestep = 567. State = [[-0.2259828  -0.00296919]]. Action = [[0.20117909 0.24819386 0.07463086 0.899968  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 567 is [True, False, False, False, True, False]
Current timestep = 568. State = [[-0.22248518 -0.00066432]]. Action = [[ 0.21224433  0.05631566 -0.22628696 -0.5782447 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 568 is [True, False, False, False, True, False]
Current timestep = 569. State = [[-0.21800973  0.00162864]]. Action = [[-0.19235443  0.22504178  0.15967259  0.33755744]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 569 is [True, False, False, False, True, False]
Current timestep = 570. State = [[-0.21631788  0.0049774 ]]. Action = [[-0.24594708 -0.1700194   0.01778385  0.95912313]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 570 is [True, False, False, False, True, False]
Current timestep = 571. State = [[-0.21642014  0.00534851]]. Action = [[ 0.02894786 -0.13907813 -0.0770655  -0.75599563]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 571 is [True, False, False, False, True, False]
Current timestep = 572. State = [[-0.21629244  0.00462805]]. Action = [[ 0.18232715  0.13351572 -0.21198758 -0.93242776]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 572 is [True, False, False, False, True, False]
Current timestep = 573. State = [[-0.21602777  0.00519439]]. Action = [[-0.24847509 -0.1159956  -0.1466111  -0.4695809 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 573 is [True, False, False, False, True, False]
Human Feedback received at timestep 573 of 1
Current timestep = 574. State = [[-0.2160322   0.00525704]]. Action = [[-0.11825994  0.12182397  0.02055046 -0.7906668 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 574 is [True, False, False, False, True, False]
Current timestep = 575. State = [[-0.21633475  0.00603528]]. Action = [[ 0.01120037  0.20826459  0.04074037 -0.07018977]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 575 is [True, False, False, False, True, False]
Current timestep = 576. State = [[-0.21716659  0.00883417]]. Action = [[-0.00422604  0.12493417  0.19166523  0.86328936]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 576 is [True, False, False, False, True, False]
Current timestep = 577. State = [[-0.21823685  0.0124726 ]]. Action = [[-0.09327751  0.09229064 -0.07041106  0.34023416]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 577 is [True, False, False, False, True, False]
Current timestep = 578. State = [[-0.21933012  0.01589283]]. Action = [[-0.23164049 -0.08428027 -0.1938676   0.61675453]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 578 is [True, False, False, False, True, False]
Current timestep = 579. State = [[-0.22092743  0.01762499]]. Action = [[-0.11346632  0.1456154   0.12507367 -0.6046331 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 579 is [True, False, False, False, True, False]
Current timestep = 580. State = [[-0.22275227  0.02053583]]. Action = [[ 0.14550906 -0.17310289  0.20267755  0.11334574]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 580 is [True, False, False, False, True, False]
Current timestep = 581. State = [[-0.22296077  0.02069048]]. Action = [[-0.0477546  0.2002261 -0.1260594 -0.9759839]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 581 is [True, False, False, False, True, False]
Current timestep = 582. State = [[-0.22360392  0.02272828]]. Action = [[-0.21565759  0.15965402  0.03843623 -0.55508363]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 582 is [True, False, False, False, True, False]
Current timestep = 583. State = [[-0.2259153   0.02689619]]. Action = [[0.07420358 0.13809383 0.21366245 0.09730029]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 583 is [True, False, False, False, True, False]
Scene graph at timestep 583 is [True, False, False, False, True, False]
State prediction error at timestep 583 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 584. State = [[-0.22746216  0.03124783]]. Action = [[ 0.04822272 -0.00377357  0.11320111  0.77724147]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 584 is [True, False, False, False, True, False]
Current timestep = 585. State = [[-0.22820193  0.03342679]]. Action = [[-0.23197526  0.11070254 -0.18354417  0.8213284 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 585 is [True, False, False, False, True, False]
Scene graph at timestep 585 is [True, False, False, False, True, False]
State prediction error at timestep 585 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 586. State = [[-0.2302899   0.03672483]]. Action = [[ 0.17528412 -0.20382507  0.17871422 -0.84323597]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 586 is [True, False, False, False, True, False]
Current timestep = 587. State = [[-0.23032662  0.0362578 ]]. Action = [[ 0.15884721 -0.15966499 -0.22644235 -0.2915039 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 587 is [True, False, False, False, True, False]
Current timestep = 588. State = [[-0.22987543  0.03424656]]. Action = [[-0.01392965 -0.22161984  0.13219655  0.8491211 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 588 is [True, False, False, False, True, False]
Current timestep = 589. State = [[-0.2295036   0.03075603]]. Action = [[-0.05615716  0.22079587  0.04253113 -0.3408383 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 589 is [True, False, False, False, True, False]
Scene graph at timestep 589 is [True, False, False, False, True, False]
State prediction error at timestep 589 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 590. State = [[-0.22980885  0.03096747]]. Action = [[-0.2120993   0.17448932 -0.2144507  -0.7465775 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 590 is [True, False, False, False, True, False]
Current timestep = 591. State = [[-0.23090453  0.03278954]]. Action = [[ 0.09416926 -0.0201579   0.149831   -0.7963047 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 591 is [True, False, False, False, True, False]
Scene graph at timestep 591 is [True, False, False, False, True, False]
State prediction error at timestep 591 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 592. State = [[-0.23114374  0.03324158]]. Action = [[ 0.10460466 -0.22094952  0.12600741 -0.86073875]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 592 is [True, False, False, False, True, False]
Scene graph at timestep 592 is [True, False, False, False, True, False]
State prediction error at timestep 592 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 593. State = [[-0.23104447  0.03165419]]. Action = [[-0.07616329 -0.05219111 -0.13065785  0.09692287]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 593 is [True, False, False, False, True, False]
Current timestep = 594. State = [[-0.23098396  0.03028573]]. Action = [[0.12387598 0.16300309 0.00429615 0.9160986 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 594 is [True, False, False, False, True, False]
Current timestep = 595. State = [[-0.23098184  0.03023629]]. Action = [[-0.10492891 -0.2221342  -0.14619751 -0.7552694 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 595 is [True, False, False, False, True, False]
Current timestep = 596. State = [[-0.23095626  0.02883597]]. Action = [[-0.18419518 -0.01971024 -0.18170476  0.83477354]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 596 is [True, False, False, False, True, False]
Current timestep = 597. State = [[-0.23220915  0.02741059]]. Action = [[-0.18185519 -0.22183055 -0.24131197 -0.1702742 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 597 is [True, False, False, False, True, False]
Current timestep = 598. State = [[-0.23499544  0.02439665]]. Action = [[-0.21794012  0.21783698 -0.13522391 -0.28186   ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 598 is [True, False, False, False, True, False]
Scene graph at timestep 598 is [True, False, False, False, True, False]
State prediction error at timestep 598 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 599. State = [[-0.2405866   0.02413039]]. Action = [[-0.00552043  0.01569337  0.0572359  -0.5282818 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 599 is [True, False, False, False, True, False]
Current timestep = 600. State = [[-0.24380127  0.02366036]]. Action = [[ 0.07728791 -0.13271596 -0.14044178 -0.9011754 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 600 is [True, False, False, False, True, False]
Scene graph at timestep 600 is [True, False, False, False, True, False]
State prediction error at timestep 600 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 601. State = [[-0.24516197  0.02209613]]. Action = [[-0.22200999  0.06428996 -0.03840305 -0.75037324]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 601 is [True, False, False, False, True, False]
Current timestep = 602. State = [[-0.24803676  0.02133482]]. Action = [[-0.24707337 -0.24157567 -0.00159045 -0.7266285 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 602 is [True, False, False, False, True, False]
Current timestep = 603. State = [[-0.2520034   0.01849073]]. Action = [[ 0.12583518  0.19073051 -0.0520108   0.21712112]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 603 is [True, False, False, False, True, False]
Current timestep = 604. State = [[-0.2536175   0.01877106]]. Action = [[ 0.16953552  0.15692693  0.18002015 -0.25772846]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 604 is [True, False, False, False, True, False]
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.25377977  0.01993956]]. Action = [[ 0.07906151 -0.0805721  -0.08274837  0.80678606]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 605 is [True, False, False, False, True, False]
Current timestep = 606. State = [[-0.25373617  0.02010284]]. Action = [[-0.03612593  0.17780441 -0.0253299   0.4631245 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 606 is [True, False, False, False, True, False]
Current timestep = 607. State = [[-0.25425854  0.02173181]]. Action = [[ 0.1277389  -0.23362768  0.17636031 -0.01215088]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 607 is [True, False, False, False, True, False]
Current timestep = 608. State = [[-0.25394478  0.02102137]]. Action = [[ 0.09184849 -0.17094766  0.08449847 -0.25339442]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 608 is [True, False, False, False, True, False]
Scene graph at timestep 608 is [True, False, False, False, True, False]
State prediction error at timestep 608 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 609. State = [[-0.2533329   0.01909658]]. Action = [[-0.06971756  0.1897595   0.13756633  0.94335055]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 609 is [True, False, False, False, True, False]
Scene graph at timestep 609 is [True, False, False, False, True, False]
State prediction error at timestep 609 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 610. State = [[-0.25344327  0.01931417]]. Action = [[-0.08604756  0.01945484 -0.05801579 -0.8969022 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 610 is [True, False, False, False, True, False]
Current timestep = 611. State = [[-0.25353116  0.01979909]]. Action = [[-0.09516808  0.1126231   0.12998223 -0.5724772 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 611 is [True, False, False, False, True, False]
Current timestep = 612. State = [[-0.25395656  0.02103924]]. Action = [[ 0.09618816 -0.10960782  0.11926287  0.88781035]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 612 is [True, False, False, False, True, False]
Current timestep = 613. State = [[-0.2538441   0.02104746]]. Action = [[0.16999632 0.20620057 0.1346229  0.10118175]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 613 is [True, False, False, False, True, False]
Current timestep = 614. State = [[-0.25379646  0.02193258]]. Action = [[ 0.24756336 -0.1243591   0.07474449 -0.90570086]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 614 is [True, False, False, False, True, False]
Current timestep = 615. State = [[-0.25197256  0.02185994]]. Action = [[-0.07847928 -0.21550667  0.18329382 -0.8386966 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 615 is [True, False, False, False, True, False]
Current timestep = 616. State = [[-0.25088754  0.02015804]]. Action = [[ 0.18616152 -0.22565873  0.11217642  0.8224685 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 616 is [True, False, False, False, True, False]
Current timestep = 617. State = [[-0.24877183  0.01707504]]. Action = [[0.1413888  0.21041545 0.15922225 0.42051446]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 617 is [True, False, False, False, True, False]
Current timestep = 618. State = [[-0.24672374  0.01710853]]. Action = [[ 0.00840175 -0.21136266  0.02074999  0.2508533 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 618 is [True, False, False, False, True, False]
Current timestep = 619. State = [[-0.24474828  0.01506525]]. Action = [[-0.1755854  -0.10035802 -0.11341554  0.93326783]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 619 is [True, False, False, False, True, False]
Current timestep = 620. State = [[-0.24398628  0.01265738]]. Action = [[-0.0399137   0.00928181 -0.2365157   0.38927495]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 620 is [True, False, False, False, True, False]
Current timestep = 621. State = [[-0.2438274  0.0114834]]. Action = [[-0.05910692 -0.13817741 -0.12301089 -0.5856628 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 621 is [True, False, False, False, True, False]
Scene graph at timestep 621 is [True, False, False, False, True, False]
State prediction error at timestep 621 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 622. State = [[-0.24354346  0.00877313]]. Action = [[-0.05473793 -0.1858108   0.22140807  0.15981328]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 622 is [True, False, False, False, True, False]
Scene graph at timestep 622 is [True, False, False, False, True, False]
State prediction error at timestep 622 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Current timestep = 623. State = [[-0.2435205   0.00425026]]. Action = [[-0.1179401  -0.00137517  0.19241917  0.09767246]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 623 is [True, False, False, False, True, False]
Scene graph at timestep 623 is [True, False, False, False, True, False]
State prediction error at timestep 623 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 624. State = [[-0.2443239   0.00079631]]. Action = [[ 0.23705813 -0.22090954 -0.09102453  0.32815742]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 624 is [True, False, False, False, True, False]
Current timestep = 625. State = [[-0.2439279  -0.00350513]]. Action = [[0.08732653 0.21562952 0.05196577 0.78095543]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 625 is [True, False, False, False, True, False]
Current timestep = 626. State = [[-0.24375749 -0.00381809]]. Action = [[ 0.04487875  0.06657398  0.14986902 -0.17096442]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 626 is [True, False, False, False, True, False]
Scene graph at timestep 626 is [True, False, False, False, True, False]
State prediction error at timestep 626 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 627. State = [[-0.2436431  -0.00378799]]. Action = [[-0.21874878  0.00328618  0.01410508  0.8504343 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 627 is [True, False, False, False, True, False]
Scene graph at timestep 627 is [True, False, False, False, True, False]
State prediction error at timestep 627 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 628. State = [[-0.24368109 -0.00379278]]. Action = [[-0.20942728 -0.12489593 -0.10183731  0.31113338]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 628 is [True, False, False, False, True, False]
Current timestep = 629. State = [[-0.24501857 -0.00464599]]. Action = [[ 0.0371128   0.15143174 -0.01064105 -0.25360614]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 629 is [True, False, False, False, True, False]
Current timestep = 630. State = [[-0.2452306  -0.00451425]]. Action = [[-0.08994731 -0.21641761 -0.20384626  0.97531366]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 630 is [True, False, False, False, True, False]
Current timestep = 631. State = [[-0.24596256 -0.00578497]]. Action = [[-0.097866   -0.06109007  0.14988488  0.88100135]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 631 is [True, False, False, False, True, False]
Current timestep = 632. State = [[-0.24744017 -0.00764899]]. Action = [[-0.19890645  0.01855406 -0.11132617  0.45165277]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 632 is [True, False, False, False, True, False]
Current timestep = 633. State = [[-0.24921983 -0.00895843]]. Action = [[-0.02709018 -0.03878097 -0.09765597  0.44621623]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 633 is [True, False, False, False, True, False]
Scene graph at timestep 633 is [True, False, False, False, True, False]
State prediction error at timestep 633 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 634. State = [[-0.25066775 -0.01073049]]. Action = [[-0.14915152  0.03995427  0.10097313 -0.13064998]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 634 is [True, False, False, False, True, False]
Scene graph at timestep 634 is [True, False, False, False, True, False]
State prediction error at timestep 634 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 635. State = [[-0.25274372 -0.01137288]]. Action = [[ 0.2210801   0.20511603  0.10541973 -0.8165324 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 635 is [True, False, False, False, True, False]
Scene graph at timestep 635 is [True, False, False, False, True, False]
State prediction error at timestep 635 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 636. State = [[-0.25315082 -0.01018276]]. Action = [[ 0.10855517 -0.16782798 -0.22643572 -0.83097833]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 636 is [True, False, False, False, True, False]
Scene graph at timestep 636 is [True, False, False, False, True, False]
State prediction error at timestep 636 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 637. State = [[-0.25325572 -0.01048044]]. Action = [[-0.15599738  0.05748171 -0.00146437  0.8215828 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 637 is [True, False, False, False, True, False]
Scene graph at timestep 637 is [True, False, False, False, True, False]
State prediction error at timestep 637 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 638. State = [[-0.253745   -0.01037574]]. Action = [[0.15579152 0.15586358 0.03020957 0.7473419 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 638 is [True, False, False, False, True, False]
Current timestep = 639. State = [[-0.25392523 -0.00932249]]. Action = [[ 0.09727779  0.21014923  0.08441335 -0.92907125]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 639 is [True, False, False, False, True, False]
Current timestep = 640. State = [[-0.25454232 -0.00665084]]. Action = [[-0.19493462 -0.20632505  0.17875716  0.5374187 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 640 is [True, False, False, False, True, False]
Current timestep = 641. State = [[-0.25472805 -0.00606094]]. Action = [[-0.20125704  0.1425097  -0.18241587 -0.11727321]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 641 is [True, False, False, False, True, False]
Current timestep = 642. State = [[-0.25601628 -0.00381757]]. Action = [[ 0.22404158  0.23099455  0.23454788 -0.90356386]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 642 is [True, False, False, False, True, False]
Current timestep = 643. State = [[-0.25693434 -0.0005914 ]]. Action = [[ 0.1844073  -0.14207985 -0.1068332   0.89233685]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 643 is [True, False, False, False, True, False]
Scene graph at timestep 643 is [True, False, False, False, True, False]
State prediction error at timestep 643 is tensor(7.9137e-05, grad_fn=<MseLossBackward0>)
Current timestep = 644. State = [[-2.5696400e-01 -2.5430674e-04]]. Action = [[ 0.12388295 -0.01153222 -0.09726682  0.9614506 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 644 is [True, False, False, False, True, False]
Current timestep = 645. State = [[-2.5682443e-01 -7.4228446e-05]]. Action = [[-0.15489757 -0.11230576 -0.07489094  0.907452  ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 645 is [True, False, False, False, True, False]
Current timestep = 646. State = [[-2.5681201e-01 -1.9728133e-04]]. Action = [[-0.23401827  0.03691947  0.17970207  0.9028554 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 646 is [True, False, False, False, True, False]
Current timestep = 647. State = [[-2.5683352e-01 -1.0277938e-04]]. Action = [[-0.11958396  0.03799433 -0.11587685 -0.26027226]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 647 is [True, False, False, False, True, False]
Current timestep = 648. State = [[-0.25727218  0.00043065]]. Action = [[-0.24543613  0.1145466   0.17815751  0.17404199]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 648 is [True, False, False, False, True, False]
Current timestep = 649. State = [[-0.25994378  0.00253466]]. Action = [[-0.04666899  0.23115069  0.17056775 -0.9152391 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 649 is [True, False, False, False, True, False]
Current timestep = 650. State = [[-0.26319537  0.00634046]]. Action = [[ 0.08988428 -0.18600133  0.07148769 -0.21176296]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 650 is [True, False, False, False, True, False]
Scene graph at timestep 650 is [True, False, False, False, True, False]
State prediction error at timestep 650 is tensor(2.5272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 651. State = [[-0.26471296  0.00678466]]. Action = [[ 0.10738331  0.11037815  0.22527862 -0.8531184 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 651 is [True, False, False, False, True, False]
Scene graph at timestep 651 is [True, False, False, False, True, False]
State prediction error at timestep 651 is tensor(1.8263e-05, grad_fn=<MseLossBackward0>)
Current timestep = 652. State = [[-0.26518115  0.0079162 ]]. Action = [[-0.2351516   0.05114543  0.05134624  0.86783934]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 652 is [True, False, False, False, True, False]
Current timestep = 653. State = [[-0.2667014   0.00945646]]. Action = [[ 0.1894179  -0.24367714  0.06426701  0.98193693]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 653 is [True, False, False, False, True, False]
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.26699436  0.00832207]]. Action = [[ 0.03847086  0.14879018 -0.17710546  0.68697166]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 654 is [True, False, False, False, True, False]
Current timestep = 655. State = [[-0.26703045  0.00853875]]. Action = [[0.14686489 0.15483683 0.09807158 0.1617409 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 655 is [True, False, False, False, True, False]
Current timestep = 656. State = [[-0.26719877  0.00965794]]. Action = [[-0.19383156 -0.18095893 -0.15112087  0.11105227]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 656 is [True, False, False, False, True, False]
Scene graph at timestep 656 is [True, False, False, False, True, False]
State prediction error at timestep 656 is tensor(1.1410e-05, grad_fn=<MseLossBackward0>)
Current timestep = 657. State = [[-0.26719883  0.00945375]]. Action = [[ 0.04799014 -0.16131024 -0.1495256   0.00254667]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 657 is [True, False, False, False, True, False]
Current timestep = 658. State = [[-0.2671435   0.00807333]]. Action = [[ 0.09648937  0.10581255  0.15793234 -0.37651217]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 658 is [True, False, False, False, True, False]
Scene graph at timestep 658 is [True, False, False, False, True, False]
State prediction error at timestep 658 is tensor(3.7854e-06, grad_fn=<MseLossBackward0>)
Current timestep = 659. State = [[-0.26717088  0.00786733]]. Action = [[-0.21652623 -0.18137108 -0.04454912 -0.24538398]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 659 is [True, False, False, False, True, False]
Scene graph at timestep 659 is [True, False, False, False, True, False]
State prediction error at timestep 659 is tensor(1.0391e-06, grad_fn=<MseLossBackward0>)
Current timestep = 660. State = [[-0.26766014  0.0057516 ]]. Action = [[ 0.13218218 -0.18980162  0.21094167  0.44602036]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 660 is [True, False, False, False, True, False]
Current timestep = 661. State = [[-0.26754868  0.00302886]]. Action = [[-0.18952681  0.15533802 -0.14860946  0.53835   ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 661 is [True, False, False, False, True, False]
Current timestep = 662. State = [[-0.26844037  0.00259251]]. Action = [[ 0.23845923 -0.16742009  0.03954533  0.6508734 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 662 is [True, False, False, False, True, False]
Current timestep = 663. State = [[-0.2681523   0.00108036]]. Action = [[-0.01952794  0.09328294 -0.05588056  0.7960701 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 663 is [True, False, False, False, True, False]
Scene graph at timestep 663 is [True, False, False, False, True, False]
State prediction error at timestep 663 is tensor(1.0286e-05, grad_fn=<MseLossBackward0>)
Current timestep = 664. State = [[-0.26812327  0.00093313]]. Action = [[ 0.00089189 -0.05478391  0.15307635 -0.8611643 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 664 is [True, False, False, False, True, False]
Current timestep = 665. State = [[-0.2681551   0.00036428]]. Action = [[ 0.22964436  0.04165494 -0.158622    0.93129635]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 665 is [True, False, False, False, True, False]
Current timestep = 666. State = [[-2.6725650e-01  1.9985106e-04]]. Action = [[-0.17928466 -0.20338835 -0.10451701  0.21812534]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 666 is [True, False, False, False, True, False]
Current timestep = 667. State = [[-0.26721942 -0.001938  ]]. Action = [[0.13773    0.09231809 0.07484537 0.35109377]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 667 is [True, False, False, False, True, False]
Scene graph at timestep 667 is [True, False, False, False, True, False]
State prediction error at timestep 667 is tensor(3.4309e-05, grad_fn=<MseLossBackward0>)
Current timestep = 668. State = [[-0.26684782 -0.00303652]]. Action = [[ 0.17026329 -0.18624587  0.08087698 -0.3171575 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 668 is [True, False, False, False, True, False]
Current timestep = 669. State = [[-0.26538584 -0.0054816 ]]. Action = [[ 0.2147198  -0.01204823 -0.01815644 -0.9149709 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 669 is [True, False, False, False, True, False]
Current timestep = 670. State = [[-0.26285198 -0.0074735 ]]. Action = [[0.11065012 0.03458452 0.02227932 0.14966798]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 670 is [True, False, False, False, True, False]
Current timestep = 671. State = [[-0.26017547 -0.00882071]]. Action = [[ 0.04534221  0.22654396 -0.2240025   0.36386812]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 671 is [True, False, False, False, True, False]
Current timestep = 672. State = [[-0.2581943  -0.00817258]]. Action = [[ 0.13715386 -0.16667344  0.15895766 -0.07856643]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 672 is [True, False, False, False, True, False]
Current timestep = 673. State = [[-0.25557595 -0.0090626 ]]. Action = [[-0.11664119 -0.18157992  0.12799966 -0.9313771 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 673 is [True, False, False, False, True, False]
Current timestep = 674. State = [[-0.25419936 -0.01174002]]. Action = [[ 0.09710348 -0.10990083  0.15937838 -0.5707286 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 674 is [True, False, False, False, True, False]
Current timestep = 675. State = [[-0.252464   -0.01472277]]. Action = [[ 0.08258992  0.16544446 -0.0473721   0.40171456]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 675 is [True, False, False, False, True, False]
Current timestep = 676. State = [[-0.25071675 -0.01519073]]. Action = [[ 0.14546919 -0.15968364 -0.22097586  0.44972634]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 676 is [True, False, False, False, True, False]
Current timestep = 677. State = [[-0.24717641 -0.01650968]]. Action = [[ 0.2306819  -0.03189883 -0.13086575 -0.21265912]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 677 is [True, False, False, False, True, False]
Human Feedback received at timestep 677 of 1
Current timestep = 678. State = [[-0.24098106 -0.01811876]]. Action = [[ 0.15327281 -0.12801279 -0.16036546  0.2397008 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 678 is [True, False, False, False, True, False]
Scene graph at timestep 678 is [True, False, False, False, True, False]
State prediction error at timestep 678 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 679. State = [[-0.23582453 -0.02088768]]. Action = [[-0.01255699 -0.0488952  -0.08958626 -0.9108621 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 679 is [True, False, False, False, True, False]
Scene graph at timestep 679 is [True, False, False, False, True, False]
State prediction error at timestep 679 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 680. State = [[-0.23300165 -0.0234163 ]]. Action = [[-0.00746952  0.19667631 -0.21803544  0.13037872]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 680 is [True, False, False, False, True, False]
Current timestep = 681. State = [[-0.23184511 -0.02331112]]. Action = [[-0.04522187 -0.2168815  -0.16014245 -0.9091923 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 681 is [True, False, False, False, True, False]
Scene graph at timestep 681 is [True, False, False, False, True, False]
State prediction error at timestep 681 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 682. State = [[-0.23058307 -0.02556664]]. Action = [[-0.07869282 -0.13335015  0.1380314  -0.9693167 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 682 is [True, False, False, False, True, False]
Scene graph at timestep 682 is [True, False, False, False, True, False]
State prediction error at timestep 682 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 682 of 1
Current timestep = 683. State = [[-0.22976577 -0.0290241 ]]. Action = [[ 0.05449238 -0.17614555 -0.04123725  0.54366875]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 683 is [True, False, False, False, True, False]
Scene graph at timestep 683 is [True, False, False, False, True, False]
State prediction error at timestep 683 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 684. State = [[-0.22884694 -0.03289076]]. Action = [[ 0.20245975  0.18677813 -0.08279829  0.5149522 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 684 is [True, False, False, False, True, False]
Scene graph at timestep 684 is [True, False, False, False, True, False]
State prediction error at timestep 684 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 685. State = [[-0.22756714 -0.03357199]]. Action = [[-0.06572761  0.06846491 -0.22123663 -0.41601896]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 685 is [True, False, False, False, True, False]
Current timestep = 686. State = [[-0.22757491 -0.03321476]]. Action = [[-0.18908508  0.10187048 -0.15308194 -0.49882156]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 686 is [True, False, False, False, True, False]
Current timestep = 687. State = [[-0.2276605  -0.03253013]]. Action = [[-0.13032241  0.14847323 -0.21583483 -0.74868053]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 687 is [True, False, False, False, True, False]
Current timestep = 688. State = [[-0.22802977 -0.03042525]]. Action = [[-0.04395385  0.22375816  0.19941247  0.12955523]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 688 is [True, False, False, False, True, False]
Scene graph at timestep 688 is [True, False, False, False, True, False]
State prediction error at timestep 688 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 689. State = [[-0.22889584 -0.02629014]]. Action = [[-0.146789   -0.09252253  0.19731328 -0.9560661 ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 689 is [True, False, False, False, True, False]
Current timestep = 690. State = [[-0.22927077 -0.02494757]]. Action = [[ 0.07951838 -0.22453535  0.04720294  0.4920044 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 690 is [True, False, False, False, True, False]
Current timestep = 691. State = [[-0.2292247  -0.02531856]]. Action = [[ 0.09610108  0.05749705 -0.06716847  0.20212114]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 691 is [True, False, False, False, True, False]
Current timestep = 692. State = [[-0.22924648 -0.02544004]]. Action = [[ 0.05018902  0.00343123  0.09328443 -0.6610179 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 692 is [True, False, False, False, True, False]
Current timestep = 693. State = [[-0.22920379 -0.02558861]]. Action = [[-0.21700004 -0.20873377  0.18549126 -0.8397511 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 693 is [True, False, False, False, True, False]
Current timestep = 694. State = [[-0.22948791 -0.02743933]]. Action = [[-0.1839638   0.005418   -0.1585466   0.45703435]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 694 is [True, False, False, False, True, False]
Current timestep = 695. State = [[-0.23092566 -0.02884821]]. Action = [[ 0.18381679  0.18298411 -0.21550551  0.21343946]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 695 is [True, False, False, False, True, False]
Current timestep = 696. State = [[-0.23104037 -0.02818824]]. Action = [[-0.2349809   0.18612954  0.01515287 -0.3749616 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 696 is [True, False, False, False, True, False]
Current timestep = 697. State = [[-0.23281015 -0.02568525]]. Action = [[0.11219493 0.13582337 0.15428811 0.30408096]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 697 is [True, False, False, False, True, False]
Current timestep = 698. State = [[-0.23373762 -0.02277955]]. Action = [[-0.23055957 -0.23518372  0.2088696  -0.05333197]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 698 is [True, False, False, False, True, False]
Current timestep = 699. State = [[-0.23511091 -0.02288559]]. Action = [[ 0.19025242  0.02737796 -0.14776975  0.45747733]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 699 is [True, False, False, False, True, False]
Current timestep = 700. State = [[-0.2353019  -0.02299436]]. Action = [[ 0.05533621 -0.08824557  0.07691211  0.12384272]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 700 is [True, False, False, False, True, False]
Current timestep = 701. State = [[-0.23523146 -0.02351712]]. Action = [[ 0.0082441  -0.23725235 -0.16905324  0.3192191 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 701 is [True, False, False, False, True, False]
Current timestep = 702. State = [[-0.23507316 -0.02587714]]. Action = [[ 0.23642653  0.22333038 -0.0387762  -0.19570911]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 702 is [True, False, False, False, True, False]
Current timestep = 703. State = [[-0.23499171 -0.0258474 ]]. Action = [[ 0.22325337 -0.01721692  0.14924532 -0.7781335 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 703 is [True, False, False, False, True, False]
Scene graph at timestep 703 is [True, False, False, False, True, False]
State prediction error at timestep 703 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 704. State = [[-0.23378474 -0.02563506]]. Action = [[-0.23106577  0.17594671  0.15154272 -0.15289748]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 704 is [True, False, False, False, True, False]
Scene graph at timestep 704 is [True, False, False, False, True, False]
State prediction error at timestep 704 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 705. State = [[-0.23394701 -0.02434551]]. Action = [[-0.01268817 -0.00297518  0.22335273 -0.4239928 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 705 is [True, False, False, False, True, False]
Current timestep = 706. State = [[-0.23402654 -0.02341495]]. Action = [[0.1289877  0.09213838 0.07681528 0.52772975]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 706 is [True, False, False, False, True, False]
Current timestep = 707. State = [[-0.23391704 -0.02235245]]. Action = [[ 0.01141199  0.02053663  0.1472351  -0.3835426 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 707 is [True, False, False, False, True, False]
Current timestep = 708. State = [[-0.23372513 -0.02138117]]. Action = [[ 0.19237712 -0.12009686 -0.06900345  0.9180138 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 708 is [True, False, False, False, True, False]
Current timestep = 709. State = [[-0.23225151 -0.02122484]]. Action = [[-0.2146313   0.14066237  0.05128649 -0.29391313]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 709 is [True, False, False, False, True, False]
Scene graph at timestep 709 is [True, False, False, False, True, False]
State prediction error at timestep 709 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 710. State = [[-0.23217826 -0.02052268]]. Action = [[ 0.18246192  0.02224466 -0.10559008 -0.6307108 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 710 is [True, False, False, False, True, False]
Current timestep = 711. State = [[-0.2310816  -0.01948201]]. Action = [[ 0.12881973  0.20217195 -0.1853868   0.9719819 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 711 is [True, False, False, False, True, False]
Current timestep = 712. State = [[-0.22894016 -0.01659827]]. Action = [[ 0.22583055  0.08040449  0.08139637 -0.66702324]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 712 is [True, False, False, False, True, False]
Current timestep = 713. State = [[-0.22445415 -0.0129759 ]]. Action = [[ 0.131892    0.01877946 -0.12394041 -0.2801447 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 713 is [True, False, False, False, True, False]
Current timestep = 714. State = [[-0.22033042 -0.01027632]]. Action = [[ 0.06786674 -0.15361024 -0.18954413  0.3586254 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 714 is [True, False, False, False, True, False]
Current timestep = 715. State = [[-0.21617118 -0.01050491]]. Action = [[ 0.12204894 -0.20130168 -0.12437522  0.06521416]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 715 is [True, False, False, False, True, False]
Scene graph at timestep 715 is [True, False, False, False, True, False]
State prediction error at timestep 715 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 716. State = [[-0.2116465  -0.01195967]]. Action = [[ 0.21030366  0.00369191  0.02699551 -0.0940488 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 716 is [True, False, False, False, True, False]
Scene graph at timestep 716 is [True, False, False, False, True, False]
State prediction error at timestep 716 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 717. State = [[-0.2065672  -0.01261407]]. Action = [[-0.08797905  0.03927535  0.01086691 -0.06634027]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 717 is [True, False, False, False, True, False]
Current timestep = 718. State = [[-0.20498417 -0.01278048]]. Action = [[ 0.12621516 -0.22506364 -0.02519    -0.31812632]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 718 is [True, False, False, False, True, False]
Current timestep = 719. State = [[-0.20276566 -0.01589517]]. Action = [[-0.0867109  -0.20024635 -0.18301754 -0.5123219 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 719 is [True, False, False, False, True, False]
Human Feedback received at timestep 719 of 1
Current timestep = 720. State = [[-0.20135057 -0.02051049]]. Action = [[-0.04191142 -0.23344842 -0.02186882 -0.88665485]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 720 is [True, False, False, False, True, False]
Scene graph at timestep 720 is [True, False, False, False, True, False]
State prediction error at timestep 720 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 721. State = [[-0.19988754 -0.02624906]]. Action = [[ 0.16743642  0.15318447 -0.12353942  0.69342315]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 721 is [True, False, False, False, True, False]
Current timestep = 722. State = [[-0.19857067 -0.0282615 ]]. Action = [[-0.05905825 -0.17856196 -0.03466231 -0.14727288]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 722 is [True, False, False, False, True, False]
Current timestep = 723. State = [[-0.1977233  -0.03099976]]. Action = [[ 0.13406715  0.0851143   0.06476361 -0.10339046]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 723 is [True, False, False, False, True, False]
Current timestep = 724. State = [[-0.19661176 -0.03236987]]. Action = [[-0.17596366 -0.15413545 -0.08760184  0.53008795]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 724 is [True, False, False, False, True, False]
Current timestep = 725. State = [[-0.19612604 -0.03456141]]. Action = [[-0.09304836 -0.18369284  0.07398263 -0.88293654]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 725 is [True, False, False, False, True, False]
Current timestep = 726. State = [[-0.19562173 -0.03834405]]. Action = [[ 0.18039283 -0.05535091 -0.21248607  0.695312  ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 726 is [True, False, False, False, True, False]
Current timestep = 727. State = [[-0.19494328 -0.04162719]]. Action = [[ 0.17075789 -0.07349029 -0.0994916  -0.6122692 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 727 is [True, False, False, False, True, False]
Current timestep = 728. State = [[-0.1933388  -0.04505328]]. Action = [[-0.0195663   0.05676094  0.22923118  0.73452616]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 728 is [True, False, False, False, True, False]
Scene graph at timestep 728 is [True, False, False, False, True, False]
State prediction error at timestep 728 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 729. State = [[-0.19238432 -0.04629285]]. Action = [[-0.12936594  0.22504258  0.11580288  0.8597692 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 729 is [True, False, False, False, True, False]
Current timestep = 730. State = [[-0.19243006 -0.04526654]]. Action = [[-0.01561943 -0.10720477  0.14487562  0.7085856 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 730 is [True, False, False, False, True, False]
Current timestep = 731. State = [[-0.19243006 -0.04526654]]. Action = [[-0.01481009  0.18380415  0.15216917 -0.42566454]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 731 is [True, False, False, False, True, False]
Current timestep = 732. State = [[-0.1923313  -0.04407991]]. Action = [[ 0.19868624  0.17022663 -0.02424268 -0.876919  ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 732 is [True, False, False, False, True, False]
Current timestep = 733. State = [[-0.19140655 -0.04135192]]. Action = [[-0.08704808 -0.24247645 -0.21291809  0.817935  ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 733 is [True, False, False, False, True, False]
Scene graph at timestep 733 is [True, False, False, False, True, False]
State prediction error at timestep 733 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 734. State = [[-0.19114009 -0.04181273]]. Action = [[-0.00523266  0.0923453  -0.14942595 -0.92689264]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 734 is [True, False, False, False, True, False]
Current timestep = 735. State = [[-0.19105522 -0.04180374]]. Action = [[ 0.16109228 -0.06746805 -0.19424677 -0.14883745]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 735 is [True, False, False, False, True, False]
Current timestep = 736. State = [[-0.1900266  -0.04213846]]. Action = [[ 0.10088205 -0.15904789 -0.10073555 -0.6683817 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 736 is [True, False, False, False, True, False]
Current timestep = 737. State = [[-0.18827869 -0.04359882]]. Action = [[ 0.207156   -0.10268515 -0.16328424  0.8254528 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 737 is [True, False, False, False, True, False]
Scene graph at timestep 737 is [True, False, False, False, True, False]
State prediction error at timestep 737 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Current timestep = 738. State = [[-0.184477   -0.04563527]]. Action = [[-0.10648279  0.05494434 -0.06668478 -0.049842  ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 738 is [True, False, False, False, True, False]
Current timestep = 739. State = [[-0.18166447 -0.04623435]]. Action = [[ 0.1047059   0.19245145  0.1559684  -0.876958  ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 739 is [True, False, False, False, True, False]
Current timestep = 740. State = [[-0.17908661 -0.04513626]]. Action = [[-0.03786632  0.23495406  0.17686617 -0.9020801 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 740 is [True, False, False, False, True, False]
Current timestep = 741. State = [[-0.17705774 -0.04179445]]. Action = [[0.04715988 0.1107766  0.01647201 0.35950089]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 741 is [True, False, False, False, True, False]
Current timestep = 742. State = [[-0.17593263 -0.03910245]]. Action = [[-0.1317371  -0.17444843  0.03148437  0.11664104]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 742 is [True, False, False, False, True, False]
Current timestep = 743. State = [[-0.17621034 -0.0384668 ]]. Action = [[-0.06433424  0.24440151  0.18502858  0.1386379 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 743 is [True, False, False, False, True, False]
Human Feedback received at timestep 743 of 1
Current timestep = 744. State = [[-0.17663008 -0.03624937]]. Action = [[-0.07016072 -0.21579131  0.11181888  0.98009086]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 744 is [True, False, False, False, True, False]
Current timestep = 745. State = [[-0.17688902 -0.03633321]]. Action = [[ 0.09738624  0.17940763  0.11382991 -0.69073004]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 745 is [True, False, False, False, True, False]
Current timestep = 746. State = [[-0.17700817 -0.03509063]]. Action = [[ 0.16406053  0.05581996  0.19883472 -0.863702  ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 746 is [True, False, False, False, True, False]
Current timestep = 747. State = [[-0.17664015 -0.03426221]]. Action = [[ 0.21737325 -0.09646764  0.11796397 -0.21337593]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 747 is [True, False, False, False, True, False]
Current timestep = 748. State = [[-0.1745834  -0.03403088]]. Action = [[-0.1444528  -0.12640834  0.05929548 -0.09188819]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 748 is [True, False, False, False, True, False]
Current timestep = 749. State = [[-0.17459112 -0.03416489]]. Action = [[-0.24501726  0.17980218 -0.09415454  0.05013263]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 749 is [True, False, False, False, True, False]
Current timestep = 750. State = [[-0.1747785  -0.03332601]]. Action = [[ 0.19972387  0.21311045  0.04118592 -0.95484   ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 750 is [True, False, False, False, True, False]
Human Feedback received at timestep 750 of 1
Current timestep = 751. State = [[-0.17497905 -0.0306466 ]]. Action = [[ 0.13934106 -0.22367723 -0.11931403  0.45377898]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 751 is [True, False, False, False, True, False]
Current timestep = 752. State = [[-0.17405944 -0.03055187]]. Action = [[-0.21352205  0.05842239 -0.00942504 -0.8028758 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 752 is [True, False, False, False, True, False]
Current timestep = 753. State = [[-0.17406225 -0.0305912 ]]. Action = [[ 0.14651328 -0.15123235 -0.06324109 -0.52004635]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 753 is [True, False, False, False, True, False]
Current timestep = 754. State = [[-0.1738969  -0.03099605]]. Action = [[ 0.1755713   0.20894533  0.11463952 -0.25537503]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 754 is [True, False, False, False, True, False]
Current timestep = 755. State = [[-0.17236824 -0.0296826 ]]. Action = [[-0.18817006  0.14497155 -0.05005813 -0.7052353 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 755 is [True, False, False, False, True, False]
Current timestep = 756. State = [[-0.17249432 -0.0270758 ]]. Action = [[-0.09384646  0.17571363  0.02724445  0.20171738]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 756 is [True, False, False, False, True, False]
Current timestep = 757. State = [[-0.1727476  -0.02299845]]. Action = [[ 0.05073461  0.1247403  -0.0160304   0.45521677]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 757 is [True, False, False, False, True, False]
Current timestep = 758. State = [[-0.17270713 -0.01871761]]. Action = [[ 0.09964907  0.06934172 -0.0833312   0.51289153]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 758 is [True, False, False, False, True, False]
Scene graph at timestep 758 is [True, False, False, False, True, False]
State prediction error at timestep 758 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Current timestep = 759. State = [[-0.17171472 -0.01475917]]. Action = [[ 0.01659849  0.14239818 -0.17313775  0.46657944]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 759 is [True, False, False, False, True, False]
Current timestep = 760. State = [[-0.17134933 -0.01105078]]. Action = [[ 0.07417685  0.03719845 -0.20667948 -0.44576412]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 760 is [True, False, False, False, True, False]
Current timestep = 761. State = [[-0.17003097 -0.00815234]]. Action = [[0.07834977 0.19216436 0.1387986  0.73216057]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 761 is [True, False, False, False, True, False]
Current timestep = 762. State = [[-0.16823883 -0.00382664]]. Action = [[ 0.21340668 -0.0020849   0.0912711  -0.9626567 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 762 is [True, False, False, False, True, False]
Current timestep = 763. State = [[-0.16520697 -0.00043934]]. Action = [[ 0.10851151 -0.1349885   0.19962388 -0.94646835]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 763 is [True, False, False, False, True, False]
Scene graph at timestep 763 is [True, False, False, False, True, False]
State prediction error at timestep 763 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 764. State = [[-0.16094942 -0.00032299]]. Action = [[ 0.24581182 -0.22293986  0.23735327 -0.782667  ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 764 is [True, False, False, False, True, False]
Current timestep = 765. State = [[-0.15554717 -0.0021194 ]]. Action = [[ 0.18928397  0.24256074 -0.18113257 -0.38674676]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 765 is [True, False, False, False, True, False]
Current timestep = 766. State = [[-0.14995536 -0.00071041]]. Action = [[ 0.04495096  0.06426641 -0.04727583 -0.1429857 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 766 is [True, False, False, False, True, False]
Current timestep = 767. State = [[-0.14467534  0.00068393]]. Action = [[-0.07917215 -0.22283362 -0.16179545  0.25337434]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 767 is [True, False, False, False, True, False]
Current timestep = 768. State = [[-0.14086816 -0.00019263]]. Action = [[ 0.05579102 -0.13500704 -0.02850641 -0.8055718 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 768 is [True, False, False, False, True, False]
Scene graph at timestep 768 is [True, False, False, False, True, False]
State prediction error at timestep 768 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 769. State = [[-0.13766555 -0.00196605]]. Action = [[-0.20623025  0.0779233   0.00362799  0.7993293 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 769 is [True, False, False, False, True, False]
Current timestep = 770. State = [[-0.13787295 -0.00195909]]. Action = [[ 0.15229833  0.19448537 -0.07579839  0.586087  ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 770 is [True, False, False, False, True, False]
Human Feedback received at timestep 770 of 1
Current timestep = 771. State = [[-0.13718034 -0.00093584]]. Action = [[ 0.22646576 -0.01235911  0.21849889 -0.39439368]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 771 is [True, False, False, False, True, False]
Scene graph at timestep 771 is [True, False, False, False, True, False]
State prediction error at timestep 771 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Current timestep = 772. State = [[-0.13477378 -0.00016848]]. Action = [[-0.03026181 -0.05777773  0.04281271 -0.25010908]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 772 is [True, False, False, False, True, False]
Scene graph at timestep 772 is [True, False, False, False, True, False]
State prediction error at timestep 772 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 773. State = [[-1.3397112e-01  5.3381533e-05]]. Action = [[ 0.03893241  0.20431703 -0.21384059 -0.74946386]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 773 is [True, False, False, False, True, False]
Current timestep = 774. State = [[-0.13341568  0.00213469]]. Action = [[-0.22284181  0.24516204  0.01426882 -0.6709282 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 774 is [True, False, False, False, True, False]
Current timestep = 775. State = [[-0.13442299  0.00625919]]. Action = [[-0.11197722  0.23889077 -0.13987567 -0.3216436 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 775 is [True, False, False, False, True, False]
Scene graph at timestep 775 is [True, False, False, False, True, False]
State prediction error at timestep 775 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Current timestep = 776. State = [[-0.13590741  0.01208065]]. Action = [[-0.11164804 -0.14655712  0.11925697 -0.26064003]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 776 is [True, False, False, False, True, False]
Current timestep = 777. State = [[-0.13652183  0.01402065]]. Action = [[-0.06963103  0.04578048  0.06225473  0.01133049]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 777 is [True, False, False, False, True, False]
Current timestep = 778. State = [[-0.13709491  0.01570161]]. Action = [[ 0.17128801 -0.10138898  0.22158611 -0.00093508]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 778 is [True, False, False, False, True, False]
Current timestep = 779. State = [[-0.13711189  0.01576011]]. Action = [[ 0.16187638  0.12077177 -0.07669084  0.9086007 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 779 is [True, False, False, False, True, False]
Scene graph at timestep 779 is [True, False, False, False, True, False]
State prediction error at timestep 779 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Current timestep = 780. State = [[-0.13708133  0.01617036]]. Action = [[ 0.12364703 -0.14469784 -0.06504798 -0.43196273]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 780 is [True, False, False, False, True, False]
Current timestep = 781. State = [[-0.13655949  0.01569925]]. Action = [[-0.13816544 -0.03185318 -0.13585852 -0.61751634]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 781 is [True, False, False, False, True, False]
Current timestep = 782. State = [[-0.13649088  0.01567029]]. Action = [[-0.19304919 -0.0345107   0.21448445  0.00888705]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 782 is [True, False, False, False, True, False]
Current timestep = 783. State = [[-0.13645387  0.01483406]]. Action = [[ 0.07167202 -0.22881448 -0.23989858  0.94605994]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 783 is [True, False, False, False, True, False]
Current timestep = 784. State = [[-0.13622703  0.01191994]]. Action = [[-0.1422079  -0.24659812 -0.08115873  0.1466949 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 784 is [True, False, False, False, True, False]
Current timestep = 785. State = [[-0.13600746  0.00690859]]. Action = [[-0.17013788 -0.22669253 -0.12151529 -0.58415544]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 785 is [True, False, False, False, True, False]
Current timestep = 786. State = [[-0.1362184   0.00158531]]. Action = [[ 0.0710808  -0.10021155  0.06630144 -0.20759797]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 786 is [True, False, False, False, True, False]
Current timestep = 787. State = [[-0.13639554 -0.00320449]]. Action = [[0.05287296 0.06222212 0.07777989 0.80359197]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 787 is [True, False, False, False, True, False]
Scene graph at timestep 787 is [True, False, False, False, True, False]
State prediction error at timestep 787 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Current timestep = 788. State = [[-0.13661136 -0.00560537]]. Action = [[ 0.04210636  0.03949323 -0.12478685  0.63281655]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 788 is [True, False, False, False, True, False]
Current timestep = 789. State = [[-0.13675232 -0.00669259]]. Action = [[-0.14067315 -0.02922523  0.14414349  0.91227424]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 789 is [True, False, False, False, True, False]
Current timestep = 790. State = [[-0.1369255  -0.00769611]]. Action = [[0.05888987 0.02402699 0.24193859 0.8706105 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 790 is [True, False, False, False, True, False]
Current timestep = 791. State = [[-0.13707568 -0.00766279]]. Action = [[-0.2271672   0.22707397  0.18394732  0.7828944 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 791 is [True, False, False, False, True, False]
Current timestep = 792. State = [[-0.13794301 -0.00584369]]. Action = [[0.00565472 0.0412994  0.1741246  0.08927894]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 792 is [True, False, False, False, True, False]
Current timestep = 793. State = [[-0.13869399 -0.00462701]]. Action = [[-0.02944474  0.1119239  -0.06891972  0.53072965]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 793 is [True, False, False, False, True, False]
Current timestep = 794. State = [[-0.13931625 -0.00274896]]. Action = [[ 0.05483699  0.22633052 -0.15316902 -0.6915074 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 794 is [True, False, False, False, True, False]
Current timestep = 795. State = [[-0.14028364  0.00117737]]. Action = [[ 0.18361524 -0.19608037 -0.1817035  -0.9475382 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 795 is [True, False, False, False, True, False]
Current timestep = 796. State = [[-0.14038017  0.00167949]]. Action = [[ 0.11071804  0.15758806 -0.15323447 -0.63526475]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 796 is [True, False, False, False, True, False]
Current timestep = 797. State = [[-0.14036293  0.00287148]]. Action = [[-0.09499992 -0.0763166  -0.08501741 -0.6529087 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 797 is [True, False, False, False, True, False]
Current timestep = 798. State = [[-0.14034942  0.00307935]]. Action = [[-0.0982378  -0.13764122  0.21992785  0.44834268]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 798 is [True, False, False, False, True, False]
Current timestep = 799. State = [[-0.14023581  0.00267551]]. Action = [[ 0.20733076 -0.22352989 -0.22881652 -0.90813166]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 799 is [True, False, False, False, True, False]
Current timestep = 800. State = [[-1.3977009e-01  1.1436530e-04]]. Action = [[-0.1160877   0.11973396 -0.05618134  0.17139602]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 800 is [True, False, False, False, True, False]
Current timestep = 801. State = [[-0.13982487  0.00021727]]. Action = [[ 0.08620983  0.10416296 -0.05066289  0.3397689 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 801 is [True, False, False, False, True, False]
Current timestep = 802. State = [[-0.13986647  0.00046035]]. Action = [[ 0.01738548 -0.0682604  -0.21208543 -0.05118942]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 802 is [True, False, False, False, True, False]
Current timestep = 803. State = [[-0.13989495  0.00037842]]. Action = [[-0.20166045  0.05834091 -0.10341948  0.11345196]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 803 is [True, False, False, False, True, False]
Current timestep = 804. State = [[-0.13993044  0.00042742]]. Action = [[-0.14138068  0.06226453 -0.22213154  0.81468105]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 804 is [True, False, False, False, True, False]
Current timestep = 805. State = [[-0.14022423  0.00125879]]. Action = [[-0.21634746 -0.08272484 -0.08986616  0.7597244 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 805 is [True, False, False, False, True, False]
Scene graph at timestep 805 is [True, False, False, False, True, False]
State prediction error at timestep 805 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Current timestep = 806. State = [[-0.14186735  0.00077218]]. Action = [[ 0.00804743 -0.24176723  0.1356276   0.00376654]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 806 is [True, False, False, False, True, False]
Current timestep = 807. State = [[-0.14319475 -0.0016942 ]]. Action = [[ 0.22304595  0.14091486 -0.22117238 -0.6023669 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 807 is [True, False, False, False, True, False]
Current timestep = 808. State = [[-0.14325683 -0.00152496]]. Action = [[ 0.10297671  0.10635239  0.07535756 -0.10639793]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 808 is [True, False, False, False, True, False]
Current timestep = 809. State = [[-0.14335462 -0.00091272]]. Action = [[-0.24592228  0.03457731 -0.13794951  0.02291095]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 809 is [True, False, False, False, True, False]
Current timestep = 810. State = [[-0.14367105 -0.00051547]]. Action = [[ 0.21717304 -0.00367852  0.1843307   0.9491093 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 810 is [True, False, False, False, True, False]
Current timestep = 811. State = [[-0.14362949 -0.00062399]]. Action = [[-0.18220875 -0.23589198 -0.16667831 -0.57761353]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 811 is [True, False, False, False, True, False]
Current timestep = 812. State = [[-0.14368773 -0.00190197]]. Action = [[-0.10205941  0.0486322  -0.07292455  0.42297077]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 812 is [True, False, False, False, True, False]
Scene graph at timestep 812 is [True, False, False, False, True, False]
State prediction error at timestep 812 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Current timestep = 813. State = [[-0.1442699  -0.00235944]]. Action = [[ 0.09493178  0.06596032  0.0774487  -0.6573462 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 813 is [True, False, False, False, True, False]
Current timestep = 814. State = [[-0.1443689  -0.00182931]]. Action = [[-0.14118834  0.24296707 -0.23599672 -0.51614416]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 814 is [True, False, False, False, True, False]
Current timestep = 815. State = [[-0.14511476  0.00056896]]. Action = [[ 0.22699058 -0.2180319  -0.00459334  0.15041637]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 815 is [True, False, False, False, True, False]
Scene graph at timestep 815 is [True, False, False, False, True, False]
State prediction error at timestep 815 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Current timestep = 816. State = [[-0.14512947  0.00034277]]. Action = [[ 0.12608457  0.24714464  0.12943187 -0.20347136]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 816 is [True, False, False, False, True, False]
Current timestep = 817. State = [[-0.1452975   0.00180879]]. Action = [[-0.06429033  0.14415792 -0.07257962 -0.97123605]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 817 is [True, False, False, False, True, False]
Scene graph at timestep 817 is [True, False, False, False, True, False]
State prediction error at timestep 817 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Current timestep = 818. State = [[-0.14581528  0.00500541]]. Action = [[ 0.12592629  0.22121304 -0.10687046 -0.4040866 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 818 is [True, False, False, False, True, False]
Current timestep = 819. State = [[-0.14621963  0.00933377]]. Action = [[-0.09461589 -0.2473176   0.036771    0.5780612 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 819 is [True, False, False, False, True, False]
Current timestep = 820. State = [[-0.14623173  0.00930269]]. Action = [[0.17744505 0.00569946 0.08580419 0.8598838 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 820 is [True, False, False, False, True, False]
Current timestep = 821. State = [[-0.1459264   0.00942691]]. Action = [[-0.14731538  0.20547548 -0.18205123 -0.8663974 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 821 is [True, False, False, False, True, False]
Current timestep = 822. State = [[-0.14651886  0.01183554]]. Action = [[-0.1037778  -0.16910487  0.02076921 -0.88124293]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 822 is [True, False, False, False, True, False]
Current timestep = 823. State = [[-0.1464949   0.01176331]]. Action = [[-0.15587886 -0.08410659 -0.12635587 -0.29051197]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 823 is [True, False, False, False, True, False]
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[-0.14667983  0.01121937]]. Action = [[-0.16382405 -0.15274167  0.22315446 -0.06608343]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 824 is [True, False, False, False, True, False]
Current timestep = 825. State = [[-0.1478027   0.00944678]]. Action = [[-0.18046638  0.2294622   0.07995009 -0.11974508]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 825 is [True, False, False, False, True, False]
Current timestep = 826. State = [[-0.14952584  0.01079081]]. Action = [[ 0.2178374  -0.03148623 -0.1700893   0.6091478 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 826 is [True, False, False, False, True, False]
Scene graph at timestep 826 is [True, False, False, False, True, False]
State prediction error at timestep 826 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 827. State = [[-0.14956701  0.01092632]]. Action = [[ 0.2149604  -0.22143169 -0.14791307  0.11194897]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 827 is [True, False, False, False, True, False]
Current timestep = 828. State = [[-0.1491622   0.00875993]]. Action = [[ 0.08374608 -0.12629671 -0.00795832  0.3999796 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 828 is [True, False, False, False, True, False]
Current timestep = 829. State = [[-0.14869662  0.00663948]]. Action = [[ 0.11303377  0.099617   -0.11708117 -0.99500954]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 829 is [True, False, False, False, True, False]
Current timestep = 830. State = [[-0.14830926  0.00632177]]. Action = [[-0.06083456 -0.00619155  0.2048474  -0.12647319]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 830 is [True, False, False, False, True, False]
Current timestep = 831. State = [[-0.14827837  0.00627647]]. Action = [[-0.0172143   0.01015618  0.13136822  0.13926136]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 831 is [True, False, False, False, True, False]
Current timestep = 832. State = [[-0.14831436  0.00584377]]. Action = [[-0.21027714 -0.2230687   0.2181648  -0.6462144 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 832 is [True, False, False, False, True, False]
Current timestep = 833. State = [[-0.14819959  0.00286612]]. Action = [[ 0.06594786 -0.1164954  -0.07216398  0.87541544]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 833 is [True, False, False, False, True, False]
Current timestep = 834. State = [[-0.14811422 -0.00062917]]. Action = [[-0.22156762 -0.04390466 -0.11450003  0.39464593]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 834 is [True, False, False, False, True, False]
Current timestep = 835. State = [[-0.14895056 -0.00387699]]. Action = [[-0.01895458 -0.03274353  0.19966164  0.03837204]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 835 is [True, False, False, False, True, False]
Current timestep = 836. State = [[-0.149393   -0.00651794]]. Action = [[0.09320277 0.01010805 0.20416188 0.834592  ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 836 is [True, False, False, False, True, False]
Current timestep = 837. State = [[-0.14960827 -0.00759781]]. Action = [[-0.17263779 -0.05788577 -0.22000334 -0.95555276]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 837 is [True, False, False, False, True, False]
Scene graph at timestep 837 is [True, False, False, False, True, False]
State prediction error at timestep 837 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Current timestep = 838. State = [[-0.1503139  -0.00901029]]. Action = [[ 0.08284441  0.1190595   0.24000251 -0.4534521 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 838 is [True, False, False, False, True, False]
Current timestep = 839. State = [[-0.15053722 -0.00851388]]. Action = [[ 0.22910696  0.18577608  0.20171386 -0.68477005]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 839 is [True, False, False, False, True, False]
Current timestep = 840. State = [[-0.15073395 -0.00687483]]. Action = [[ 0.14894307  0.14795405  0.07364234 -0.17407298]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 840 is [True, False, False, False, True, False]
Current timestep = 841. State = [[-0.15062638 -0.00441877]]. Action = [[-0.19323985  0.15393236  0.01941839 -0.47706002]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 841 is [True, False, False, False, True, False]
Current timestep = 842. State = [[-0.15123029 -0.00100455]]. Action = [[-0.14647268  0.13127917 -0.03442711  0.0483216 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 842 is [True, False, False, False, True, False]
Current timestep = 843. State = [[-0.15219271  0.00304857]]. Action = [[-0.02129915  0.02966437  0.12119037  0.44046235]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 843 is [True, False, False, False, True, False]
Current timestep = 844. State = [[-0.15305358  0.00629407]]. Action = [[ 0.1235995  -0.16068496 -0.21764663  0.14411712]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 844 is [True, False, False, False, True, False]
Current timestep = 845. State = [[-0.15305957  0.00632853]]. Action = [[-0.13344766 -0.00729918 -0.11253045 -0.6484829 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 845 is [True, False, False, False, True, False]
Current timestep = 846. State = [[-0.15313828  0.00629975]]. Action = [[-0.15113144 -0.10277116  0.04365879  0.76881814]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 846 is [True, False, False, False, True, False]
Current timestep = 847. State = [[-0.15342084  0.00581899]]. Action = [[-0.09966655  0.21896404  0.12945494  0.5590594 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 847 is [True, False, False, False, True, False]
Current timestep = 848. State = [[-0.15443592  0.00768463]]. Action = [[ 0.00108695 -0.22429577 -0.07121041 -0.5516537 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 848 is [True, False, False, False, True, False]
Current timestep = 849. State = [[-0.15479837  0.00648982]]. Action = [[ 0.09327018 -0.15534072 -0.01600225 -0.9098947 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 849 is [True, False, False, False, True, False]
Current timestep = 850. State = [[-0.15473452  0.0039478 ]]. Action = [[-0.1554609  -0.20788158 -0.15443026 -0.54401624]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 850 is [True, False, False, False, True, False]
Current timestep = 851. State = [[-0.15574701  0.00049359]]. Action = [[0.15996224 0.19871244 0.23976582 0.8357252 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 851 is [True, False, False, False, True, False]
Scene graph at timestep 851 is [True, False, False, False, True, False]
State prediction error at timestep 851 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 852. State = [[-0.15596592  0.00041009]]. Action = [[-0.16276519  0.18215299  0.17402565 -0.6733022 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 852 is [True, False, False, False, True, False]
Scene graph at timestep 852 is [True, False, False, False, True, False]
State prediction error at timestep 852 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 853. State = [[-0.15669361  0.00186931]]. Action = [[-0.1571242  -0.14942124 -0.1579171  -0.02225041]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 853 is [True, False, False, False, True, False]
Human Feedback received at timestep 853 of -1
Current timestep = 854. State = [[-0.15780531  0.00148943]]. Action = [[ 0.06352189 -0.0498524  -0.18961181  0.5983496 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 854 is [True, False, False, False, True, False]
Current timestep = 855. State = [[-0.15808743  0.00079682]]. Action = [[ 0.02717432  0.14424741 -0.10759409  0.7914146 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 855 is [True, False, False, False, True, False]
Current timestep = 856. State = [[-0.15827534  0.00119407]]. Action = [[-0.04955928 -0.02501459 -0.12258574 -0.89747566]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 856 is [True, False, False, False, True, False]
Current timestep = 857. State = [[-0.15845123  0.00130409]]. Action = [[ 0.14586973  0.07349423 -0.21898241 -0.4087907 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 857 is [True, False, False, False, True, False]
Current timestep = 858. State = [[-0.1585578   0.00176016]]. Action = [[-0.0904655  -0.12820952 -0.23479417 -0.42646527]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 858 is [True, False, False, False, True, False]
Scene graph at timestep 858 is [True, False, False, False, True, False]
State prediction error at timestep 858 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Current timestep = 859. State = [[-0.15864895  0.00136793]]. Action = [[ 0.12966526 -0.13162054  0.03551009  0.7857467 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 859 is [True, False, False, False, True, False]
Current timestep = 860. State = [[-0.15839699 -0.00031965]]. Action = [[ 0.17608541 -0.22434542 -0.042338   -0.2549218 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 860 is [True, False, False, False, True, False]
Current timestep = 861. State = [[-0.15766233 -0.00423776]]. Action = [[ 0.09023485  0.0622673  -0.05291578  0.01269889]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 861 is [True, False, False, False, True, False]
Current timestep = 862. State = [[-0.15739006 -0.00533357]]. Action = [[-0.20042878  0.2261501  -0.12387964 -0.72421914]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 862 is [True, False, False, False, True, False]
Current timestep = 863. State = [[-0.15767013 -0.00434494]]. Action = [[ 0.17809981 -0.2395857  -0.16416085 -0.20925945]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 863 is [True, False, False, False, True, False]
Current timestep = 864. State = [[-0.15737513 -0.00600379]]. Action = [[-0.07681179  0.07387346  0.223539   -0.30208063]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 864 is [True, False, False, False, True, False]
Scene graph at timestep 864 is [True, False, False, False, True, False]
State prediction error at timestep 864 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Current timestep = 865. State = [[-0.15733148 -0.00626142]]. Action = [[-0.12131882 -0.02278753  0.04332063 -0.08973086]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 865 is [True, False, False, False, True, False]
Current timestep = 866. State = [[-0.15737502 -0.00679451]]. Action = [[-0.09584574 -0.0278416   0.08534351  0.13484037]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 866 is [True, False, False, False, True, False]
Current timestep = 867. State = [[-0.15752491 -0.00765125]]. Action = [[-0.1876085  -0.1954451   0.03234172  0.53210473]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 867 is [True, False, False, False, True, False]
Current timestep = 868. State = [[-0.15888606 -0.01049646]]. Action = [[-0.04210162 -0.03569503 -0.2150189  -0.18291944]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 868 is [True, False, False, False, True, False]
Scene graph at timestep 868 is [True, False, False, False, True, False]
State prediction error at timestep 868 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Current timestep = 869. State = [[-0.16044465 -0.01264534]]. Action = [[ 0.19006336  0.22434264 -0.14401178 -0.7142498 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 869 is [True, False, False, False, True, False]
Current timestep = 870. State = [[-0.16062829 -0.01158805]]. Action = [[0.21528137 0.15570998 0.21784306 0.98628426]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 870 is [True, False, False, False, True, False]
Current timestep = 871. State = [[-0.1607459  -0.00973971]]. Action = [[-0.05177021  0.18367496 -0.07248095  0.27022445]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 871 is [True, False, False, False, True, False]
Current timestep = 872. State = [[-0.1610444  -0.00651319]]. Action = [[ 0.07328591  0.08185896 -0.17483222  0.6529895 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 872 is [True, False, False, False, True, False]
Scene graph at timestep 872 is [True, False, False, False, True, False]
State prediction error at timestep 872 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Current timestep = 873. State = [[-0.16132678 -0.00392231]]. Action = [[-0.00814791 -0.17358674  0.17976543 -0.838865  ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 873 is [True, False, False, False, True, False]
Scene graph at timestep 873 is [True, False, False, False, True, False]
State prediction error at timestep 873 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 874. State = [[-0.1613007 -0.0037284]]. Action = [[-0.02762754  0.24292973 -0.17463346  0.6294501 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 874 is [True, False, False, False, True, False]
Current timestep = 875. State = [[-0.16152698 -0.00143514]]. Action = [[ 0.1348261 -0.1376466  0.1116133 -0.8878848]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 875 is [True, False, False, False, True, False]
Current timestep = 876. State = [[-0.16124417 -0.00127428]]. Action = [[-0.1331     -0.17641158 -0.22792709 -0.8799038 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 876 is [True, False, False, False, True, False]
Scene graph at timestep 876 is [True, False, False, False, True, False]
State prediction error at timestep 876 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 876 of -1
Current timestep = 877. State = [[-0.16091704 -0.00228788]]. Action = [[ 0.2199114  -0.17317116  0.21719253  0.5763564 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 877 is [True, False, False, False, True, False]
Current timestep = 878. State = [[-0.15973356 -0.00482502]]. Action = [[-0.08154586 -0.23547298  0.11669323  0.58220196]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 878 is [True, False, False, False, True, False]
Scene graph at timestep 878 is [True, False, False, False, True, False]
State prediction error at timestep 878 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Current timestep = 879. State = [[-0.15886922 -0.00914388]]. Action = [[-0.23021083 -0.01278293  0.22527185 -0.7735191 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 879 is [True, False, False, False, True, False]
Current timestep = 880. State = [[-0.15892164 -0.01139759]]. Action = [[-0.04393393 -0.04726055 -0.14707734  0.9729426 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 880 is [True, False, False, False, True, False]
Current timestep = 881. State = [[-0.15906602 -0.01347446]]. Action = [[ 0.10388505 -0.00436652  0.05522582 -0.6537762 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 881 is [True, False, False, False, True, False]
Current timestep = 882. State = [[-0.15910502 -0.01469531]]. Action = [[-0.04461624  0.1313971  -0.05424801  0.22770965]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 882 is [True, False, False, False, True, False]
Scene graph at timestep 882 is [True, False, False, False, True, False]
State prediction error at timestep 882 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Current timestep = 883. State = [[-0.1593085  -0.01419795]]. Action = [[ 0.16210091  0.22849482 -0.21161923  0.5877738 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 883 is [True, False, False, False, True, False]
Current timestep = 884. State = [[-0.15953586 -0.01266545]]. Action = [[-0.04594842  0.12846369  0.24117273 -0.79500777]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 884 is [True, False, False, False, True, False]
Current timestep = 885. State = [[-0.15991542 -0.01036013]]. Action = [[ 0.05745625  0.12887686 -0.03181662  0.5132693 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 885 is [True, False, False, False, True, False]
Current timestep = 886. State = [[-0.16009627 -0.00771149]]. Action = [[ 0.110367   -0.18840142 -0.17097226  0.95464134]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 886 is [True, False, False, False, True, False]
Scene graph at timestep 886 is [True, False, False, False, True, False]
State prediction error at timestep 886 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 887. State = [[-0.15980762 -0.00759086]]. Action = [[ 0.12076163  0.04997993 -0.15580012  0.48032105]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 887 is [True, False, False, False, True, False]
Current timestep = 888. State = [[-0.15915899 -0.00722455]]. Action = [[ 0.01160854 -0.16776381 -0.19906518  0.83357143]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 888 is [True, False, False, False, True, False]
Current timestep = 889. State = [[-0.15796219 -0.00826903]]. Action = [[ 0.24142385 -0.22299738  0.19988745  0.4238026 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 889 is [True, False, False, False, True, False]
Current timestep = 890. State = [[-0.15515055 -0.01126204]]. Action = [[-0.02931151  0.08155325  0.09645844 -0.38349307]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 890 is [True, False, False, False, True, False]
Current timestep = 891. State = [[-0.15374878 -0.01185345]]. Action = [[-0.188681    0.17390832  0.21709764 -0.831764  ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 891 is [True, False, False, False, True, False]
Current timestep = 892. State = [[-0.15381724 -0.01086778]]. Action = [[-0.17548305 -0.09909749 -0.17433749  0.39448285]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 892 is [True, False, False, False, True, False]
Current timestep = 893. State = [[-0.1538186  -0.01129144]]. Action = [[ 0.01088375 -0.16335078  0.16621375 -0.18913567]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 893 is [True, False, False, False, True, False]
Current timestep = 894. State = [[-0.15358336 -0.0127365 ]]. Action = [[ 0.13951933  0.13126996 -0.06325746 -0.8408939 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 894 is [True, False, False, False, True, False]
Scene graph at timestep 894 is [True, False, False, False, True, False]
State prediction error at timestep 894 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Current timestep = 895. State = [[-0.15353136 -0.01294115]]. Action = [[ 0.23024565 -0.16678463 -0.22276703  0.5749562 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 895 is [True, False, False, False, True, False]
Current timestep = 896. State = [[-0.15293449 -0.01437538]]. Action = [[-0.195285   -0.07560374  0.06280899  0.32693303]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 896 is [True, False, False, False, True, False]
Current timestep = 897. State = [[-0.15260768 -0.01586919]]. Action = [[-0.12606229  0.03674421  0.1296705   0.3677578 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 897 is [True, False, False, False, True, False]
Scene graph at timestep 897 is [True, False, False, False, True, False]
State prediction error at timestep 897 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Current timestep = 898. State = [[-0.1526288  -0.01628351]]. Action = [[-0.18744035  0.14620352  0.17863178  0.467875  ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 898 is [True, False, False, False, True, False]
Current timestep = 899. State = [[-0.15295862 -0.01574633]]. Action = [[-0.23545748  0.2232118   0.02402705 -0.3367762 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 899 is [True, False, False, False, True, False]
Current timestep = 900. State = [[-0.15495028 -0.01264926]]. Action = [[ 0.02132455  0.19378734 -0.19102137 -0.91684055]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 900 is [True, False, False, False, True, False]
Current timestep = 901. State = [[-0.15581225 -0.0088068 ]]. Action = [[-0.17102788 -0.10356459  0.17191768  0.22237563]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 901 is [True, False, False, False, True, False]
Current timestep = 902. State = [[-0.15753905 -0.00735593]]. Action = [[-0.14444266 -0.15142052 -0.16268186  0.88001156]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 902 is [True, False, False, False, True, False]
Current timestep = 903. State = [[-0.15941772 -0.00782079]]. Action = [[ 0.22336984  0.211353   -0.07085536  0.14315939]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 903 is [True, False, False, False, True, False]
Current timestep = 904. State = [[-0.1598245  -0.00657669]]. Action = [[ 0.18147981 -0.01066893  0.12675866 -0.8085218 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 904 is [True, False, False, False, True, False]
Current timestep = 905. State = [[-0.15984413 -0.00639395]]. Action = [[-0.07992251 -0.1985063   0.12500879 -0.5147697 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 905 is [True, False, False, False, True, False]
Current timestep = 906. State = [[-0.15980029 -0.00718181]]. Action = [[-0.02319211 -0.22390313 -0.11095747 -0.7812142 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 906 is [True, False, False, False, True, False]
Current timestep = 907. State = [[-0.15961717 -0.01003545]]. Action = [[-0.184484    0.16789538 -0.21256433 -0.34705758]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 907 is [True, False, False, False, True, False]
Current timestep = 908. State = [[-0.1604681  -0.00989282]]. Action = [[-0.03681096  0.22929078  0.12615958 -0.01816201]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 908 is [True, False, False, False, True, False]
Scene graph at timestep 908 is [True, False, False, False, True, False]
State prediction error at timestep 908 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 909. State = [[-0.16164553 -0.00796582]]. Action = [[-0.1080623  -0.22140318  0.16673583 -0.85481924]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 909 is [True, False, False, False, True, False]
Current timestep = 910. State = [[-0.16314468 -0.00932818]]. Action = [[ 0.172577   -0.17697218  0.11287194  0.75162387]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 910 is [True, False, False, False, True, False]
Current timestep = 911. State = [[-0.16340812 -0.01210872]]. Action = [[-0.1297095  -0.08703467 -0.22937496 -0.16833478]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 911 is [True, False, False, False, True, False]
Current timestep = 912. State = [[-0.16508107 -0.01506801]]. Action = [[-0.2291482   0.22197592  0.0860818  -0.8181928 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 912 is [True, False, False, False, True, False]
Current timestep = 913. State = [[-0.16811547 -0.01402153]]. Action = [[ 0.15145314 -0.15553857 -0.21487811 -0.36912382]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 913 is [True, False, False, False, True, False]
Current timestep = 914. State = [[-0.1689122  -0.01488432]]. Action = [[0.14214966 0.17404246 0.20927566 0.70052266]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 914 is [True, False, False, False, True, False]
Current timestep = 915. State = [[-0.16892697 -0.01472447]]. Action = [[-0.20787771  0.08069345 -0.23265903 -0.9647885 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 915 is [True, False, False, False, True, False]
Current timestep = 916. State = [[-0.16987795 -0.01303343]]. Action = [[-0.14238237  0.18286592 -0.11996821  0.05020964]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 916 is [True, False, False, False, True, False]
Current timestep = 917. State = [[-0.17176132 -0.00987854]]. Action = [[ 0.23003787  0.19730061 -0.17124087  0.7380769 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 917 is [True, False, False, False, True, False]
Current timestep = 918. State = [[-0.17278004 -0.0052838 ]]. Action = [[ 0.23895091  0.10063937 -0.06513074  0.90469015]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 918 is [True, False, False, False, True, False]
Current timestep = 919. State = [[-0.17320591 -0.00196473]]. Action = [[-0.09204394 -0.09676585 -0.05971804  0.85423183]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 919 is [True, False, False, False, True, False]
Current timestep = 920. State = [[-0.17336519 -0.00094164]]. Action = [[-0.0395349  -0.21450233  0.05696133  0.61403525]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 920 is [True, False, False, False, True, False]
Current timestep = 921. State = [[-0.17331198 -0.00182013]]. Action = [[-0.17209475  0.07822591 -0.15452142 -0.8973546 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 921 is [True, False, False, False, True, False]
Current timestep = 922. State = [[-0.17342131 -0.0016688 ]]. Action = [[ 0.05916065 -0.17677009 -0.23877436 -0.97009706]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 922 is [True, False, False, False, True, False]
Current timestep = 923. State = [[-0.17352511 -0.00271373]]. Action = [[-0.22969888  0.05658376  0.1500724  -0.27844375]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 923 is [True, False, False, False, True, False]
Current timestep = 924. State = [[-0.17462344 -0.00296468]]. Action = [[-0.19902653  0.05194885  0.03045699 -0.6454035 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 924 is [True, False, False, False, True, False]
Current timestep = 925. State = [[-0.17706768 -0.00262377]]. Action = [[-0.18152982  0.15415317 -0.2352785   0.84986305]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 925 is [True, False, False, False, True, False]
Current timestep = 926. State = [[-0.18131757 -0.00086555]]. Action = [[ 0.18929648 -0.20517103 -0.06224181 -0.8203901 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 926 is [True, False, False, False, True, False]
Current timestep = 927. State = [[-0.18270704 -0.00167472]]. Action = [[-0.21544644  0.0738956   0.19668388  0.42384934]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 927 is [True, False, False, False, True, False]
Current timestep = 928. State = [[-0.18438514 -0.00172498]]. Action = [[-0.09425634 -0.17378105 -0.13559096 -0.06977254]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 928 is [True, False, False, False, True, False]
Current timestep = 929. State = [[-0.18618834 -0.00347577]]. Action = [[-0.0340009  -0.03780663 -0.07614492  0.8541379 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 929 is [True, False, False, False, True, False]
Scene graph at timestep 929 is [True, False, False, False, True, False]
State prediction error at timestep 929 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 930. State = [[-0.18791169 -0.00530436]]. Action = [[-0.00157706 -0.14311862  0.17570686 -0.80846894]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 930 is [True, False, False, False, True, False]
Human Feedback received at timestep 930 of -1
Current timestep = 931. State = [[-0.18912917 -0.00833615]]. Action = [[ 0.0940333  -0.04999024 -0.10185242  0.1479156 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 931 is [True, False, False, False, True, False]
Current timestep = 932. State = [[-0.18986055 -0.01116276]]. Action = [[ 0.10023716  0.0049409  -0.07250836 -0.6197514 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 932 is [True, False, False, False, True, False]
Scene graph at timestep 932 is [True, False, False, False, True, False]
State prediction error at timestep 932 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 932 of -1
Current timestep = 933. State = [[-0.19011965 -0.01269109]]. Action = [[-0.10138857  0.2283527   0.14564496  0.12419999]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 933 is [True, False, False, False, True, False]
Scene graph at timestep 933 is [True, False, False, False, True, False]
State prediction error at timestep 933 is tensor(5.8932e-05, grad_fn=<MseLossBackward0>)
Current timestep = 934. State = [[-0.19073203 -0.01111229]]. Action = [[-0.1100232   0.15815026 -0.21241103 -0.23646331]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 934 is [True, False, False, False, True, False]
Current timestep = 935. State = [[-0.19213402 -0.00871087]]. Action = [[-0.04160491 -0.19052789 -0.12000126  0.46268582]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 935 is [True, False, False, False, True, False]
Current timestep = 936. State = [[-0.19341306 -0.00889237]]. Action = [[ 0.21471459 -0.09603497  0.19346666 -0.32742584]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 936 is [True, False, False, False, True, False]
Current timestep = 937. State = [[-0.19342113 -0.00983145]]. Action = [[ 0.18300673 -0.06780529  0.17988044  0.05908132]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 937 is [True, False, False, False, True, False]
Current timestep = 938. State = [[-0.19312271 -0.01089701]]. Action = [[ 0.23253459 -0.07889616  0.07740906 -0.08751678]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 938 is [True, False, False, False, True, False]
Current timestep = 939. State = [[-0.19203097 -0.01228135]]. Action = [[ 0.11802197  0.11278588 -0.17441219  0.1691643 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 939 is [True, False, False, False, True, False]
Current timestep = 940. State = [[-0.19056404 -0.01218084]]. Action = [[ 0.1371051  -0.17714077  0.09077471 -0.32914066]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 940 is [True, False, False, False, True, False]
Current timestep = 941. State = [[-0.18865788 -0.01430071]]. Action = [[ 0.10713342 -0.18857092 -0.03213552  0.7357774 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 941 is [True, False, False, False, True, False]
Current timestep = 942. State = [[-0.18597779 -0.01853845]]. Action = [[-0.08239909 -0.10016608 -0.21635719 -0.73713714]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 942 is [True, False, False, False, True, False]
Current timestep = 943. State = [[-0.1845353  -0.02239215]]. Action = [[-0.23602806 -0.03332879 -0.16183096 -0.4455148 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 943 is [True, False, False, False, True, False]
Current timestep = 944. State = [[-0.18413916 -0.02472358]]. Action = [[-0.06436858  0.14249241  0.10337842  0.40772796]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 944 is [True, False, False, False, True, False]
Current timestep = 945. State = [[-0.1842044 -0.0243917]]. Action = [[-0.02851897  0.05560529 -0.08830395 -0.01081097]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 945 is [True, False, False, False, True, False]
Scene graph at timestep 945 is [True, False, False, False, True, False]
State prediction error at timestep 945 is tensor(8.3354e-05, grad_fn=<MseLossBackward0>)
Current timestep = 946. State = [[-0.18430513 -0.0245164 ]]. Action = [[-0.13869001 -0.18337576  0.09809318  0.8626864 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 946 is [True, False, False, False, True, False]
Current timestep = 947. State = [[-0.1846322  -0.02611394]]. Action = [[-0.24281123 -0.09502384  0.21481955 -0.4307211 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 947 is [True, False, False, False, True, False]
Scene graph at timestep 947 is [True, False, False, False, True, False]
State prediction error at timestep 947 is tensor(9.5227e-05, grad_fn=<MseLossBackward0>)
Current timestep = 948. State = [[-0.18758044 -0.02877466]]. Action = [[-0.19431134 -0.15811574  0.09787607 -0.06286073]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 948 is [True, False, False, False, True, False]
Current timestep = 949. State = [[-0.19161348 -0.03265106]]. Action = [[ 0.15083635 -0.15913628  0.23519444 -0.82761914]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 949 is [True, False, False, False, True, False]
Current timestep = 950. State = [[-0.193677   -0.03780411]]. Action = [[-0.08005948 -0.18900855 -0.1715241  -0.08622617]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 950 is [True, False, False, False, True, False]
Current timestep = 951. State = [[-0.19549228 -0.04263017]]. Action = [[-0.20902516  0.06485754 -0.05382054 -0.05111992]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 951 is [True, False, False, False, True, False]
Current timestep = 952. State = [[-0.198483   -0.04527216]]. Action = [[-0.17841266 -0.0378544   0.23040098  0.6499245 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 952 is [True, False, False, False, True, False]
Current timestep = 953. State = [[-0.20293914 -0.04762404]]. Action = [[-0.18998605 -0.0971808  -0.13458423  0.40531373]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 953 is [True, False, False, False, True, False]
Current timestep = 954. State = [[-0.20783968 -0.05096413]]. Action = [[ 0.02679047 -0.16086483 -0.12109393  0.5944009 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 954 is [True, False, False, False, True, False]
Current timestep = 955. State = [[-0.21032627 -0.05537247]]. Action = [[ 0.1075851  -0.05657901 -0.0733626   0.38276613]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 955 is [True, False, False, False, True, False]
Current timestep = 956. State = [[-0.21107654 -0.05796904]]. Action = [[-0.22425069  0.08484542  0.11635554  0.86033666]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 956 is [True, False, False, False, True, False]
Scene graph at timestep 956 is [True, False, False, False, True, False]
State prediction error at timestep 956 is tensor(5.8070e-05, grad_fn=<MseLossBackward0>)
Current timestep = 957. State = [[-0.21264511 -0.05908984]]. Action = [[-0.18795066  0.21634176  0.03306821  0.9510648 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 957 is [True, False, False, False, True, False]
Current timestep = 958. State = [[-0.21550807 -0.0576907 ]]. Action = [[ 0.15847331 -0.09148589 -0.00710005 -0.277745  ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 958 is [True, False, False, False, True, False]
Current timestep = 959. State = [[-0.21652514 -0.05785604]]. Action = [[ 0.16828287 -0.11739573 -0.01689205  0.61866856]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 959 is [True, False, False, False, True, False]
Current timestep = 960. State = [[-0.2164773  -0.05925578]]. Action = [[ 0.04736012 -0.2115626   0.05889681 -0.3602631 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 960 is [True, False, False, False, True, False]
Human Feedback received at timestep 960 of -1
Current timestep = 961. State = [[-0.21629499 -0.06214951]]. Action = [[-0.11635092  0.21072137 -0.12090704  0.0709151 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 961 is [True, False, False, False, True, False]
Current timestep = 962. State = [[-0.21728486 -0.06220914]]. Action = [[ 0.10519558  0.17039111 -0.13851355 -0.6113815 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 962 is [True, False, False, False, True, False]
Current timestep = 963. State = [[-0.21741962 -0.06114923]]. Action = [[ 0.21238095 -0.03933245  0.20673913 -0.91903186]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 963 is [True, False, False, False, True, False]
Human Feedback received at timestep 963 of -1
Current timestep = 964. State = [[-0.21703842 -0.06133442]]. Action = [[-0.20923604 -0.21009536 -0.07238179  0.49244642]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 964 is [True, False, False, False, True, False]
Scene graph at timestep 964 is [True, False, False, False, True, False]
State prediction error at timestep 964 is tensor(6.7629e-06, grad_fn=<MseLossBackward0>)
Current timestep = 965. State = [[-0.21712154 -0.06222008]]. Action = [[ 0.10699299  0.08866858 -0.19095266  0.6321497 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 965 is [True, False, False, False, True, False]
Scene graph at timestep 965 is [True, False, False, False, True, False]
State prediction error at timestep 965 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 966. State = [[-0.21690586 -0.06213855]]. Action = [[ 0.02914408  0.24129629  0.01429272 -0.38836372]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 966 is [True, False, False, False, True, False]
Scene graph at timestep 966 is [True, False, False, False, True, False]
State prediction error at timestep 966 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 967. State = [[-0.21694882 -0.0606593 ]]. Action = [[ 0.07039213 -0.17051582 -0.19039007  0.07770884]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 967 is [True, False, False, False, True, False]
Current timestep = 968. State = [[-0.21660565 -0.06057879]]. Action = [[ 0.13969499  0.00804359 -0.20724247  0.5043533 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 968 is [True, False, False, False, True, False]
Current timestep = 969. State = [[-0.21587048 -0.06066396]]. Action = [[-0.03968206 -0.19336866 -0.02357663 -0.7730076 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 969 is [True, False, False, False, True, False]
Current timestep = 970. State = [[-0.21494025 -0.06213266]]. Action = [[ 0.00719038 -0.08091226  0.09147367 -0.17901349]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 970 is [True, False, False, False, True, False]
Scene graph at timestep 970 is [True, False, False, False, True, False]
State prediction error at timestep 970 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 971. State = [[-0.21441296 -0.06364717]]. Action = [[ 0.12278908  0.13339537  0.16177145 -0.3653118 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 971 is [True, False, False, False, True, False]
Scene graph at timestep 971 is [True, False, False, False, True, False]
State prediction error at timestep 971 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 972. State = [[-0.2138023  -0.06364329]]. Action = [[ 0.04024392  0.02276039 -0.22559106  0.1295569 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 972 is [True, False, False, False, True, False]
Current timestep = 973. State = [[-0.21317476 -0.06354553]]. Action = [[ 0.04254347 -0.19902025  0.05717725 -0.45299935]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 973 is [True, False, False, False, True, False]
Current timestep = 974. State = [[-0.21146007 -0.06489951]]. Action = [[-0.09399469  0.19460732 -0.16991547  0.93638706]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 974 is [True, False, False, False, True, False]
Current timestep = 975. State = [[-0.2111716  -0.06457767]]. Action = [[-0.2418177   0.18661171  0.08496398  0.7109904 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 975 is [True, False, False, False, True, False]
Current timestep = 976. State = [[-0.21163519 -0.06299841]]. Action = [[-0.2192956  -0.08985056 -0.1784613   0.22991967]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 976 is [True, False, False, False, True, False]
Scene graph at timestep 976 is [True, False, False, False, True, False]
State prediction error at timestep 976 is tensor(3.6212e-05, grad_fn=<MseLossBackward0>)
Current timestep = 977. State = [[-0.21321967 -0.06269362]]. Action = [[ 0.02660784  0.23116216 -0.09397769 -0.8406223 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 977 is [True, False, False, False, True, False]
Current timestep = 978. State = [[-0.21444432 -0.05989904]]. Action = [[ 0.19065449  0.07252976 -0.10257266 -0.5614084 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 978 is [True, False, False, False, True, False]
Current timestep = 979. State = [[-0.21487276 -0.05791778]]. Action = [[-0.10320298 -0.21413994 -0.10182354 -0.23241657]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 979 is [True, False, False, False, True, False]
Current timestep = 980. State = [[-0.21497302 -0.05841256]]. Action = [[ 0.08191913 -0.21782051  0.2366561   0.9155476 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 980 is [True, False, False, False, True, False]
Scene graph at timestep 980 is [True, False, False, False, True, False]
State prediction error at timestep 980 is tensor(1.9488e-05, grad_fn=<MseLossBackward0>)
Current timestep = 981. State = [[-0.21469995 -0.06008698]]. Action = [[-0.18799004  0.12839505 -0.23881893 -0.7555443 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 981 is [True, False, False, False, True, False]
Current timestep = 982. State = [[-0.2157279  -0.06018402]]. Action = [[-0.19255434  0.14542723 -0.22088197 -0.72154295]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 982 is [True, False, False, False, True, False]
Scene graph at timestep 982 is [True, False, False, False, True, False]
State prediction error at timestep 982 is tensor(7.2679e-05, grad_fn=<MseLossBackward0>)
Current timestep = 983. State = [[-0.21747932 -0.05900327]]. Action = [[-0.02162698 -0.18841356  0.05049899 -0.8160996 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 983 is [True, False, False, False, True, False]
Current timestep = 984. State = [[-0.21900965 -0.06004348]]. Action = [[-0.0303188  -0.23124956 -0.17649734  0.6265793 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 984 is [True, False, False, False, True, False]
Current timestep = 985. State = [[-0.22030011 -0.06313951]]. Action = [[ 0.19522667  0.05663577 -0.01568498 -0.7308554 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 985 is [True, False, False, False, True, False]
Current timestep = 986. State = [[-0.22029279 -0.06372489]]. Action = [[ 0.00808397  0.2177453   0.24417865 -0.42637014]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 986 is [True, False, False, False, True, False]
Current timestep = 987. State = [[-0.22029953 -0.06299601]]. Action = [[ 0.05198401  0.21774209  0.18488324 -0.0827651 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 987 is [True, False, False, False, True, False]
Current timestep = 988. State = [[-0.2204059  -0.06031144]]. Action = [[ 0.2079404  -0.2143398  -0.00782686  0.13718462]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 988 is [True, False, False, False, True, False]
Current timestep = 989. State = [[-0.22003658 -0.06059079]]. Action = [[-0.1235629  -0.05272697 -0.15436004  0.2544477 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 989 is [True, False, False, False, True, False]
Scene graph at timestep 989 is [True, False, False, False, True, False]
State prediction error at timestep 989 is tensor(6.6404e-05, grad_fn=<MseLossBackward0>)
Current timestep = 990. State = [[-0.21987316 -0.06080441]]. Action = [[-0.14832045  0.04729694  0.12211859 -0.5329898 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 990 is [True, False, False, False, True, False]
Current timestep = 991. State = [[-0.2199254  -0.06085682]]. Action = [[-0.17306337 -0.06543344 -0.07994571  0.61572266]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 991 is [True, False, False, False, True, False]
Scene graph at timestep 991 is [True, False, False, False, True, False]
State prediction error at timestep 991 is tensor(1.8110e-05, grad_fn=<MseLossBackward0>)
Current timestep = 992. State = [[-0.22124302 -0.06109587]]. Action = [[-0.23291753  0.21196026  0.03522217  0.5697974 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 992 is [True, False, False, False, True, False]
Current timestep = 993. State = [[-0.22403347 -0.0597977 ]]. Action = [[ 0.19323194 -0.08134668  0.12296888  0.7398863 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 993 is [True, False, False, False, True, False]
Current timestep = 994. State = [[-0.22495382 -0.05985922]]. Action = [[0.05962542 0.15747356 0.11246938 0.4564644 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 994 is [True, False, False, False, True, False]
Current timestep = 995. State = [[-0.22508685 -0.05802426]]. Action = [[-0.07553387  0.24492148  0.2318548  -0.84693587]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 995 is [True, False, False, False, True, False]
Scene graph at timestep 995 is [True, False, False, False, True, False]
State prediction error at timestep 995 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 996. State = [[-0.2260044  -0.05360094]]. Action = [[-0.20320487  0.17379498  0.02829862 -0.2156384 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 996 is [True, False, False, False, True, False]
Current timestep = 997. State = [[-0.2280096  -0.04799294]]. Action = [[ 0.06713939  0.10263503  0.06648609 -0.66681784]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 997 is [True, False, False, False, True, False]
Current timestep = 998. State = [[-0.22892044 -0.04335607]]. Action = [[ 0.23679537 -0.23197164  0.01430577  0.52013254]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 998 is [True, False, False, False, True, False]
Human Feedback received at timestep 998 of -1
Current timestep = 999. State = [[-0.22863774 -0.04316015]]. Action = [[ 0.13716862 -0.07818288 -0.14681257 -0.44738317]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 999 is [True, False, False, False, True, False]
Current timestep = 1000. State = [[-0.22813131 -0.04356857]]. Action = [[ 0.14947301  0.13288972 -0.02863272 -0.5131675 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 1000 is [True, False, False, False, True, False]
Scene graph at timestep 1000 is [True, False, False, False, True, False]
State prediction error at timestep 1000 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1001. State = [[-0.22709695 -0.04316613]]. Action = [[ 0.2128442  -0.18570796  0.03512964  0.25009096]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 1001 is [True, False, False, False, True, False]
Current timestep = 1002. State = [[-0.225097   -0.04391424]]. Action = [[-0.00115031  0.1853441   0.20244214  0.32818472]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 1002 is [True, False, False, False, True, False]
Current timestep = 1003. State = [[-0.22853275  0.0848122 ]]. Action = [[ 0.04042256  0.14543372 -0.128085    0.15129256]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 1003 is [True, False, False, False, True, False]
Human Feedback received at timestep 1003 of -1
Current timestep = 1004. State = [[-0.22044952  0.09322835]]. Action = [[ 0.15977967  0.09824827 -0.02350454  0.80761003]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1004 is [True, False, False, False, True, False]
Current timestep = 1005. State = [[-0.21918415  0.09359829]]. Action = [[ 0.10652691 -0.22243263  0.16398406 -0.39821947]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1005 is [True, False, False, False, True, False]
Current timestep = 1006. State = [[-0.21734573  0.09255284]]. Action = [[ 0.23890746 -0.20570731  0.04110584  0.6870575 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1006 is [True, False, False, False, True, False]
Scene graph at timestep 1006 is [True, False, False, False, True, False]
State prediction error at timestep 1006 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1007. State = [[-0.21321933  0.08970937]]. Action = [[ 0.19900608 -0.19789326 -0.15892766 -0.04590046]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1007 is [True, False, False, False, True, False]
Scene graph at timestep 1007 is [True, False, False, False, True, False]
State prediction error at timestep 1007 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1008. State = [[-0.20709634  0.08585431]]. Action = [[0.03899205 0.07561871 0.20458311 0.58233917]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1008 is [True, False, False, False, True, False]
Current timestep = 1009. State = [[-0.20330445  0.08465766]]. Action = [[ 0.02369049 -0.23368236 -0.0837072   0.53015566]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1009 is [True, False, False, False, True, False]
Current timestep = 1010. State = [[-0.19982214  0.0811391 ]]. Action = [[ 0.12492171  0.00543633  0.21013513 -0.07055992]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1010 is [True, False, False, False, True, False]
Current timestep = 1011. State = [[-0.19665183  0.07881623]]. Action = [[-0.11730853  0.03677675 -0.23690622  0.44549978]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1011 is [True, False, False, False, True, False]
Current timestep = 1012. State = [[-0.19466272  0.07785751]]. Action = [[ 0.1552732   0.04690105  0.10458001 -0.43990612]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1012 is [True, False, False, False, True, False]
Current timestep = 1013. State = [[-0.1916363   0.07780705]]. Action = [[0.19979358 0.01312628 0.14648181 0.331159  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1013 is [True, False, False, False, True, False]
Scene graph at timestep 1013 is [True, False, False, False, True, False]
State prediction error at timestep 1013 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1014. State = [[-0.1885832   0.07788388]]. Action = [[-0.06743583 -0.00462016  0.05436343  0.8031665 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1014 is [True, False, False, False, True, False]
Current timestep = 1015. State = [[-0.1873764   0.07763764]]. Action = [[-0.16293406 -0.14083566  0.20133704 -0.69130206]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1015 is [True, False, False, False, True, False]
Scene graph at timestep 1015 is [True, False, False, False, True, False]
State prediction error at timestep 1015 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Current timestep = 1016. State = [[-0.1871947  0.076045 ]]. Action = [[ 1.3092160e-04 -6.0167342e-02  6.6176474e-02 -2.2623074e-01]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1016 is [True, False, False, False, True, False]
Scene graph at timestep 1016 is [True, False, False, False, True, False]
State prediction error at timestep 1016 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[-0.18731323  0.07421685]]. Action = [[ 0.08586833  0.24147251 -0.03614707 -0.59547657]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1017 is [True, False, False, False, True, False]
Current timestep = 1018. State = [[-0.1874804   0.07502782]]. Action = [[0.04800421 0.191805   0.22081223 0.9491863 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1018 is [True, False, False, False, True, False]
Scene graph at timestep 1018 is [True, False, False, False, True, False]
State prediction error at timestep 1018 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1019. State = [[-0.18742567  0.07716291]]. Action = [[ 0.01713985 -0.02835973 -0.23418085 -0.33783025]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1019 is [True, False, False, False, True, False]
Current timestep = 1020. State = [[-0.18671691  0.07824571]]. Action = [[-0.13086104  0.22211531 -0.21216618 -0.8210778 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1020 is [True, False, False, False, True, False]
Scene graph at timestep 1020 is [True, False, False, False, True, False]
State prediction error at timestep 1020 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Current timestep = 1021. State = [[-0.18782087  0.08192091]]. Action = [[-0.08314273  0.00388426 -0.01394078  0.1650728 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1021 is [True, False, False, False, True, False]
Scene graph at timestep 1021 is [True, False, False, False, True, False]
State prediction error at timestep 1021 is tensor(5.2970e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1021 of 1
Current timestep = 1022. State = [[-0.18890247  0.08448049]]. Action = [[-0.0756906   0.13482904 -0.17476185 -0.25786197]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1022 is [True, False, False, False, True, False]
Current timestep = 1023. State = [[-0.19058627  0.08805673]]. Action = [[-0.21683574  0.15113801  0.02102655  0.83615756]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1023 is [True, False, False, False, True, False]
Current timestep = 1024. State = [[-0.19272384  0.09252667]]. Action = [[-0.23049445  0.15867224 -0.20953338  0.47055387]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1024 is [True, False, False, False, True, False]
Scene graph at timestep 1024 is [True, False, False, False, True, False]
State prediction error at timestep 1024 is tensor(3.1127e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1025. State = [[-0.19597942  0.09864885]]. Action = [[ 0.04562837  0.06130353  0.12959403 -0.66966313]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1025 is [True, False, False, False, True, False]
Current timestep = 1026. State = [[-0.19824825  0.10295248]]. Action = [[-0.2071111   0.1405456   0.08505079 -0.2303245 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1026 is [True, False, False, False, True, False]
Scene graph at timestep 1026 is [True, False, False, False, True, False]
State prediction error at timestep 1026 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1027. State = [[-0.20102026  0.10830705]]. Action = [[-0.10897556  0.02189514  0.19771734 -0.11098731]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1027 is [True, False, False, False, True, False]
Human Feedback received at timestep 1027 of -1
Current timestep = 1028. State = [[-0.20309836  0.11198329]]. Action = [[-0.06226008 -0.12462211 -0.17378537  0.93243074]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1028 is [True, False, False, False, True, False]
Scene graph at timestep 1028 is [True, False, False, False, True, False]
State prediction error at timestep 1028 is tensor(7.3854e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1029. State = [[-0.20435795  0.11303527]]. Action = [[-0.07996204  0.11111715 -0.13223718  0.8061657 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1029 is [True, False, False, False, True, False]
Scene graph at timestep 1029 is [True, False, False, False, True, False]
State prediction error at timestep 1029 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1030. State = [[-0.20587943  0.11517828]]. Action = [[ 0.13628465  0.00499406 -0.01583272 -0.36576462]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1030 is [True, False, False, False, True, False]
Current timestep = 1031. State = [[-0.20643137  0.11578184]]. Action = [[ 0.02333724 -0.18553433 -0.13962132 -0.91050714]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1031 is [True, False, False, False, True, False]
Scene graph at timestep 1031 is [True, False, False, False, True, False]
State prediction error at timestep 1031 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[-0.20613492  0.11408908]]. Action = [[ 0.20584849 -0.16567105 -0.063783   -0.6792302 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1032 is [True, False, False, False, True, False]
Current timestep = 1033. State = [[-0.2052782   0.11056445]]. Action = [[0.1845926  0.1084092  0.03910571 0.5759485 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1033 is [True, False, False, False, True, False]
Scene graph at timestep 1033 is [True, False, False, False, True, False]
State prediction error at timestep 1033 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1034. State = [[-0.20459211  0.10912083]]. Action = [[-0.11299904 -0.05033293  0.08637494 -0.04295534]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1034 is [True, False, False, False, True, False]
Scene graph at timestep 1034 is [True, False, False, False, True, False]
State prediction error at timestep 1034 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1035. State = [[-0.20461228  0.10785772]]. Action = [[-0.1277922   0.08807275  0.11573026  0.8644558 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1035 is [True, False, False, False, True, False]
Current timestep = 1036. State = [[-0.2050587   0.10831332]]. Action = [[-0.00394475 -0.17599596 -0.13203518  0.56221676]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1036 is [True, False, False, False, True, False]
Scene graph at timestep 1036 is [True, False, False, False, True, False]
State prediction error at timestep 1036 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1037. State = [[-0.20476767  0.10662559]]. Action = [[ 0.20928973  0.06935015 -0.08081231  0.7579087 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1037 is [True, False, False, False, True, False]
Current timestep = 1038. State = [[-0.20432808  0.10591064]]. Action = [[-0.14460516 -0.22868526 -0.19337036 -0.27533424]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1038 is [True, False, False, False, True, False]
Scene graph at timestep 1038 is [True, False, False, False, True, False]
State prediction error at timestep 1038 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 1039. State = [[-0.20384996  0.1031843 ]]. Action = [[ 0.12720555 -0.01801491  0.19255656 -0.7004366 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1039 is [True, False, False, False, True, False]
Scene graph at timestep 1039 is [True, False, False, False, True, False]
State prediction error at timestep 1039 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1040. State = [[-0.20330705  0.1010479 ]]. Action = [[-0.06488582 -0.07311416  0.14526996  0.9907596 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1040 is [True, False, False, False, True, False]
Current timestep = 1041. State = [[-0.20298427  0.09883774]]. Action = [[ 0.10181883 -0.20678382 -0.18333267  0.27095628]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1041 is [True, False, False, False, True, False]
Current timestep = 1042. State = [[-0.20203961  0.09466653]]. Action = [[-0.13459715  0.0345217  -0.1366067  -0.4986654 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1042 is [True, False, False, False, True, False]
Current timestep = 1043. State = [[-0.20199157  0.09264845]]. Action = [[-0.24752258  0.06451961  0.02695087  0.8549285 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1043 is [True, False, False, False, True, False]
Current timestep = 1044. State = [[-0.20269278  0.09226991]]. Action = [[-0.15797277 -0.16094917 -0.19951059  0.01861477]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1044 is [True, False, False, False, True, False]
Current timestep = 1045. State = [[-0.20388319  0.09013957]]. Action = [[-0.13390599 -0.21826173  0.1540916   0.59672976]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1045 is [True, False, False, False, True, False]
Current timestep = 1046. State = [[-0.20607057  0.08593626]]. Action = [[ 0.20594427  0.16186386 -0.13977218  0.58060265]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1046 is [True, False, False, False, True, False]
Scene graph at timestep 1046 is [True, False, False, False, True, False]
State prediction error at timestep 1046 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1047. State = [[-0.20633227  0.08479966]]. Action = [[-0.08956027  0.167709    0.08665586 -0.2790236 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1047 is [True, False, False, False, True, False]
Current timestep = 1048. State = [[-0.2074205   0.08621427]]. Action = [[-0.21939507 -0.18255106 -0.18221483 -0.93098384]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1048 is [True, False, False, False, True, False]
Current timestep = 1049. State = [[-0.20937888  0.08459936]]. Action = [[-0.1448435  -0.21295851  0.08605394 -0.09196192]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1049 is [True, False, False, False, True, False]
Scene graph at timestep 1049 is [True, False, False, False, True, False]
State prediction error at timestep 1049 is tensor(7.0200e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1050. State = [[-0.21182437  0.08124287]]. Action = [[-0.18686913 -0.03369567 -0.17708322 -0.00400877]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1050 is [True, False, False, False, True, False]
Current timestep = 1051. State = [[-0.21467523  0.07929668]]. Action = [[-0.0038231   0.21439886  0.0444155   0.72116756]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1051 is [True, False, False, False, True, False]
Current timestep = 1052. State = [[-0.21710084  0.08098632]]. Action = [[0.21838081 0.18826094 0.05202579 0.79269457]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1052 is [True, False, False, False, True, False]
Current timestep = 1053. State = [[-0.21827981  0.08359979]]. Action = [[ 0.1565924  -0.18595918  0.1892258   0.75190544]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1053 is [True, False, False, False, True, False]
Current timestep = 1054. State = [[-0.21818721  0.08302938]]. Action = [[0.08222431 0.04364377 0.09864378 0.6185839 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1054 is [True, False, False, False, True, False]
Current timestep = 1055. State = [[-0.21794714  0.0826515 ]]. Action = [[ 0.15797573 -0.19519097  0.21744505  0.6591852 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1055 is [True, False, False, False, True, False]
Scene graph at timestep 1055 is [True, False, False, False, True, False]
State prediction error at timestep 1055 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1056. State = [[-0.21694788  0.08037368]]. Action = [[ 0.17250976 -0.01337299 -0.15329924  0.34784102]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1056 is [True, False, False, False, True, False]
Current timestep = 1057. State = [[-0.2150605   0.07864435]]. Action = [[ 0.06856173 -0.12990785  0.02294272 -0.42695403]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1057 is [True, False, False, False, True, False]
Current timestep = 1058. State = [[-0.21350126  0.07598138]]. Action = [[-0.14434177 -0.14143674 -0.02538553  0.05185926]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1058 is [True, False, False, False, True, False]
Current timestep = 1059. State = [[-0.21288389  0.07265189]]. Action = [[ 0.06124699 -0.10571057 -0.03967689 -0.7321717 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1059 is [True, False, False, False, True, False]
Current timestep = 1060. State = [[-0.21263148  0.06936156]]. Action = [[ 0.02217492  0.03081694 -0.22950622  0.26788366]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1060 is [True, False, False, False, True, False]
Current timestep = 1061. State = [[-0.21231939  0.06784973]]. Action = [[-0.07102916  0.09644562 -0.24732551  0.82621694]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1061 is [True, False, False, False, True, False]
Scene graph at timestep 1061 is [True, False, False, False, True, False]
State prediction error at timestep 1061 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1062. State = [[-0.21243054  0.06781103]]. Action = [[-0.24571656  0.03909236  0.0074622   0.04261363]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1062 is [True, False, False, False, True, False]
Scene graph at timestep 1062 is [True, False, False, False, True, False]
State prediction error at timestep 1062 is tensor(1.7088e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1063. State = [[-0.21278888  0.06826887]]. Action = [[-0.0322137   0.2368601  -0.23880094 -0.04662764]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1063 is [True, False, False, False, True, False]
Current timestep = 1064. State = [[-0.21386504  0.07077377]]. Action = [[ 0.04342282 -0.07326481  0.10617518 -0.01476014]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1064 is [True, False, False, False, True, False]
Scene graph at timestep 1064 is [True, False, False, False, True, False]
State prediction error at timestep 1064 is tensor(1.5952e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1065. State = [[-0.21399571  0.07094491]]. Action = [[-0.0277788  -0.07499787  0.09955591 -0.10091078]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1065 is [True, False, False, False, True, False]
Current timestep = 1066. State = [[-0.21405937  0.07115564]]. Action = [[-0.19403666  0.20004791  0.05733851  0.4212104 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1066 is [True, False, False, False, True, False]
Current timestep = 1067. State = [[-0.21528769  0.0737403 ]]. Action = [[ 0.00137123  0.03027675 -0.18690196  0.18503022]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1067 is [True, False, False, False, True, False]
Current timestep = 1068. State = [[-0.21647009  0.07597005]]. Action = [[-0.22178975  0.04398745 -0.1392861  -0.58692616]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1068 is [True, False, False, False, True, False]
Current timestep = 1069. State = [[-0.21868593  0.07785673]]. Action = [[ 0.20769387 -0.24172568  0.17534202 -0.648733  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1069 is [True, False, False, False, True, False]
Current timestep = 1070. State = [[-0.21879923  0.076267  ]]. Action = [[ 0.06132552  0.04848984 -0.1561614  -0.49146986]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1070 is [True, False, False, False, True, False]
Human Feedback received at timestep 1070 of 1
Current timestep = 1071. State = [[-0.21884392  0.07582643]]. Action = [[-0.1960234   0.20853418 -0.00532654 -0.65207165]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1071 is [True, False, False, False, True, False]
Current timestep = 1072. State = [[-0.21990484  0.07804172]]. Action = [[ 0.01546749 -0.03494082 -0.21638891  0.7656001 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1072 is [True, False, False, False, True, False]
Current timestep = 1073. State = [[-0.22048678  0.07882734]]. Action = [[-0.19921258 -0.15403886 -0.10719267  0.24304008]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1073 is [True, False, False, False, True, False]
Scene graph at timestep 1073 is [True, False, False, False, True, False]
State prediction error at timestep 1073 is tensor(5.0061e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1074. State = [[-0.22193259  0.07766795]]. Action = [[-0.15129474 -0.06281294  0.15523148  0.53417265]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1074 is [True, False, False, False, True, False]
Current timestep = 1075. State = [[-0.2237067   0.07662788]]. Action = [[ 0.08211768  0.14702168 -0.11199857 -0.80376875]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1075 is [True, False, False, False, True, False]
Current timestep = 1076. State = [[-0.22509418  0.07750064]]. Action = [[ 0.21078533 -0.0098636  -0.13964325 -0.21655053]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1076 is [True, False, False, False, True, False]
Current timestep = 1077. State = [[-0.22519816  0.07753003]]. Action = [[-0.22881733 -0.08754379  0.22640207 -0.5820318 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1077 is [True, False, False, False, True, False]
Current timestep = 1078. State = [[-0.22586049  0.07676862]]. Action = [[ 0.1308209  -0.23757462 -0.14605933 -0.27019203]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1078 is [True, False, False, False, True, False]
Current timestep = 1079. State = [[-0.22547306  0.07376893]]. Action = [[ 0.19730294 -0.18015032 -0.14423786  0.8897958 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1079 is [True, False, False, False, True, False]
Scene graph at timestep 1079 is [True, False, False, False, True, False]
State prediction error at timestep 1079 is tensor(7.3600e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1080. State = [[-0.22458565  0.06919385]]. Action = [[ 0.12509906  0.13986987 -0.05111459  0.29065478]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1080 is [True, False, False, False, True, False]
Current timestep = 1081. State = [[-0.22391483  0.06784332]]. Action = [[ 0.21735772 -0.04924707  0.22710973 -0.6360591 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1081 is [True, False, False, False, True, False]
Scene graph at timestep 1081 is [True, False, False, False, True, False]
State prediction error at timestep 1081 is tensor(4.4194e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1082. State = [[-0.22276488  0.06680355]]. Action = [[-0.17396489  0.19765759  0.08124518  0.26122928]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1082 is [True, False, False, False, True, False]
Current timestep = 1083. State = [[-0.2228848   0.06757376]]. Action = [[ 0.20190251 -0.22480737 -0.16797219  0.8933902 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1083 is [True, False, False, False, True, False]
Current timestep = 1084. State = [[-0.22164454  0.06600315]]. Action = [[ 0.00546086  0.01911861 -0.12966113  0.5685129 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1084 is [True, False, False, False, True, False]
Current timestep = 1085. State = [[-0.22089523  0.06556738]]. Action = [[ 0.03555107  0.12994742 -0.23816733 -0.4409209 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1085 is [True, False, False, False, True, False]
Scene graph at timestep 1085 is [True, False, False, False, True, False]
State prediction error at timestep 1085 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1086. State = [[-0.22022912  0.06610157]]. Action = [[0.07257748 0.10455796 0.12198997 0.75565815]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1086 is [True, False, False, False, True, False]
Current timestep = 1087. State = [[-0.21889526  0.06669649]]. Action = [[ 0.13229507 -0.21947858  0.13616177  0.90978074]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1087 is [True, False, False, False, True, False]
Current timestep = 1088. State = [[-0.21685287  0.06563982]]. Action = [[ 0.13305289 -0.07943222 -0.15639153  0.8456719 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1088 is [True, False, False, False, True, False]
Current timestep = 1089. State = [[-0.2147677   0.06426346]]. Action = [[-0.21780948  0.15627283 -0.20993832 -0.8950465 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1089 is [True, False, False, False, True, False]
Scene graph at timestep 1089 is [True, False, False, False, True, False]
State prediction error at timestep 1089 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1090. State = [[-0.21489365  0.06461218]]. Action = [[0.17554551 0.00115693 0.19208068 0.076002  ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1090 is [True, False, False, False, True, False]
Current timestep = 1091. State = [[-0.21425785  0.06477539]]. Action = [[-0.08852121 -0.17480187  0.2250393   0.03035522]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1091 is [True, False, False, False, True, False]
Scene graph at timestep 1091 is [True, False, False, False, True, False]
State prediction error at timestep 1091 is tensor(3.8201e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1092. State = [[-0.2136135   0.06319939]]. Action = [[ 0.20970434 -0.07638597  0.01982275  0.52324426]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1092 is [True, False, False, False, True, False]
Current timestep = 1093. State = [[-0.21175326  0.06128358]]. Action = [[-0.06444338 -0.1200839   0.06896034 -0.8336251 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1093 is [True, False, False, False, True, False]
Scene graph at timestep 1093 is [True, False, False, False, True, False]
State prediction error at timestep 1093 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Current timestep = 1094. State = [[-0.21060875  0.05870774]]. Action = [[-0.00504382  0.11655274  0.1452452  -0.97500527]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1094 is [True, False, False, False, True, False]
Scene graph at timestep 1094 is [True, False, False, False, True, False]
State prediction error at timestep 1094 is tensor(2.4794e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1095. State = [[-0.21044444  0.05857056]]. Action = [[-0.23739137 -0.0052601  -0.04992084 -0.4285915 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1095 is [True, False, False, False, True, False]
Current timestep = 1096. State = [[-0.2104658   0.05862154]]. Action = [[0.10522148 0.05313215 0.05917719 0.1970855 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1096 is [True, False, False, False, True, False]
Human Feedback received at timestep 1096 of 1
Current timestep = 1097. State = [[-0.21052615  0.05875741]]. Action = [[-0.18964325  0.21418864  0.24180043 -0.72443837]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1097 is [True, False, False, False, True, False]
Current timestep = 1098. State = [[-0.21131223  0.06093208]]. Action = [[0.16521639 0.10589331 0.21827036 0.6643208 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1098 is [True, False, False, False, True, False]
Current timestep = 1099. State = [[-0.21177243  0.06297798]]. Action = [[-0.08489072  0.16703731 -0.17858015  0.54277563]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1099 is [True, False, False, False, True, False]
Current timestep = 1100. State = [[-0.21273278  0.06621123]]. Action = [[-0.0056098  -0.18255809  0.10224301  0.19792914]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1100 is [True, False, False, False, True, False]
Current timestep = 1101. State = [[-0.21277432  0.06624869]]. Action = [[-0.24035993 -0.00765218  0.0735527   0.49324822]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1101 is [True, False, False, False, True, False]
Scene graph at timestep 1101 is [True, False, False, False, True, False]
State prediction error at timestep 1101 is tensor(4.6077e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1102. State = [[-0.21306305  0.06684157]]. Action = [[-0.012824   -0.01041907 -0.12223932 -0.5886192 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1102 is [True, False, False, False, True, False]
Current timestep = 1103. State = [[-0.21325912  0.06727054]]. Action = [[-0.08764488  0.0044367  -0.18858683 -0.7950752 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1103 is [True, False, False, False, True, False]
Current timestep = 1104. State = [[-0.21370576  0.06752376]]. Action = [[ 0.24137717 -0.13071284 -0.24298732  0.7014351 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1104 is [True, False, False, False, True, False]
Current timestep = 1105. State = [[-0.21356344  0.06666417]]. Action = [[-0.189707   -0.21222973  0.23615468 -0.9923791 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1105 is [True, False, False, False, True, False]
Current timestep = 1106. State = [[-0.2137367   0.06391256]]. Action = [[ 0.10020986  0.05868721 -0.15254337 -0.8765039 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1106 is [True, False, False, False, True, False]
Current timestep = 1107. State = [[-0.21356937  0.06288231]]. Action = [[-0.16550417  0.02213731  0.11313394  0.8834977 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1107 is [True, False, False, False, True, False]
Current timestep = 1108. State = [[-0.214225    0.06304009]]. Action = [[-0.2097711   0.2033993  -0.00257891 -0.8234824 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1108 is [True, False, False, False, True, False]
Current timestep = 1109. State = [[-0.21631765  0.06476033]]. Action = [[-0.11715102 -0.07358292 -0.04209396  0.6750705 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1109 is [True, False, False, False, True, False]
Current timestep = 1110. State = [[-0.21817741  0.06545694]]. Action = [[-0.13573056  0.20016706 -0.20235021  0.73480654]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1110 is [True, False, False, False, True, False]
Current timestep = 1111. State = [[-0.22121562  0.06846485]]. Action = [[ 0.11428586  0.05280074 -0.15005066 -0.36925197]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1111 is [True, False, False, False, True, False]
Scene graph at timestep 1111 is [True, False, False, False, True, False]
State prediction error at timestep 1111 is tensor(3.2070e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1112. State = [[-0.22258389  0.07045896]]. Action = [[ 0.22795525 -0.17835593  0.07585737 -0.3227185 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1112 is [True, False, False, False, True, False]
Current timestep = 1113. State = [[-0.22248833  0.07003859]]. Action = [[-0.0659966   0.23997232 -0.00355422 -0.38163614]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1113 is [True, False, False, False, True, False]
Scene graph at timestep 1113 is [True, False, False, False, True, False]
State prediction error at timestep 1113 is tensor(6.1741e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1114. State = [[-0.2233      0.07167339]]. Action = [[ 0.08164683 -0.08616248  0.03967237 -0.18916315]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1114 is [True, False, False, False, True, False]
Scene graph at timestep 1114 is [True, False, False, False, True, False]
State prediction error at timestep 1114 is tensor(9.1400e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1115. State = [[-0.22334029  0.07174237]]. Action = [[-0.09009436  0.2347737   0.11955211  0.06402206]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1115 is [True, False, False, False, True, False]
Current timestep = 1116. State = [[-0.22460996  0.07450625]]. Action = [[-0.1355198   0.11573678 -0.1123212  -0.5859284 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1116 is [True, False, False, False, True, False]
Current timestep = 1117. State = [[-0.22618844  0.07815413]]. Action = [[ 0.01994109 -0.03790146  0.23183948  0.7794156 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1117 is [True, False, False, False, True, False]
Current timestep = 1118. State = [[-0.22703381  0.07994039]]. Action = [[-0.17660636 -0.08952551  0.16689879  0.10394156]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1118 is [True, False, False, False, True, False]
Current timestep = 1119. State = [[-0.22813952  0.0803838 ]]. Action = [[ 0.06589782 -0.20855358  0.23000097 -0.09019005]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1119 is [True, False, False, False, True, False]
Current timestep = 1120. State = [[-0.2282346   0.07885588]]. Action = [[-0.2246428   0.15633386  0.10724455 -0.57646227]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1120 is [True, False, False, False, True, False]
Current timestep = 1121. State = [[-0.23049852  0.07971874]]. Action = [[-0.09799871  0.07477242 -0.16625124 -0.6268368 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1121 is [True, False, False, False, True, False]
Current timestep = 1122. State = [[-0.23291731  0.08121826]]. Action = [[-0.05630767  0.03725135  0.22317612 -0.8076123 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1122 is [True, False, False, False, True, False]
Current timestep = 1123. State = [[-0.23538788  0.08225974]]. Action = [[-0.17610517 -0.18460038 -0.0308864   0.1952138 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1123 is [True, False, False, False, True, False]
Current timestep = 1124. State = [[-0.2391002  0.0806243]]. Action = [[-0.20330082 -0.06126872 -0.11439259 -0.46429306]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1124 is [True, False, False, False, True, False]
Current timestep = 1125. State = [[-0.24370141  0.07877091]]. Action = [[ 0.07947195 -0.09221627 -0.09726593 -0.25338584]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1125 is [True, False, False, False, True, False]
Scene graph at timestep 1125 is [True, False, False, False, True, False]
State prediction error at timestep 1125 is tensor(1.0791e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1126. State = [[-0.24739692  0.07601916]]. Action = [[ 0.04344034 -0.13005131 -0.14420417  0.02955687]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1126 is [True, False, False, False, True, False]
Current timestep = 1127. State = [[-0.24883276  0.0730647 ]]. Action = [[ 0.24802136 -0.03740633 -0.01474124 -0.9470886 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1127 is [True, False, False, False, True, False]
Current timestep = 1128. State = [[-0.24849036  0.07055984]]. Action = [[ 0.14931267 -0.19109736  0.1405347   0.1164912 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1128 is [True, False, False, False, True, False]
Human Feedback received at timestep 1128 of -1
Current timestep = 1129. State = [[-0.24768062  0.066571  ]]. Action = [[ 0.05874237  0.03592589 -0.16080011  0.33620882]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1129 is [True, False, False, False, True, False]
Scene graph at timestep 1129 is [True, False, False, False, True, False]
State prediction error at timestep 1129 is tensor(8.3929e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1130. State = [[-0.2471349  0.0641884]]. Action = [[-0.05544332 -0.19052443  0.13726345 -0.39787644]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1130 is [True, False, False, False, True, False]
Human Feedback received at timestep 1130 of -1
Current timestep = 1131. State = [[-0.24663618  0.06071763]]. Action = [[-0.04616037  0.14088866 -0.21783628 -0.5401212 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1131 is [True, False, False, False, True, False]
Current timestep = 1132. State = [[-0.2464402   0.06036532]]. Action = [[-0.03008614  0.0619137   0.1262402   0.38543367]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1132 is [True, False, False, False, True, False]
Current timestep = 1133. State = [[-0.24651188  0.06052218]]. Action = [[-0.06091097 -0.02718186  0.09216213  0.64551973]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1133 is [True, False, False, False, True, False]
Scene graph at timestep 1133 is [True, False, False, False, True, False]
State prediction error at timestep 1133 is tensor(3.1057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1134. State = [[-0.24655321  0.06052909]]. Action = [[ 0.21025133 -0.03746587  0.17619824 -0.92699546]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1134 is [True, False, False, False, True, False]
Current timestep = 1135. State = [[-0.24581937  0.06010214]]. Action = [[ 0.08250356 -0.18875085  0.11401197 -0.01228851]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1135 is [True, False, False, False, True, False]
Current timestep = 1136. State = [[-0.24456419  0.05789629]]. Action = [[ 0.21357885 -0.08949231  0.07069463  0.33971798]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1136 is [True, False, False, False, True, False]
Current timestep = 1137. State = [[-0.24260601  0.05507327]]. Action = [[-0.04472549  0.16190326  0.09517616  0.06821752]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1137 is [True, False, False, False, True, False]
Scene graph at timestep 1137 is [True, False, False, False, True, False]
State prediction error at timestep 1137 is tensor(2.8326e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1138. State = [[-0.24225701  0.05451717]]. Action = [[-0.10269085 -0.08562118  0.05732936 -0.67141575]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1138 is [True, False, False, False, True, False]
Current timestep = 1139. State = [[-0.24208425  0.05394318]]. Action = [[-0.09860441 -0.12243205 -0.0998551   0.5459006 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1139 is [True, False, False, False, True, False]
Scene graph at timestep 1139 is [True, False, False, False, True, False]
State prediction error at timestep 1139 is tensor(2.7913e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1140. State = [[-0.24183476  0.05186572]]. Action = [[-0.212805   -0.16831063 -0.08954842 -0.13726169]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1140 is [True, False, False, False, True, False]
Current timestep = 1141. State = [[-0.24252678  0.04810956]]. Action = [[-0.00238273 -0.05773193  0.17972326 -0.3398038 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1141 is [True, False, False, False, True, False]
Scene graph at timestep 1141 is [True, False, False, False, True, False]
State prediction error at timestep 1141 is tensor(2.9797e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1142. State = [[-0.2429017   0.04448153]]. Action = [[ 0.06883729 -0.12023565  0.11515328  0.36762154]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1142 is [True, False, False, False, True, False]
Current timestep = 1143. State = [[-0.24269997  0.04123519]]. Action = [[ 0.16370475 -0.21855195  0.08686942  0.06236601]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1143 is [True, False, False, False, True, False]
Current timestep = 1144. State = [[-0.24151662  0.03706956]]. Action = [[ 0.03916892  0.19181001 -0.09374139  0.97467136]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1144 is [True, False, False, False, True, False]
Current timestep = 1145. State = [[-0.24126747  0.03641086]]. Action = [[-0.09665066 -0.05862966 -0.13927059  0.78807473]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1145 is [True, False, False, False, True, False]
Current timestep = 1146. State = [[-0.24122024  0.03560023]]. Action = [[-0.16406728  0.1091947  -0.02243283 -0.32116282]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1146 is [True, False, False, False, True, False]
Current timestep = 1147. State = [[-0.241819    0.03537055]]. Action = [[ 0.16376537 -0.21916023 -0.21501458 -0.34334123]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1147 is [True, False, False, False, True, False]
Current timestep = 1148. State = [[-0.24128905  0.03298213]]. Action = [[ 0.20416766 -0.21651581 -0.21343859  0.3068545 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1148 is [True, False, False, False, True, False]
Scene graph at timestep 1148 is [True, False, False, False, True, False]
State prediction error at timestep 1148 is tensor(2.2929e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1149. State = [[-0.24001369  0.02806335]]. Action = [[ 0.08515018 -0.24873188  0.05897599  0.68558204]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1149 is [True, False, False, False, True, False]
Scene graph at timestep 1149 is [True, False, False, False, True, False]
State prediction error at timestep 1149 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1150. State = [[-0.23826914  0.02127394]]. Action = [[0.03682154 0.14949346 0.1211018  0.876961  ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1150 is [True, False, False, False, True, False]
Scene graph at timestep 1150 is [True, False, False, False, True, False]
State prediction error at timestep 1150 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1151. State = [[-0.23709755  0.01826871]]. Action = [[ 0.19434807 -0.22842208 -0.08764973 -0.5211693 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1151 is [True, False, False, False, True, False]
Current timestep = 1152. State = [[-0.23481472  0.01390362]]. Action = [[0.18935817 0.20822209 0.23978281 0.8017597 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1152 is [True, False, False, False, True, False]
Current timestep = 1153. State = [[-0.23223871  0.01331533]]. Action = [[-0.1273344   0.07586998  0.10375056  0.88434386]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1153 is [True, False, False, False, True, False]
Current timestep = 1154. State = [[-0.23141213  0.01374256]]. Action = [[ 0.24677914  0.1176866  -0.07632738 -0.7511966 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1154 is [True, False, False, False, True, False]
Current timestep = 1155. State = [[-0.22876379  0.01481912]]. Action = [[ 0.18867275  0.02084687 -0.2000499  -0.46196395]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1155 is [True, False, False, False, True, False]
Current timestep = 1156. State = [[-0.22444464  0.01579412]]. Action = [[ 0.23006016 -0.19750586  0.02114868 -0.50239354]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1156 is [True, False, False, False, True, False]
Current timestep = 1157. State = [[-0.2192377   0.01490556]]. Action = [[ 0.1967212   0.10240284  0.05068764 -0.3450951 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1157 is [True, False, False, False, True, False]
Human Feedback received at timestep 1157 of 1
Current timestep = 1158. State = [[-0.2132261   0.01557696]]. Action = [[ 0.06429386 -0.22824821  0.17767155  0.2423346 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1158 is [True, False, False, False, True, False]
Current timestep = 1159. State = [[-0.20795733  0.01397548]]. Action = [[-0.00258435  0.21120128 -0.02312428 -0.78533655]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1159 is [True, False, False, False, True, False]
Scene graph at timestep 1159 is [True, False, False, False, True, False]
State prediction error at timestep 1159 is tensor(2.8894e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1160. State = [[-0.20414117  0.01445596]]. Action = [[-0.1615983  -0.01044601  0.18524516  0.6898911 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1160 is [True, False, False, False, True, False]
Current timestep = 1161. State = [[-0.20333709  0.01467319]]. Action = [[0.12145782 0.0003629  0.06002507 0.00191092]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1161 is [True, False, False, False, True, False]
Human Feedback received at timestep 1161 of 1
Current timestep = 1162. State = [[-0.20228475  0.0146611 ]]. Action = [[ 0.16907924  0.07930988 -0.22503494  0.0200212 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1162 is [True, False, False, False, True, False]
Current timestep = 1163. State = [[-0.199745    0.01514701]]. Action = [[ 0.15637243  0.00083748 -0.1855563  -0.54267496]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1163 is [True, False, False, False, True, False]
Current timestep = 1164. State = [[-0.19689134  0.01550078]]. Action = [[-0.06175576 -0.22738306  0.02188993 -0.28118014]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1164 is [True, False, False, False, True, False]
Current timestep = 1165. State = [[-0.19505402  0.01451935]]. Action = [[-0.10814705 -0.00109345 -0.19333693 -0.42921686]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1165 is [True, False, False, False, True, False]
Scene graph at timestep 1165 is [True, False, False, False, True, False]
State prediction error at timestep 1165 is tensor(2.9194e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1165 of 1
Current timestep = 1166. State = [[-0.19443096  0.01383689]]. Action = [[ 0.15204906 -0.19392833  0.03278363 -0.65938085]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1166 is [True, False, False, False, True, False]
Current timestep = 1167. State = [[-0.1925254   0.01059087]]. Action = [[ 0.21421477  0.15674096 -0.02641919 -0.46123075]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1167 is [True, False, False, False, True, False]
Current timestep = 1168. State = [[-0.19020455  0.00985526]]. Action = [[ 0.06904632  0.22689039  0.18549216 -0.12616837]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1168 is [True, False, False, False, True, False]
Scene graph at timestep 1168 is [True, False, False, False, True, False]
State prediction error at timestep 1168 is tensor(1.8346e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1169. State = [[-0.18717709  0.01126916]]. Action = [[ 0.24167073 -0.09261219  0.2468577   0.68708634]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1169 is [True, False, False, False, True, False]
Current timestep = 1170. State = [[-0.18285029  0.01193145]]. Action = [[-0.15185252 -0.12106708 -0.1794754   0.23432565]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1170 is [True, False, False, False, True, False]
Scene graph at timestep 1170 is [True, False, False, False, True, False]
State prediction error at timestep 1170 is tensor(2.3181e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1171. State = [[-0.17969158  0.01135583]]. Action = [[ 0.05684099 -0.18304396  0.1970101  -0.9723195 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1171 is [True, False, False, False, True, False]
Scene graph at timestep 1171 is [True, False, False, False, True, False]
State prediction error at timestep 1171 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1172. State = [[-0.17707798  0.00893063]]. Action = [[-0.20323433  0.17312574  0.18675745 -0.68935525]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1172 is [True, False, False, False, True, False]
Human Feedback received at timestep 1172 of 1
Current timestep = 1173. State = [[-0.17708385  0.00904662]]. Action = [[-0.24348019  0.00852296  0.12338069 -0.6710798 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1173 is [True, False, False, False, True, False]
Current timestep = 1174. State = [[-0.1772481   0.00912935]]. Action = [[ 0.21264851  0.01092711 -0.06097284  0.14208698]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1174 is [True, False, False, False, True, False]
Scene graph at timestep 1174 is [True, False, False, False, True, False]
State prediction error at timestep 1174 is tensor(6.0888e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1175. State = [[-0.17726703  0.00901185]]. Action = [[ 0.02391228 -0.22083445 -0.15272999  0.04579937]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1175 is [True, False, False, False, True, False]
Current timestep = 1176. State = [[-0.17682151  0.00715324]]. Action = [[ 0.07725233 -0.12256297  0.11964667 -0.8934712 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1176 is [True, False, False, False, True, False]
Scene graph at timestep 1176 is [True, False, False, False, True, False]
State prediction error at timestep 1176 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1177. State = [[-0.17621836  0.00454242]]. Action = [[-0.11006853 -0.04394428  0.07773823 -0.2885996 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1177 is [True, False, False, False, True, False]
Current timestep = 1178. State = [[-0.17588744  0.00227948]]. Action = [[ 0.00660032 -0.13286246 -0.15101328 -0.14602512]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1178 is [True, False, False, False, True, False]
Current timestep = 1179. State = [[-0.17537607 -0.00067227]]. Action = [[-0.23329008 -0.2195899   0.05762532 -0.30943775]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1179 is [True, False, False, False, True, False]
Current timestep = 1180. State = [[-0.17530423 -0.00548157]]. Action = [[ 0.0992507  -0.03480045  0.12595919 -0.8043571 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1180 is [True, False, False, False, True, False]
Current timestep = 1181. State = [[-0.17492741 -0.00870527]]. Action = [[-0.12165377  0.1584782  -0.0911893  -0.7182268 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1181 is [True, False, False, False, True, False]
Current timestep = 1182. State = [[-0.17505363 -0.00924521]]. Action = [[ 0.15216252 -0.23489974  0.00893265 -0.06062961]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1182 is [True, False, False, False, True, False]
Scene graph at timestep 1182 is [True, False, False, False, True, False]
State prediction error at timestep 1182 is tensor(2.8711e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1183. State = [[-0.17444101 -0.01249875]]. Action = [[-0.17160991  0.00253341 -0.12594287 -0.98464507]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1183 is [True, False, False, False, True, False]
Current timestep = 1184. State = [[-0.17458242 -0.01446729]]. Action = [[-0.03612734 -0.1656894   0.10246247  0.2735039 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1184 is [True, False, False, False, True, False]
Scene graph at timestep 1184 is [True, False, False, False, True, False]
State prediction error at timestep 1184 is tensor(1.2552e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1185. State = [[-0.1752033  -0.01787577]]. Action = [[-0.19585866  0.16258386 -0.2284068  -0.43420982]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1185 is [True, False, False, False, True, False]
Scene graph at timestep 1185 is [True, False, False, False, True, False]
State prediction error at timestep 1185 is tensor(5.9228e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1186. State = [[-0.17660674 -0.01914912]]. Action = [[-0.14155954 -0.00832236 -0.10524842  0.53008103]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1186 is [True, False, False, False, True, False]
Current timestep = 1187. State = [[-0.17858198 -0.01988975]]. Action = [[0.2297636  0.1488252  0.18852404 0.9692316 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1187 is [True, False, False, False, True, False]
Current timestep = 1188. State = [[-0.1788524  -0.01936144]]. Action = [[-0.0317606   0.0706307  -0.17209426 -0.03367466]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1188 is [True, False, False, False, True, False]
Current timestep = 1189. State = [[-0.17937116 -0.01816085]]. Action = [[ 0.21765244  0.19830436 -0.11432017  0.6009859 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1189 is [True, False, False, False, True, False]
Current timestep = 1190. State = [[-0.17981681 -0.01572482]]. Action = [[ 0.23978233 -0.09482756 -0.01755814 -0.7211854 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1190 is [True, False, False, False, True, False]
Current timestep = 1191. State = [[-0.17961651 -0.01517272]]. Action = [[ 0.1495296   0.05029392 -0.0326308  -0.25796235]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1191 is [True, False, False, False, True, False]
Current timestep = 1192. State = [[-0.17794424 -0.01457479]]. Action = [[ 0.18568647 -0.09997559  0.22264779  0.6513432 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1192 is [True, False, False, False, True, False]
Scene graph at timestep 1192 is [True, False, False, False, True, False]
State prediction error at timestep 1192 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1193. State = [[-0.17533317 -0.01437849]]. Action = [[-0.19853498 -0.06201401 -0.07275282 -0.35277283]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1193 is [True, False, False, False, True, False]
Scene graph at timestep 1193 is [True, False, False, False, True, False]
State prediction error at timestep 1193 is tensor(1.1793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1194. State = [[-0.1751682  -0.01476079]]. Action = [[ 0.16975898 -0.1332222  -0.02969682 -0.13622212]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1194 is [True, False, False, False, True, False]
Current timestep = 1195. State = [[-0.17432578 -0.016423  ]]. Action = [[-0.09387413  0.23158422  0.14843154  0.9460039 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1195 is [True, False, False, False, True, False]
Current timestep = 1196. State = [[-0.17430668 -0.01516175]]. Action = [[-0.01756623  0.07972825  0.03204212  0.07470608]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1196 is [True, False, False, False, True, False]
Human Feedback received at timestep 1196 of 1
Current timestep = 1197. State = [[-0.17443424 -0.01352592]]. Action = [[ 0.24452919 -0.13897811 -0.10472631 -0.89402276]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1197 is [True, False, False, False, True, False]
Current timestep = 1198. State = [[-0.17232153 -0.01450041]]. Action = [[ 0.08413452 -0.24843867  0.17405224  0.82501054]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1198 is [True, False, False, False, True, False]
Current timestep = 1199. State = [[-0.17009883 -0.01742439]]. Action = [[ 0.04041985 -0.1456306   0.05487299 -0.28986782]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1199 is [True, False, False, False, True, False]
Scene graph at timestep 1199 is [True, False, False, False, True, False]
State prediction error at timestep 1199 is tensor(4.6896e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1200. State = [[-0.16691838 -0.02105417]]. Action = [[ 0.18218955  0.08836293  0.04566127 -0.6572643 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1200 is [True, False, False, False, True, False]
Current timestep = 1201. State = [[-0.16408986 -0.02166617]]. Action = [[-0.22490336  0.06057081  0.23497659  0.00153947]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1201 is [True, False, False, False, True, False]
Scene graph at timestep 1201 is [True, False, False, False, True, False]
State prediction error at timestep 1201 is tensor(3.0268e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1202. State = [[-0.16377585 -0.02166908]]. Action = [[-0.09306261 -0.01853904  0.17234454  0.09969115]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1202 is [True, False, False, False, True, False]
Current timestep = 1203. State = [[-0.16380413 -0.021761  ]]. Action = [[ 0.02166128 -0.1830993  -0.03830943  0.09464204]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1203 is [True, False, False, False, True, False]
Current timestep = 1204. State = [[-0.16339706 -0.02366755]]. Action = [[-0.10170914  0.04629984  0.23079687 -0.9180464 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1204 is [True, False, False, False, True, False]
Current timestep = 1205. State = [[-0.16343279 -0.02439107]]. Action = [[-0.02696198 -0.15279001  0.18177906 -0.951179  ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1205 is [True, False, False, False, True, False]
Current timestep = 1206. State = [[-0.16337503 -0.02678904]]. Action = [[-0.22454211 -0.03625084 -0.21804774 -0.6286668 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1206 is [True, False, False, False, True, False]
Current timestep = 1207. State = [[-0.16417769 -0.02894019]]. Action = [[ 0.03768215 -0.20755155 -0.0377152   0.45413637]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1207 is [True, False, False, False, True, False]
Current timestep = 1208. State = [[-0.16474342 -0.03279158]]. Action = [[ 0.00438455 -0.07540417  0.03506893  0.61063826]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1208 is [True, False, False, False, True, False]
Scene graph at timestep 1208 is [True, False, False, False, True, False]
State prediction error at timestep 1208 is tensor(8.7857e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1209. State = [[-0.16513267 -0.0360578 ]]. Action = [[ 0.13062292 -0.14894621  0.20155722  0.94947493]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1209 is [True, False, False, False, True, False]
Current timestep = 1210. State = [[-0.16420195 -0.0403373 ]]. Action = [[ 0.11771557 -0.15929934 -0.20403743 -0.00927925]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1210 is [True, False, False, False, True, False]
Scene graph at timestep 1210 is [True, False, False, False, True, False]
State prediction error at timestep 1210 is tensor(3.0728e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1211. State = [[-0.16298251 -0.04617515]]. Action = [[-0.09657477  0.07429215 -0.1162986  -0.09446532]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1211 is [True, False, False, False, True, False]
Current timestep = 1212. State = [[-0.16260745 -0.04803322]]. Action = [[-0.09104584  0.10437161 -0.21690698 -0.8528973 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1212 is [True, False, False, False, True, False]
Current timestep = 1213. State = [[-0.16287503 -0.04801951]]. Action = [[-0.1000976   0.0171212   0.19891602  0.9483088 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1213 is [True, False, False, False, True, False]
Current timestep = 1214. State = [[-0.16341287 -0.04812194]]. Action = [[-0.07736987 -0.0820199   0.21583411 -0.61412454]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1214 is [True, False, False, False, True, False]
Current timestep = 1215. State = [[-0.16432765 -0.04857751]]. Action = [[ 0.06039158  0.14748847 -0.04210582 -0.5956453 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1215 is [True, False, False, False, True, False]
Current timestep = 1216. State = [[-0.16443163 -0.04851175]]. Action = [[-0.18077838 -0.2415384   0.13249922 -0.679509  ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1216 is [True, False, False, False, True, False]
Current timestep = 1217. State = [[-0.16581148 -0.04989195]]. Action = [[ 0.14415532  0.11669976 -0.00268607 -0.18189305]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1217 is [True, False, False, False, True, False]
Scene graph at timestep 1217 is [True, False, False, False, True, False]
State prediction error at timestep 1217 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1218. State = [[-0.16575477 -0.05004864]]. Action = [[ 0.06890315 -0.14955434 -0.04101877  0.4346391 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1218 is [True, False, False, False, True, False]
Scene graph at timestep 1218 is [True, False, False, False, True, False]
State prediction error at timestep 1218 is tensor(5.0195e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1219. State = [[-0.16550998 -0.05121256]]. Action = [[ 0.20896459 -0.02367489 -0.11033151  0.27334595]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1219 is [True, False, False, False, True, False]
Scene graph at timestep 1219 is [True, False, False, False, True, False]
State prediction error at timestep 1219 is tensor(8.3823e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1220. State = [[-0.16502647 -0.05293403]]. Action = [[ 0.17828935 -0.20350158 -0.01314798 -0.5315178 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1220 is [True, False, False, False, True, False]
Current timestep = 1221. State = [[-0.16355398 -0.05719745]]. Action = [[ 0.24105683 -0.11553839 -0.00899908  0.38730276]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1221 is [True, False, False, False, True, False]
Current timestep = 1222. State = [[-0.16095717 -0.06154245]]. Action = [[-0.232523    0.0092403  -0.03216569 -0.9495547 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1222 is [True, False, False, False, True, False]
Current timestep = 1223. State = [[-0.1605636  -0.06373999]]. Action = [[-0.13310997  0.11501473 -0.19662483  0.39900708]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1223 is [True, False, False, False, True, False]
Scene graph at timestep 1223 is [True, False, False, False, True, False]
State prediction error at timestep 1223 is tensor(3.5467e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1224. State = [[-0.16057476 -0.06381696]]. Action = [[ 0.2031036  -0.13102873  0.14148402  0.22573066]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 1224 is [True, False, False, False, True, False]
Current timestep = 1225. State = [[-0.16020134 -0.06493224]]. Action = [[-0.01711653  0.2243168  -0.03233518  0.65580714]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 1225 is [True, False, False, False, True, False]
Current timestep = 1226. State = [[-0.16015498 -0.06423316]]. Action = [[-0.18917835 -0.21857463  0.08413458 -0.2614348 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 1226 is [True, False, False, False, True, False]
Current timestep = 1227. State = [[-0.15996012 -0.06521857]]. Action = [[-0.03329155  0.09347802 -0.12041345 -0.8632776 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 1227 is [True, False, False, False, True, False]
Current timestep = 1228. State = [[-0.15996993 -0.06521757]]. Action = [[-0.21723667 -0.02153091  0.18647993 -0.97548527]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 1228 is [True, False, False, False, True, False]
Current timestep = 1229. State = [[-0.16104467 -0.06533117]]. Action = [[-0.20112942  0.07746783 -0.15757346  0.02724874]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 1229 is [True, False, False, False, True, False]
Current timestep = 1230. State = [[-0.16267762 -0.06506646]]. Action = [[-0.17773461  0.22821069  0.04013979  0.11976337]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 1230 is [True, False, False, False, True, False]
Current timestep = 1231. State = [[-0.16475071 -0.06260673]]. Action = [[ 0.08782646 -0.00532433  0.06666052  0.93378544]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 1231 is [True, False, False, False, True, False]
Human Feedback received at timestep 1231 of 1
Current timestep = 1232. State = [[-0.16525176 -0.06152565]]. Action = [[-0.12297097  0.01467207 -0.15165725 -0.9370561 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 1232 is [True, False, False, False, True, False]
Current timestep = 1233. State = [[-0.16664645 -0.06072988]]. Action = [[ 0.02044725  0.01270911 -0.19446635 -0.9669975 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 1233 is [True, False, False, False, True, False]
Current timestep = 1234. State = [[-0.16704676 -0.06025364]]. Action = [[ 0.20601991 -0.13923496  0.17324483  0.4043008 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 1234 is [True, False, False, False, True, False]
Current timestep = 1235. State = [[-0.16710983 -0.06056907]]. Action = [[-0.22808543  0.15733212  0.07936656 -0.33807915]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 1235 is [True, False, False, False, True, False]
Scene graph at timestep 1235 is [True, False, False, False, True, False]
State prediction error at timestep 1235 is tensor(6.1974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1236. State = [[-0.16759174 -0.05966699]]. Action = [[-0.01971768 -0.00883836 -0.21507859 -0.97612035]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 1236 is [True, False, False, False, True, False]
Scene graph at timestep 1236 is [True, False, False, False, True, False]
State prediction error at timestep 1236 is tensor(3.4213e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1237. State = [[-0.16794924 -0.05950527]]. Action = [[ 0.15299514 -0.04854839 -0.16013351 -0.49136543]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 1237 is [True, False, False, False, True, False]
Current timestep = 1238. State = [[-0.16790399 -0.05949274]]. Action = [[ 0.06291011 -0.1501178  -0.18985632  0.0519532 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 1238 is [True, False, False, False, True, False]
Current timestep = 1239. State = [[-0.16779086 -0.06006684]]. Action = [[ 0.22700131 -0.04097675  0.04858044 -0.98720324]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 1239 is [True, False, False, False, True, False]
Current timestep = 1240. State = [[-0.16758749 -0.06118241]]. Action = [[-0.10728961 -0.2010826   0.16666117 -0.15356004]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 1240 is [True, False, False, False, True, False]
Scene graph at timestep 1240 is [True, False, False, False, True, False]
State prediction error at timestep 1240 is tensor(5.6056e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1241. State = [[-0.16731296 -0.06402525]]. Action = [[ 0.11178771  0.10118595 -0.19534142  0.85843   ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 1241 is [True, False, False, False, True, False]
Current timestep = 1242. State = [[-0.16723932 -0.06419262]]. Action = [[-0.232524    0.04628131 -0.0412055   0.5198401 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 1242 is [True, False, False, False, True, False]
Current timestep = 1243. State = [[-0.1673085  -0.06425562]]. Action = [[-0.21811633  0.10461169  0.24714196  0.84038424]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 1243 is [True, False, False, False, True, False]
Current timestep = 1244. State = [[-0.16801812 -0.06371427]]. Action = [[-0.23940852  0.18889984  0.06167728 -0.84424716]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 1244 is [True, False, False, False, True, False]
Scene graph at timestep 1244 is [True, False, False, False, True, False]
State prediction error at timestep 1244 is tensor(9.2958e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1245. State = [[-0.17044051 -0.06105413]]. Action = [[ 0.11010048  0.03677061 -0.20471579  0.90567946]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 1245 is [True, False, False, False, True, False]
Current timestep = 1246. State = [[-0.17196761 -0.05939931]]. Action = [[-0.15376471  0.0437558   0.00544396 -0.84897757]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 1246 is [True, False, False, False, True, False]
Scene graph at timestep 1246 is [True, False, False, False, True, False]
State prediction error at timestep 1246 is tensor(6.1960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1247. State = [[-0.17486447 -0.0578821 ]]. Action = [[-0.0374207  -0.06256989  0.16802353  0.30926514]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 1247 is [True, False, False, False, True, False]
Current timestep = 1248. State = [[-0.17735815 -0.05743163]]. Action = [[-0.11882353  0.14078361  0.01548961  0.8593048 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1248 is [True, False, False, False, True, False]
Current timestep = 1249. State = [[-0.18076895 -0.0555531 ]]. Action = [[-0.0929313   0.02995995  0.12624586 -0.36965543]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1249 is [True, False, False, False, True, False]
Current timestep = 1250. State = [[-0.18445086 -0.05406052]]. Action = [[-0.24383405  0.12557447  0.12415883 -0.93975675]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1250 is [True, False, False, False, True, False]
Current timestep = 1251. State = [[-0.18959546 -0.05128372]]. Action = [[ 0.03763127 -0.22608544  0.21685761 -0.58031154]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1251 is [True, False, False, False, True, False]
Current timestep = 1252. State = [[-0.19199249 -0.05196238]]. Action = [[ 0.12534758  0.06934008  0.19900954 -0.8917351 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1252 is [True, False, False, False, True, False]
Current timestep = 1253. State = [[-0.19236109 -0.05191052]]. Action = [[-0.1814705   0.00085187 -0.2275057  -0.16935855]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1253 is [True, False, False, False, True, False]
Current timestep = 1254. State = [[-0.19344893 -0.05199837]]. Action = [[ 0.05099714 -0.21122077 -0.00595295 -0.1227783 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1254 is [True, False, False, False, True, False]
Current timestep = 1255. State = [[-0.19433396 -0.05409155]]. Action = [[ 0.03768376 -0.1024967  -0.02853554  0.7080164 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1255 is [True, False, False, False, True, False]
Human Feedback received at timestep 1255 of -1
Current timestep = 1256. State = [[-0.1949501 -0.0572922]]. Action = [[-0.11111397 -0.21001938 -0.09709507 -0.55636656]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1256 is [True, False, False, False, True, False]
Current timestep = 1257. State = [[-0.19647767 -0.06192017]]. Action = [[0.11452881 0.06973931 0.1981582  0.51751745]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1257 is [True, False, False, False, True, False]
Human Feedback received at timestep 1257 of -1
Current timestep = 1258. State = [[-0.19691141 -0.06376862]]. Action = [[-0.1594276  -0.11377349 -0.14158195 -0.19109827]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1258 is [True, False, False, False, True, False]
Current timestep = 1259. State = [[-0.1982315 -0.0658083]]. Action = [[ 0.14275566  0.17844018 -0.13620587  0.57668936]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1259 is [True, False, False, False, True, False]
Current timestep = 1260. State = [[-0.19843163 -0.06570567]]. Action = [[-0.24392481 -0.06513453 -0.07624212 -0.14180136]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1260 is [True, False, False, False, True, False]
Scene graph at timestep 1260 is [True, False, False, False, True, False]
State prediction error at timestep 1260 is tensor(5.8491e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1261. State = [[-0.19995283 -0.06604297]]. Action = [[ 0.09654412  0.22239152  0.14958936 -0.75552756]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1261 is [True, False, False, False, True, False]
Scene graph at timestep 1261 is [True, False, False, False, True, False]
State prediction error at timestep 1261 is tensor(6.3803e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1262. State = [[-0.20118701 -0.06457683]]. Action = [[-0.10471705 -0.02116433  0.01669827  0.11798954]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1262 is [True, False, False, False, True, False]
Scene graph at timestep 1262 is [True, False, False, False, True, False]
State prediction error at timestep 1262 is tensor(4.5390e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1263. State = [[-0.20256276 -0.06388489]]. Action = [[-0.16945113 -0.11393428 -0.09465382  0.6939268 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1263 is [True, False, False, False, True, False]
Current timestep = 1264. State = [[-0.20589636 -0.06403413]]. Action = [[-0.23905148  0.15843344 -0.05853118 -0.7441915 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1264 is [True, False, False, False, True, False]
Current timestep = 1265. State = [[-0.21069582 -0.06256468]]. Action = [[-0.0257082   0.20221162 -0.11546063 -0.37089765]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1265 is [True, False, False, False, True, False]
Human Feedback received at timestep 1265 of -1
Current timestep = 1266. State = [[-0.21504368 -0.0587949 ]]. Action = [[-0.15949897  0.22638908  0.19943619 -0.30004692]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1266 is [True, False, False, False, True, False]
Current timestep = 1267. State = [[-0.21954034 -0.05347763]]. Action = [[ 0.03984118 -0.22855598 -0.16392225  0.5407175 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1267 is [True, False, False, False, True, False]
Scene graph at timestep 1267 is [True, False, False, False, True, False]
State prediction error at timestep 1267 is tensor(4.6272e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1268. State = [[-0.22171153 -0.05316383]]. Action = [[ 0.09130597  0.00624645 -0.15942782 -0.32538736]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1268 is [True, False, False, False, True, False]
Scene graph at timestep 1268 is [True, False, False, False, True, False]
State prediction error at timestep 1268 is tensor(9.4105e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1269. State = [[-0.22244315 -0.05294226]]. Action = [[ 0.10204864  0.18223643 -0.04185008  0.6367092 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1269 is [True, False, False, False, True, False]
Current timestep = 1270. State = [[-0.22281429 -0.05114724]]. Action = [[ 0.06829908  0.02530321 -0.02355243  0.2613511 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1270 is [True, False, False, False, True, False]
Current timestep = 1271. State = [[-0.2230725  -0.04955188]]. Action = [[0.24107087 0.17606819 0.20314354 0.55094504]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1271 is [True, False, False, False, True, False]
Current timestep = 1272. State = [[-0.2226406 -0.0464809]]. Action = [[-0.07795589 -0.00793976 -0.0352802  -0.2812909 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1272 is [True, False, False, False, True, False]
Scene graph at timestep 1272 is [True, False, False, False, True, False]
State prediction error at timestep 1272 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1273. State = [[-0.22297923 -0.04447433]]. Action = [[ 0.21566033  0.01250863  0.05489913 -0.80540395]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1273 is [True, False, False, False, True, False]
Current timestep = 1274. State = [[-0.22138616 -0.04345503]]. Action = [[ 0.20371372 -0.07493055 -0.1695133  -0.5096089 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1274 is [True, False, False, False, True, False]
Current timestep = 1275. State = [[-0.21919653 -0.04289623]]. Action = [[ 0.06673872 -0.06224385  0.03242272  0.02231061]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1275 is [True, False, False, False, True, False]
Current timestep = 1276. State = [[-0.21739961 -0.0429685 ]]. Action = [[-0.22223686 -0.12511848 -0.18719885 -0.739545  ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1276 is [True, False, False, False, True, False]
Human Feedback received at timestep 1276 of -1
Current timestep = 1277. State = [[-0.21726198 -0.04355371]]. Action = [[ 0.09027585  0.1932168   0.22090617 -0.94544154]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1277 is [True, False, False, False, True, False]
Current timestep = 1278. State = [[-0.21715784 -0.04253972]]. Action = [[ 0.22927529  0.22707826  0.22267729 -0.21245205]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1278 is [True, False, False, False, True, False]
Current timestep = 1279. State = [[-0.21490853 -0.03929006]]. Action = [[-0.08700576 -0.11124897 -0.24435522  0.6824168 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1279 is [True, False, False, False, True, False]
Current timestep = 1280. State = [[-0.21471249 -0.03825472]]. Action = [[ 0.21050969  0.03581578  0.19743228 -0.42465186]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1280 is [True, False, False, False, True, False]
Scene graph at timestep 1280 is [True, False, False, False, True, False]
State prediction error at timestep 1280 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1281. State = [[-0.21271865 -0.03736312]]. Action = [[ 0.07683998 -0.15607414 -0.12510033 -0.83201385]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1281 is [True, False, False, False, True, False]
Scene graph at timestep 1281 is [True, False, False, False, True, False]
State prediction error at timestep 1281 is tensor(7.0387e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1282. State = [[-0.21000384 -0.03779924]]. Action = [[ 0.15604848  0.1822505  -0.08679593  0.36925042]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1282 is [True, False, False, False, True, False]
Current timestep = 1283. State = [[-0.20598537 -0.03713175]]. Action = [[ 0.04491988 -0.24110395  0.05037302 -0.52280456]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1283 is [True, False, False, False, True, False]
Current timestep = 1284. State = [[-0.20236027 -0.03821764]]. Action = [[-0.0606235  -0.14314818  0.20004416  0.8389509 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1284 is [True, False, False, False, True, False]
Current timestep = 1285. State = [[-0.20001212 -0.04012648]]. Action = [[ 0.24817532  0.02770251 -0.19540739  0.20625687]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1285 is [True, False, False, False, True, False]
Current timestep = 1286. State = [[-0.19601925 -0.04057711]]. Action = [[ 0.09737056  0.22676694 -0.087107    0.914644  ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1286 is [True, False, False, False, True, False]
Current timestep = 1287. State = [[-0.19282703 -0.03960934]]. Action = [[ 0.00872353 -0.06169039  0.09910181 -0.59168535]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1287 is [True, False, False, False, True, False]
Human Feedback received at timestep 1287 of 1
Current timestep = 1288. State = [[-0.19067216 -0.0396587 ]]. Action = [[ 0.13690436 -0.2169418  -0.21390006 -0.96471745]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1288 is [True, False, False, False, True, False]
Current timestep = 1289. State = [[-0.18809305 -0.04172045]]. Action = [[-0.21168292 -0.17833978  0.02365747 -0.5639049 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1289 is [True, False, False, False, True, False]
Current timestep = 1290. State = [[-0.18766919 -0.04492719]]. Action = [[-0.14528884  0.19445902 -0.01660061  0.6138973 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1290 is [True, False, False, False, True, False]
Scene graph at timestep 1290 is [True, False, False, False, True, False]
State prediction error at timestep 1290 is tensor(2.6607e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1291. State = [[-0.18766654 -0.0449966 ]]. Action = [[ 0.04684636 -0.12538747 -0.09367043  0.13927078]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1291 is [True, False, False, False, True, False]
Current timestep = 1292. State = [[-0.18757938 -0.04602984]]. Action = [[-0.2039879   0.06741196  0.04152796  0.4043603 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1292 is [True, False, False, False, True, False]
Current timestep = 1293. State = [[-0.18776818 -0.04599869]]. Action = [[-0.1474793   0.2325024   0.24213567  0.73529553]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1293 is [True, False, False, False, True, False]
Current timestep = 1294. State = [[-0.18893161 -0.04448071]]. Action = [[ 0.11970842 -0.08275108  0.03947273 -0.5929498 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1294 is [True, False, False, False, True, False]
Current timestep = 1295. State = [[-0.18905306 -0.04438443]]. Action = [[-0.19707412  0.01635391 -0.10143913 -0.1948381 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1295 is [True, False, False, False, True, False]
Current timestep = 1296. State = [[-0.19013608 -0.04452365]]. Action = [[ 0.04085541 -0.14322059 -0.18948264  0.38578105]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1296 is [True, False, False, False, True, False]
Current timestep = 1297. State = [[-0.19032802 -0.04525651]]. Action = [[ 0.17528254 -0.0670101   0.13487634  0.5475633 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1297 is [True, False, False, False, True, False]
Current timestep = 1298. State = [[-0.19021395 -0.0463744 ]]. Action = [[-0.1450525  -0.16315183  0.05095413 -0.18663949]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1298 is [True, False, False, False, True, False]
Current timestep = 1299. State = [[-0.19045004 -0.04901867]]. Action = [[ 0.11875248  0.03320649 -0.03811419 -0.2390396 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1299 is [True, False, False, False, True, False]
Current timestep = 1300. State = [[-0.19057752 -0.05036704]]. Action = [[-0.12555102 -0.19421585 -0.04979041 -0.04924083]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1300 is [True, False, False, False, True, False]
Current timestep = 1301. State = [[-0.1907956  -0.05343252]]. Action = [[ 0.0878537  -0.16850531  0.18962768  0.84859097]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1301 is [True, False, False, False, True, False]
Current timestep = 1302. State = [[-0.19069058 -0.05778418]]. Action = [[-0.03489418 -0.05856435  0.03718939 -0.67442954]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1302 is [True, False, False, False, True, False]
Current timestep = 1303. State = [[-0.1907949  -0.06121498]]. Action = [[-0.2051673  -0.05587944  0.09910893  0.87334037]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1303 is [True, False, False, False, True, False]
Current timestep = 1304. State = [[-0.19247998 -0.06422172]]. Action = [[-0.0200859   0.12613147 -0.11071974 -0.7639632 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1304 is [True, False, False, False, True, False]
Current timestep = 1305. State = [[-0.1933766  -0.06516758]]. Action = [[ 0.21181363  0.03259069  0.04986045 -0.00898755]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1305 is [True, False, False, False, True, False]
Current timestep = 1306. State = [[-0.19333212 -0.06526484]]. Action = [[-0.10808286 -0.21257654 -0.19809411 -0.15393418]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1306 is [True, False, False, False, True, False]
Current timestep = 1307. State = [[-0.19351919 -0.0674485 ]]. Action = [[-0.23763154  0.03206477  0.01054299  0.9713156 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1307 is [True, False, False, False, True, False]
Current timestep = 1308. State = [[-0.19501165 -0.06879852]]. Action = [[-0.22866268  0.03178635 -0.08907104  0.09375095]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1308 is [True, False, False, False, True, False]
Current timestep = 1309. State = [[-0.19745837 -0.06967322]]. Action = [[-0.02500835 -0.09309152  0.17333922 -0.8247878 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1309 is [True, False, False, False, True, False]
Human Feedback received at timestep 1309 of 1
Current timestep = 1310. State = [[-0.19945662 -0.07142643]]. Action = [[ 0.0457319   0.06627762 -0.10122767  0.5745758 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1310 is [True, False, False, False, True, False]
Scene graph at timestep 1310 is [True, False, False, False, True, False]
State prediction error at timestep 1310 is tensor(1.6752e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1311. State = [[-0.20112868 -0.07165316]]. Action = [[-0.20299374  0.04093075 -0.14311482 -0.52767617]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1311 is [True, False, False, False, True, False]
Scene graph at timestep 1311 is [True, False, False, False, True, False]
State prediction error at timestep 1311 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1312. State = [[-0.20415942 -0.07158531]]. Action = [[-0.19551553 -0.05636512 -0.14852512  0.02767384]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1312 is [True, False, False, False, True, False]
Current timestep = 1313. State = [[-0.20885138 -0.07237259]]. Action = [[ 0.23045689 -0.0027436   0.12884569 -0.9331859 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1313 is [True, False, False, False, True, False]
Current timestep = 1314. State = [[-0.2102856  -0.07306167]]. Action = [[-0.21995394 -0.14862299 -0.07328454  0.941998  ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1314 is [True, False, False, False, True, False]
Scene graph at timestep 1314 is [True, False, False, False, True, False]
State prediction error at timestep 1314 is tensor(1.7097e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1315. State = [[-0.21288149 -0.07566867]]. Action = [[-0.05672997 -0.21561967  0.24370578 -0.07805246]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1315 is [True, False, False, False, True, False]
Scene graph at timestep 1315 is [True, False, False, False, True, False]
State prediction error at timestep 1315 is tensor(4.7086e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1315 of -1
Current timestep = 1316. State = [[-0.21668236 -0.08036023]]. Action = [[-0.03884     0.11705136  0.18037486  0.537107  ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1316 is [True, False, False, False, True, False]
Scene graph at timestep 1316 is [True, False, False, False, True, False]
State prediction error at timestep 1316 is tensor(1.3742e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1317. State = [[-0.21898459 -0.08149958]]. Action = [[ 0.23484021  0.04346475 -0.14476219  0.12723875]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1317 is [True, False, False, False, True, False]
Current timestep = 1318. State = [[-0.21883468 -0.0816375 ]]. Action = [[ 0.11638254 -0.13833283  0.10572374  0.7569548 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1318 is [True, False, False, False, True, False]
Scene graph at timestep 1318 is [True, False, False, False, True, False]
State prediction error at timestep 1318 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1318 of -1
Current timestep = 1319. State = [[-0.21860297 -0.08260797]]. Action = [[ 0.24131027 -0.24022937 -0.15428503 -0.65864104]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1319 is [True, False, False, False, True, False]
Current timestep = 1320. State = [[-0.21688862 -0.08643463]]. Action = [[ 0.05273008 -0.22251545  0.05318817  0.6633942 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1320 is [True, False, False, False, True, False]
Current timestep = 1321. State = [[-0.21481459 -0.09150991]]. Action = [[ 0.14792857  0.20536822 -0.15700476 -0.4442858 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1321 is [True, False, False, False, True, False]
Current timestep = 1322. State = [[-0.21300356 -0.09326468]]. Action = [[-0.10840447 -0.14894865 -0.10608226  0.95917165]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 1322 is [True, False, False, False, True, False]
Scene graph at timestep 1322 is [True, False, False, False, True, False]
State prediction error at timestep 1322 is tensor(3.8849e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1323. State = [[-0.21187878 -0.09551097]]. Action = [[ 0.22992766 -0.12251908 -0.21413493  0.35779738]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 1323 is [True, False, False, False, True, False]
Current timestep = 1324. State = [[-0.21012516 -0.0987241 ]]. Action = [[ 0.17605495  0.09429032 -0.12424231  0.23622668]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 1324 is [True, False, False, False, True, False]
Current timestep = 1325. State = [[-0.20803292 -0.09967965]]. Action = [[ 0.09198141 -0.00923257  0.21201876 -0.67014325]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 1325 is [True, False, False, False, True, False]
Scene graph at timestep 1325 is [True, False, False, False, True, False]
State prediction error at timestep 1325 is tensor(3.4667e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1326. State = [[-0.20545949 -0.10020544]]. Action = [[-0.10597363 -0.07551274 -0.21841416 -0.9669398 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 1326 is [True, False, False, False, True, False]
Current timestep = 1327. State = [[-0.20432614 -0.10109361]]. Action = [[-0.19372861 -0.00335677  0.05886018  0.01025021]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 1327 is [True, False, False, False, True, False]
Scene graph at timestep 1327 is [True, False, False, False, True, False]
State prediction error at timestep 1327 is tensor(1.6970e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1327 of -1
Current timestep = 1328. State = [[-0.20423047 -0.10164317]]. Action = [[-0.12055725  0.18508604 -0.16870761  0.7485094 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 1328 is [True, False, False, False, True, False]
Current timestep = 1329. State = [[-0.20433141 -0.1015394 ]]. Action = [[-0.06077465  0.01269835 -0.16940786 -0.42519915]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 1329 is [True, False, False, False, True, False]
Current timestep = 1330. State = [[-0.20446149 -0.10137001]]. Action = [[ 0.10276681 -0.02641305 -0.01678747  0.6941737 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 1330 is [True, False, False, False, True, False]
Current timestep = 1331. State = [[-0.20444424 -0.10130938]]. Action = [[-0.06040648  0.06630221  0.20658755 -0.67992306]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 1331 is [True, False, False, False, True, False]
Scene graph at timestep 1331 is [True, False, False, False, True, False]
State prediction error at timestep 1331 is tensor(5.1911e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1332. State = [[-0.20440547 -0.10099435]]. Action = [[0.21354336 0.14767584 0.22363305 0.05238903]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 1332 is [True, False, False, False, True, False]
Current timestep = 1333. State = [[-0.20417432 -0.09899317]]. Action = [[-0.15575078 -0.08794864  0.11687267 -0.94505733]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 1333 is [True, False, False, False, True, False]
Current timestep = 1334. State = [[-0.2041348  -0.09884045]]. Action = [[-0.14633687  0.11881033  0.22550803 -0.65008426]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 1334 is [True, False, False, False, True, False]
Current timestep = 1335. State = [[-0.20417424 -0.09777761]]. Action = [[0.10244924 0.02781644 0.2253524  0.01127958]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 1335 is [True, False, False, False, True, False]
Scene graph at timestep 1335 is [True, False, False, False, True, False]
State prediction error at timestep 1335 is tensor(9.1091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1336. State = [[-0.20415388 -0.09701046]]. Action = [[ 0.06682125 -0.20979041  0.03017452  0.8935456 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 1336 is [True, False, False, False, True, False]
Current timestep = 1337. State = [[-0.20405817 -0.09732657]]. Action = [[0.14048791 0.16062337 0.11072081 0.7146535 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 1337 is [True, False, False, False, True, False]
Current timestep = 1338. State = [[-0.20384988 -0.09645481]]. Action = [[ 0.08296552  0.09959146 -0.07378882 -0.23186398]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 1338 is [True, False, False, False, True, False]
Current timestep = 1339. State = [[-0.20331362 -0.09482216]]. Action = [[-0.24667068  0.18137622  0.17688093  0.91288793]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 1339 is [True, False, False, False, True, False]
Current timestep = 1340. State = [[-0.2033177  -0.09196386]]. Action = [[ 0.11036867  0.13977781 -0.13519365  0.25643158]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 1340 is [True, False, False, False, True, False]
Current timestep = 1341. State = [[-0.2034732  -0.08809941]]. Action = [[ 0.12631544  0.01844263 -0.15185484  0.29941332]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 1341 is [True, False, False, False, True, False]
Current timestep = 1342. State = [[-0.20323423 -0.08585438]]. Action = [[-0.07663184 -0.23463205 -0.1476456  -0.4480945 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 1342 is [True, False, False, False, True, False]
Current timestep = 1343. State = [[-0.20314236 -0.08619952]]. Action = [[-0.10083269  0.16141218 -0.09863099 -0.5405633 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 1343 is [True, False, False, False, True, False]
Current timestep = 1344. State = [[-0.20322663 -0.08517268]]. Action = [[ 0.05789727 -0.05887981 -0.12509577 -0.22286838]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 1344 is [True, False, False, False, True, False]
Current timestep = 1345. State = [[-0.2031961  -0.08504999]]. Action = [[-0.19769989 -0.07139052  0.05680734  0.35504842]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 1345 is [True, False, False, False, True, False]
Current timestep = 1346. State = [[-0.20323972 -0.08515345]]. Action = [[-0.1465443  -0.16046432  0.01703021 -0.31283462]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 1346 is [True, False, False, False, True, False]
Current timestep = 1347. State = [[-0.20370099 -0.08711073]]. Action = [[ 0.04745105 -0.14793013  0.0755105  -0.52689403]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 1347 is [True, False, False, False, True, False]
Current timestep = 1348. State = [[-0.20401508 -0.08971988]]. Action = [[ 0.23966932 -0.01879579 -0.07031181 -0.46576113]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 1348 is [True, False, False, False, True, False]
Current timestep = 1349. State = [[-0.20408875 -0.09109955]]. Action = [[-0.12580673 -0.22193354  0.18020004  0.3959316 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 1349 is [True, False, False, False, True, False]
Current timestep = 1350. State = [[-0.20428266 -0.09470913]]. Action = [[ 0.13496915  0.22193116  0.16419598 -0.31890756]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 1350 is [True, False, False, False, True, False]
Current timestep = 1351. State = [[-0.20436083 -0.09452082]]. Action = [[-0.10807671  0.24255115  0.15770197 -0.740778  ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 1351 is [True, False, False, False, True, False]
Current timestep = 1352. State = [[-0.20446828 -0.09277993]]. Action = [[ 0.11640376 -0.13826154  0.24792427 -0.02251041]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 1352 is [True, False, False, False, True, False]
Current timestep = 1353. State = [[-0.20443492 -0.09279138]]. Action = [[-0.04538363  0.17186683 -0.20414798 -0.11424297]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 1353 is [True, False, False, False, True, False]
Current timestep = 1354. State = [[-0.20420958 -0.09158967]]. Action = [[-0.11682048 -0.03426348 -0.08797871  0.967015  ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 1354 is [True, False, False, False, True, False]
Current timestep = 1355. State = [[-0.20431903 -0.09133861]]. Action = [[-0.1537577  -0.17279361 -0.19296525  0.3791926 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 1355 is [True, False, False, False, True, False]
Scene graph at timestep 1355 is [True, False, False, False, True, False]
State prediction error at timestep 1355 is tensor(9.4104e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1356. State = [[-0.20478556 -0.09238411]]. Action = [[ 0.03844538 -0.23933953  0.23718491  0.9133761 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 1356 is [True, False, False, False, True, False]
Current timestep = 1357. State = [[-0.20478584 -0.09553307]]. Action = [[ 0.14738107 -0.02116153 -0.13147849  0.02512538]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 1357 is [True, False, False, False, True, False]
Current timestep = 1358. State = [[-0.20487244 -0.09696941]]. Action = [[-0.1666432   0.03601608 -0.14285898 -0.0406785 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 1358 is [True, False, False, False, True, False]
Current timestep = 1359. State = [[-0.20509404 -0.09759031]]. Action = [[-0.10290006  0.15502119 -0.05214763 -0.623328  ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 1359 is [True, False, False, False, True, False]
Current timestep = 1360. State = [[-0.20599572 -0.09730708]]. Action = [[-0.05926299 -0.01377346  0.22626472 -0.78187805]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 1360 is [True, False, False, False, True, False]
Scene graph at timestep 1360 is [True, False, False, False, True, False]
State prediction error at timestep 1360 is tensor(2.3873e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1361. State = [[-0.20695114 -0.09698159]]. Action = [[ 0.1890778   0.12461078  0.02189112 -0.09539986]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 1361 is [True, False, False, False, True, False]
Current timestep = 1362. State = [[-0.20698458 -0.09596914]]. Action = [[ 0.1703698   0.20819965 -0.1572227   0.29701114]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 1362 is [True, False, False, False, True, False]
Current timestep = 1363. State = [[-0.20685293 -0.09298232]]. Action = [[ 0.01699468 -0.06125654 -0.14902137  0.5220119 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 1363 is [True, False, False, False, True, False]
Current timestep = 1364. State = [[-0.20654273 -0.09195884]]. Action = [[ 0.23902601  0.03550547 -0.03037786 -0.82087183]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 1364 is [True, False, False, False, True, False]
Current timestep = 1365. State = [[-0.20499676 -0.0905768 ]]. Action = [[0.22198209 0.1709525  0.16131175 0.6106415 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 1365 is [True, False, False, False, True, False]
Current timestep = 1366. State = [[-0.20175102 -0.0871989 ]]. Action = [[-0.08939427  0.24750665 -0.22522262 -0.55295295]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 1366 is [True, False, False, False, True, False]
Current timestep = 1367. State = [[-0.20008421 -0.0827948 ]]. Action = [[-0.22746682 -0.01814489  0.24278444  0.743428  ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 1367 is [True, False, False, False, True, False]
Scene graph at timestep 1367 is [True, False, False, False, True, False]
State prediction error at timestep 1367 is tensor(7.3841e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1368. State = [[-0.20006374 -0.08017548]]. Action = [[ 0.20028299 -0.05009773 -0.22248013 -0.1786483 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 1368 is [True, False, False, False, True, False]
Current timestep = 1369. State = [[-0.19962564 -0.07891359]]. Action = [[-0.20957308 -0.02633666 -0.21538746 -0.3933257 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 1369 is [True, False, False, False, True, False]
Current timestep = 1370. State = [[-0.19946148 -0.07848448]]. Action = [[-0.03293876 -0.18949899 -0.07218793  0.8438325 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 1370 is [True, False, False, False, True, False]
Scene graph at timestep 1370 is [True, False, False, False, True, False]
State prediction error at timestep 1370 is tensor(3.4839e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1370 of -1
Current timestep = 1371. State = [[-0.19941337 -0.07899548]]. Action = [[-0.07954489 -0.14914733  0.16400644  0.84105635]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 1371 is [True, False, False, False, True, False]
Current timestep = 1372. State = [[-0.19930874 -0.08089525]]. Action = [[ 0.04636052 -0.11639634 -0.133216   -0.8329589 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 1372 is [True, False, False, False, True, False]
Current timestep = 1373. State = [[-0.19910972 -0.08281676]]. Action = [[0.19620997 0.10719293 0.19094718 0.635357  ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 1373 is [True, False, False, False, True, False]
Scene graph at timestep 1373 is [True, False, False, False, True, False]
State prediction error at timestep 1373 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1374. State = [[-0.19910398 -0.08303497]]. Action = [[ 0.02344525  0.08544531 -0.03311044  0.42899954]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 1374 is [True, False, False, False, True, False]
Scene graph at timestep 1374 is [True, False, False, False, True, False]
State prediction error at timestep 1374 is tensor(6.4549e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1375. State = [[-0.19910012 -0.08306626]]. Action = [[-0.05300209 -0.2166654  -0.12594742 -0.40366435]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 1375 is [True, False, False, False, True, False]
Current timestep = 1376. State = [[-0.1988963  -0.08456346]]. Action = [[-0.00149882  0.10707542 -0.20415875 -0.3670889 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 1376 is [True, False, False, False, True, False]
Scene graph at timestep 1376 is [True, False, False, False, True, False]
State prediction error at timestep 1376 is tensor(7.5259e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1377. State = [[-0.19888963 -0.08461563]]. Action = [[-0.07593323  0.1075508   0.23228323 -0.5309888 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 1377 is [True, False, False, False, True, False]
Current timestep = 1378. State = [[-0.19895297 -0.08433437]]. Action = [[ 0.1781224  -0.09702599  0.18644923  0.11521447]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 1378 is [True, False, False, False, True, False]
Current timestep = 1379. State = [[-0.19885983 -0.0844283 ]]. Action = [[ 0.1849792  -0.22323017 -0.08155385  0.9859896 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 1379 is [True, False, False, False, True, False]
Scene graph at timestep 1379 is [True, False, False, False, True, False]
State prediction error at timestep 1379 is tensor(5.0197e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1380. State = [[-0.19747114 -0.08728959]]. Action = [[ 0.09401429 -0.12834865  0.16659972  0.5193603 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 1380 is [True, False, False, False, True, False]
Current timestep = 1381. State = [[-0.1953721  -0.09083758]]. Action = [[0.05392313 0.05888134 0.1495868  0.8072033 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 1381 is [True, False, False, False, True, False]
Current timestep = 1382. State = [[-0.19325076 -0.09221061]]. Action = [[-0.22946449 -0.12823437  0.1671387  -0.68766344]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 1382 is [True, False, False, False, True, False]
Current timestep = 1383. State = [[-0.19303955 -0.09400976]]. Action = [[ 0.24783146  0.19048774  0.16632622 -0.05183721]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 1383 is [True, False, False, False, True, False]
Current timestep = 1384. State = [[-0.1912912  -0.09398033]]. Action = [[-0.0383728   0.09232628  0.19330704  0.0018003 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 1384 is [True, False, False, False, True, False]
Scene graph at timestep 1384 is [True, False, False, False, True, False]
State prediction error at timestep 1384 is tensor(5.8082e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1385. State = [[-0.189324   -0.09365653]]. Action = [[ 0.16522777 -0.23594491  0.05447641  0.46152687]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 1385 is [True, False, False, False, True, False]
Current timestep = 1386. State = [[-0.1865991  -0.09493043]]. Action = [[-0.18758154 -0.08422166  0.08992314  0.23394299]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 1386 is [True, False, False, False, True, False]
Current timestep = 1387. State = [[-0.18634343 -0.09655206]]. Action = [[0.22155464 0.20432568 0.23704869 0.2546482 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 1387 is [True, False, False, False, True, False]
Current timestep = 1388. State = [[-0.1848037  -0.09582519]]. Action = [[ 0.10673708  0.19635028 -0.03806044 -0.08233666]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 1388 is [True, False, False, False, True, False]
Scene graph at timestep 1388 is [True, False, False, False, True, False]
State prediction error at timestep 1388 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 1389. State = [[-0.18291198 -0.09328684]]. Action = [[-0.23359615  0.20339301 -0.19431204  0.79488504]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 1389 is [True, False, False, False, True, False]
Human Feedback received at timestep 1389 of 1
Current timestep = 1390. State = [[-0.1827359  -0.08945364]]. Action = [[ 0.06614605  0.2304      0.19913948 -0.588974  ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 1390 is [True, False, False, False, True, False]
Current timestep = 1391. State = [[-0.18256323 -0.08485247]]. Action = [[ 0.2347478  -0.13241635  0.0930284   0.50922203]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 1391 is [True, False, False, False, True, False]
Scene graph at timestep 1391 is [True, False, False, False, True, False]
State prediction error at timestep 1391 is tensor(7.8855e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1392. State = [[-0.18082312 -0.0835413 ]]. Action = [[-0.08741042 -0.16762167  0.05026907  0.14191484]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 1392 is [True, False, False, False, True, False]
Scene graph at timestep 1392 is [True, False, False, False, True, False]
State prediction error at timestep 1392 is tensor(2.8899e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1393. State = [[-0.18043838 -0.0837261 ]]. Action = [[-0.1418279   0.17149568 -0.09048504 -0.48804593]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 1393 is [True, False, False, False, True, False]
Current timestep = 1394. State = [[-0.18038237 -0.08290567]]. Action = [[ 0.05126908 -0.00113599  0.12668997 -0.5102093 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 1394 is [True, False, False, False, True, False]
Current timestep = 1395. State = [[-0.18035172 -0.08246364]]. Action = [[-0.13094018 -0.13443339  0.18965787 -0.7632172 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 1395 is [True, False, False, False, True, False]
Current timestep = 1396. State = [[-0.1803268  -0.08247561]]. Action = [[ 0.06671971  0.17198592  0.09590283 -0.91833234]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 1396 is [True, False, False, False, True, False]
Current timestep = 1397. State = [[-0.1803954  -0.08185177]]. Action = [[-0.18800233  0.18958077  0.05659711  0.9188819 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 1397 is [True, False, False, False, True, False]
Current timestep = 1398. State = [[-0.18077503 -0.07875548]]. Action = [[-0.12193377 -0.22769353  0.04889366  0.93007565]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 1398 is [True, False, False, False, True, False]
Scene graph at timestep 1398 is [True, False, False, False, True, False]
State prediction error at timestep 1398 is tensor(3.4368e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1399. State = [[-0.1813447  -0.07876553]]. Action = [[-0.2171261  -0.02505587  0.17422533 -0.40797544]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 1399 is [True, False, False, False, True, False]
Current timestep = 1400. State = [[-0.18363458 -0.07958014]]. Action = [[-0.03260648 -0.1026753  -0.13060264 -0.4755178 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 1400 is [True, False, False, False, True, False]
Current timestep = 1401. State = [[-0.18511379 -0.08114428]]. Action = [[ 0.07230854 -0.03438421 -0.14031082 -0.7718862 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 1401 is [True, False, False, False, True, False]
Scene graph at timestep 1401 is [True, False, False, False, True, False]
State prediction error at timestep 1401 is tensor(3.3632e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1402. State = [[-0.18537621 -0.08236576]]. Action = [[ 0.19283265 -0.14063303  0.24701446  0.1973871 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 1402 is [True, False, False, False, True, False]
Current timestep = 1403. State = [[-0.18536194 -0.08370547]]. Action = [[0.15414709 0.15978602 0.19474083 0.96659184]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 1403 is [True, False, False, False, True, False]
Current timestep = 1404. State = [[-0.18531065 -0.08354007]]. Action = [[0.19530839 0.02340221 0.21960899 0.6776751 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 1404 is [True, False, False, False, True, False]
Current timestep = 1405. State = [[-0.18407075 -0.08307466]]. Action = [[-0.03141309  0.20777059  0.02697048 -0.18418694]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 1405 is [True, False, False, False, True, False]
Scene graph at timestep 1405 is [True, False, False, False, True, False]
State prediction error at timestep 1405 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1406. State = [[-0.18372533 -0.08081905]]. Action = [[-0.2426479   0.10396415  0.05488268  0.18200362]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 1406 is [True, False, False, False, True, False]
Current timestep = 1407. State = [[-0.18381454 -0.07895662]]. Action = [[-0.13771658 -0.04259481  0.11724168  0.6346531 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 1407 is [True, False, False, False, True, False]
Current timestep = 1408. State = [[-0.18400389 -0.07772215]]. Action = [[-0.24785706  0.1437065  -0.20523247  0.01521575]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 1408 is [True, False, False, False, True, False]
Scene graph at timestep 1408 is [True, False, False, False, True, False]
State prediction error at timestep 1408 is tensor(5.1692e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1409. State = [[-0.18601106 -0.07466093]]. Action = [[ 0.09781584  0.12960899  0.06007153 -0.9694843 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 1409 is [True, False, False, False, True, False]
Current timestep = 1410. State = [[-0.18640737 -0.07137559]]. Action = [[ 0.18517485 -0.21686788  0.11074355 -0.07482505]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 1410 is [True, False, False, False, True, False]
Scene graph at timestep 1410 is [True, False, False, False, True, False]
State prediction error at timestep 1410 is tensor(3.8681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1411. State = [[-0.18649879 -0.07131653]]. Action = [[-0.17153507  0.09550911 -0.04184796  0.12493241]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 1411 is [True, False, False, False, True, False]
Scene graph at timestep 1411 is [True, False, False, False, True, False]
State prediction error at timestep 1411 is tensor(3.0631e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1412. State = [[-0.18669716 -0.07101076]]. Action = [[-0.16773155 -0.11823922 -0.1876831  -0.904509  ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 1412 is [True, False, False, False, True, False]
Current timestep = 1413. State = [[-0.18771061 -0.07165453]]. Action = [[-0.1802895   0.23164982 -0.18408266  0.48308635]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 1413 is [True, False, False, False, True, False]
Current timestep = 1414. State = [[-0.1894279 -0.0696409]]. Action = [[ 0.23404801 -0.21896514 -0.05211276 -0.30642664]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 1414 is [True, False, False, False, True, False]
Scene graph at timestep 1414 is [True, False, False, False, True, False]
State prediction error at timestep 1414 is tensor(6.2608e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1415. State = [[-0.18951835 -0.07018122]]. Action = [[-0.11397648 -0.03773943 -0.19938792 -0.9065916 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 1415 is [True, False, False, False, True, False]
Current timestep = 1416. State = [[-0.18996604 -0.07100102]]. Action = [[-0.0719997   0.03826267  0.04998785  0.70284605]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 1416 is [True, False, False, False, True, False]
Current timestep = 1417. State = [[-0.19074348 -0.07170713]]. Action = [[-0.05441228 -0.20787339 -0.16466063  0.54734826]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 1417 is [True, False, False, False, True, False]
Scene graph at timestep 1417 is [True, False, False, False, True, False]
State prediction error at timestep 1417 is tensor(4.1626e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1417 of 1
Current timestep = 1418. State = [[-0.19201869 -0.07408572]]. Action = [[-0.18337205  0.02710465 -0.1186039  -0.84882194]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 1418 is [True, False, False, False, True, False]
Current timestep = 1419. State = [[-0.19377574 -0.07521981]]. Action = [[ 0.1088433   0.22461462  0.06823426 -0.56429225]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 1419 is [True, False, False, False, True, False]
Scene graph at timestep 1419 is [True, False, False, False, True, False]
State prediction error at timestep 1419 is tensor(9.1020e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1420. State = [[-0.19510506 -0.07360283]]. Action = [[ 1.2900120e-01  1.3166916e-01 -2.6030838e-04  2.9260957e-01]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 1420 is [True, False, False, False, True, False]
Current timestep = 1421. State = [[-0.1952714 -0.0714383]]. Action = [[0.14859408 0.03119278 0.15111256 0.88923645]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 1421 is [True, False, False, False, True, False]
Current timestep = 1422. State = [[-0.1954208  -0.06998923]]. Action = [[-0.18485792 -0.21463645 -0.02609293 -0.78745383]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 1422 is [True, False, False, False, True, False]
Current timestep = 1423. State = [[-0.19539753 -0.07028338]]. Action = [[ 0.09642416  0.03376672 -0.16107371  0.852489  ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 1423 is [True, False, False, False, True, False]
Current timestep = 1424. State = [[-0.19536756 -0.07059104]]. Action = [[ 0.16755894 -0.2406526   0.19185421 -0.8138924 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 1424 is [True, False, False, False, True, False]
Current timestep = 1425. State = [[-0.1951671  -0.07241459]]. Action = [[ 0.04943356 -0.10774513  0.00873682  0.63457704]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 1425 is [True, False, False, False, True, False]
Current timestep = 1426. State = [[-0.19502336 -0.07422996]]. Action = [[-0.16155405  0.20057386 -0.24223903  0.49835503]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 1426 is [True, False, False, False, True, False]
Current timestep = 1427. State = [[-0.19503708 -0.07417427]]. Action = [[ 0.04647157  0.01423717 -0.09732679  0.05985069]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 1427 is [True, False, False, False, True, False]
Current timestep = 1428. State = [[-0.19508973 -0.07416931]]. Action = [[ 0.20686758 -0.04867816 -0.0170192   0.3364693 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 1428 is [True, False, False, False, True, False]
Current timestep = 1429. State = [[-0.19507171 -0.07425641]]. Action = [[ 0.0317452  -0.17125173 -0.03777805 -0.16676605]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 1429 is [True, False, False, False, True, False]
Scene graph at timestep 1429 is [True, False, False, False, True, False]
State prediction error at timestep 1429 is tensor(2.9255e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1430. State = [[-0.19458474 -0.07576452]]. Action = [[ 0.21738216 -0.22087613 -0.07188109  0.3416102 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 1430 is [True, False, False, False, True, False]
Current timestep = 1431. State = [[-0.19239436 -0.0796987 ]]. Action = [[-0.18673472  0.24709928 -0.18689428 -0.37193596]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 1431 is [True, False, False, False, True, False]
Current timestep = 1432. State = [[-0.19207667 -0.0802607 ]]. Action = [[-0.22223683 -0.22033544 -0.20313899  0.61386776]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 1432 is [True, False, False, False, True, False]
Current timestep = 1433. State = [[-0.19215947 -0.08210956]]. Action = [[-0.11375004 -0.00267927  0.01550511 -0.7078906 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 1433 is [True, False, False, False, True, False]
Current timestep = 1434. State = [[-0.19251335 -0.08360936]]. Action = [[-0.18460992  0.07377177  0.13599306 -0.6586656 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 1434 is [True, False, False, False, True, False]
Current timestep = 1435. State = [[-0.19418381 -0.08464265]]. Action = [[-0.19405738 -0.20845425 -0.23446772 -0.56533104]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 1435 is [True, False, False, False, True, False]
Current timestep = 1436. State = [[-0.19735904 -0.08827066]]. Action = [[ 0.14183593  0.08621085 -0.21496032 -0.5819467 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 1436 is [True, False, False, False, True, False]
Current timestep = 1437. State = [[-0.19857842 -0.08938644]]. Action = [[ 0.00230035  0.22113419  0.21638119 -0.7780106 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 1437 is [True, False, False, False, True, False]
Scene graph at timestep 1437 is [True, False, False, False, True, False]
State prediction error at timestep 1437 is tensor(3.1736e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1438. State = [[-0.19934623 -0.08778498]]. Action = [[0.01555732 0.13654965 0.06241158 0.33780026]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 1438 is [True, False, False, False, True, False]
Current timestep = 1439. State = [[-0.19982985 -0.0855521 ]]. Action = [[ 0.07726479  0.13557065  0.13689452 -0.14943182]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 1439 is [True, False, False, False, True, False]
Current timestep = 1440. State = [[-0.20010348 -0.0829217 ]]. Action = [[-0.1388836  -0.0676128  -0.10371968 -0.6044231 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 1440 is [True, False, False, False, True, False]
Scene graph at timestep 1440 is [True, False, False, False, True, False]
State prediction error at timestep 1440 is tensor(5.9098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1441. State = [[-0.20101602 -0.08159892]]. Action = [[-0.15589035  0.0781031   0.24082655 -0.17543489]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 1441 is [True, False, False, False, True, False]
Current timestep = 1442. State = [[-0.2024029  -0.08004253]]. Action = [[ 0.04687637 -0.14726423 -0.06537491 -0.88933915]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 1442 is [True, False, False, False, True, False]
Current timestep = 1443. State = [[-0.20326065 -0.07988407]]. Action = [[ 0.1919663   0.11874706 -0.06028751  0.5458493 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 1443 is [True, False, False, False, True, False]
Current timestep = 1444. State = [[-0.20328052 -0.07950906]]. Action = [[ 0.23086035 -0.18967925 -0.13460594 -0.5750091 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 1444 is [True, False, False, False, True, False]
Current timestep = 1445. State = [[-0.20316741 -0.07982681]]. Action = [[ 0.0785892  -0.07122874 -0.22202185 -0.41517317]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 1445 is [True, False, False, False, True, False]
Current timestep = 1446. State = [[-0.20250422 -0.08064244]]. Action = [[ 0.23289514 -0.2322375   0.00534701  0.5569501 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 1446 is [True, False, False, False, True, False]
Scene graph at timestep 1446 is [True, False, False, False, True, False]
State prediction error at timestep 1446 is tensor(8.7281e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1447. State = [[-0.20011365 -0.08380678]]. Action = [[-2.26935238e-01 -1.06200576e-04 -1.61808565e-01  4.71747279e-01]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 1447 is [True, False, False, False, True, False]
Current timestep = 1448. State = [[-0.19983588 -0.08555178]]. Action = [[ 0.15163362  0.0126234  -0.05571324 -0.43949413]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 1448 is [True, False, False, False, True, False]
Human Feedback received at timestep 1448 of -1
Current timestep = 1449. State = [[-0.19930775 -0.08638935]]. Action = [[0.0960089  0.21393985 0.02402055 0.7494235 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 1449 is [True, False, False, False, True, False]
Current timestep = 1450. State = [[-0.19843751 -0.08557296]]. Action = [[-0.04452094 -0.20128943 -0.15697378 -0.61991733]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 1450 is [True, False, False, False, True, False]
Scene graph at timestep 1450 is [True, False, False, False, True, False]
State prediction error at timestep 1450 is tensor(3.0095e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1451. State = [[-0.19781183 -0.08671472]]. Action = [[ 0.07833031 -0.14284901 -0.22216342 -0.31034046]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 1451 is [True, False, False, False, True, False]
Scene graph at timestep 1451 is [True, False, False, False, True, False]
State prediction error at timestep 1451 is tensor(7.6681e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1452. State = [[-0.19689256 -0.08874881]]. Action = [[ 0.09343424  0.1568411   0.10866186 -0.8407939 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 1452 is [True, False, False, False, True, False]
Scene graph at timestep 1452 is [True, False, False, False, True, False]
State prediction error at timestep 1452 is tensor(1.1043e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1453. State = [[-0.19514544 -0.08911008]]. Action = [[ 0.09917438 -0.20486043  0.00225678 -0.87935394]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 1453 is [True, False, False, False, True, False]
Current timestep = 1454. State = [[-0.19323537 -0.09113755]]. Action = [[-0.17232679  0.18682384  0.22081894 -0.41269046]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 1454 is [True, False, False, False, True, False]
Current timestep = 1455. State = [[-0.19304675 -0.09058268]]. Action = [[ 0.17874402  0.12183067 -0.04724875  0.73326266]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 1455 is [True, False, False, False, True, False]
Current timestep = 1456. State = [[-0.19124803 -0.08945406]]. Action = [[ 0.02622914 -0.16323589 -0.09716558  0.6366873 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 1456 is [True, False, False, False, True, False]
Current timestep = 1457. State = [[-0.19002794 -0.09006492]]. Action = [[-0.0388954   0.13416922  0.15773076  0.9408716 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 1457 is [True, False, False, False, True, False]
Human Feedback received at timestep 1457 of -1
Current timestep = 1458. State = [[-0.18899405 -0.08920581]]. Action = [[ 0.02310908  0.15116096  0.1774053  -0.1797812 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 1458 is [True, False, False, False, True, False]
Current timestep = 1459. State = [[-0.18834333 -0.08738127]]. Action = [[ 0.1699062   0.05760372 -0.22195974 -0.68348557]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 1459 is [True, False, False, False, True, False]
Scene graph at timestep 1459 is [True, False, False, False, True, False]
State prediction error at timestep 1459 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1460. State = [[-0.185821   -0.08539578]]. Action = [[ 0.23721349 -0.19266595  0.09471932  0.5342957 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 1460 is [True, False, False, False, True, False]
Current timestep = 1461. State = [[-0.1825587  -0.08577561]]. Action = [[-0.20330818  0.03723034 -0.03606015  0.33048284]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 1461 is [True, False, False, False, True, False]
Current timestep = 1462. State = [[-0.18137555 -0.08582465]]. Action = [[-0.16757616  0.21797475  0.1145618   0.77069545]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 1462 is [True, False, False, False, True, False]
Current timestep = 1463. State = [[-0.18159467 -0.08398562]]. Action = [[ 0.22827479 -0.17172348  0.21473944 -0.23623133]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 1463 is [True, False, False, False, True, False]
Current timestep = 1464. State = [[-0.18096387 -0.08422431]]. Action = [[-0.17187645  0.10808793  0.13153759 -0.49621373]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 1464 is [True, False, False, False, True, False]
Scene graph at timestep 1464 is [True, False, False, False, True, False]
State prediction error at timestep 1464 is tensor(2.3723e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1464 of 1
Current timestep = 1465. State = [[-0.1810255  -0.08373825]]. Action = [[-0.10189803 -0.01224317 -0.13574517 -0.4457276 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 1465 is [True, False, False, False, True, False]
Current timestep = 1466. State = [[-0.18107331 -0.08358176]]. Action = [[-0.16695409 -0.17954442  0.17052534 -0.07969493]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 1466 is [True, False, False, False, True, False]
Current timestep = 1467. State = [[-0.18130988 -0.08486365]]. Action = [[-0.07413217 -0.24151458 -0.0550084   0.71626484]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 1467 is [True, False, False, False, True, False]
Current timestep = 1468. State = [[-0.18239756 -0.08846299]]. Action = [[-0.02323824 -0.2278025  -0.12506865 -0.6025479 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 1468 is [True, False, False, False, True, False]
Current timestep = 1469. State = [[-0.18344197 -0.09341051]]. Action = [[ 0.06327009  0.1762901  -0.01338921  0.16785872]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 1469 is [True, False, False, False, True, False]
Current timestep = 1470. State = [[-0.18375726 -0.09426834]]. Action = [[-0.17580533  0.22750878 -0.04940349 -0.75420403]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 1470 is [True, False, False, False, True, False]
Current timestep = 1471. State = [[-0.18472105 -0.09290373]]. Action = [[0.19225198 0.10159034 0.00327834 0.06690252]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 1471 is [True, False, False, False, True, False]
Current timestep = 1472. State = [[-0.18480827 -0.09156061]]. Action = [[-0.12302046 -0.14124925  0.07021394  0.05445528]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 1472 is [True, False, False, False, True, False]
Current timestep = 1473. State = [[-0.18506578 -0.09164558]]. Action = [[-0.20664433 -0.18188186 -0.1550435   0.19580495]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 1473 is [True, False, False, False, True, False]
Current timestep = 1474. State = [[-0.18661167 -0.09412611]]. Action = [[ 0.1409359  -0.14411467 -0.18639417 -0.58311284]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 1474 is [True, False, False, False, True, False]
Current timestep = 1475. State = [[-0.1868118  -0.09669208]]. Action = [[-0.08537114 -0.0657953  -0.19961397 -0.4364177 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 1475 is [True, False, False, False, True, False]
Current timestep = 1476. State = [[-0.18774128 -0.09930694]]. Action = [[-0.02554013 -0.18259533 -0.06170835  0.8732065 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 1476 is [True, False, False, False, True, False]
Current timestep = 1477. State = [[-0.18849878 -0.10242783]]. Action = [[-0.07044765 -0.00845927 -0.21887998  0.2710153 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 1477 is [True, False, False, False, True, False]
Scene graph at timestep 1477 is [True, False, False, False, True, False]
State prediction error at timestep 1477 is tensor(3.7792e-08, grad_fn=<MseLossBackward0>)
Current timestep = 1478. State = [[-0.18996835 -0.1050203 ]]. Action = [[ 0.08849055 -0.1051905  -0.1534151   0.28447044]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 1478 is [True, False, False, False, True, False]
Current timestep = 1479. State = [[-0.19041166 -0.10779567]]. Action = [[ 0.08960345  0.13676596 -0.03860626 -0.9403675 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 1479 is [True, False, False, False, True, False]
Current timestep = 1480. State = [[-0.19049115 -0.10813716]]. Action = [[ 0.09367988 -0.07652849 -0.02521454  0.42559493]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 1480 is [True, False, False, False, True, False]
Current timestep = 1481. State = [[-0.19048996 -0.10901707]]. Action = [[ 0.08643863  0.02471501 -0.01164025  0.62495124]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 1481 is [True, False, False, False, True, False]
Current timestep = 1482. State = [[-0.19041663 -0.10915063]]. Action = [[-0.23815453  0.14440554  0.02090806 -0.98700756]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 1482 is [True, False, False, False, True, False]
Scene graph at timestep 1482 is [True, False, False, False, True, False]
State prediction error at timestep 1482 is tensor(6.8487e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1483. State = [[-0.190474   -0.10878281]]. Action = [[ 0.16310042  0.24595791 -0.14802924 -0.8463934 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 1483 is [True, False, False, False, True, False]
Current timestep = 1484. State = [[-0.1905668  -0.10690046]]. Action = [[-0.22543406 -0.1443972   0.20022154 -0.44540548]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 1484 is [True, False, False, False, True, False]
Current timestep = 1485. State = [[-0.19059302 -0.10685594]]. Action = [[ 0.22833669 -0.08531103  0.1314078   0.35544264]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 1485 is [True, False, False, False, True, False]
Current timestep = 1486. State = [[-0.19050384 -0.10717978]]. Action = [[ 0.09631926 -0.02074637  0.05255994  0.5677278 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 1486 is [True, False, False, False, True, False]
Current timestep = 1487. State = [[-0.19049507 -0.10728633]]. Action = [[ 0.04608977 -0.02837306 -0.04619567  0.5479634 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 1487 is [True, False, False, False, True, False]
Current timestep = 1488. State = [[-0.19038257 -0.10765961]]. Action = [[ 0.1722318  -0.08669303 -0.1545704   0.8616506 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 1488 is [True, False, False, False, True, False]
Current timestep = 1489. State = [[-0.18874541 -0.10858756]]. Action = [[0.03387317 0.06183645 0.07836753 0.8653306 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 1489 is [True, False, False, False, True, False]
Scene graph at timestep 1489 is [True, False, False, False, True, False]
State prediction error at timestep 1489 is tensor(9.4649e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1490. State = [[-0.18748428 -0.10861235]]. Action = [[-0.15795588  0.05415225 -0.05909903  0.1084007 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 1490 is [True, False, False, False, True, False]
Current timestep = 1491. State = [[-0.18747188 -0.10859263]]. Action = [[-0.01123354  0.00369477  0.20018518  0.2148192 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 1491 is [True, False, False, False, True, False]
Current timestep = 1492. State = [[-0.18750776 -0.10848098]]. Action = [[-0.21717568  0.23585042 -0.05467413  0.30434895]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 1492 is [True, False, False, False, True, False]
Current timestep = 1493. State = [[-0.18772054 -0.10655005]]. Action = [[ 0.1847331  -0.00353488 -0.05302224  0.26973498]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 1493 is [True, False, False, False, True, False]
Current timestep = 1494. State = [[-0.18749738 -0.1052927 ]]. Action = [[ 0.22824547  0.13234645  0.05411464 -0.09517288]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 1494 is [True, False, False, False, True, False]
Current timestep = 1495. State = [[-0.18620403 -0.10273295]]. Action = [[ 0.18344045 -0.17359841 -0.15133293 -0.8905954 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 1495 is [True, False, False, False, True, False]
Current timestep = 1496. State = [[-0.18425728 -0.10238515]]. Action = [[-0.20121287  0.01630425  0.08736345 -0.48988187]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 1496 is [True, False, False, False, True, False]
Scene graph at timestep 1496 is [True, False, False, False, True, False]
State prediction error at timestep 1496 is tensor(2.5102e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1497. State = [[-0.18408525 -0.10214855]]. Action = [[ 0.13982907 -0.10544088  0.09995902  0.21925163]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 1497 is [True, False, False, False, True, False]
Current timestep = 1498. State = [[-0.1833625  -0.10271423]]. Action = [[ 0.18208805 -0.18953723  0.03574806 -0.24858403]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 1498 is [True, False, False, False, True, False]
Current timestep = 1499. State = [[-0.1812191  -0.10485644]]. Action = [[ 0.20225954 -0.15257695  0.0245814   0.6445893 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 1499 is [True, False, False, False, True, False]
Current timestep = 1500. State = [[-0.17520814 -0.11065238]]. Action = [[-0.17448431 -0.21060383  0.07825506  0.9138112 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 1500 is [True, False, False, False, True, False]
Current timestep = 1501. State = [[-0.17384481 -0.11450992]]. Action = [[ 0.20964396 -0.06537502 -0.1918691   0.02908492]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 1501 is [True, False, False, False, True, False]
Human Feedback received at timestep 1501 of 1
Current timestep = 1502. State = [[-0.17065285 -0.11816687]]. Action = [[ 0.22437006  0.2110011  -0.02913712 -0.68689525]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 1502 is [True, False, False, False, True, False]
Scene graph at timestep 1502 is [True, False, False, False, True, False]
State prediction error at timestep 1502 is tensor(3.6069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1503. State = [[-0.1660291 -0.1177702]]. Action = [[-0.15774848  0.10462323  0.05831069  0.24075937]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 1503 is [True, False, False, False, True, False]
Current timestep = 1504. State = [[-0.16418014 -0.11683868]]. Action = [[ 0.12406373 -0.24282934 -0.1891307  -0.44486064]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 1504 is [True, False, False, False, True, False]
Current timestep = 1505. State = [[-0.2151905  -0.06297965]]. Action = [[ 0.05152571  0.15035355 -0.1990302  -0.83957094]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 1505 is [True, False, False, False, True, False]
Scene graph at timestep 1505 is [True, False, False, False, True, False]
State prediction error at timestep 1505 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1505 of 1
Current timestep = 1506. State = [[-0.21094322 -0.06759287]]. Action = [[ 0.24558681  0.17078859 -0.14830993 -0.9407842 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1506 is [True, False, False, False, True, False]
Scene graph at timestep 1506 is [True, False, False, False, True, False]
State prediction error at timestep 1506 is tensor(1.9833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1507. State = [[-0.20898606 -0.06679072]]. Action = [[-0.1318606   0.20600653 -0.2122859   0.6167097 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1507 is [True, False, False, False, True, False]
Current timestep = 1508. State = [[-0.20857288 -0.06434115]]. Action = [[0.17189169 0.01605824 0.01395041 0.31730473]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1508 is [True, False, False, False, True, False]
Current timestep = 1509. State = [[-0.2076355  -0.06283628]]. Action = [[-0.19012852 -0.09847395 -0.22369975 -0.24855858]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 1509 is [True, False, False, False, True, False]
Current timestep = 1510. State = [[-0.20762116 -0.06285973]]. Action = [[-0.02230616  0.14806807  0.20264041  0.15562415]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1510 is [True, False, False, False, True, False]
Current timestep = 1511. State = [[-0.2076822  -0.06182359]]. Action = [[-0.11573179 -0.1931063  -0.16203697  0.43360758]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 1511 is [True, False, False, False, True, False]
Current timestep = 1512. State = [[-0.20789848 -0.06195525]]. Action = [[ 0.03004682 -0.06886895  0.13765246 -0.8157529 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1512 is [True, False, False, False, True, False]
Current timestep = 1513. State = [[-0.20793025 -0.06263122]]. Action = [[ 0.15463278  0.23190778  0.05047739 -0.29358375]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 1513 is [True, False, False, False, True, False]
Current timestep = 1514. State = [[-0.20789813 -0.06140041]]. Action = [[-0.11486012  0.1915308   0.13943112  0.19034028]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 1514 is [True, False, False, False, True, False]
Current timestep = 1515. State = [[-0.20812005 -0.05898268]]. Action = [[-0.1053964  -0.13894674 -0.19913796  0.43946838]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1515 is [True, False, False, False, True, False]
Current timestep = 1516. State = [[-0.20818111 -0.05868319]]. Action = [[ 0.12497199  0.11928159 -0.18694954 -0.4005282 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1516 is [True, False, False, False, True, False]
Current timestep = 1517. State = [[-0.20828013 -0.05762143]]. Action = [[-0.147897    0.14617974 -0.05554919  0.32756817]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1517 is [True, False, False, False, True, False]
Current timestep = 1518. State = [[-0.20875204 -0.05489657]]. Action = [[-0.19121978  0.09671664 -0.15001339  0.24360275]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 1518 is [True, False, False, False, True, False]
Current timestep = 1519. State = [[-0.20975308 -0.05149893]]. Action = [[ 0.01197508  0.05779681 -0.23466788 -0.7504376 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 1519 is [True, False, False, False, True, False]
Current timestep = 1520. State = [[-0.21048501 -0.04829169]]. Action = [[-0.05773027  0.12459862  0.06085992  0.18757963]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 1520 is [True, False, False, False, True, False]
Scene graph at timestep 1520 is [True, False, False, False, True, False]
State prediction error at timestep 1520 is tensor(2.3454e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1521. State = [[-0.21147949 -0.04452277]]. Action = [[-0.22489917 -0.00301646  0.05963743 -0.7409688 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1521 is [True, False, False, False, True, False]
Scene graph at timestep 1521 is [True, False, False, False, True, False]
State prediction error at timestep 1521 is tensor(1.3693e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1522. State = [[-0.21346454 -0.04204227]]. Action = [[ 0.22686961  0.02066928 -0.08145374 -0.0880487 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 1522 is [True, False, False, False, True, False]
Current timestep = 1523. State = [[-0.21372911 -0.04058981]]. Action = [[ 0.15772504  0.20149648 -0.05652116 -0.23687339]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 1523 is [True, False, False, False, True, False]
Current timestep = 1524. State = [[-0.2142408  -0.03761926]]. Action = [[ 0.0713414   0.03203204 -0.14988534 -0.22310561]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 1524 is [True, False, False, False, True, False]
Current timestep = 1525. State = [[-0.21422753 -0.03561739]]. Action = [[-0.16096976 -0.10986091 -0.11407885  0.9822583 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 1525 is [True, False, False, False, True, False]
Current timestep = 1526. State = [[-0.21436669 -0.03505903]]. Action = [[ 0.09392345  0.14340621  0.13838905 -0.84066063]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 1526 is [True, False, False, False, True, False]
Current timestep = 1527. State = [[-0.21462254 -0.03336821]]. Action = [[ 0.18808323 -0.04522543  0.03441674  0.96799254]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 1527 is [True, False, False, False, True, False]
Current timestep = 1528. State = [[-0.21354404 -0.03298608]]. Action = [[ 0.13792357 -0.1836637  -0.02846508  0.20655596]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 1528 is [True, False, False, False, True, False]
Human Feedback received at timestep 1528 of -1
Current timestep = 1529. State = [[-0.21203686 -0.03336585]]. Action = [[-0.05764246  0.03930739 -0.17362483  0.5497767 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 1529 is [True, False, False, False, True, False]
Current timestep = 1530. State = [[-0.21151215 -0.03335372]]. Action = [[0.24637902 0.17796865 0.12716222 0.05040586]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 1530 is [True, False, False, False, True, False]
Current timestep = 1531. State = [[-0.20912096 -0.03231421]]. Action = [[ 0.02059644 -0.14133269 -0.08771169  0.74143434]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 1531 is [True, False, False, False, True, False]
Current timestep = 1532. State = [[-0.20674993 -0.03230065]]. Action = [[-0.17918485 -0.01914446 -0.19451632  0.83164227]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 1532 is [True, False, False, False, True, False]
Current timestep = 1533. State = [[-0.20617148 -0.03245661]]. Action = [[-0.16108094 -0.12251815  0.19716635 -0.97374535]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 1533 is [True, False, False, False, True, False]
Scene graph at timestep 1533 is [True, False, False, False, True, False]
State prediction error at timestep 1533 is tensor(2.4472e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1534. State = [[-0.2063147  -0.03340202]]. Action = [[ 0.13719392  0.02153352 -0.15024865  0.6866183 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 1534 is [True, False, False, False, True, False]
Current timestep = 1535. State = [[-0.20613517 -0.033674  ]]. Action = [[ 0.15834022 -0.05849907  0.07357711 -0.04611528]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 1535 is [True, False, False, False, True, False]
Current timestep = 1536. State = [[-0.20487058 -0.03422984]]. Action = [[ 0.22664496  0.04596445 -0.12711874 -0.32185638]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 1536 is [True, False, False, False, True, False]
Current timestep = 1537. State = [[-0.201582   -0.03429633]]. Action = [[ 0.06110981  0.11234421 -0.01735416 -0.48671985]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 1537 is [True, False, False, False, True, False]
Current timestep = 1538. State = [[-0.19794105 -0.03399057]]. Action = [[-0.17784362  0.08119801  0.05685458 -0.9118881 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 1538 is [True, False, False, False, True, False]
Scene graph at timestep 1538 is [True, False, False, False, True, False]
State prediction error at timestep 1538 is tensor(8.3979e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1539. State = [[-0.19666895 -0.03325969]]. Action = [[-0.17891848 -0.10854633 -0.00399141  0.8115816 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 1539 is [True, False, False, False, True, False]
Current timestep = 1540. State = [[-0.19673128 -0.03332686]]. Action = [[ 0.22720402  0.05714774 -0.03016536 -0.25595462]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 1540 is [True, False, False, False, True, False]
Human Feedback received at timestep 1540 of 1
Current timestep = 1541. State = [[-0.19563295 -0.03344542]]. Action = [[-0.08203208 -0.21957722  0.05010882  0.9707329 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 1541 is [True, False, False, False, True, False]
Current timestep = 1542. State = [[-0.19560829 -0.03472726]]. Action = [[-0.18758678  0.06414673 -0.02235451  0.2214185 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1542 is [True, False, False, False, True, False]
Current timestep = 1543. State = [[-0.19596188 -0.03493927]]. Action = [[-0.09722465  0.20413846  0.1125316   0.22074962]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1543 is [True, False, False, False, True, False]
Scene graph at timestep 1543 is [True, False, False, False, True, False]
State prediction error at timestep 1543 is tensor(7.0802e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1544. State = [[-0.19652602 -0.03356946]]. Action = [[ 0.12068924 -0.18172063  0.03358585 -0.441517  ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1544 is [True, False, False, False, True, False]
Human Feedback received at timestep 1544 of 1
Current timestep = 1545. State = [[-0.19659993 -0.03398786]]. Action = [[-0.0145746   0.02987295 -0.2144809   0.62046885]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1545 is [True, False, False, False, True, False]
Current timestep = 1546. State = [[-0.1966826  -0.03397914]]. Action = [[-0.12218139  0.1242575   0.1639064   0.06154668]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1546 is [True, False, False, False, True, False]
Current timestep = 1547. State = [[-0.19706394 -0.0334488 ]]. Action = [[-0.19727114  0.19969457  0.24637097  0.37417746]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1547 is [True, False, False, False, True, False]
Current timestep = 1548. State = [[-0.19804464 -0.03022867]]. Action = [[ 0.23757076 -0.19243366  0.11152712  0.49600613]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1548 is [True, False, False, False, True, False]
Current timestep = 1549. State = [[-0.1980773  -0.03033477]]. Action = [[-0.24755834  0.02597621 -0.1456248   0.31738758]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 1549 is [True, False, False, False, True, False]
Current timestep = 1550. State = [[-0.19834828 -0.03023075]]. Action = [[-0.05313659  0.00987154  0.1777708  -0.31141114]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 1550 is [True, False, False, False, True, False]
Current timestep = 1551. State = [[-0.19857368 -0.03013242]]. Action = [[ 0.12286073 -0.14509293 -0.24643995  0.6188483 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 1551 is [True, False, False, False, True, False]
Current timestep = 1552. State = [[-0.19877344 -0.0314309 ]]. Action = [[ 0.10789001 -0.00887379 -0.12670338 -0.9221273 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 1552 is [True, False, False, False, True, False]
Scene graph at timestep 1552 is [True, False, False, False, True, False]
State prediction error at timestep 1552 is tensor(7.1849e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1553. State = [[-0.19878384 -0.03198726]]. Action = [[0.21200734 0.09283713 0.170685   0.21681643]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 1553 is [True, False, False, False, True, False]
Scene graph at timestep 1553 is [True, False, False, False, True, False]
State prediction error at timestep 1553 is tensor(4.9374e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1554. State = [[-0.19864294 -0.03199419]]. Action = [[ 0.13015443 -0.1464572  -0.05705665 -0.8685853 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 1554 is [True, False, False, False, True, False]
Scene graph at timestep 1554 is [True, False, False, False, True, False]
State prediction error at timestep 1554 is tensor(7.6023e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1555. State = [[-0.19771963 -0.03267629]]. Action = [[ 0.14302981  0.21739107 -0.08653693 -0.73045903]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 1555 is [True, False, False, False, True, False]
Current timestep = 1556. State = [[-0.19542468 -0.03207986]]. Action = [[ 0.19866312 -0.1850268   0.11572888 -0.4948895 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 1556 is [True, False, False, False, True, False]
Scene graph at timestep 1556 is [True, False, False, False, True, False]
State prediction error at timestep 1556 is tensor(5.6737e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1557. State = [[-0.1914669  -0.03256144]]. Action = [[-0.0929731   0.2433202   0.07861805  0.7523544 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 1557 is [True, False, False, False, True, False]
Current timestep = 1558. State = [[-0.18954371 -0.03133749]]. Action = [[ 0.16843599 -0.02742553 -0.05908877 -0.4955486 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1558 is [True, False, False, False, True, False]
Human Feedback received at timestep 1558 of 1
Current timestep = 1559. State = [[-0.18791181 -0.03084329]]. Action = [[-0.19528064  0.10113987 -0.22947982  0.6677394 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 1559 is [True, False, False, False, True, False]
Current timestep = 1560. State = [[-0.18815203 -0.02972204]]. Action = [[ 0.08379838 -0.13234504  0.01177478  0.78714263]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 1560 is [True, False, False, False, True, False]
Current timestep = 1561. State = [[-0.18783046 -0.02990785]]. Action = [[-0.04137099 -0.23444842 -0.19973958  0.13576996]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 1561 is [True, False, False, False, True, False]
Current timestep = 1562. State = [[-0.18735169 -0.03184743]]. Action = [[-0.21387169  0.05439475 -0.15601629  0.5916071 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 1562 is [True, False, False, False, True, False]
Scene graph at timestep 1562 is [True, False, False, False, True, False]
State prediction error at timestep 1562 is tensor(2.0109e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1563. State = [[-0.18749438 -0.03250493]]. Action = [[-0.0746929   0.04061943  0.06204894 -0.15285492]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 1563 is [True, False, False, False, True, False]
Current timestep = 1564. State = [[-0.18788487 -0.03248666]]. Action = [[-0.12298298 -0.18732366  0.22322142  0.26807415]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 1564 is [True, False, False, False, True, False]
Current timestep = 1565. State = [[-0.18859927 -0.03482463]]. Action = [[-0.02699935 -0.08580297  0.19137713 -0.7934422 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 1565 is [True, False, False, False, True, False]
Current timestep = 1566. State = [[-0.18934636 -0.03676412]]. Action = [[-0.20550995  0.11770275 -0.06612553 -0.9168795 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1566 is [True, False, False, False, True, False]
Current timestep = 1567. State = [[-0.19127816 -0.03693972]]. Action = [[-0.0547398  -0.03775403  0.19238982 -0.9083605 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 1567 is [True, False, False, False, True, False]
Current timestep = 1568. State = [[-0.19266775 -0.03743994]]. Action = [[ 0.2106792  -0.02695516  0.02934164 -0.48531556]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 1568 is [True, False, False, False, True, False]
Current timestep = 1569. State = [[-0.19261616 -0.03769517]]. Action = [[ 0.1984489   0.18198377 -0.1920673   0.8250563 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1569 is [True, False, False, False, True, False]
Scene graph at timestep 1569 is [True, False, False, False, True, False]
State prediction error at timestep 1569 is tensor(3.6768e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1570. State = [[-0.1925243  -0.03704895]]. Action = [[-0.15158777  0.01553068  0.0946511   0.24789429]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1570 is [True, False, False, False, True, False]
Current timestep = 1571. State = [[-0.19261052 -0.03633287]]. Action = [[ 0.22912145  0.05288893 -0.17288654  0.45035756]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 1571 is [True, False, False, False, True, False]
Current timestep = 1572. State = [[-0.19254425 -0.03557999]]. Action = [[-0.08012973  0.08035383 -0.17514619 -0.41585362]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1572 is [True, False, False, False, True, False]
Current timestep = 1573. State = [[-0.19249363 -0.03460672]]. Action = [[ 0.2446979  -0.23500296 -0.24170744 -0.5625438 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1573 is [True, False, False, False, True, False]
Current timestep = 1574. State = [[-0.19093968 -0.03558975]]. Action = [[-0.01005307 -0.00882402 -0.11446312 -0.14646089]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 1574 is [True, False, False, False, True, False]
Current timestep = 1575. State = [[-0.19051369 -0.03603854]]. Action = [[-0.11834535  0.21359381 -0.05530593  0.92329407]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1575 is [True, False, False, False, True, False]
Current timestep = 1576. State = [[-0.1906563  -0.03449048]]. Action = [[-0.22334777  0.11037165 -0.21275789 -0.45234835]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 1576 is [True, False, False, False, True, False]
Current timestep = 1577. State = [[-0.19111633 -0.03254694]]. Action = [[-0.06642014 -0.04039609  0.10007977 -0.596427  ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1577 is [True, False, False, False, True, False]
Current timestep = 1578. State = [[-0.19136147 -0.03183498]]. Action = [[-0.24080388 -0.13756502  0.02379775  0.8447069 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1578 is [True, False, False, False, True, False]
Scene graph at timestep 1578 is [True, False, False, False, True, False]
State prediction error at timestep 1578 is tensor(8.4455e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1579. State = [[-0.19298539 -0.03221656]]. Action = [[ 0.16493708  0.12184522 -0.07191899 -0.18069386]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 1579 is [True, False, False, False, True, False]
Current timestep = 1580. State = [[-0.19310156 -0.0317665 ]]. Action = [[-0.13391484 -0.0551389   0.05653763  0.7639501 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 1580 is [True, False, False, False, True, False]
Current timestep = 1581. State = [[-0.19397451 -0.03143293]]. Action = [[-0.2077649   0.19491649  0.09786031  0.83610797]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 1581 is [True, False, False, False, True, False]
Current timestep = 1582. State = [[-0.19557244 -0.02917525]]. Action = [[ 0.05817193 -0.18672505 -0.04256986 -0.2951926 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 1582 is [True, False, False, False, True, False]
Scene graph at timestep 1582 is [True, False, False, False, True, False]
State prediction error at timestep 1582 is tensor(4.3330e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1583. State = [[-0.19630337 -0.02965448]]. Action = [[ 0.0828484  -0.22523218  0.21789879  0.16569042]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 1583 is [True, False, False, False, True, False]
Scene graph at timestep 1583 is [True, False, False, False, True, False]
State prediction error at timestep 1583 is tensor(1.8791e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1584. State = [[-0.19639531 -0.03246381]]. Action = [[-0.05712253 -0.21921402 -0.12063861 -0.6185146 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1584 is [True, False, False, False, True, False]
Scene graph at timestep 1584 is [True, False, False, False, True, False]
State prediction error at timestep 1584 is tensor(1.1202e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1584 of 1
Current timestep = 1585. State = [[-0.19685389 -0.0373157 ]]. Action = [[ 0.19021755 -0.19522744  0.18936813 -0.4584883 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 1585 is [True, False, False, False, True, False]
Scene graph at timestep 1585 is [True, False, False, False, True, False]
State prediction error at timestep 1585 is tensor(3.0874e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1586. State = [[-0.19721489 -0.04268423]]. Action = [[ 0.07777959  0.11119241 -0.22975254 -0.9344038 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 1586 is [True, False, False, False, True, False]
Current timestep = 1587. State = [[-0.19739072 -0.04384252]]. Action = [[-0.0046574   0.05795762  0.11273098  0.3562187 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1587 is [True, False, False, False, True, False]
Current timestep = 1588. State = [[-0.19740121 -0.04404891]]. Action = [[ 0.06610665 -0.14503637 -0.14819993 -0.06983131]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 1588 is [True, False, False, False, True, False]
Current timestep = 1589. State = [[-0.19724317 -0.04576828]]. Action = [[ 0.00732741 -0.21435024 -0.12661986  0.64627314]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1589 is [True, False, False, False, True, False]
Current timestep = 1590. State = [[-0.19689493 -0.04946727]]. Action = [[-0.17445363  0.21785194  0.20590651  0.91370976]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 1590 is [True, False, False, False, True, False]
Current timestep = 1591. State = [[-0.19700542 -0.04951494]]. Action = [[ 0.02383691 -0.13851778 -0.15835235 -0.19407672]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1591 is [True, False, False, False, True, False]
Current timestep = 1592. State = [[-0.19715697 -0.05082389]]. Action = [[ 0.20292443  0.21449375 -0.24036904 -0.51285917]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 1592 is [True, False, False, False, True, False]
Current timestep = 1593. State = [[-0.1970989  -0.05032587]]. Action = [[0.13924325 0.1712187  0.12012088 0.2815528 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1593 is [True, False, False, False, True, False]
Current timestep = 1594. State = [[-0.19641006 -0.04798935]]. Action = [[ 0.18273333 -0.02777962 -0.14588214  0.85699797]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1594 is [True, False, False, False, True, False]
Current timestep = 1595. State = [[-0.19446233 -0.04703697]]. Action = [[-0.00974189 -0.20630549 -0.2087168   0.94741416]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 1595 is [True, False, False, False, True, False]
Scene graph at timestep 1595 is [True, False, False, False, True, False]
State prediction error at timestep 1595 is tensor(3.3221e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1596. State = [[-0.19304997 -0.0480526 ]]. Action = [[-0.02259943  0.10561395  0.1795918   0.13200247]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1596 is [True, False, False, False, True, False]
Current timestep = 1597. State = [[-0.19280367 -0.04816555]]. Action = [[-0.22777583 -0.15066703  0.22689733  0.49403977]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1597 is [True, False, False, False, True, False]
Current timestep = 1598. State = [[-0.19277495 -0.04863808]]. Action = [[ 0.23431402  0.10356301  0.0547865  -0.00560921]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 1598 is [True, False, False, False, True, False]
Current timestep = 1599. State = [[-0.1926331 -0.0485681]]. Action = [[-0.17571327 -0.21446572  0.12099159 -0.5206761 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1599 is [True, False, False, False, True, False]
Scene graph at timestep 1599 is [True, False, False, False, True, False]
State prediction error at timestep 1599 is tensor(1.3677e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1600. State = [[-0.19233513 -0.05067824]]. Action = [[ 0.21538132 -0.22915931 -0.18593243 -0.29337758]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1600 is [True, False, False, False, True, False]
Current timestep = 1601. State = [[-0.19122247 -0.05491089]]. Action = [[-0.20243879  0.17185086 -0.16234998 -0.35934603]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 1601 is [True, False, False, False, True, False]
Scene graph at timestep 1601 is [True, False, False, False, True, False]
State prediction error at timestep 1601 is tensor(6.7610e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1602. State = [[-0.1911859  -0.05559104]]. Action = [[-0.24234968 -0.16406468  0.20993921  0.9148905 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1602 is [True, False, False, False, True, False]
Current timestep = 1603. State = [[-0.19180615 -0.05768051]]. Action = [[ 0.04471776  0.05061531 -0.07621129  0.7128099 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1603 is [True, False, False, False, True, False]
Current timestep = 1604. State = [[-0.19196972 -0.05860742]]. Action = [[-0.16936973  0.14877796  0.09942791  0.21476328]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 1604 is [True, False, False, False, True, False]
Current timestep = 1605. State = [[-0.19336087 -0.05811768]]. Action = [[0.10209936 0.07525814 0.22179034 0.23592567]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 1605 is [True, False, False, False, True, False]
Current timestep = 1606. State = [[-0.19341439 -0.05768567]]. Action = [[ 0.20044792  0.05045006 -0.05436157 -0.6603879 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1606 is [True, False, False, False, True, False]
Current timestep = 1607. State = [[-0.19334286 -0.05675078]]. Action = [[-0.17075922  0.01476336 -0.07025382  0.35562468]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 1607 is [True, False, False, False, True, False]
Current timestep = 1608. State = [[-0.19331652 -0.05588045]]. Action = [[ 0.15725169  0.2057155  -0.15893826  0.31893015]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1608 is [True, False, False, False, True, False]
Scene graph at timestep 1608 is [True, False, False, False, True, False]
State prediction error at timestep 1608 is tensor(7.2827e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1609. State = [[-0.1932759  -0.05288158]]. Action = [[-0.10497192  0.00747773 -0.07098582  0.21256542]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1609 is [True, False, False, False, True, False]
Scene graph at timestep 1609 is [True, False, False, False, True, False]
State prediction error at timestep 1609 is tensor(1.3509e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1609 of 1
Current timestep = 1610. State = [[-0.19318727 -0.05115305]]. Action = [[ 0.22801584  0.06448451  0.23399827 -0.74733293]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 1610 is [True, False, False, False, True, False]
Current timestep = 1611. State = [[-0.19283207 -0.04960675]]. Action = [[-0.06173511 -0.22524281 -0.1136865   0.6724031 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1611 is [True, False, False, False, True, False]
Current timestep = 1612. State = [[-0.19278094 -0.05011799]]. Action = [[-0.13815545  0.13110074  0.10209525  0.9749892 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 1612 is [True, False, False, False, True, False]
Current timestep = 1613. State = [[-0.19288242 -0.04947369]]. Action = [[ 0.08791599  0.07895479 -0.03785275  0.33603048]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 1613 is [True, False, False, False, True, False]
Current timestep = 1614. State = [[-0.19297807 -0.04814209]]. Action = [[ 0.02476645  0.09516704 -0.14971116  0.9128362 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 1614 is [True, False, False, False, True, False]
Current timestep = 1615. State = [[-0.1931456  -0.04647113]]. Action = [[ 0.2390452   0.24060339  0.02713287 -0.4778183 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1615 is [True, False, False, False, True, False]
Current timestep = 1616. State = [[-0.1920131  -0.04225507]]. Action = [[ 0.12614685  0.00261366  0.02616945 -0.54220927]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1616 is [True, False, False, False, True, False]
Current timestep = 1617. State = [[-0.1904961  -0.03986011]]. Action = [[-0.12676212 -0.11770673 -0.04116511 -0.1132139 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1617 is [True, False, False, False, True, False]
Current timestep = 1618. State = [[-0.19015084 -0.03910652]]. Action = [[ 0.19268024  0.16604906 -0.18600343 -0.06608945]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1618 is [True, False, False, False, True, False]
Current timestep = 1619. State = [[-0.1887972  -0.03683126]]. Action = [[-0.13183263 -0.17619282 -0.23267528 -0.3078208 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 1619 is [True, False, False, False, True, False]
Current timestep = 1620. State = [[-0.1887657  -0.03681799]]. Action = [[ 0.10253146 -0.02471119  0.00412384 -0.6353751 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 1620 is [True, False, False, False, True, False]
Current timestep = 1621. State = [[-0.18808496 -0.03710414]]. Action = [[ 0.18468547 -0.04973973 -0.1887064   0.49003053]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 1621 is [True, False, False, False, True, False]
Current timestep = 1622. State = [[-0.18616214 -0.03705853]]. Action = [[-0.02202672  0.22687125 -0.10572806 -0.32318068]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 1622 is [True, False, False, False, True, False]
Current timestep = 1623. State = [[-0.18493429 -0.03490267]]. Action = [[-0.20864691  0.2209602  -0.06976756 -0.74585915]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 1623 is [True, False, False, False, True, False]
Current timestep = 1624. State = [[-0.18489371 -0.03157454]]. Action = [[-0.04895797 -0.24061713  0.24871495 -0.33235812]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1624 is [True, False, False, False, True, False]
Scene graph at timestep 1624 is [True, False, False, False, True, False]
State prediction error at timestep 1624 is tensor(8.7089e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1625. State = [[-0.1850325  -0.03162436]]. Action = [[-0.24130048 -0.07357764 -0.22473334 -0.8638059 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1625 is [True, False, False, False, True, False]
Scene graph at timestep 1625 is [True, False, False, False, True, False]
State prediction error at timestep 1625 is tensor(2.0133e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1626. State = [[-0.18506648 -0.03242156]]. Action = [[-0.00210312 -0.20209785  0.07675353  0.44389057]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 1626 is [True, False, False, False, True, False]
Current timestep = 1627. State = [[-0.18513013 -0.03504348]]. Action = [[ 0.01115057 -0.10891303  0.00682792  0.74738264]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1627 is [True, False, False, False, True, False]
Current timestep = 1628. State = [[-0.18526143 -0.03738234]]. Action = [[ 0.07831824  0.15976912 -0.17518744 -0.54433864]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1628 is [True, False, False, False, True, False]
Scene graph at timestep 1628 is [True, False, False, False, True, False]
State prediction error at timestep 1628 is tensor(7.9327e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1629. State = [[-0.18529303 -0.03740108]]. Action = [[ 0.23509467 -0.18373431  0.0323543  -0.04225028]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 1629 is [True, False, False, False, True, False]
Current timestep = 1630. State = [[-0.18500374 -0.03921514]]. Action = [[-0.13540347 -0.02491057 -0.09833783 -0.8063143 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1630 is [True, False, False, False, True, False]
Current timestep = 1631. State = [[-0.18483606 -0.04086676]]. Action = [[-0.19137949 -0.22899683  0.08585897 -0.7520279 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 1631 is [True, False, False, False, True, False]
Current timestep = 1632. State = [[-0.18494609 -0.04444488]]. Action = [[ 0.11094618  0.21532363 -0.10608402  0.30719543]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1632 is [True, False, False, False, True, False]
Current timestep = 1633. State = [[-0.18495706 -0.0446867 ]]. Action = [[ 0.14163035 -0.17048882 -0.22146364 -0.01307964]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 1633 is [True, False, False, False, True, False]
Current timestep = 1634. State = [[-0.18466817 -0.04694989]]. Action = [[-0.24179019 -0.23516548 -0.11903316 -0.43900847]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 1634 is [True, False, False, False, True, False]
Current timestep = 1635. State = [[-0.18488725 -0.05114479]]. Action = [[ 0.21367961 -0.1417166   0.14136359  0.75303936]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 1635 is [True, False, False, False, True, False]
Scene graph at timestep 1635 is [True, False, False, False, True, False]
State prediction error at timestep 1635 is tensor(3.9007e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1636. State = [[-0.18470022 -0.05574072]]. Action = [[ 0.2036761   0.05386984 -0.18285587 -0.1734575 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 1636 is [True, False, False, False, True, False]
Current timestep = 1637. State = [[-0.18425657 -0.05793666]]. Action = [[ 0.20333654 -0.04158714  0.12022713 -0.9246374 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 1637 is [True, False, False, False, True, False]
Current timestep = 1638. State = [[-0.18212917 -0.0599735 ]]. Action = [[ 0.18081024 -0.15712322 -0.24566205  0.4624884 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 1638 is [True, False, False, False, True, False]
Human Feedback received at timestep 1638 of 1
Current timestep = 1639. State = [[-0.17742066 -0.06407386]]. Action = [[-0.2010291  -0.23440805  0.01064435 -0.8731284 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 1639 is [True, False, False, False, True, False]
Current timestep = 1640. State = [[-0.17498651 -0.06909372]]. Action = [[ 0.01253578 -0.00833653 -0.17675312 -0.41108972]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 1640 is [True, False, False, False, True, False]
Current timestep = 1641. State = [[-0.17362662 -0.0725013 ]]. Action = [[-0.1972508  -0.1218884   0.22034702 -0.50792783]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 1641 is [True, False, False, False, True, False]
Scene graph at timestep 1641 is [True, False, False, False, True, False]
State prediction error at timestep 1641 is tensor(1.8319e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1642. State = [[-0.17358859 -0.07582624]]. Action = [[0.22623962 0.09556738 0.13186026 0.6884153 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 1642 is [True, False, False, False, True, False]
Scene graph at timestep 1642 is [True, False, False, False, True, False]
State prediction error at timestep 1642 is tensor(2.8709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1643. State = [[-0.17320584 -0.07702596]]. Action = [[-0.17230678 -0.1237427  -0.20458846 -0.11891901]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 1643 is [True, False, False, False, True, False]
Current timestep = 1644. State = [[-0.17350332 -0.07888854]]. Action = [[-0.24560358  0.18595034 -0.2213827   0.31168962]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 1644 is [True, False, False, False, True, False]
Current timestep = 1645. State = [[-0.17385091 -0.07853364]]. Action = [[-0.02076449  0.24747169  0.09693685  0.41039777]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 1645 is [True, False, False, False, True, False]
Current timestep = 1646. State = [[-0.17428653 -0.07675869]]. Action = [[ 0.05633315 -0.20494203 -0.10625999  0.8740616 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 1646 is [True, False, False, False, True, False]
Scene graph at timestep 1646 is [True, False, False, False, True, False]
State prediction error at timestep 1646 is tensor(1.8301e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1647. State = [[-0.17436725 -0.07683666]]. Action = [[ 0.24610582  0.23013371 -0.02114743  0.4760821 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 1647 is [True, False, False, False, True, False]
Current timestep = 1648. State = [[-0.17422332 -0.07562879]]. Action = [[-0.21844779 -0.14217386 -0.16807713  0.01240289]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 1648 is [True, False, False, False, True, False]
Scene graph at timestep 1648 is [True, False, False, False, True, False]
State prediction error at timestep 1648 is tensor(2.9452e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1649. State = [[-0.1742255  -0.07580554]]. Action = [[ 0.22248054 -0.12399094  0.20288193 -0.72575957]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 1649 is [True, False, False, False, True, False]
Current timestep = 1650. State = [[-0.17404884 -0.07653756]]. Action = [[ 0.21887356 -0.0107654   0.06000087  0.8293686 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 1650 is [True, False, False, False, True, False]
Current timestep = 1651. State = [[-0.17290965 -0.0773871 ]]. Action = [[-0.19829157  0.07177982 -0.0264844   0.33019888]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 1651 is [True, False, False, False, True, False]
Current timestep = 1652. State = [[-0.17292528 -0.07710157]]. Action = [[-0.10224    -0.07919669 -0.03114575 -0.9703306 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 1652 is [True, False, False, False, True, False]
Current timestep = 1653. State = [[-0.17300297 -0.07732812]]. Action = [[-0.13102412 -0.04296075 -0.21456651 -0.63273114]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 1653 is [True, False, False, False, True, False]
Current timestep = 1654. State = [[-0.17308715 -0.07809163]]. Action = [[-0.09350856 -0.21131974 -0.17046796  0.27882206]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 1654 is [True, False, False, False, True, False]
Current timestep = 1655. State = [[-0.17338161 -0.08153726]]. Action = [[-0.07954295 -0.11348099 -0.16429803 -0.1927424 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 1655 is [True, False, False, False, True, False]
Current timestep = 1656. State = [[-0.17405185 -0.08517866]]. Action = [[0.20207578 0.06309265 0.12431541 0.05237556]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 1656 is [True, False, False, False, True, False]
Current timestep = 1657. State = [[-0.1743263  -0.08657756]]. Action = [[-0.07106861  0.00944075 -0.08179229  0.22459137]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 1657 is [True, False, False, False, True, False]
Scene graph at timestep 1657 is [True, False, False, False, True, False]
State prediction error at timestep 1657 is tensor(8.1015e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1658. State = [[-0.1745873  -0.08742858]]. Action = [[-0.21261136 -0.02135466  0.17059898 -0.5670594 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 1658 is [True, False, False, False, True, False]
Current timestep = 1659. State = [[-0.17635857 -0.08925667]]. Action = [[-0.17961118 -0.2398288  -0.16177154  0.62220526]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 1659 is [True, False, False, False, True, False]
Current timestep = 1660. State = [[-0.1788107  -0.09315462]]. Action = [[ 0.08167091  0.14449912  0.17867678 -0.6141454 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 1660 is [True, False, False, False, True, False]
Current timestep = 1661. State = [[-0.18009703 -0.09406906]]. Action = [[-0.16727835 -0.02369735  0.14772043 -0.72028893]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 1661 is [True, False, False, False, True, False]
Scene graph at timestep 1661 is [True, False, False, False, True, False]
State prediction error at timestep 1661 is tensor(2.0761e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1662. State = [[-0.18195063 -0.09506378]]. Action = [[ 0.06173575 -0.13031836 -0.09755757  0.76274824]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 1662 is [True, False, False, False, True, False]
Current timestep = 1663. State = [[-0.18267512 -0.09678812]]. Action = [[ 0.05565226 -0.11982194  0.12234601  0.5405514 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 1663 is [True, False, False, False, True, False]
Scene graph at timestep 1663 is [True, False, False, False, True, False]
State prediction error at timestep 1663 is tensor(3.9463e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1664. State = [[-0.18323873 -0.09953036]]. Action = [[-0.04613012 -0.21685152  0.19299138 -0.72097623]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 1664 is [True, False, False, False, True, False]
Current timestep = 1665. State = [[-0.18434641 -0.10428228]]. Action = [[-0.22478606 -0.04907382  0.04536679 -0.70215225]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 1665 is [True, False, False, False, True, False]
Current timestep = 1666. State = [[-0.18686046 -0.10840563]]. Action = [[0.23170912 0.08556262 0.00135773 0.80548835]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 1666 is [True, False, False, False, True, False]
Current timestep = 1667. State = [[-0.18717831 -0.10929006]]. Action = [[-0.16922691  0.05460641  0.14177239 -0.6749394 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 1667 is [True, False, False, False, True, False]
Human Feedback received at timestep 1667 of -1
Current timestep = 1668. State = [[-0.18779461 -0.10934189]]. Action = [[-0.12844963 -0.20783746  0.20887268 -0.87362146]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 1668 is [True, False, False, False, True, False]
Current timestep = 1669. State = [[-0.18982562 -0.11199079]]. Action = [[-0.17910582  0.11235809  0.03576994  0.39040005]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 1669 is [True, False, False, False, True, False]
Current timestep = 1670. State = [[-0.19213423 -0.1132397 ]]. Action = [[ 0.14081544 -0.19837792  0.09524715  0.734267  ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 1670 is [True, False, False, False, True, False]
Current timestep = 1671. State = [[-0.19353081 -0.1156418 ]]. Action = [[ 0.09547234  0.12882268 -0.11943001 -0.70854115]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 1671 is [True, False, False, False, True, False]
Human Feedback received at timestep 1671 of -1
Current timestep = 1672. State = [[-0.19387496 -0.11636255]]. Action = [[-0.01970252 -0.18712638 -0.11664602  0.37022948]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 1672 is [True, False, False, False, True, False]
Current timestep = 1673. State = [[-0.19446534 -0.11840277]]. Action = [[-0.22223726  0.187204   -0.20271276  0.81580544]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 1673 is [True, False, False, False, True, False]
Current timestep = 1674. State = [[-0.19647503 -0.11875968]]. Action = [[-0.17039594 -0.20689066  0.08251601  0.1755333 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 1674 is [True, False, False, False, True, False]
Current timestep = 1675. State = [[-0.19951212 -0.12078904]]. Action = [[ 0.04814175  0.09813961  0.18098062 -0.0175451 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 1675 is [True, False, False, False, True, False]
Human Feedback received at timestep 1675 of -1
Current timestep = 1676. State = [[-0.20111205 -0.12130679]]. Action = [[ 0.11244074  0.13238227 -0.13079618 -0.40578222]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 1676 is [True, False, False, False, True, False]
Current timestep = 1677. State = [[-0.20126237 -0.12047823]]. Action = [[-0.11960828  0.11958241 -0.18417135 -0.6812611 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 1677 is [True, False, False, False, True, False]
Scene graph at timestep 1677 is [True, False, False, False, True, False]
State prediction error at timestep 1677 is tensor(3.0369e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1678. State = [[-0.20227651 -0.11870547]]. Action = [[-0.22141625 -0.10971504  0.19756892 -0.8413138 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 1678 is [True, False, False, False, True, False]
Current timestep = 1679. State = [[-0.20385204 -0.11867045]]. Action = [[-0.19598189 -0.05207095  0.12776998  0.42686653]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 1679 is [True, False, False, False, True, False]
Scene graph at timestep 1679 is [True, False, False, False, True, False]
State prediction error at timestep 1679 is tensor(1.2773e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1679 of -1
Current timestep = 1680. State = [[-0.20770359 -0.11922814]]. Action = [[ 0.20407116  0.07158333  0.2224139  -0.59472907]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 1680 is [True, False, False, False, True, False]
Current timestep = 1681. State = [[-0.2080946  -0.11905774]]. Action = [[-0.20831227  0.00121936  0.21451777  0.33025694]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 1681 is [True, False, False, False, True, False]
Human Feedback received at timestep 1681 of -1
Current timestep = 1682. State = [[-0.20959625 -0.11893424]]. Action = [[-0.00929354 -0.10800344 -0.16767076  0.05282342]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 1682 is [True, False, False, False, True, False]
Current timestep = 1683. State = [[-0.21114099 -0.11944823]]. Action = [[-0.19990352  0.04222971  0.21935326  0.9469757 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 1683 is [True, False, False, False, True, False]
Current timestep = 1684. State = [[-0.21519767 -0.11909623]]. Action = [[ 0.15335861  0.0504339  -0.07680926  0.2414751 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 1684 is [True, False, False, False, True, False]
Current timestep = 1685. State = [[-0.2172208  -0.11868107]]. Action = [[ 0.24549633 -0.17506248  0.08404735 -0.9256046 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 1685 is [True, False, False, False, True, False]
Scene graph at timestep 1685 is [True, False, False, False, True, False]
State prediction error at timestep 1685 is tensor(1.1550e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1685 of -1
Current timestep = 1686. State = [[-0.2171328 -0.1197994]]. Action = [[ 0.17255947 -0.22331561  0.03611425 -0.8147845 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 1686 is [True, False, False, False, True, False]
Current timestep = 1687. State = [[-0.21681198 -0.12289865]]. Action = [[-0.19298516  0.02899379  0.22773254  0.16907763]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 1687 is [True, False, False, False, True, False]
Current timestep = 1688. State = [[-0.21685627 -0.12433053]]. Action = [[ 0.01599169  0.05130893 -0.00665762 -0.5025459 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 1688 is [True, False, False, False, True, False]
Human Feedback received at timestep 1688 of -1
Current timestep = 1689. State = [[-0.21696466 -0.12456923]]. Action = [[0.0841313  0.24083325 0.19365284 0.621593  ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 1689 is [True, False, False, False, True, False]
Current timestep = 1690. State = [[-0.21683808 -0.12354927]]. Action = [[-0.04991001  0.0092853  -0.19320257  0.4930122 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 1690 is [True, False, False, False, True, False]
Current timestep = 1691. State = [[-0.21678244 -0.12315109]]. Action = [[ 0.0783253  -0.19545649 -0.08948743  0.49453914]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 1691 is [True, False, False, False, True, False]
Current timestep = 1692. State = [[-0.2167578  -0.12357373]]. Action = [[-0.07902059  0.00659272 -0.12106118  0.6329154 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 1692 is [True, False, False, False, True, False]
Scene graph at timestep 1692 is [True, False, False, False, True, False]
State prediction error at timestep 1692 is tensor(1.0221e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1693. State = [[-0.21681851 -0.12384903]]. Action = [[ 0.00863272  0.02425027 -0.18504108  0.23538268]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 1693 is [True, False, False, False, True, False]
Current timestep = 1694. State = [[-0.21681583 -0.12389983]]. Action = [[-0.10058901  0.02002442 -0.04954123  0.16654265]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 1694 is [True, False, False, False, True, False]
Current timestep = 1695. State = [[-0.21685082 -0.12375787]]. Action = [[ 0.21782935  0.21539354 -0.18916346  0.6264992 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 1695 is [True, False, False, False, True, False]
Scene graph at timestep 1695 is [True, False, False, False, True, False]
State prediction error at timestep 1695 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 1696. State = [[-0.21678953 -0.12180445]]. Action = [[-0.1463336   0.21352762  0.24860883 -0.880557  ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 1696 is [True, False, False, False, True, False]
Scene graph at timestep 1696 is [True, False, False, False, True, False]
State prediction error at timestep 1696 is tensor(4.2958e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1696 of -1
Current timestep = 1697. State = [[-0.21688968 -0.11791296]]. Action = [[-0.03716978  0.14413261 -0.02087098  0.89088726]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 1697 is [True, False, False, False, True, False]
Current timestep = 1698. State = [[-0.21700856 -0.11399644]]. Action = [[-0.05243745  0.14434093 -0.08236876  0.29945564]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 1698 is [True, False, False, False, True, False]
Current timestep = 1699. State = [[-0.21720833 -0.10948318]]. Action = [[-0.03763306  0.24090186 -0.02072346 -0.7175351 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 1699 is [True, False, False, False, True, False]
Current timestep = 1700. State = [[-0.21797559 -0.10284577]]. Action = [[-0.23413421  0.1838454  -0.08022855  0.02517009]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 1700 is [True, False, False, False, True, False]
Current timestep = 1701. State = [[-0.21980037 -0.09632782]]. Action = [[ 0.1687488  -0.14438266  0.1832434   0.55938554]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 1701 is [True, False, False, False, True, False]
Current timestep = 1702. State = [[-0.22012584 -0.09436073]]. Action = [[-0.00630502 -0.18167354  0.20954871 -0.11236286]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 1702 is [True, False, False, False, True, False]
Current timestep = 1703. State = [[-0.22021346 -0.09508874]]. Action = [[-0.10629062 -0.21565306 -0.0821147  -0.6968045 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 1703 is [True, False, False, False, True, False]
Current timestep = 1704. State = [[-0.22095436 -0.09728661]]. Action = [[ 0.20925891  0.23389852 -0.19918598 -0.17291659]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 1704 is [True, False, False, False, True, False]
Scene graph at timestep 1704 is [True, False, False, False, True, False]
State prediction error at timestep 1704 is tensor(9.9406e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1705. State = [[-0.22100285 -0.09636694]]. Action = [[-0.01872329  0.24701276 -0.19834852 -0.12642324]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 1705 is [True, False, False, False, True, False]
Current timestep = 1706. State = [[-0.22111017 -0.09338219]]. Action = [[-0.01346549 -0.01609468 -0.09788531  0.974656  ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 1706 is [True, False, False, False, True, False]
Current timestep = 1707. State = [[-0.22120343 -0.09174072]]. Action = [[0.18162715 0.07647693 0.05704483 0.8623955 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 1707 is [True, False, False, False, True, False]
Current timestep = 1708. State = [[-0.2210554  -0.08954387]]. Action = [[0.06921613 0.24678451 0.16701967 0.3466431 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 1708 is [True, False, False, False, True, False]
Current timestep = 1709. State = [[-0.22044471 -0.08484371]]. Action = [[ 0.1812259  -0.01221734 -0.13244067  0.77618504]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 1709 is [True, False, False, False, True, False]
Current timestep = 1710. State = [[-0.21864767 -0.08188035]]. Action = [[-0.00743884  0.07960644  0.11810625  0.42660296]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 1710 is [True, False, False, False, True, False]
Current timestep = 1711. State = [[-0.21771942 -0.07969346]]. Action = [[0.08239639 0.14049059 0.24832714 0.7409539 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 1711 is [True, False, False, False, True, False]
Current timestep = 1712. State = [[-0.21611394 -0.07657312]]. Action = [[ 0.21595585 -0.2380206  -0.14390895  0.24172187]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 1712 is [True, False, False, False, True, False]
Current timestep = 1713. State = [[-0.21333262 -0.07641684]]. Action = [[0.15270323 0.20418102 0.02834743 0.15929818]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 1713 is [True, False, False, False, True, False]
Current timestep = 1714. State = [[-0.20945878 -0.07460063]]. Action = [[ 0.13600498 -0.00537179 -0.10205781  0.11216462]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 1714 is [True, False, False, False, True, False]
Current timestep = 1715. State = [[-0.20562224 -0.07326113]]. Action = [[-0.10360909 -0.01383609  0.12723261  0.33791363]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 1715 is [True, False, False, False, True, False]
Current timestep = 1716. State = [[-0.20350508 -0.07183331]]. Action = [[-0.14801498 -0.00979796 -0.06556112 -0.17521495]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 1716 is [True, False, False, False, True, False]
Current timestep = 1717. State = [[-0.20339495 -0.07138778]]. Action = [[-0.12767577 -0.2050177   0.145154   -0.61436397]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 1717 is [True, False, False, False, True, False]
Current timestep = 1718. State = [[-0.2032658  -0.07234746]]. Action = [[ 0.00717556 -0.08864632 -0.05338252 -0.14297777]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 1718 is [True, False, False, False, True, False]
Current timestep = 1719. State = [[-0.20318618 -0.07370774]]. Action = [[ 0.16783208 -0.15287393  0.10915592 -0.67240834]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 1719 is [True, False, False, False, True, False]
Human Feedback received at timestep 1719 of 1
Current timestep = 1720. State = [[-0.20216438 -0.07647086]]. Action = [[ 0.14116839  0.18241492  0.18092889 -0.3381797 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 1720 is [True, False, False, False, True, False]
Current timestep = 1721. State = [[-0.20104049 -0.07642126]]. Action = [[ 0.13527685 -0.20281902 -0.06641927 -0.47981745]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 1721 is [True, False, False, False, True, False]
Scene graph at timestep 1721 is [True, False, False, False, True, False]
State prediction error at timestep 1721 is tensor(6.4040e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1722. State = [[-0.19878522 -0.07826237]]. Action = [[ 0.05503225  0.06071049 -0.04341047  0.6134558 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 1722 is [True, False, False, False, True, False]
Current timestep = 1723. State = [[-0.19679151 -0.07869574]]. Action = [[ 0.18064529  0.02876186  0.10704112 -0.4345265 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 1723 is [True, False, False, False, True, False]
Current timestep = 1724. State = [[-0.19376725 -0.07866132]]. Action = [[-0.11086455  0.15817493  0.19743717  0.29839826]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 1724 is [True, False, False, False, True, False]
Current timestep = 1725. State = [[-0.19236906 -0.0780475 ]]. Action = [[-0.02896661  0.05992964 -0.16810466 -0.74262965]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 1725 is [True, False, False, False, True, False]
Current timestep = 1726. State = [[-0.19185342 -0.07679427]]. Action = [[ 0.2303034   0.15710026  0.16076559 -0.73140275]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 1726 is [True, False, False, False, True, False]
Current timestep = 1727. State = [[-0.18982814 -0.07392984]]. Action = [[-0.23679416 -0.22077683  0.137811    0.412207  ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 1727 is [True, False, False, False, True, False]
Current timestep = 1728. State = [[-0.18979497 -0.07447377]]. Action = [[ 0.14276966  0.124612   -0.12509996 -0.11826062]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 1728 is [True, False, False, False, True, False]
Human Feedback received at timestep 1728 of 1
Current timestep = 1729. State = [[-0.18959026 -0.07395191]]. Action = [[ 0.08987725 -0.0297533  -0.06949264 -0.75352305]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 1729 is [True, False, False, False, True, False]
Scene graph at timestep 1729 is [True, False, False, False, True, False]
State prediction error at timestep 1729 is tensor(8.3027e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1730. State = [[-0.18888819 -0.07377817]]. Action = [[-0.22413526  0.05110139  0.2047069   0.00238037]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 1730 is [True, False, False, False, True, False]
Current timestep = 1731. State = [[-0.18891427 -0.07313097]]. Action = [[-0.1924006   0.12528706  0.09536642 -0.56642735]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 1731 is [True, False, False, False, True, False]
Current timestep = 1732. State = [[-0.18912521 -0.07175526]]. Action = [[ 0.03821507 -0.15505895 -0.07392541  0.41440094]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 1732 is [True, False, False, False, True, False]
Current timestep = 1733. State = [[-0.18907999 -0.07188896]]. Action = [[ 0.13831162  0.01997516 -0.10254668  0.17741692]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 1733 is [True, False, False, False, True, False]
Scene graph at timestep 1733 is [True, False, False, False, True, False]
State prediction error at timestep 1733 is tensor(5.1279e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1734. State = [[-0.1890496  -0.07193415]]. Action = [[-2.0315042e-01 -1.2415451e-01 -1.6209483e-04 -8.2632911e-01]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 1734 is [True, False, False, False, True, False]
Current timestep = 1735. State = [[-0.18904972 -0.07272203]]. Action = [[ 0.0956009  -0.04386634 -0.09349172  0.6702889 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 1735 is [True, False, False, False, True, False]
Scene graph at timestep 1735 is [True, False, False, False, True, False]
State prediction error at timestep 1735 is tensor(2.0879e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1736. State = [[-0.18912359 -0.07337341]]. Action = [[-0.0856666   0.09887624  0.24366376 -0.3081507 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 1736 is [True, False, False, False, True, False]
Current timestep = 1737. State = [[-0.1891298  -0.07322019]]. Action = [[0.01653454 0.11323109 0.2252908  0.00267482]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 1737 is [True, False, False, False, True, False]
Scene graph at timestep 1737 is [True, False, False, False, True, False]
State prediction error at timestep 1737 is tensor(1.8136e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1738. State = [[-0.18917917 -0.07255956]]. Action = [[ 0.11127919  0.05873388 -0.09076408  0.65089273]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 1738 is [True, False, False, False, True, False]
Scene graph at timestep 1738 is [True, False, False, False, True, False]
State prediction error at timestep 1738 is tensor(1.3091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1739. State = [[-0.18920712 -0.07127483]]. Action = [[-0.06128699  0.18119186  0.17501086  0.801628  ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 1739 is [True, False, False, False, True, False]
Current timestep = 1740. State = [[-0.18943584 -0.06842843]]. Action = [[ 0.06146064  0.21327037 -0.03015001  0.11686122]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 1740 is [True, False, False, False, True, False]
Scene graph at timestep 1740 is [True, False, False, False, True, False]
State prediction error at timestep 1740 is tensor(5.0550e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1741. State = [[-0.189902   -0.06391428]]. Action = [[-0.1047525   0.07241225 -0.17988586  0.8095622 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 1741 is [True, False, False, False, True, False]
Current timestep = 1742. State = [[-0.19038826 -0.05999976]]. Action = [[ 0.04257974 -0.21585245  0.1657666   0.39425862]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 1742 is [True, False, False, False, True, False]
Current timestep = 1743. State = [[-0.1904092  -0.05982784]]. Action = [[ 0.04532897  0.19207078 -0.0183607  -0.020576  ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 1743 is [True, False, False, False, True, False]
Current timestep = 1744. State = [[-0.19058512 -0.05801947]]. Action = [[-0.17413898 -0.13884497  0.24086925  0.1864984 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 1744 is [True, False, False, False, True, False]
Current timestep = 1745. State = [[-0.19069555 -0.05776242]]. Action = [[-0.12033355  0.14981136  0.00590441 -0.4332763 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 1745 is [True, False, False, False, True, False]
Current timestep = 1746. State = [[-0.19129762 -0.05615455]]. Action = [[-0.10091752  0.21489024  0.16604367  0.08806455]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 1746 is [True, False, False, False, True, False]
Current timestep = 1747. State = [[-0.193107   -0.05198682]]. Action = [[-0.04391462  0.23480067  0.22715366 -0.35016692]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 1747 is [True, False, False, False, True, False]
Scene graph at timestep 1747 is [True, False, False, False, True, False]
State prediction error at timestep 1747 is tensor(3.8095e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1748. State = [[-0.1951086  -0.04597717]]. Action = [[ 0.08265913 -0.08445036  0.16413212  0.75822663]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 1748 is [True, False, False, False, True, False]
Scene graph at timestep 1748 is [True, False, False, False, True, False]
State prediction error at timestep 1748 is tensor(4.7033e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1749. State = [[-0.19569953 -0.04305987]]. Action = [[-0.09329598  0.22506225  0.13154042 -0.49806762]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 1749 is [True, False, False, False, True, False]
Scene graph at timestep 1749 is [True, False, False, False, True, False]
State prediction error at timestep 1749 is tensor(1.1946e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1750. State = [[-0.19701101 -0.03808782]]. Action = [[ 0.09657696  0.09168789 -0.17547695  0.32707107]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 1750 is [True, False, False, False, True, False]
Current timestep = 1751. State = [[-0.1975786  -0.03468029]]. Action = [[ 0.1263321  -0.17701136 -0.24519525 -0.39191914]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 1751 is [True, False, False, False, True, False]
Current timestep = 1752. State = [[-0.19756268 -0.0344894 ]]. Action = [[0.11351249 0.1453811  0.1660729  0.02702069]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 1752 is [True, False, False, False, True, False]
Current timestep = 1753. State = [[-0.19760953 -0.03309128]]. Action = [[ 0.1870437   0.17341593  0.10216093 -0.92170393]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 1753 is [True, False, False, False, True, False]
Current timestep = 1754. State = [[-0.19624788 -0.03027808]]. Action = [[ 0.16399124  0.0072729  -0.2233642  -0.3139044 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 1754 is [True, False, False, False, True, False]
Scene graph at timestep 1754 is [True, False, False, False, True, False]
State prediction error at timestep 1754 is tensor(3.3308e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1755. State = [[-0.1935812  -0.02808664]]. Action = [[ 0.23010832 -0.00150755 -0.03685412 -0.93623334]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 1755 is [True, False, False, False, True, False]
Current timestep = 1756. State = [[-0.18950608 -0.02611029]]. Action = [[ 0.20497176  0.15946311  0.00912368 -0.47060072]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 1756 is [True, False, False, False, True, False]
Current timestep = 1757. State = [[-0.18429725 -0.02310407]]. Action = [[ 0.22199556 -0.205825   -0.08752139 -0.2816621 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 1757 is [True, False, False, False, True, False]
Current timestep = 1758. State = [[-0.17790286 -0.02347338]]. Action = [[ 0.06091598  0.02220392  0.073208   -0.9082538 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 1758 is [True, False, False, False, True, False]
Human Feedback received at timestep 1758 of 1
Current timestep = 1759. State = [[-0.17171548 -0.0232119 ]]. Action = [[-0.01195958  0.12370497 -0.07553475 -0.76407164]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 1759 is [True, False, False, False, True, False]
Current timestep = 1760. State = [[-0.16855781 -0.02200375]]. Action = [[ 0.06637853  0.00673223  0.24524325 -0.73288393]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 1760 is [True, False, False, False, True, False]
Current timestep = 1761. State = [[-0.16629893 -0.02137657]]. Action = [[-0.23492596 -0.08390409 -0.00366618  0.88758206]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 1761 is [True, False, False, False, True, False]
Current timestep = 1762. State = [[-0.16611934 -0.02128604]]. Action = [[-0.1971733   0.19540656  0.24488103  0.30750418]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 1762 is [True, False, False, False, True, False]
Current timestep = 1763. State = [[-0.16655266 -0.01930761]]. Action = [[ 0.18544644  0.04030004 -0.11972776  0.24466133]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 1763 is [True, False, False, False, True, False]
Scene graph at timestep 1763 is [True, False, False, False, True, False]
State prediction error at timestep 1763 is tensor(1.9719e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1764. State = [[-0.16677672 -0.0174911 ]]. Action = [[ 0.23183936  0.00671834 -0.1834159  -0.8668669 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 1764 is [True, False, False, False, True, False]
Scene graph at timestep 1764 is [True, False, False, False, True, False]
State prediction error at timestep 1764 is tensor(2.0608e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1764 of 1
Current timestep = 1765. State = [[-0.16505578 -0.01626238]]. Action = [[ 0.01025331  0.13927028 -0.12142035 -0.23172784]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 1765 is [True, False, False, False, True, False]
Current timestep = 1766. State = [[-0.16395248 -0.01401921]]. Action = [[ 0.13680702  0.15148643 -0.14285769  0.60105777]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 1766 is [True, False, False, False, True, False]
Scene graph at timestep 1766 is [True, False, False, False, True, False]
State prediction error at timestep 1766 is tensor(3.8177e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1767. State = [[-0.16172285 -0.01052065]]. Action = [[ 0.05723536 -0.04390615  0.20789713  0.38367152]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 1767 is [True, False, False, False, True, False]
Current timestep = 1768. State = [[-0.15908168 -0.00942597]]. Action = [[ 0.2314786  -0.19967899 -0.02390426 -0.00850987]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 1768 is [True, False, False, False, True, False]
Human Feedback received at timestep 1768 of 1
Current timestep = 1769. State = [[-0.15477064 -0.00960918]]. Action = [[ 0.04742154  0.2018627  -0.09791923 -0.10719764]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 1769 is [True, False, False, False, True, False]
Current timestep = 1770. State = [[-0.15057278 -0.0081568 ]]. Action = [[ 0.15686446  0.04476884  0.23374432 -0.8096236 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 1770 is [True, False, False, False, True, False]
Scene graph at timestep 1770 is [True, False, False, False, True, False]
State prediction error at timestep 1770 is tensor(7.9140e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1771. State = [[-0.14618687 -0.00695474]]. Action = [[ 0.05398405 -0.03690274  0.1410875  -0.35286897]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 1771 is [True, False, False, False, True, False]
Human Feedback received at timestep 1771 of 1
Current timestep = 1772. State = [[-0.14274532 -0.00636482]]. Action = [[-0.21635066 -0.0793808  -0.15881552  0.23749864]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 1772 is [True, False, False, False, True, False]
Current timestep = 1773. State = [[-0.14191318 -0.00628852]]. Action = [[-0.13337123  0.07223415  0.03902528 -0.96518856]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 1773 is [True, False, False, False, True, False]
Current timestep = 1774. State = [[-0.1426136  -0.00587845]]. Action = [[-0.24316742  0.15232441 -0.06619263  0.02773452]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 1774 is [True, False, False, False, True, False]
Current timestep = 1775. State = [[-0.14384787 -0.00267466]]. Action = [[ 0.07150319  0.1730403  -0.0287396   0.3706038 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 1775 is [True, False, False, False, True, False]
Human Feedback received at timestep 1775 of 1
Current timestep = 1776. State = [[-0.14503561  0.00164842]]. Action = [[ 0.22219852  0.21876451 -0.00901367  0.0158453 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 1776 is [True, False, False, False, True, False]
Current timestep = 1777. State = [[-0.14554888  0.00627153]]. Action = [[-0.06468484 -0.06446612  0.02972841 -0.7476929 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 1777 is [True, False, False, False, True, False]
Scene graph at timestep 1777 is [True, False, False, False, True, False]
State prediction error at timestep 1777 is tensor(1.3160e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1778. State = [[-0.14624786  0.00909486]]. Action = [[-0.11412983  0.20692152 -0.07563831  0.24235177]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 1778 is [True, False, False, False, True, False]
Current timestep = 1779. State = [[-0.14708287  0.01292655]]. Action = [[-0.143628   -0.19377512 -0.22393273  0.90518916]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 1779 is [True, False, False, False, True, False]
Current timestep = 1780. State = [[-0.14727545  0.01334237]]. Action = [[ 0.10867977 -0.23090309  0.05763441  0.27631366]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 1780 is [True, False, False, False, True, False]
Current timestep = 1781. State = [[-0.14697962  0.01213995]]. Action = [[ 0.06496894  0.21339461 -0.1878769  -0.8093164 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 1781 is [True, False, False, False, True, False]
Current timestep = 1782. State = [[-0.14698572  0.01262401]]. Action = [[ 0.18062282 -0.16965453  0.13098514  0.948594  ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 1782 is [True, False, False, False, True, False]
Current timestep = 1783. State = [[-0.14643052  0.01225603]]. Action = [[-0.00868762  0.02488041  0.1637336  -0.1619134 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 1783 is [True, False, False, False, True, False]
Current timestep = 1784. State = [[-0.14615068  0.01252354]]. Action = [[ 0.08457109  0.20474303 -0.05401002  0.38361633]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 1784 is [True, False, False, False, True, False]
Current timestep = 1785. State = [[-0.14505532  0.01342378]]. Action = [[ 0.08862484  0.09876058 -0.20973733 -0.53903544]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 1785 is [True, False, False, False, True, False]
Current timestep = 1786. State = [[-0.14332575  0.01532402]]. Action = [[ 0.08151904  0.24250752 -0.19214553 -0.05270988]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 1786 is [True, False, False, False, True, False]
Current timestep = 1787. State = [[-0.14089432  0.01940964]]. Action = [[-0.22455592 -0.23198116 -0.09158491 -0.99226475]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 1787 is [True, False, False, False, True, False]
Current timestep = 1788. State = [[-0.14006333  0.01970392]]. Action = [[-0.19293402 -0.06327504 -0.18538573 -0.37312782]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 1788 is [True, False, False, False, True, False]
Current timestep = 1789. State = [[-0.1405471   0.01947359]]. Action = [[-0.1110719  -0.09065604  0.21336633  0.29201317]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 1789 is [True, False, False, False, True, False]
Human Feedback received at timestep 1789 of 1
Current timestep = 1790. State = [[-0.14060982  0.01891331]]. Action = [[ 0.20852554  0.0516696  -0.1277363  -0.77013665]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 1790 is [True, False, False, False, True, False]
Current timestep = 1791. State = [[-0.14062467  0.01913216]]. Action = [[ 0.0138588  -0.18439555  0.02450645  0.8102249 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 1791 is [True, False, False, False, True, False]
Scene graph at timestep 1791 is [True, False, False, False, True, False]
State prediction error at timestep 1791 is tensor(1.2939e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1792. State = [[-0.14036825  0.01725614]]. Action = [[ 0.13384035  0.20310578  0.2132253  -0.2945261 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 1792 is [True, False, False, False, True, False]
Current timestep = 1793. State = [[-0.14003685  0.01734897]]. Action = [[-0.07390068 -0.12361959 -0.05134717  0.8166816 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 1793 is [True, False, False, False, True, False]
Current timestep = 1794. State = [[-0.14002049  0.01722859]]. Action = [[-0.22014117 -0.06383716 -0.08726186  0.21449399]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 1794 is [True, False, False, False, True, False]
Current timestep = 1795. State = [[-0.14014083  0.01600137]]. Action = [[-0.22201823 -0.08860712  0.1846003   0.11278236]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 1795 is [True, False, False, False, True, False]
Scene graph at timestep 1795 is [True, False, False, False, True, False]
State prediction error at timestep 1795 is tensor(1.9702e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1796. State = [[-0.14091186  0.01390524]]. Action = [[-0.07013854  0.21953142 -0.11037976 -0.9846148 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 1796 is [True, False, False, False, True, False]
Human Feedback received at timestep 1796 of 1
Current timestep = 1797. State = [[-0.14196026  0.01534963]]. Action = [[ 0.15886116  0.11733174 -0.2051086   0.30177557]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 1797 is [True, False, False, False, True, False]
Current timestep = 1798. State = [[-0.14252312  0.0169727 ]]. Action = [[-0.07972232 -0.14403355 -0.13654205  0.8607477 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 1798 is [True, False, False, False, True, False]
Scene graph at timestep 1798 is [True, False, False, False, True, False]
State prediction error at timestep 1798 is tensor(9.8147e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1799. State = [[-0.14276867  0.01639747]]. Action = [[ 0.23692185  0.03240973  0.07161945 -0.4755267 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 1799 is [True, False, False, False, True, False]
Current timestep = 1800. State = [[-0.14276323  0.0164714 ]]. Action = [[-0.24352023 -0.04983914  0.1321032  -0.38880777]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 1800 is [True, False, False, False, True, False]
Current timestep = 1801. State = [[-0.14292854  0.01572177]]. Action = [[-0.00661533 -0.0665358  -0.11286542  0.02662539]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 1801 is [True, False, False, False, True, False]
Current timestep = 1802. State = [[-0.14305046  0.01486363]]. Action = [[0.13580498 0.16133663 0.21243447 0.55737805]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 1802 is [True, False, False, False, True, False]
Scene graph at timestep 1802 is [True, False, False, False, True, False]
State prediction error at timestep 1802 is tensor(1.5912e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1803. State = [[-0.14328642  0.01576551]]. Action = [[-0.04201494  0.13510048  0.16368246 -0.0673033 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 1803 is [True, False, False, False, True, False]
Current timestep = 1804. State = [[-0.14379132  0.01771025]]. Action = [[-0.03039069  0.10114115  0.09606588  0.00776541]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 1804 is [True, False, False, False, True, False]
Current timestep = 1805. State = [[-0.14472839  0.02090892]]. Action = [[ 0.15625057  0.23174715  0.05690384 -0.9191216 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 1805 is [True, False, False, False, True, False]
Scene graph at timestep 1805 is [True, False, False, False, True, False]
State prediction error at timestep 1805 is tensor(1.2879e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1806. State = [[-0.14579493  0.02555556]]. Action = [[-0.19500023  0.0769043  -0.09087959  0.87775517]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 1806 is [True, False, False, False, True, False]
Current timestep = 1807. State = [[-0.14717375  0.02995089]]. Action = [[0.08327499 0.09045738 0.15107399 0.9570377 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 1807 is [True, False, False, False, True, False]
Scene graph at timestep 1807 is [True, False, False, False, True, False]
State prediction error at timestep 1807 is tensor(1.2895e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1808. State = [[-0.14838816  0.0339147 ]]. Action = [[-0.16994531  0.21356773 -0.1042926  -0.65216464]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 1808 is [True, False, False, False, True, False]
Human Feedback received at timestep 1808 of 1
Current timestep = 1809. State = [[-0.15049659  0.03986668]]. Action = [[-0.16087705  0.069107    0.16327688 -0.162781  ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 1809 is [True, False, False, False, True, False]
Current timestep = 1810. State = [[-0.1522446   0.04458045]]. Action = [[ 0.18094221  0.08070642 -0.15232709  0.10060561]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 1810 is [True, False, False, False, True, False]
Current timestep = 1811. State = [[-0.15350851  0.04846974]]. Action = [[ 0.04237586  0.19753557 -0.14021297 -0.89623284]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 1811 is [True, False, False, False, True, False]
Current timestep = 1812. State = [[-0.15473133  0.05241735]]. Action = [[-0.18074621 -0.1217538  -0.00351448 -0.54442525]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 1812 is [True, False, False, False, True, False]
Current timestep = 1813. State = [[-0.15539442  0.05392767]]. Action = [[-0.22515231 -0.21205917 -0.15957513  0.8537426 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 1813 is [True, False, False, False, True, False]
Current timestep = 1814. State = [[-0.15564226  0.05268019]]. Action = [[-0.05368033  0.02275297  0.14516926  0.9009564 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 1814 is [True, False, False, False, True, False]
Scene graph at timestep 1814 is [True, False, False, False, True, False]
State prediction error at timestep 1814 is tensor(1.5949e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1815. State = [[-0.15607066  0.05231947]]. Action = [[-0.23163123 -0.03329521  0.19168788  0.66573286]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 1815 is [True, False, False, False, True, False]
Current timestep = 1816. State = [[-0.15811753  0.05211232]]. Action = [[0.0292758  0.13512343 0.07713923 0.25980186]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 1816 is [True, False, False, False, True, False]
Current timestep = 1817. State = [[-0.15960735  0.0538768 ]]. Action = [[-0.09071863  0.2087726  -0.15118484  0.68157184]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 1817 is [True, False, False, False, True, False]
Current timestep = 1818. State = [[-0.16168092  0.0579206 ]]. Action = [[ 0.12007156 -0.21272498 -0.09074372 -0.5974309 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 1818 is [True, False, False, False, True, False]
Current timestep = 1819. State = [[-0.16201644  0.05745884]]. Action = [[-0.05072311 -0.18992873 -0.03972213 -0.41979814]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 1819 is [True, False, False, False, True, False]
Scene graph at timestep 1819 is [True, False, False, False, True, False]
State prediction error at timestep 1819 is tensor(1.0825e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1820. State = [[-0.16222733  0.05502845]]. Action = [[-0.24041347 -0.02822953 -0.21225618  0.00445879]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 1820 is [True, False, False, False, True, False]
Current timestep = 1821. State = [[-0.16357285  0.05313738]]. Action = [[-0.17001112  0.1708929  -0.14348239  0.39028323]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 1821 is [True, False, False, False, True, False]
Current timestep = 1822. State = [[-0.16702536  0.05445022]]. Action = [[-0.10255989  0.17857265  0.17034754  0.60781693]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 1822 is [True, False, False, False, True, False]
Current timestep = 1823. State = [[-0.17193998  0.05709508]]. Action = [[ 0.01081464 -0.18350746  0.03079784 -0.22389078]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 1823 is [True, False, False, False, True, False]
Current timestep = 1824. State = [[-0.17519592  0.05606923]]. Action = [[-0.00943011 -0.14018604 -0.10379848 -0.14505744]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 1824 is [True, False, False, False, True, False]
Current timestep = 1825. State = [[-0.17718649  0.05404224]]. Action = [[-0.1417024   0.18235114  0.12082449 -0.74470127]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 1825 is [True, False, False, False, True, False]
Scene graph at timestep 1825 is [True, False, False, False, True, False]
State prediction error at timestep 1825 is tensor(3.2534e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1826. State = [[-0.1802685   0.05559601]]. Action = [[-0.13520186  0.2363104  -0.15261449 -0.8952454 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 1826 is [True, False, False, False, True, False]
Current timestep = 1827. State = [[-0.18355896  0.05951096]]. Action = [[ 0.21462703 -0.00351135  0.19176266 -0.5650536 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 1827 is [True, False, False, False, True, False]
Scene graph at timestep 1827 is [True, False, False, False, True, False]
State prediction error at timestep 1827 is tensor(1.5383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1828. State = [[-0.18478444  0.06191296]]. Action = [[-0.0659402   0.13599241 -0.15955156  0.51827645]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 1828 is [True, False, False, False, True, False]
Current timestep = 1829. State = [[-0.18583371  0.06476104]]. Action = [[ 0.11895803  0.01545417  0.16882944 -0.297693  ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 1829 is [True, False, False, False, True, False]
Current timestep = 1830. State = [[-0.18665022  0.06683281]]. Action = [[-0.05972084  0.2443791   0.18570149  0.76882744]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 1830 is [True, False, False, False, True, False]
Current timestep = 1831. State = [[-0.18816757  0.07092211]]. Action = [[ 0.11061415 -0.04008891  0.07766557  0.24420571]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 1831 is [True, False, False, False, True, False]
Current timestep = 1832. State = [[-0.18834327  0.07282279]]. Action = [[ 0.23858982 -0.23004219 -0.21697405  0.47883308]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 1832 is [True, False, False, False, True, False]
Current timestep = 1833. State = [[-0.18708149  0.07204932]]. Action = [[ 0.1097559   0.03797209 -0.21650447  0.84715104]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 1833 is [True, False, False, False, True, False]
Current timestep = 1834. State = [[-0.18630695  0.07212014]]. Action = [[ 4.8300624e-04  1.6536254e-01 -1.8126351e-01 -8.6306161e-01]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 1834 is [True, False, False, False, True, False]
Current timestep = 1835. State = [[-0.18642364  0.07277928]]. Action = [[ 0.1251409  -0.22982687 -0.16875747 -0.66486305]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 1835 is [True, False, False, False, True, False]
Current timestep = 1836. State = [[-0.1853648   0.07180883]]. Action = [[-0.22355922  0.20998067  0.23040092  0.5099392 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 1836 is [True, False, False, False, True, False]
Scene graph at timestep 1836 is [True, False, False, False, True, False]
State prediction error at timestep 1836 is tensor(2.5956e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1837. State = [[-0.18577996  0.07284188]]. Action = [[-0.01944679  0.13848925  0.03977945  0.68231964]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 1837 is [True, False, False, False, True, False]
Current timestep = 1838. State = [[-0.18651804  0.07470176]]. Action = [[-0.17831312  0.17089492 -0.13031565 -0.97925013]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 1838 is [True, False, False, False, True, False]
Scene graph at timestep 1838 is [True, False, False, False, True, False]
State prediction error at timestep 1838 is tensor(2.0504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1839. State = [[-0.18797149  0.07832965]]. Action = [[ 0.00434262 -0.17014787  0.10408047  0.8948827 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 1839 is [True, False, False, False, True, False]
Current timestep = 1840. State = [[-0.18812573  0.07908387]]. Action = [[ 0.22704852  0.00851211 -0.17191029 -0.87590426]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 1840 is [True, False, False, False, True, False]
Current timestep = 1841. State = [[-0.18787703  0.07897481]]. Action = [[-0.17306785  0.06245565 -0.13794635 -0.22532964]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 1841 is [True, False, False, False, True, False]
Current timestep = 1842. State = [[-0.18821082  0.07961477]]. Action = [[-0.00322731 -0.01878321  0.17674488 -0.85036623]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 1842 is [True, False, False, False, True, False]
Current timestep = 1843. State = [[-0.18841372  0.08010601]]. Action = [[-0.0820758   0.22378269 -0.0269929   0.40450716]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 1843 is [True, False, False, False, True, False]
Current timestep = 1844. State = [[-0.18949467  0.08283655]]. Action = [[ 0.19922104 -0.06604654 -0.08693664 -0.17569274]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 1844 is [True, False, False, False, True, False]
Current timestep = 1845. State = [[-0.18950523  0.08375135]]. Action = [[-0.19557141  0.17795086  0.1070253  -0.12306529]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 1845 is [True, False, False, False, True, False]
Current timestep = 1846. State = [[-0.1905123   0.08654663]]. Action = [[-0.19781898 -0.20044681  0.24459177 -0.38073158]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 1846 is [True, False, False, False, True, False]
Current timestep = 1847. State = [[-0.19097379  0.08738586]]. Action = [[0.20327353 0.17115575 0.01664406 0.31008136]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 1847 is [True, False, False, False, True, False]
Scene graph at timestep 1847 is [True, False, False, False, True, False]
State prediction error at timestep 1847 is tensor(2.0177e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1848. State = [[-0.19141507  0.08856218]]. Action = [[ 0.08283028 -0.06039891 -0.21977676 -0.89271855]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 1848 is [True, False, False, False, True, False]
Scene graph at timestep 1848 is [True, False, False, False, True, False]
State prediction error at timestep 1848 is tensor(3.3107e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1848 of 1
Current timestep = 1849. State = [[-0.19145198  0.08868363]]. Action = [[ 0.00363675 -0.02866983  0.12816536  0.34496164]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 1849 is [True, False, False, False, True, False]
Scene graph at timestep 1849 is [True, False, False, False, True, False]
State prediction error at timestep 1849 is tensor(2.7599e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1850. State = [[-0.19143705  0.0887315 ]]. Action = [[ 0.17603233  0.21492964 -0.19375645 -0.8236586 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 1850 is [True, False, False, False, True, False]
Current timestep = 1851. State = [[-0.19074443  0.09051897]]. Action = [[ 0.11572722  0.02223623 -0.02123983  0.33799756]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 1851 is [True, False, False, False, True, False]
Current timestep = 1852. State = [[-0.18891367  0.09175725]]. Action = [[ 0.21966666 -0.22376809 -0.18286575  0.8730285 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 1852 is [True, False, False, False, True, False]
Scene graph at timestep 1852 is [True, False, False, False, True, False]
State prediction error at timestep 1852 is tensor(1.8904e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1853. State = [[-0.18522045  0.0914821 ]]. Action = [[-0.17656262  0.20342302 -0.2215722  -0.13316405]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 1853 is [True, False, False, False, True, False]
Current timestep = 1854. State = [[-0.18428887  0.09322187]]. Action = [[-0.13845246  0.14160079 -0.10912649 -0.5377571 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 1854 is [True, False, False, False, True, False]
Current timestep = 1855. State = [[-0.18519397  0.09565382]]. Action = [[ 0.23254001 -0.03961655  0.18769512  0.3524282 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 1855 is [True, False, False, False, True, False]
Current timestep = 1856. State = [[-0.18428959  0.09648625]]. Action = [[-0.24122211  0.24884832 -0.20172366 -0.13224977]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 1856 is [True, False, False, False, True, False]
Current timestep = 1857. State = [[-0.18547982  0.10051261]]. Action = [[ 0.02840227 -0.19777904  0.08628452 -0.6258187 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 1857 is [True, False, False, False, True, False]
Scene graph at timestep 1857 is [True, False, False, False, True, False]
State prediction error at timestep 1857 is tensor(8.0865e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1858. State = [[-0.18565674  0.10086516]]. Action = [[-0.00733432 -0.2150909   0.13580272  0.72959614]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 1858 is [True, False, False, False, True, False]
Current timestep = 1859. State = [[-0.18500857  0.09932889]]. Action = [[ 0.19301611  0.20867133  0.1588696  -0.03498423]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 1859 is [True, False, False, False, True, False]
Current timestep = 1860. State = [[-0.18465498  0.10036206]]. Action = [[-0.12284389  0.08007079 -0.19744796  0.39258194]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 1860 is [True, False, False, False, True, False]
Current timestep = 1861. State = [[-0.18518674  0.10161654]]. Action = [[-0.02397296  0.04735291  0.07360777 -0.5179798 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 1861 is [True, False, False, False, True, False]
Current timestep = 1862. State = [[-0.18554534  0.10257286]]. Action = [[ 0.2246232   0.15340185 -0.04267788 -0.31422883]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 1862 is [True, False, False, False, True, False]
Scene graph at timestep 1862 is [True, False, False, False, True, False]
State prediction error at timestep 1862 is tensor(8.0933e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1863. State = [[-0.18398666  0.10440058]]. Action = [[-0.15026048  0.00946522  0.08642313 -0.30193365]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 1863 is [True, False, False, False, True, False]
Current timestep = 1864. State = [[-0.1835971   0.10586112]]. Action = [[-0.09949911 -0.18614927  0.06243855  0.7341969 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 1864 is [True, False, False, False, True, False]
Scene graph at timestep 1864 is [True, False, False, False, True, False]
State prediction error at timestep 1864 is tensor(2.7406e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1865. State = [[-0.18354349  0.10580345]]. Action = [[ 0.17256564  0.01538122  0.10632521 -0.39641416]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 1865 is [True, False, False, False, True, False]
Scene graph at timestep 1865 is [True, False, False, False, True, False]
State prediction error at timestep 1865 is tensor(4.2072e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1866. State = [[-0.18321154  0.10582391]]. Action = [[-0.19957092 -0.00324562  0.04297829 -0.20292956]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 1866 is [True, False, False, False, True, False]
Current timestep = 1867. State = [[-0.18336318  0.10606044]]. Action = [[-0.14572684  0.19299337  0.03043211 -0.965075  ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 1867 is [True, False, False, False, True, False]
Scene graph at timestep 1867 is [True, False, False, False, True, False]
State prediction error at timestep 1867 is tensor(3.0374e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1868. State = [[-0.18452641  0.10852373]]. Action = [[-0.22106463 -0.08346076 -0.04250963  0.8191829 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 1868 is [True, False, False, False, True, False]
Current timestep = 1869. State = [[-0.18570808  0.11028652]]. Action = [[-0.22056893  0.02132151  0.22872806 -0.4021666 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 1869 is [True, False, False, False, True, False]
Scene graph at timestep 1869 is [True, False, False, False, True, False]
State prediction error at timestep 1869 is tensor(5.9542e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1870. State = [[-0.18861051  0.11228772]]. Action = [[ 0.20906338  0.11051342 -0.00632583  0.8751923 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 1870 is [True, False, False, False, True, False]
Current timestep = 1871. State = [[-0.18959086  0.11418749]]. Action = [[-0.21480608 -0.02572888 -0.22313838  0.3972683 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 1871 is [True, False, False, False, True, False]
Current timestep = 1872. State = [[-0.19126247  0.11587612]]. Action = [[-0.16633339  0.01559898 -0.21469632 -0.9030074 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 1872 is [True, False, False, False, True, False]
Current timestep = 1873. State = [[-0.19390006  0.11694898]]. Action = [[ 0.22240233 -0.21797936  0.23274115  0.9576862 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 1873 is [True, False, False, False, True, False]
Current timestep = 1874. State = [[-0.19387954  0.11494143]]. Action = [[-0.19584446 -0.06400663 -0.08831367 -0.50779235]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 1874 is [True, False, False, False, True, False]
Scene graph at timestep 1874 is [True, False, False, False, True, False]
State prediction error at timestep 1874 is tensor(2.0511e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1875. State = [[-0.1953002   0.11260929]]. Action = [[ 0.1320548  -0.05198608 -0.19895272  0.20360494]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 1875 is [True, False, False, False, True, False]
Current timestep = 1876. State = [[-0.19530275  0.11083458]]. Action = [[0.07671809 0.19482273 0.10754004 0.7122799 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 1876 is [True, False, False, False, True, False]
Current timestep = 1877. State = [[-0.19573998  0.11164855]]. Action = [[ 0.12138841  0.17889166  0.21429712 -0.6587901 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 1877 is [True, False, False, False, True, False]
Scene graph at timestep 1877 is [True, False, False, False, True, False]
State prediction error at timestep 1877 is tensor(5.7626e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1878. State = [[-0.1963626  0.1129614]]. Action = [[-0.08799958  0.22841024 -0.06338616 -0.3620404 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 1878 is [True, False, False, False, True, False]
Scene graph at timestep 1878 is [True, False, False, False, True, False]
State prediction error at timestep 1878 is tensor(8.7015e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1879. State = [[-0.19872239  0.11767797]]. Action = [[ 0.04126272 -0.0636209  -0.17642     0.2888105 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 1879 is [True, False, False, False, True, False]
Human Feedback received at timestep 1879 of -1
Current timestep = 1880. State = [[-0.1994467   0.11911839]]. Action = [[-0.19857904 -0.1880425  -0.09570533 -0.9435609 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 1880 is [True, False, False, False, True, False]
Current timestep = 1881. State = [[-0.19942993  0.11879379]]. Action = [[ 0.111646   -0.11038354  0.03937218  0.60904276]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 1881 is [True, False, False, False, True, False]
Current timestep = 1882. State = [[-0.19912995  0.1175311 ]]. Action = [[-0.19848505 -0.18032826  0.0636988   0.29608595]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 1882 is [True, False, False, False, True, False]
Current timestep = 1883. State = [[-0.19931705  0.11491974]]. Action = [[ 0.06262892  0.21343178  0.19035006 -0.15400231]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 1883 is [True, False, False, False, True, False]
Current timestep = 1884. State = [[-0.19981425  0.11548709]]. Action = [[-0.15046676  0.15181687  0.10729951 -0.45595407]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 1884 is [True, False, False, False, True, False]
Current timestep = 1885. State = [[-0.20116973  0.11757065]]. Action = [[-0.1997124  -0.19625036  0.12671977 -0.71439   ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 1885 is [True, False, False, False, True, False]
Scene graph at timestep 1885 is [True, False, False, False, True, False]
State prediction error at timestep 1885 is tensor(1.1019e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1886. State = [[-0.20292614  0.11679576]]. Action = [[ 0.17121145 -0.13004097  0.13487321  0.17223668]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 1886 is [True, False, False, False, True, False]
Current timestep = 1887. State = [[-0.2028415   0.11512045]]. Action = [[0.08724728 0.14002234 0.06387442 0.12207544]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 1887 is [True, False, False, False, True, False]
Current timestep = 1888. State = [[-0.20291843  0.11508996]]. Action = [[-0.07513395 -0.0730252   0.06967369 -0.78969437]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 1888 is [True, False, False, False, True, False]
Current timestep = 1889. State = [[-0.20290893  0.11502376]]. Action = [[0.12508452 0.04517695 0.11063462 0.7031026 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 1889 is [True, False, False, False, True, False]
Scene graph at timestep 1889 is [True, False, False, False, True, False]
State prediction error at timestep 1889 is tensor(1.0267e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1890. State = [[-0.20288862  0.1150431 ]]. Action = [[ 0.05775219 -0.07884112  0.05466565  0.5017345 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 1890 is [True, False, False, False, True, False]
Scene graph at timestep 1890 is [True, False, False, False, True, False]
State prediction error at timestep 1890 is tensor(1.4595e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1891. State = [[-0.20265147  0.1141608 ]]. Action = [[ 0.18222547 -0.12945464  0.2079004  -0.2851395 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 1891 is [True, False, False, False, True, False]
Scene graph at timestep 1891 is [True, False, False, False, True, False]
State prediction error at timestep 1891 is tensor(1.5074e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1892. State = [[-0.20148733  0.11202355]]. Action = [[ 0.06979838 -0.09007911  0.02213687 -0.79279774]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 1892 is [True, False, False, False, True, False]
Current timestep = 1893. State = [[-0.20024115  0.10916445]]. Action = [[-0.15445733 -0.19282143  0.06667188  0.1875912 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 1893 is [True, False, False, False, True, False]
Current timestep = 1894. State = [[-0.19954638  0.10506187]]. Action = [[-0.10888955 -0.11821157 -0.15513122 -0.5206237 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 1894 is [True, False, False, False, True, False]
Current timestep = 1895. State = [[-0.1992917   0.10101333]]. Action = [[ 0.05249724  0.11811173 -0.21833882 -0.29784775]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 1895 is [True, False, False, False, True, False]
Current timestep = 1896. State = [[-0.19951941  0.09966733]]. Action = [[-0.10154974 -0.04965082 -0.19949983 -0.8223384 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 1896 is [True, False, False, False, True, False]
Current timestep = 1897. State = [[-0.19963762  0.0988417 ]]. Action = [[-0.06644586 -0.00816923  0.12276876 -0.27815664]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 1897 is [True, False, False, False, True, False]
Current timestep = 1898. State = [[-0.20021056  0.09815777]]. Action = [[-0.13937287 -0.00338925  0.16331464  0.8239026 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 1898 is [True, False, False, False, True, False]
Current timestep = 1899. State = [[-0.2011331   0.09745546]]. Action = [[-0.00381623 -0.18376291 -0.12661436 -0.75997317]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 1899 is [True, False, False, False, True, False]
Scene graph at timestep 1899 is [True, False, False, False, True, False]
State prediction error at timestep 1899 is tensor(3.2672e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1900. State = [[-0.20204546  0.09469745]]. Action = [[-0.10860422  0.04266959  0.167517   -0.6803811 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 1900 is [True, False, False, False, True, False]
Scene graph at timestep 1900 is [True, False, False, False, True, False]
State prediction error at timestep 1900 is tensor(4.4882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1901. State = [[-0.2036421   0.09306073]]. Action = [[ 0.09051976 -0.03571101  0.18583035  0.35485852]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 1901 is [True, False, False, False, True, False]
Current timestep = 1902. State = [[-0.20453067  0.09113225]]. Action = [[-0.07740635 -0.20243135 -0.12319347 -0.66523844]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 1902 is [True, False, False, False, True, False]
Current timestep = 1903. State = [[-0.20561536  0.08766187]]. Action = [[-0.20716588  0.03176463 -0.17989512  0.2337147 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 1903 is [True, False, False, False, True, False]
Scene graph at timestep 1903 is [True, False, False, False, True, False]
State prediction error at timestep 1903 is tensor(1.1791e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1904. State = [[-0.2088031   0.08555418]]. Action = [[ 4.9502283e-02 -7.5845271e-02 -4.5359135e-05 -4.8135424e-01]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 1904 is [True, False, False, False, True, False]
Current timestep = 1905. State = [[-0.21023268  0.08362834]]. Action = [[-0.24366818 -0.07975495  0.20098323  0.74855185]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 1905 is [True, False, False, False, True, False]
Current timestep = 1906. State = [[-0.21249557  0.0819506 ]]. Action = [[-0.13888907  0.19424456  0.22512567  0.6130991 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 1906 is [True, False, False, False, True, False]
Scene graph at timestep 1906 is [True, False, False, False, True, False]
State prediction error at timestep 1906 is tensor(1.9257e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1907. State = [[-0.21573296  0.08315141]]. Action = [[-0.12550177 -0.11640321  0.07243684  0.92366767]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 1907 is [True, False, False, False, True, False]
Current timestep = 1908. State = [[-0.21895869  0.08241058]]. Action = [[-0.02689944  0.13131589 -0.18036798  0.46249712]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 1908 is [True, False, False, False, True, False]
Scene graph at timestep 1908 is [True, False, False, False, True, False]
State prediction error at timestep 1908 is tensor(9.4895e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1909. State = [[-0.2234919   0.08296164]]. Action = [[ 0.22888339  0.05351999  0.04381073 -0.70161927]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 1909 is [True, False, False, False, True, False]
Current timestep = 1910. State = [[-0.22417946  0.08302721]]. Action = [[ 0.01048121 -0.2369478  -0.20769225  0.5125797 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 1910 is [True, False, False, False, True, False]
Current timestep = 1911. State = [[-0.2241289  0.081555 ]]. Action = [[-0.03935799  0.05109566  0.20303923  0.3406937 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 1911 is [True, False, False, False, True, False]
Current timestep = 1912. State = [[-0.22440347  0.08073976]]. Action = [[ 0.11681926 -0.21446118 -0.17927672 -0.4795735 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 1912 is [True, False, False, False, True, False]
Scene graph at timestep 1912 is [True, False, False, False, True, False]
State prediction error at timestep 1912 is tensor(2.4299e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1913. State = [[-0.22413066  0.07778049]]. Action = [[-0.19099802  0.10280693  0.02953002  0.5053642 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 1913 is [True, False, False, False, True, False]
Current timestep = 1914. State = [[-0.2253331   0.07672309]]. Action = [[-0.14567772  0.05774611  0.16453865  0.9001807 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 1914 is [True, False, False, False, True, False]
Current timestep = 1915. State = [[-0.22696498  0.07699512]]. Action = [[-0.14908536  0.14709637  0.09812468  0.9906633 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 1915 is [True, False, False, False, True, False]
Scene graph at timestep 1915 is [True, False, False, False, True, False]
State prediction error at timestep 1915 is tensor(6.0402e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1916. State = [[-0.22991058  0.07917196]]. Action = [[ 0.14925498 -0.08354419  0.14435506  0.23488998]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 1916 is [True, False, False, False, True, False]
Scene graph at timestep 1916 is [True, False, False, False, True, False]
State prediction error at timestep 1916 is tensor(3.3679e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1916 of -1
Current timestep = 1917. State = [[-0.23085216  0.0790732 ]]. Action = [[-0.11314827 -0.06157048  0.1688329  -0.27230185]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 1917 is [True, False, False, False, True, False]
Current timestep = 1918. State = [[-0.23233767  0.07847716]]. Action = [[ 0.12475353 -0.0324266  -0.2039812   0.45354164]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 1918 is [True, False, False, False, True, False]
Scene graph at timestep 1918 is [True, False, False, False, True, False]
State prediction error at timestep 1918 is tensor(1.2979e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1919. State = [[-0.2326391   0.07777228]]. Action = [[-2.10803732e-01 -1.26456454e-01 -1.02728605e-04  2.05151439e-01]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 1919 is [True, False, False, False, True, False]
Current timestep = 1920. State = [[-0.23431428  0.07612721]]. Action = [[-0.03759617  0.06965095 -0.18588825 -0.23560351]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 1920 is [True, False, False, False, True, False]
Current timestep = 1921. State = [[-0.23584883  0.07554595]]. Action = [[-0.19362627 -0.17655034 -0.16630505  0.92817116]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 1921 is [True, False, False, False, True, False]
Current timestep = 1922. State = [[-0.23835787  0.07373951]]. Action = [[0.19235623 0.21228597 0.16028285 0.4117434 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 1922 is [True, False, False, False, True, False]
Current timestep = 1923. State = [[-0.23910491  0.07402352]]. Action = [[-0.18653665  0.11800823 -0.19979918 -0.30837446]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 1923 is [True, False, False, False, True, False]
Current timestep = 1924. State = [[-0.24101901  0.07628696]]. Action = [[-0.08681241  0.07577139 -0.09084818  0.92117035]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 1924 is [True, False, False, False, True, False]
Scene graph at timestep 1924 is [True, False, False, False, True, False]
State prediction error at timestep 1924 is tensor(6.5234e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1925. State = [[-0.24343327  0.07901432]]. Action = [[ 0.14147595  0.22534394 -0.10671741  0.9346062 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 1925 is [True, False, False, False, True, False]
Current timestep = 1926. State = [[-0.24498975  0.08284383]]. Action = [[ 0.04733777 -0.06986555 -0.19945455 -0.01178718]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 1926 is [True, False, False, False, True, False]
Current timestep = 1927. State = [[-0.24564905  0.08424175]]. Action = [[-0.06533048 -0.05679601  0.06594363 -0.94677985]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 1927 is [True, False, False, False, True, False]
Current timestep = 1928. State = [[-0.24575444  0.08461144]]. Action = [[0.20724562 0.00869876 0.09702685 0.46947277]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 1928 is [True, False, False, False, True, False]
Current timestep = 1929. State = [[-0.2456858   0.08452718]]. Action = [[ 0.10564354 -0.20672262 -0.20727113 -0.95693856]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 1929 is [True, False, False, False, True, False]
Current timestep = 1930. State = [[-0.24463812  0.08331001]]. Action = [[-0.09750746 -0.01057655  0.13579029  0.42535818]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 1930 is [True, False, False, False, True, False]
Current timestep = 1931. State = [[-0.24429405  0.08223587]]. Action = [[ 0.18334383 -0.04882729 -0.03283533  0.07589614]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 1931 is [True, False, False, False, True, False]
Current timestep = 1932. State = [[-0.24355397  0.08100306]]. Action = [[ 0.22888535 -0.12125656  0.07457748  0.55963564]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 1932 is [True, False, False, False, True, False]
Scene graph at timestep 1932 is [True, False, False, False, True, False]
State prediction error at timestep 1932 is tensor(4.1830e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1933. State = [[-0.24065058  0.07780443]]. Action = [[ 0.12701255 -0.16206637  0.11010012  0.23711395]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 1933 is [True, False, False, False, True, False]
Current timestep = 1934. State = [[-0.23814529  0.07441878]]. Action = [[0.20883861 0.1385062  0.11260909 0.01868308]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 1934 is [True, False, False, False, True, False]
Scene graph at timestep 1934 is [True, False, False, False, True, False]
State prediction error at timestep 1934 is tensor(1.1109e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1935. State = [[-0.235024    0.07381459]]. Action = [[-0.19641677  0.13967279  0.1042811   0.33480728]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 1935 is [True, False, False, False, True, False]
Current timestep = 1936. State = [[-0.23467234  0.0741968 ]]. Action = [[ 0.08060563 -0.16614257 -0.21209806 -0.7649473 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 1936 is [True, False, False, False, True, False]
Current timestep = 1937. State = [[-0.23375529  0.07303488]]. Action = [[ 0.16798824 -0.04960603 -0.18054679 -0.8595243 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 1937 is [True, False, False, False, True, False]
Scene graph at timestep 1937 is [True, False, False, False, True, False]
State prediction error at timestep 1937 is tensor(1.0406e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1938. State = [[-0.23188367  0.07158242]]. Action = [[ 0.0389761  -0.10187975 -0.19492346  0.17895663]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 1938 is [True, False, False, False, True, False]
Current timestep = 1939. State = [[-0.22998445  0.06955791]]. Action = [[-0.01058985  0.03305241  0.14705437  0.81341505]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 1939 is [True, False, False, False, True, False]
Scene graph at timestep 1939 is [True, False, False, False, True, False]
State prediction error at timestep 1939 is tensor(4.0333e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1940. State = [[-0.22841178  0.06893602]]. Action = [[ 0.11681518  0.14881924  0.15277934 -0.62819755]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 1940 is [True, False, False, False, True, False]
Current timestep = 1941. State = [[-0.22671433  0.06955442]]. Action = [[ 0.14866549  0.04824618  0.01687515 -0.6872583 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 1941 is [True, False, False, False, True, False]
Current timestep = 1942. State = [[-0.2244195   0.07045737]]. Action = [[-0.02857399  0.20875248 -0.10774195  0.6578841 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 1942 is [True, False, False, False, True, False]
Current timestep = 1943. State = [[-0.22329625  0.07266412]]. Action = [[-0.11913744  0.08678335  0.1102879   0.68860424]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 1943 is [True, False, False, False, True, False]
Current timestep = 1944. State = [[-0.22378649  0.07471307]]. Action = [[ 0.14798042  0.15978485 -0.15044715  0.43092263]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 1944 is [True, False, False, False, True, False]
Current timestep = 1945. State = [[-0.222964    0.07738334]]. Action = [[ 0.07224196  0.02425882 -0.20251925 -0.26861393]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 1945 is [True, False, False, False, True, False]
Current timestep = 1946. State = [[-0.22196962  0.07967547]]. Action = [[-0.04671374  0.18100923 -0.0107228   0.80715084]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 1946 is [True, False, False, False, True, False]
Human Feedback received at timestep 1946 of 1
Current timestep = 1947. State = [[-0.22184451  0.08276433]]. Action = [[-0.20992687 -0.2465183   0.0295572  -0.7574623 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 1947 is [True, False, False, False, True, False]
Current timestep = 1948. State = [[-0.22198138  0.08283313]]. Action = [[-0.07454087 -0.06934771 -0.07165569 -0.12044579]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 1948 is [True, False, False, False, True, False]
Current timestep = 1949. State = [[-0.22194323  0.08263514]]. Action = [[-0.00196478  0.12571633 -0.01934668  0.9078069 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 1949 is [True, False, False, False, True, False]
Human Feedback received at timestep 1949 of 1
Current timestep = 1950. State = [[-0.22228125  0.08335264]]. Action = [[ 0.09457266 -0.23401731  0.12220079 -0.4489408 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 1950 is [True, False, False, False, True, False]
Current timestep = 1951. State = [[-0.22207172  0.08259858]]. Action = [[-0.07013214  0.19970065  0.13586217 -0.6769428 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 1951 is [True, False, False, False, True, False]
Current timestep = 1952. State = [[-0.22230613  0.08314291]]. Action = [[ 0.035763    0.07978708  0.10738426 -0.33174574]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 1952 is [True, False, False, False, True, False]
Human Feedback received at timestep 1952 of 1
Current timestep = 1953. State = [[-0.2227009   0.08411941]]. Action = [[ 0.24077064 -0.10729416 -0.0207447   0.79190886]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 1953 is [True, False, False, False, True, False]
Current timestep = 1954. State = [[-0.22239304  0.0840807 ]]. Action = [[-0.2438593   0.01489401 -0.12565336 -0.9825937 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 1954 is [True, False, False, False, True, False]
Current timestep = 1955. State = [[-0.22233711  0.08396365]]. Action = [[-0.10897858 -0.01233886 -0.03756791  0.26323903]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 1955 is [True, False, False, False, True, False]
Current timestep = 1956. State = [[-0.22253563  0.08442781]]. Action = [[ 0.05722505  0.20704585 -0.04033822  0.13951802]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 1956 is [True, False, False, False, True, False]
Scene graph at timestep 1956 is [True, False, False, False, True, False]
State prediction error at timestep 1956 is tensor(5.4094e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1957. State = [[-0.22326905  0.08614987]]. Action = [[ 0.12845162 -0.00975384  0.05323887 -0.7316406 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 1957 is [True, False, False, False, True, False]
Current timestep = 1958. State = [[-0.22334187  0.08680461]]. Action = [[ 0.11504191  0.13274312  0.24324173 -0.52575076]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 1958 is [True, False, False, False, True, False]
Current timestep = 1959. State = [[-0.22290738  0.08856042]]. Action = [[0.1588856  0.07453278 0.1038993  0.8272927 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 1959 is [True, False, False, False, True, False]
Current timestep = 1960. State = [[-0.22091404  0.09134024]]. Action = [[-0.15522343  0.036935   -0.17619507 -0.10042495]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 1960 is [True, False, False, False, True, False]
Scene graph at timestep 1960 is [True, False, False, False, True, False]
State prediction error at timestep 1960 is tensor(1.0436e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1961. State = [[-0.22016764  0.09389064]]. Action = [[-0.22972003  0.12836367 -0.22961608  0.1921432 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 1961 is [True, False, False, False, True, False]
Scene graph at timestep 1961 is [True, False, False, False, True, False]
State prediction error at timestep 1961 is tensor(2.6144e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1962. State = [[-0.22099788  0.09709668]]. Action = [[ 0.01276189 -0.247932   -0.11001788 -0.12858075]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 1962 is [True, False, False, False, True, False]
Scene graph at timestep 1962 is [True, False, False, False, True, False]
State prediction error at timestep 1962 is tensor(3.3406e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1963. State = [[-0.22098233  0.09703524]]. Action = [[ 0.16294217 -0.13653898  0.11401415 -0.23578358]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 1963 is [True, False, False, False, True, False]
Current timestep = 1964. State = [[-0.2203088   0.09585616]]. Action = [[ 0.21200171  0.19736618 -0.19670027 -0.94811934]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 1964 is [True, False, False, False, True, False]
Current timestep = 1965. State = [[-0.21903297  0.09638147]]. Action = [[-0.09674387  0.07113796  0.16378105  0.74859977]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 1965 is [True, False, False, False, True, False]
Current timestep = 1966. State = [[-0.21837921  0.09737312]]. Action = [[ 0.09980547 -0.11976905  0.07557893  0.23733652]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 1966 is [True, False, False, False, True, False]
Current timestep = 1967. State = [[-0.21702974  0.09764154]]. Action = [[-0.11983238  0.17511463  0.01730716 -0.39583826]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 1967 is [True, False, False, False, True, False]
Current timestep = 1968. State = [[-0.21658942  0.09914454]]. Action = [[-0.17861232  0.20330241  0.06178659  0.66317725]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 1968 is [True, False, False, False, True, False]
Scene graph at timestep 1968 is [True, False, False, False, True, False]
State prediction error at timestep 1968 is tensor(4.6382e-07, grad_fn=<MseLossBackward0>)
Current timestep = 1969. State = [[-0.21816725  0.10257795]]. Action = [[ 0.11197168 -0.0266739  -0.06394772 -0.34635472]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 1969 is [True, False, False, False, True, False]
Current timestep = 1970. State = [[-0.21835737  0.10391347]]. Action = [[-0.21656798 -0.10716802 -0.03510222 -0.8109029 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 1970 is [True, False, False, False, True, False]
Current timestep = 1971. State = [[-0.21848647  0.10420438]]. Action = [[ 0.13961011 -0.19555916 -0.02469184  0.45072627]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 1971 is [True, False, False, False, True, False]
Current timestep = 1972. State = [[-0.2182633   0.10356387]]. Action = [[ 0.00219274  0.06396508 -0.21873654 -0.69117975]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 1972 is [True, False, False, False, True, False]
Current timestep = 1973. State = [[-0.21815081  0.10334646]]. Action = [[-0.20882142 -0.05636406 -0.09655765  0.01465607]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 1973 is [True, False, False, False, True, False]
Scene graph at timestep 1973 is [True, False, False, False, True, False]
State prediction error at timestep 1973 is tensor(1.1829e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1974. State = [[-0.21829496  0.1031874 ]]. Action = [[-0.05618832  0.17974445 -0.24610296  0.9864352 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 1974 is [True, False, False, False, True, False]
Current timestep = 1975. State = [[-0.21894641  0.10447276]]. Action = [[-0.09690653 -0.17827725  0.07300389  0.6416793 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 1975 is [True, False, False, False, True, False]
Current timestep = 1976. State = [[-0.21910533  0.1039169 ]]. Action = [[ 0.07557359 -0.21392551  0.04732698 -0.09319919]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 1976 is [True, False, False, False, True, False]
Current timestep = 1977. State = [[-0.21881153  0.10139654]]. Action = [[ 0.18501484  0.16850004 -0.14334452 -0.42192632]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 1977 is [True, False, False, False, True, False]
Current timestep = 1978. State = [[-0.21886455  0.10128302]]. Action = [[-0.13820565  0.12634635 -0.23955855 -0.7545521 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 1978 is [True, False, False, False, True, False]
Scene graph at timestep 1978 is [True, False, False, False, True, False]
State prediction error at timestep 1978 is tensor(1.8541e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1978 of 1
Current timestep = 1979. State = [[-0.21929641  0.10198583]]. Action = [[ 0.07954839 -0.09603539  0.05448997  0.16789174]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 1979 is [True, False, False, False, True, False]
Scene graph at timestep 1979 is [True, False, False, False, True, False]
State prediction error at timestep 1979 is tensor(2.5482e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1980. State = [[-0.21937063  0.10213806]]. Action = [[ 0.04602197  0.23692963 -0.20078202  0.88981795]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 1980 is [True, False, False, False, True, False]
Scene graph at timestep 1980 is [True, False, False, False, True, False]
State prediction error at timestep 1980 is tensor(1.7460e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1981. State = [[-0.21992113  0.10339227]]. Action = [[-0.216517   -0.16095626  0.11581033  0.09262192]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 1981 is [True, False, False, False, True, False]
Scene graph at timestep 1981 is [True, False, False, False, True, False]
State prediction error at timestep 1981 is tensor(1.5426e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1982. State = [[-0.21992867  0.10331677]]. Action = [[-0.13127016 -0.2054395   0.20305914  0.6552346 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 1982 is [True, False, False, False, True, False]
Current timestep = 1983. State = [[-0.22116445  0.10166474]]. Action = [[-0.05499682 -0.03331362  0.19858736  0.8978027 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 1983 is [True, False, False, False, True, False]
Current timestep = 1984. State = [[-0.22231404  0.10018441]]. Action = [[-0.01873159 -0.13632414  0.11590266  0.25622714]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 1984 is [True, False, False, False, True, False]
Scene graph at timestep 1984 is [True, False, False, False, True, False]
State prediction error at timestep 1984 is tensor(1.3204e-05, grad_fn=<MseLossBackward0>)
Current timestep = 1985. State = [[-0.22337687  0.09761894]]. Action = [[0.09688759 0.22649536 0.23428369 0.11993837]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 1985 is [True, False, False, False, True, False]
Current timestep = 1986. State = [[-0.2236664   0.09792498]]. Action = [[-0.0282014  -0.21276641 -0.20664242 -0.3726881 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 1986 is [True, False, False, False, True, False]
Current timestep = 1987. State = [[-0.22386658  0.09629455]]. Action = [[-0.19723879  0.17580563  0.12789178  0.06890488]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 1987 is [True, False, False, False, True, False]
Scene graph at timestep 1987 is [True, False, False, False, True, False]
State prediction error at timestep 1987 is tensor(8.6577e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1988. State = [[-0.22571197  0.09728993]]. Action = [[ 0.08664149  0.22538906  0.24335575 -0.08206654]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 1988 is [True, False, False, False, True, False]
Scene graph at timestep 1988 is [True, False, False, False, True, False]
State prediction error at timestep 1988 is tensor(5.1049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1989. State = [[-0.22714464  0.10009006]]. Action = [[-0.13226688 -0.02307534  0.16495842  0.60697746]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 1989 is [True, False, False, False, True, False]
Current timestep = 1990. State = [[-0.22852433  0.1021997 ]]. Action = [[-0.03759916  0.16435975 -0.05522579  0.7955117 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 1990 is [True, False, False, False, True, False]
Scene graph at timestep 1990 is [True, False, False, False, True, False]
State prediction error at timestep 1990 is tensor(1.9388e-06, grad_fn=<MseLossBackward0>)
Current timestep = 1991. State = [[-0.23055203  0.10551309]]. Action = [[ 0.21737349  0.18657464 -0.06729855  0.7798078 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 1991 is [True, False, False, False, True, False]
Current timestep = 1992. State = [[-0.2323651   0.10909554]]. Action = [[-0.07814053  0.14107397  0.06334922 -0.14452255]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 1992 is [True, False, False, False, True, False]
Current timestep = 1993. State = [[-0.23416366  0.11282706]]. Action = [[0.10737279 0.05740771 0.08838069 0.52414644]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 1993 is [True, False, False, False, True, False]
Current timestep = 1994. State = [[-0.23530771  0.11518681]]. Action = [[0.13913912 0.1842643  0.01049548 0.75732374]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 1994 is [True, False, False, False, True, False]
Current timestep = 1995. State = [[-0.23517779  0.11849449]]. Action = [[ 0.0200783  -0.0807924  -0.20849982  0.55915713]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 1995 is [True, False, False, False, True, False]
Current timestep = 1996. State = [[-0.23505555  0.11934555]]. Action = [[-0.17120306  0.06760341  0.18100148  0.901876  ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 1996 is [True, False, False, False, True, False]
Current timestep = 1997. State = [[-0.2358844   0.12133362]]. Action = [[ 0.21753225  0.14180502 -0.12862384  0.86122036]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 1997 is [True, False, False, False, True, False]
Human Feedback received at timestep 1997 of -1
Current timestep = 1998. State = [[-0.23571031  0.12397477]]. Action = [[-0.13546497  0.06902441  0.11438328  0.6578567 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 1998 is [True, False, False, False, True, False]
Current timestep = 1999. State = [[-0.23692629  0.12666924]]. Action = [[-0.02558208  0.16688275  0.18124783 -0.65912664]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 1999 is [True, False, False, False, True, False]
Human Feedback received at timestep 1999 of -1
Current timestep = 2000. State = [[-0.23781903  0.13123171]]. Action = [[-0.06200565  0.18332914 -0.20035799  0.9771249 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 2000 is [True, False, False, False, False, True]
Scene graph at timestep 2000 is [True, False, False, False, False, True]
State prediction error at timestep 2000 is tensor(7.8680e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2001. State = [[-0.23902087  0.1367493 ]]. Action = [[-0.22838356  0.06658795  0.18763167 -0.6830729 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 2001 is [True, False, False, False, False, True]
Current timestep = 2002. State = [[-0.24192002  0.14147173]]. Action = [[-0.21464811  0.13121313  0.18848675 -0.280019  ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 2002 is [True, False, False, False, False, True]
Scene graph at timestep 2002 is [True, False, False, False, False, True]
State prediction error at timestep 2002 is tensor(1.6221e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2002 of -1
Current timestep = 2003. State = [[-0.24556738  0.14739238]]. Action = [[ 0.1097998  -0.11815982 -0.1028423   0.04800427]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 2003 is [True, False, False, False, False, True]
Current timestep = 2004. State = [[-0.24655691  0.14890045]]. Action = [[-0.24553934  0.04322276 -0.06624433 -0.9613233 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 2004 is [True, False, False, False, False, True]
Scene graph at timestep 2004 is [True, False, False, False, False, True]
State prediction error at timestep 2004 is tensor(5.1228e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2005. State = [[-0.24819967  0.15137306]]. Action = [[-0.21348068  0.11612093 -0.23036166  0.4412825 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 2005 is [True, False, False, False, False, True]
Human Feedback received at timestep 2005 of -1
Current timestep = 2006. State = [[-0.25092128  0.15524516]]. Action = [[-0.14635378 -0.11152834 -0.18929745 -0.95563674]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 2006 is [True, False, False, False, False, True]
Current timestep = 2007. State = [[-0.24090944  0.03421206]]. Action = [[ 0.07811785  0.1975815  -0.21151765  0.30013514]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 2007 is [True, False, False, False, False, True]
Human Feedback received at timestep 2007 of -1
Current timestep = 2008. State = [[-0.23976019  0.03626109]]. Action = [[-0.23665325  0.13181525  0.01410079  0.69669557]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 2008 is [True, False, False, False, True, False]
Current timestep = 2009. State = [[-0.24057522  0.03857606]]. Action = [[ 0.18846148  0.14125061  0.07944214 -0.7026611 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 2009 is [True, False, False, False, True, False]
Current timestep = 2010. State = [[-0.24139065  0.04151647]]. Action = [[ 0.10337868 -0.06116164  0.19897616  0.8685124 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2010 is [True, False, False, False, True, False]
Current timestep = 2011. State = [[-0.24096707  0.04293074]]. Action = [[-0.23632917  0.22556281 -0.2319938   0.73528385]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 2011 is [True, False, False, False, True, False]
Current timestep = 2012. State = [[-0.24241647  0.04674028]]. Action = [[-0.19261919 -0.1895747   0.08418703  0.15641177]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 2012 is [True, False, False, False, True, False]
Current timestep = 2013. State = [[-0.24303074  0.0480269 ]]. Action = [[ 0.22709727 -0.06830528 -0.1084094   0.23818707]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 2013 is [True, False, False, False, True, False]
Current timestep = 2014. State = [[-0.24299677  0.04800356]]. Action = [[-0.11938646  0.24151862  0.12248468  0.6815982 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 2014 is [True, False, False, False, True, False]
Current timestep = 2015. State = [[-0.24389482  0.05030575]]. Action = [[0.14259455 0.12576258 0.12204212 0.3524828 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 2015 is [True, False, False, False, True, False]
Current timestep = 2016. State = [[-0.2448544   0.05276122]]. Action = [[-0.21835598  0.11872032 -0.19757873  0.07929349]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 2016 is [True, False, False, False, True, False]
Current timestep = 2017. State = [[-0.24630295  0.05631946]]. Action = [[ 0.21824044 -0.23650406 -0.15336823 -0.8536485 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 2017 is [True, False, False, False, True, False]
Scene graph at timestep 2017 is [True, False, False, False, True, False]
State prediction error at timestep 2017 is tensor(1.2437e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2018. State = [[-0.24613261  0.05642378]]. Action = [[ 0.1704241   0.04560935 -0.15473251  0.22891927]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 2018 is [True, False, False, False, True, False]
Current timestep = 2019. State = [[-0.24430795  0.05623455]]. Action = [[-0.19109397 -0.19137894 -0.10499644  0.7068398 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 2019 is [True, False, False, False, True, False]
Current timestep = 2020. State = [[-0.24415348  0.05497745]]. Action = [[ 0.24127638 -0.06816907  0.13962    -0.6788981 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 2020 is [True, False, False, False, True, False]
Current timestep = 2021. State = [[-0.24230367  0.05367239]]. Action = [[0.21609616 0.13167635 0.2188288  0.81822455]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 2021 is [True, False, False, False, True, False]
Current timestep = 2022. State = [[-0.23829453  0.05412489]]. Action = [[-0.16729425  0.06135795  0.08637154  0.81082153]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 2022 is [True, False, False, False, True, False]
Current timestep = 2023. State = [[-0.23592219  0.0544191 ]]. Action = [[ 0.11215591 -0.20659377  0.05728355  0.36456513]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 2023 is [True, False, False, False, True, False]
Current timestep = 2024. State = [[-0.23416021  0.05358409]]. Action = [[-0.17485559  0.15535426 -0.0659897   0.45795202]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 2024 is [True, False, False, False, True, False]
Current timestep = 2025. State = [[-0.23405145  0.05372667]]. Action = [[-0.09608413 -0.14247829  0.01897556  0.708856  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 2025 is [True, False, False, False, True, False]
Current timestep = 2026. State = [[-0.23408572  0.05301021]]. Action = [[ 0.0281662  -0.2125005  -0.15866487 -0.05572236]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 2026 is [True, False, False, False, True, False]
Current timestep = 2027. State = [[-0.23400636  0.05028193]]. Action = [[-0.23966856  0.06796524 -0.18634063  0.06156862]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 2027 is [True, False, False, False, True, False]
Current timestep = 2028. State = [[-0.23434168  0.04934605]]. Action = [[-0.10519662  0.14926839  0.05813354 -0.46471125]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 2028 is [True, False, False, False, True, False]
Current timestep = 2029. State = [[-0.23549686  0.05001618]]. Action = [[ 0.1746189  -0.18536617 -0.22235616 -0.28927964]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 2029 is [True, False, False, False, True, False]
Scene graph at timestep 2029 is [True, False, False, False, True, False]
State prediction error at timestep 2029 is tensor(9.6132e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2030. State = [[-0.23537433  0.04869856]]. Action = [[ 0.15706936  0.00759023  0.03927407 -0.6367968 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 2030 is [True, False, False, False, True, False]
Scene graph at timestep 2030 is [True, False, False, False, True, False]
State prediction error at timestep 2030 is tensor(5.4847e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2031. State = [[-0.23532723  0.04771071]]. Action = [[-0.12614639 -0.06325473  0.14836544  0.00855494]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 2031 is [True, False, False, False, True, False]
Scene graph at timestep 2031 is [True, False, False, False, True, False]
State prediction error at timestep 2031 is tensor(1.8361e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2032. State = [[-0.23530029  0.04623193]]. Action = [[ 0.08161628  0.0110788   0.00784537 -0.86817086]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 2032 is [True, False, False, False, True, False]
Current timestep = 2033. State = [[-0.23529859  0.04540006]]. Action = [[ 0.01296321 -0.08398438 -0.18660444 -0.06015897]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 2033 is [True, False, False, False, True, False]
Current timestep = 2034. State = [[-0.23530999  0.04389622]]. Action = [[ 0.05426863  0.04484895  0.15733963 -0.6816737 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 2034 is [True, False, False, False, True, False]
Current timestep = 2035. State = [[-0.23533128  0.04346976]]. Action = [[-0.05559583  0.16267711 -0.10105646 -0.90931606]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 2035 is [True, False, False, False, True, False]
Scene graph at timestep 2035 is [True, False, False, False, True, False]
State prediction error at timestep 2035 is tensor(6.4571e-08, grad_fn=<MseLossBackward0>)
Current timestep = 2036. State = [[-0.23546058  0.0439343 ]]. Action = [[0.22762692 0.21532488 0.02845058 0.82876635]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 2036 is [True, False, False, False, True, False]
Current timestep = 2037. State = [[-0.23454458  0.04596179]]. Action = [[ 0.1965164  -0.07330474  0.09703127  0.42486584]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 2037 is [True, False, False, False, True, False]
Scene graph at timestep 2037 is [True, False, False, False, True, False]
State prediction error at timestep 2037 is tensor(1.8113e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2038. State = [[-0.23081455  0.04657096]]. Action = [[0.15556747 0.09881398 0.18695575 0.53924155]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 2038 is [True, False, False, False, True, False]
Current timestep = 2039. State = [[-0.2264202   0.04812086]]. Action = [[ 0.06540856  0.22009772  0.21646395 -0.24782628]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 2039 is [True, False, False, False, True, False]
Current timestep = 2040. State = [[-0.22314818  0.05163204]]. Action = [[-0.05734649 -0.06793274 -0.03837612  0.8275801 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 2040 is [True, False, False, False, True, False]
Current timestep = 2041. State = [[-0.2218997   0.05351892]]. Action = [[-0.20966771  0.12151793 -0.05102843  0.8629203 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 2041 is [True, False, False, False, True, False]
Current timestep = 2042. State = [[-0.22302456  0.05611229]]. Action = [[ 0.08484304 -0.04828177 -0.21510825  0.29619384]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 2042 is [True, False, False, False, True, False]
Scene graph at timestep 2042 is [True, False, False, False, True, False]
State prediction error at timestep 2042 is tensor(3.5220e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2043. State = [[-0.22331542  0.05699486]]. Action = [[-0.01583442  0.00896698  0.22628021  0.46027493]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 2043 is [True, False, False, False, True, False]
Human Feedback received at timestep 2043 of 1
Current timestep = 2044. State = [[-0.22361474  0.05752736]]. Action = [[-0.21192937  0.01179937 -0.13303655 -0.66384906]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 2044 is [True, False, False, False, True, False]
Current timestep = 2045. State = [[-0.22423698  0.05879992]]. Action = [[0.04512239 0.20532542 0.00959975 0.7256634 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 2045 is [True, False, False, False, True, False]
Current timestep = 2046. State = [[-0.22560933  0.06223816]]. Action = [[0.21640363 0.19071895 0.07100317 0.88671803]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 2046 is [True, False, False, False, True, False]
Current timestep = 2047. State = [[-0.22563666  0.06614497]]. Action = [[ 0.1254633   0.12721229  0.241862   -0.8151589 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 2047 is [True, False, False, False, True, False]
Current timestep = 2048. State = [[-0.22428522  0.07013576]]. Action = [[ 0.14685279 -0.11475107  0.07732695 -0.39902484]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 2048 is [True, False, False, False, True, False]
Current timestep = 2049. State = [[-0.22042398  0.07173038]]. Action = [[0.00993413 0.06718212 0.24424586 0.4422567 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 2049 is [True, False, False, False, True, False]
Current timestep = 2050. State = [[-0.21690199  0.07344799]]. Action = [[ 0.17446893  0.09481385 -0.13980727  0.79787755]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 2050 is [True, False, False, False, True, False]
Scene graph at timestep 2050 is [True, False, False, False, True, False]
State prediction error at timestep 2050 is tensor(1.4969e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2050 of -1
Current timestep = 2051. State = [[-0.21221836  0.07590407]]. Action = [[ 0.012862   -0.14967604 -0.04403128  0.39899993]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 2051 is [True, False, False, False, True, False]
Current timestep = 2052. State = [[-0.20890938  0.07618468]]. Action = [[-0.00610435 -0.14239752 -0.20022795 -0.38768613]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 2052 is [True, False, False, False, True, False]
Current timestep = 2053. State = [[-0.20739605  0.07567834]]. Action = [[ 0.03723899 -0.06367493  0.09949696 -0.53873867]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 2053 is [True, False, False, False, True, False]
Scene graph at timestep 2053 is [True, False, False, False, True, False]
State prediction error at timestep 2053 is tensor(5.7483e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2054. State = [[-0.20650697  0.07456155]]. Action = [[-0.12475616 -0.16969593 -0.07440156 -0.8824266 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 2054 is [True, False, False, False, True, False]
Current timestep = 2055. State = [[-0.20614068  0.07183306]]. Action = [[-0.14509067  0.09989294  0.08500975  0.06190622]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 2055 is [True, False, False, False, True, False]
Current timestep = 2056. State = [[-0.2066509   0.07161888]]. Action = [[-0.19421074  0.23357803 -0.18212605 -0.13808066]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 2056 is [True, False, False, False, True, False]
Scene graph at timestep 2056 is [True, False, False, False, True, False]
State prediction error at timestep 2056 is tensor(1.0621e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2057. State = [[-0.20796654  0.0739961 ]]. Action = [[-0.15212002  0.2285493  -0.21042599  0.5507884 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 2057 is [True, False, False, False, True, False]
Current timestep = 2058. State = [[-0.21069519  0.07892781]]. Action = [[-0.22299536 -0.11619619  0.01376235 -0.53131735]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 2058 is [True, False, False, False, True, False]
Scene graph at timestep 2058 is [True, False, False, False, True, False]
State prediction error at timestep 2058 is tensor(8.0094e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2059. State = [[-0.21348867  0.0809774 ]]. Action = [[ 0.03943032 -0.02827358 -0.2189943  -0.93190515]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 2059 is [True, False, False, False, True, False]
Current timestep = 2060. State = [[-0.21460627  0.08177419]]. Action = [[ 0.09443748  0.19153601 -0.1632004   0.13834131]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 2060 is [True, False, False, False, True, False]
Current timestep = 2061. State = [[-0.21576285  0.08394871]]. Action = [[ 0.21763718 -0.01876231 -0.05634484  0.64665294]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 2061 is [True, False, False, False, True, False]
Current timestep = 2062. State = [[-0.21594511  0.08439279]]. Action = [[-0.20734136  0.06486911  0.107362   -0.61935866]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 2062 is [True, False, False, False, True, False]
Current timestep = 2063. State = [[-0.21655017  0.08563515]]. Action = [[-0.00596894 -0.07083783 -0.12747683 -0.5616839 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 2063 is [True, False, False, False, True, False]
Current timestep = 2064. State = [[-0.21668553  0.08580533]]. Action = [[ 0.14668801 -0.11017188  0.15552124  0.20899272]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 2064 is [True, False, False, False, True, False]
Human Feedback received at timestep 2064 of -1
Current timestep = 2065. State = [[-0.21653259  0.08519002]]. Action = [[-0.04387595 -0.16402477  0.07753313  0.8303561 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 2065 is [True, False, False, False, True, False]
Current timestep = 2066. State = [[-0.21616955  0.08305755]]. Action = [[ 0.1679492  -0.05285923  0.03933892 -0.8681766 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 2066 is [True, False, False, False, True, False]
Current timestep = 2067. State = [[-0.2154067   0.08078295]]. Action = [[ 0.20046777 -0.10528417  0.11600825 -0.09613055]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 2067 is [True, False, False, False, True, False]
Current timestep = 2068. State = [[-0.21403211  0.07837366]]. Action = [[-0.03151008 -0.1911048  -0.05452234 -0.7491282 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 2068 is [True, False, False, False, True, False]
Scene graph at timestep 2068 is [True, False, False, False, True, False]
State prediction error at timestep 2068 is tensor(3.1809e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2069. State = [[-0.21286918  0.07416407]]. Action = [[-0.11823416  0.15539551  0.19249132 -0.5032722 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 2069 is [True, False, False, False, True, False]
Current timestep = 2070. State = [[-0.21315095  0.07331587]]. Action = [[-0.13628276  0.12989068 -0.14520109 -0.17271602]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 2070 is [True, False, False, False, True, False]
Current timestep = 2071. State = [[-0.21386187  0.07439297]]. Action = [[-0.24337284 -0.11197484 -0.00266236  0.25708747]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 2071 is [True, False, False, False, True, False]
Current timestep = 2072. State = [[-0.21455127  0.07328819]]. Action = [[-0.14044307 -0.15957274 -0.01531671 -0.84883785]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 2072 is [True, False, False, False, True, False]
Current timestep = 2073. State = [[-0.21580131  0.07133181]]. Action = [[ 0.09732699  0.2184385  -0.20438944 -0.8507202 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 2073 is [True, False, False, False, True, False]
Current timestep = 2074. State = [[-0.2166523   0.07221572]]. Action = [[-0.189172   -0.00875391  0.10799205 -0.54468054]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 2074 is [True, False, False, False, True, False]
Current timestep = 2075. State = [[-0.21831547  0.07302508]]. Action = [[ 0.0779812   0.14765954 -0.18175994 -0.7185721 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 2075 is [True, False, False, False, True, False]
Current timestep = 2076. State = [[-0.21929269  0.07467106]]. Action = [[-0.00482853  0.091582   -0.07080126 -0.31637537]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 2076 is [True, False, False, False, True, False]
Scene graph at timestep 2076 is [True, False, False, False, True, False]
State prediction error at timestep 2076 is tensor(6.3333e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2077. State = [[-0.22067617  0.07710746]]. Action = [[-0.19514517 -0.246076    0.15816993  0.1674695 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 2077 is [True, False, False, False, True, False]
Scene graph at timestep 2077 is [True, False, False, False, True, False]
State prediction error at timestep 2077 is tensor(8.6465e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2078. State = [[-0.22218557  0.0762125 ]]. Action = [[-0.17993939  0.08212256 -0.1259511   0.9874866 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 2078 is [True, False, False, False, True, False]
Current timestep = 2079. State = [[-0.22479205  0.07675762]]. Action = [[-0.04692116 -0.18409891 -0.04401585 -0.11799049]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 2079 is [True, False, False, False, True, False]
Current timestep = 2080. State = [[-0.22638479  0.07512687]]. Action = [[ 0.1636725   0.09093949 -0.02472483 -0.45475137]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 2080 is [True, False, False, False, True, False]
Current timestep = 2081. State = [[-0.22645463  0.0749453 ]]. Action = [[ 0.14362657  0.11412561  0.20303059 -0.27316082]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 2081 is [True, False, False, False, True, False]
Current timestep = 2082. State = [[-0.2266388   0.07524548]]. Action = [[ 0.07669339  0.03701967 -0.08105391 -0.03853691]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 2082 is [True, False, False, False, True, False]
Scene graph at timestep 2082 is [True, False, False, False, True, False]
State prediction error at timestep 2082 is tensor(3.6701e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2083. State = [[-0.22675805  0.07556205]]. Action = [[ 0.01571998 -0.19354656  0.05283144 -0.51087755]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 2083 is [True, False, False, False, True, False]
Current timestep = 2084. State = [[-0.22653584  0.07418793]]. Action = [[ 0.03260976 -0.23819222  0.08675712 -0.7976812 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 2084 is [True, False, False, False, True, False]
Current timestep = 2085. State = [[-0.22595273  0.07071151]]. Action = [[-0.0851213  -0.09352177 -0.0360734   0.00942433]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 2085 is [True, False, False, False, True, False]
Current timestep = 2086. State = [[-0.22584988  0.06722153]]. Action = [[-0.06718707  0.04592377  0.14395845  0.41162086]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 2086 is [True, False, False, False, True, False]
Current timestep = 2087. State = [[-0.22586252  0.06529529]]. Action = [[ 0.19045126 -0.10315999 -0.17431445  0.28040528]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 2087 is [True, False, False, False, True, False]
Current timestep = 2088. State = [[-0.22560047  0.0628748 ]]. Action = [[0.18819642 0.04649639 0.09489143 0.713835  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 2088 is [True, False, False, False, True, False]
Current timestep = 2089. State = [[-0.22482637  0.06201619]]. Action = [[-0.01754887 -0.11083172 -0.09842147 -0.74541116]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 2089 is [True, False, False, False, True, False]
Current timestep = 2090. State = [[-0.22425862  0.06038519]]. Action = [[-0.2469967  -0.1588145  -0.13006231 -0.01659644]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 2090 is [True, False, False, False, True, False]
Current timestep = 2091. State = [[-0.22414118  0.05710941]]. Action = [[ 0.2160868   0.12936571  0.00586283 -0.04964828]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 2091 is [True, False, False, False, True, False]
Human Feedback received at timestep 2091 of 1
Current timestep = 2092. State = [[-0.22413813  0.05617247]]. Action = [[-0.13451104  0.09830886 -0.09492852  0.1325196 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 2092 is [True, False, False, False, True, False]
Current timestep = 2093. State = [[-0.22444181  0.05674219]]. Action = [[-0.20090166  0.19462734 -0.09515202  0.91014504]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 2093 is [True, False, False, False, True, False]
Current timestep = 2094. State = [[-0.22541173  0.05913797]]. Action = [[ 0.15943184  0.01787519  0.14012384 -0.09405541]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 2094 is [True, False, False, False, True, False]
Scene graph at timestep 2094 is [True, False, False, False, True, False]
State prediction error at timestep 2094 is tensor(3.3350e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2095. State = [[-0.22584313  0.06028711]]. Action = [[ 0.01286247 -0.04649019  0.18872654 -0.01713634]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 2095 is [True, False, False, False, True, False]
Current timestep = 2096. State = [[-0.22589183  0.06032634]]. Action = [[ 0.22664088 -0.21646738 -0.06539762 -0.533475  ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 2096 is [True, False, False, False, True, False]
Scene graph at timestep 2096 is [True, False, False, False, True, False]
State prediction error at timestep 2096 is tensor(1.8013e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2097. State = [[-0.2249457  0.0588766]]. Action = [[-0.07279834  0.17643875  0.24078828  0.23246038]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 2097 is [True, False, False, False, True, False]
Current timestep = 2098. State = [[-0.2249672   0.05938383]]. Action = [[-0.23142116  0.17890263 -0.22088914 -0.5190781 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 2098 is [True, False, False, False, True, False]
Current timestep = 2099. State = [[-0.22588947  0.06198145]]. Action = [[-0.06984332 -0.01929235  0.01081282  0.7886157 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 2099 is [True, False, False, False, True, False]
Current timestep = 2100. State = [[-0.22676888  0.06367251]]. Action = [[-0.10485278 -0.14799985 -0.1877756   0.58910084]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 2100 is [True, False, False, False, True, False]
Current timestep = 2101. State = [[-0.2269044   0.06328004]]. Action = [[ 0.16826001 -0.16651273  0.15223074  0.73768246]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 2101 is [True, False, False, False, True, False]
Current timestep = 2102. State = [[-0.22661306  0.06120156]]. Action = [[-0.08245409 -0.00126429 -0.10403875 -0.96787816]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 2102 is [True, False, False, False, True, False]
Scene graph at timestep 2102 is [True, False, False, False, True, False]
State prediction error at timestep 2102 is tensor(2.6922e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2103. State = [[-0.22665192  0.06004447]]. Action = [[-0.17414238  0.19998884  0.05568874 -0.5180629 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 2103 is [True, False, False, False, True, False]
Scene graph at timestep 2103 is [True, False, False, False, True, False]
State prediction error at timestep 2103 is tensor(7.0397e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2104. State = [[-0.22801384  0.06156441]]. Action = [[-0.20390436 -0.19572009  0.21577641  0.92359316]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 2104 is [True, False, False, False, True, False]
Current timestep = 2105. State = [[-0.22961202  0.06023419]]. Action = [[ 0.03076625 -0.15448453 -0.1200963  -0.8909986 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 2105 is [True, False, False, False, True, False]
Current timestep = 2106. State = [[-0.23035401  0.05801914]]. Action = [[-0.1933372  -0.10795276  0.0707761   0.6082578 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 2106 is [True, False, False, False, True, False]
Current timestep = 2107. State = [[-0.23238344  0.05517095]]. Action = [[-0.1773101   0.1935972   0.01815146  0.2435025 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 2107 is [True, False, False, False, True, False]
Current timestep = 2108. State = [[-0.23619215  0.05566248]]. Action = [[-0.06993908  0.0625523   0.16817743 -0.38958907]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 2108 is [True, False, False, False, True, False]
Current timestep = 2109. State = [[-0.24050625  0.05668379]]. Action = [[-0.1330874   0.12256208 -0.21447049  0.7129617 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 2109 is [True, False, False, False, True, False]
Current timestep = 2110. State = [[-0.24561912  0.05856086]]. Action = [[ 0.1392448  -0.19489643  0.10563889 -0.4288776 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 2110 is [True, False, False, False, True, False]
Scene graph at timestep 2110 is [True, False, False, False, True, False]
State prediction error at timestep 2110 is tensor(2.3244e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2111. State = [[-0.2475637   0.05734964]]. Action = [[-0.10267988  0.1470033  -0.00237     0.40586424]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 2111 is [True, False, False, False, True, False]
Current timestep = 2112. State = [[-0.24979311  0.05820388]]. Action = [[-0.03255728 -0.01221429 -0.20267448  0.6107657 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 2112 is [True, False, False, False, True, False]
Scene graph at timestep 2112 is [True, False, False, False, True, False]
State prediction error at timestep 2112 is tensor(9.0263e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2113. State = [[-0.25184393  0.05858823]]. Action = [[0.03119162 0.02607465 0.19709814 0.6023948 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 2113 is [True, False, False, False, True, False]
Current timestep = 2114. State = [[-0.25322914  0.05881913]]. Action = [[ 0.06978357  0.10690624 -0.12732217 -0.2640906 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 2114 is [True, False, False, False, True, False]
Human Feedback received at timestep 2114 of -1
Current timestep = 2115. State = [[-0.25378388  0.05999949]]. Action = [[-0.18757614 -0.00542     0.04126838  0.4870453 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 2115 is [True, False, False, False, True, False]
Current timestep = 2116. State = [[-0.2555449   0.06141289]]. Action = [[ 0.11339578  0.20234686  0.18850288 -0.8866199 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 2116 is [True, False, False, False, True, False]
Current timestep = 2117. State = [[-0.25674498  0.06398071]]. Action = [[-0.21707317 -0.17516087 -0.16022858  0.9286108 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 2117 is [True, False, False, False, True, False]
Human Feedback received at timestep 2117 of -1
Current timestep = 2118. State = [[-0.25852987  0.06466902]]. Action = [[-0.0610998   0.05013907 -0.18893112  0.43812823]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 2118 is [True, False, False, False, True, False]
Current timestep = 2119. State = [[-0.2603272   0.06549489]]. Action = [[-0.04637548 -0.16063818 -0.07759459  0.8319571 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 2119 is [True, False, False, False, True, False]
Current timestep = 2120. State = [[-0.26137134  0.06409349]]. Action = [[ 0.1609605  -0.17887343  0.11811972 -0.23056972]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 2120 is [True, False, False, False, True, False]
Scene graph at timestep 2120 is [True, False, False, False, True, False]
State prediction error at timestep 2120 is tensor(2.5070e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2121. State = [[-0.26117665  0.06105242]]. Action = [[ 0.07872111 -0.14730014 -0.13882674  0.3462993 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 2121 is [True, False, False, False, True, False]
Current timestep = 2122. State = [[-0.2608105   0.05727277]]. Action = [[-0.17523834 -0.17619361 -0.20409612 -0.8003236 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 2122 is [True, False, False, False, True, False]
Scene graph at timestep 2122 is [True, False, False, False, True, False]
State prediction error at timestep 2122 is tensor(3.3248e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2123. State = [[-0.26169372  0.05238618]]. Action = [[-0.22923605  0.18044266 -0.07688645  0.66737807]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 2123 is [True, False, False, False, True, False]
Current timestep = 2124. State = [[-0.26400682  0.0521076 ]]. Action = [[-0.21794829  0.1365878  -0.19575411  0.99073684]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 2124 is [True, False, False, False, True, False]
Current timestep = 2125. State = [[-0.2675482   0.05388987]]. Action = [[ 0.12646326  0.18295509 -0.2055699   0.98257935]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 2125 is [True, False, False, False, True, False]
Current timestep = 2126. State = [[-0.26935622  0.05691951]]. Action = [[ 0.02055562 -0.1801409  -0.03044333  0.03977704]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 2126 is [True, False, False, False, True, False]
Scene graph at timestep 2126 is [True, False, False, False, True, False]
State prediction error at timestep 2126 is tensor(7.0205e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2127. State = [[-0.2698142   0.05647053]]. Action = [[-0.02463931  0.07910958 -0.06304309 -0.7028568 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 2127 is [True, False, False, False, True, False]
Current timestep = 2128. State = [[-0.27045813  0.05699325]]. Action = [[ 0.01908326  0.01170242  0.22321793 -0.83357507]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 2128 is [True, False, False, False, True, False]
Current timestep = 2129. State = [[-0.27085784  0.05735837]]. Action = [[0.0324125  0.01334754 0.19800013 0.55403256]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 2129 is [True, False, False, False, True, False]
Current timestep = 2130. State = [[-0.2709157   0.05774388]]. Action = [[ 0.1760253   0.16796035 -0.21316971  0.08302844]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 2130 is [True, False, False, False, True, False]
Current timestep = 2131. State = [[-0.2713711   0.05879812]]. Action = [[ 0.06319526 -0.02544345 -0.20316738  0.77845573]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 2131 is [True, False, False, False, True, False]
Scene graph at timestep 2131 is [True, False, False, False, True, False]
State prediction error at timestep 2131 is tensor(6.7507e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2132. State = [[-0.27143833  0.05925916]]. Action = [[ 0.03955224  0.03739679  0.10114554 -0.9349121 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 2132 is [True, False, False, False, True, False]
Scene graph at timestep 2132 is [True, False, False, False, True, False]
State prediction error at timestep 2132 is tensor(1.0186e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2133. State = [[-0.2715574   0.05976429]]. Action = [[-0.09284553  0.13486087 -0.1362631  -0.6641251 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 2133 is [True, False, False, False, True, False]
Scene graph at timestep 2133 is [True, False, False, False, True, False]
State prediction error at timestep 2133 is tensor(3.1676e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2134. State = [[-0.27219585  0.06206482]]. Action = [[ 0.20644382 -0.1646753  -0.2328049  -0.89961255]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 2134 is [True, False, False, False, True, False]
Scene graph at timestep 2134 is [True, False, False, False, True, False]
State prediction error at timestep 2134 is tensor(1.0052e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2135. State = [[-0.27132446  0.06262671]]. Action = [[-0.19648464  0.10033709 -0.21372212 -0.25850928]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 2135 is [True, False, False, False, True, False]
Human Feedback received at timestep 2135 of -1
Current timestep = 2136. State = [[-0.2705523   0.06293854]]. Action = [[-0.02624583 -0.20501809 -0.1143465  -0.26092744]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 2136 is [True, False, False, False, True, False]
Current timestep = 2137. State = [[-0.2699863   0.06133506]]. Action = [[ 0.11957446  0.12833112 -0.14756985 -0.11512381]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 2137 is [True, False, False, False, True, False]
Current timestep = 2138. State = [[-0.26939932  0.06168213]]. Action = [[-0.16057187  0.16127613 -0.03666469  0.8661933 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 2138 is [True, False, False, False, True, False]
Current timestep = 2139. State = [[-0.2697905   0.06269812]]. Action = [[-0.08039221  0.13847905  0.2173684  -0.87357575]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 2139 is [True, False, False, False, True, False]
Current timestep = 2140. State = [[-0.2708388   0.06515297]]. Action = [[ 0.19849306  0.09341866 -0.11435154 -0.38021344]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 2140 is [True, False, False, False, True, False]
Current timestep = 2141. State = [[-0.27077454  0.06700417]]. Action = [[-0.08089578  0.06060916 -0.18138592 -0.33470356]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 2141 is [True, False, False, False, True, False]
Current timestep = 2142. State = [[-0.27126583  0.06926308]]. Action = [[0.19199425 0.0483073  0.09962618 0.12078083]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 2142 is [True, False, False, False, True, False]
Current timestep = 2143. State = [[-0.26993284  0.07093897]]. Action = [[0.20152453 0.01784763 0.1583299  0.6208168 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 2143 is [True, False, False, False, True, False]
Scene graph at timestep 2143 is [True, False, False, False, True, False]
State prediction error at timestep 2143 is tensor(2.8245e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2144. State = [[-0.2673744   0.07229242]]. Action = [[-0.2034637   0.20014882 -0.06947768  0.9534322 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 2144 is [True, False, False, False, True, False]
Current timestep = 2145. State = [[-0.26547822  0.07314148]]. Action = [[ 0.14209479 -0.09109004  0.14258602  0.89531255]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 2145 is [True, False, False, False, True, False]
Current timestep = 2146. State = [[-0.2633761   0.07373703]]. Action = [[-0.13654448 -0.22179703  0.07577515 -0.80360013]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 2146 is [True, False, False, False, True, False]
Current timestep = 2147. State = [[-0.2623298   0.07229419]]. Action = [[ 0.13600868 -0.00846563  0.15778393  0.9869096 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 2147 is [True, False, False, False, True, False]
Current timestep = 2148. State = [[-0.26100832  0.07157032]]. Action = [[ 0.07256299 -0.08476183  0.09767008 -0.32092142]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 2148 is [True, False, False, False, True, False]
Current timestep = 2149. State = [[-0.25930545  0.06981657]]. Action = [[-0.23547772 -0.24100636  0.12230632  0.28410912]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 2149 is [True, False, False, False, True, False]
Scene graph at timestep 2149 is [True, False, False, False, True, False]
State prediction error at timestep 2149 is tensor(5.6144e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2150. State = [[-0.25845903  0.06600072]]. Action = [[-0.06993416 -0.18253665  0.23319304 -0.6836834 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 2150 is [True, False, False, False, True, False]
Current timestep = 2151. State = [[-0.25792822  0.06111771]]. Action = [[-0.16241288  0.02682227 -0.16169083 -0.86487967]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 2151 is [True, False, False, False, True, False]
Current timestep = 2152. State = [[-0.2590023   0.05788974]]. Action = [[ 0.06870693 -0.14846556  0.07611385 -0.349324  ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 2152 is [True, False, False, False, True, False]
Scene graph at timestep 2152 is [True, False, False, False, True, False]
State prediction error at timestep 2152 is tensor(3.0248e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2153. State = [[-0.25937757  0.05488708]]. Action = [[-0.192729    0.23735926 -0.20160048  0.59004426]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 2153 is [True, False, False, False, True, False]
Scene graph at timestep 2153 is [True, False, False, False, True, False]
State prediction error at timestep 2153 is tensor(3.4151e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2154. State = [[-0.26110095  0.05470331]]. Action = [[ 0.15991199 -0.23754594 -0.1523629  -0.24321741]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 2154 is [True, False, False, False, True, False]
Scene graph at timestep 2154 is [True, False, False, False, True, False]
State prediction error at timestep 2154 is tensor(2.0881e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2155. State = [[-0.260983  0.052164]]. Action = [[ 0.08348846  0.07731915  0.13139507 -0.965054  ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 2155 is [True, False, False, False, True, False]
Current timestep = 2156. State = [[-0.26081198  0.05121411]]. Action = [[0.03250307 0.12638444 0.1369524  0.8052218 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 2156 is [True, False, False, False, True, False]
Current timestep = 2157. State = [[-0.26081133  0.05126131]]. Action = [[-0.2147523  -0.1784048  -0.01197655  0.11756718]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 2157 is [True, False, False, False, True, False]
Current timestep = 2158. State = [[-0.26140502  0.04972079]]. Action = [[ 0.06921414 -0.1986407  -0.22341344 -0.5537767 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 2158 is [True, False, False, False, True, False]
Current timestep = 2159. State = [[-0.2612988   0.04691452]]. Action = [[-0.13669486 -0.07061461  0.12390998 -0.74221927]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 2159 is [True, False, False, False, True, False]
Current timestep = 2160. State = [[-0.2620466   0.04401123]]. Action = [[-0.03463998 -0.16553606  0.05962038 -0.67206955]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 2160 is [True, False, False, False, True, False]
Scene graph at timestep 2160 is [True, False, False, False, True, False]
State prediction error at timestep 2160 is tensor(2.4497e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2161. State = [[-0.26293406  0.04027568]]. Action = [[ 0.06787598 -0.0797103   0.11294776 -0.19531995]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 2161 is [True, False, False, False, True, False]
Current timestep = 2162. State = [[-0.26284882  0.03691202]]. Action = [[ 0.24447829 -0.23730254 -0.1244421  -0.9113185 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 2162 is [True, False, False, False, True, False]
Scene graph at timestep 2162 is [True, False, False, False, True, False]
State prediction error at timestep 2162 is tensor(5.7113e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2163. State = [[-0.26177043  0.03171666]]. Action = [[ 0.17742461 -0.02987823  0.10033607 -0.36650348]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 2163 is [True, False, False, False, True, False]
Scene graph at timestep 2163 is [True, False, False, False, True, False]
State prediction error at timestep 2163 is tensor(2.7665e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2164. State = [[-0.2604123   0.02756218]]. Action = [[-0.22417225 -0.02711141 -0.13612473  0.59093714]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 2164 is [True, False, False, False, True, False]
Current timestep = 2165. State = [[-0.26002613  0.02527415]]. Action = [[ 0.12472087 -0.13609627 -0.2082215   0.9751954 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 2165 is [True, False, False, False, True, False]
Current timestep = 2166. State = [[-0.25939122  0.02220469]]. Action = [[-0.19526528  0.04278663 -0.14866412 -0.81567514]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 2166 is [True, False, False, False, True, False]
Current timestep = 2167. State = [[-0.2596877   0.01987597]]. Action = [[ 0.00255564 -0.17162116 -0.2317577   0.8221419 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 2167 is [True, False, False, False, True, False]
Scene graph at timestep 2167 is [True, False, False, False, True, False]
State prediction error at timestep 2167 is tensor(2.9387e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2168. State = [[-0.25978503  0.016763  ]]. Action = [[ 0.24015361  0.22801468  0.08565238 -0.6604516 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 2168 is [True, False, False, False, True, False]
Scene graph at timestep 2168 is [True, False, False, False, True, False]
State prediction error at timestep 2168 is tensor(3.7447e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2169. State = [[-0.25973132  0.01649786]]. Action = [[ 0.00501004 -0.19485046 -0.09478641 -0.7973001 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 2169 is [True, False, False, False, True, False]
Current timestep = 2170. State = [[-0.25920916  0.01496978]]. Action = [[-0.18164062  0.0626739  -0.2180476  -0.5993425 ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 2170 is [True, False, False, False, True, False]
Human Feedback received at timestep 2170 of 1
Current timestep = 2171. State = [[-0.25930634  0.01477294]]. Action = [[ 0.15568507  0.20175242  0.1464282  -0.56036586]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 2171 is [True, False, False, False, True, False]
Current timestep = 2172. State = [[-0.2592639   0.01504854]]. Action = [[ 0.12997854  0.00991562  0.00414383 -0.15527445]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 2172 is [True, False, False, False, True, False]
Scene graph at timestep 2172 is [True, False, False, False, True, False]
State prediction error at timestep 2172 is tensor(3.0249e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2173. State = [[-0.2591716   0.01547352]]. Action = [[ 0.03582883  0.0561403  -0.21304013 -0.18209392]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 2173 is [True, False, False, False, True, False]
Current timestep = 2174. State = [[-0.25913063  0.01607146]]. Action = [[-0.07320827  0.06766453 -0.00367446 -0.55109555]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 2174 is [True, False, False, False, True, False]
Current timestep = 2175. State = [[-0.2591993   0.01711325]]. Action = [[-0.11928913  0.20612389 -0.15923646  0.7968844 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 2175 is [True, False, False, False, True, False]
Current timestep = 2176. State = [[-0.25981018  0.01998998]]. Action = [[ 0.01682809 -0.00525561  0.09432364 -0.7452046 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 2176 is [True, False, False, False, True, False]
Current timestep = 2177. State = [[-0.25996548  0.02137506]]. Action = [[ 0.1992689  -0.1116364  -0.19890885  0.8961475 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 2177 is [True, False, False, False, True, False]
Current timestep = 2178. State = [[-0.25919262  0.02155341]]. Action = [[-2.2180378e-04 -1.8495440e-02 -2.2886488e-01  6.9836903e-01]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 2178 is [True, False, False, False, True, False]
Current timestep = 2179. State = [[-0.2587199   0.02174765]]. Action = [[ 0.16453522 -0.05301479 -0.03500322  0.04534495]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 2179 is [True, False, False, False, True, False]
Current timestep = 2180. State = [[-0.25711024  0.0215687 ]]. Action = [[-0.08204345 -0.17709313  0.13194737  0.7400521 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 2180 is [True, False, False, False, True, False]
Scene graph at timestep 2180 is [True, False, False, False, True, False]
State prediction error at timestep 2180 is tensor(1.7899e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2181. State = [[-0.2562316   0.02000543]]. Action = [[ 0.07626548 -0.18332951 -0.06503236  0.79485583]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 2181 is [True, False, False, False, True, False]
Scene graph at timestep 2181 is [True, False, False, False, True, False]
State prediction error at timestep 2181 is tensor(1.6194e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2182. State = [[-0.25448212  0.01712039]]. Action = [[ 0.11910611 -0.03064469 -0.11332394  0.48537207]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 2182 is [True, False, False, False, True, False]
Scene graph at timestep 2182 is [True, False, False, False, True, False]
State prediction error at timestep 2182 is tensor(7.6013e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2183. State = [[-0.25235206  0.0144805 ]]. Action = [[ 0.11360842  0.15410286  0.20596594 -0.3230061 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 2183 is [True, False, False, False, True, False]
Current timestep = 2184. State = [[-0.24964711  0.01485627]]. Action = [[ 0.24697018  0.048549    0.20759198 -0.81288743]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 2184 is [True, False, False, False, True, False]
Current timestep = 2185. State = [[-0.24539374  0.01569282]]. Action = [[ 0.20257571  0.1342744   0.17683518 -0.87437004]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 2185 is [True, False, False, False, True, False]
Current timestep = 2186. State = [[-0.23970753  0.01681212]]. Action = [[-0.01214516  0.17506501  0.02043331 -0.34371448]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 2186 is [True, False, False, False, True, False]
Current timestep = 2187. State = [[-0.23554264  0.01902165]]. Action = [[-0.15734278  0.0096865  -0.11385405 -0.70520425]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 2187 is [True, False, False, False, True, False]
Current timestep = 2188. State = [[-0.23378286  0.02031524]]. Action = [[-0.0784886  -0.06255445 -0.08477134 -0.73763585]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 2188 is [True, False, False, False, True, False]
Scene graph at timestep 2188 is [True, False, False, False, True, False]
State prediction error at timestep 2188 is tensor(4.4062e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2189. State = [[-0.23312244  0.0209138 ]]. Action = [[ 0.2177077   0.19577733 -0.13210289  0.8899555 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 2189 is [True, False, False, False, True, False]
Current timestep = 2190. State = [[-0.23139489  0.02354313]]. Action = [[-0.19625661  0.2372315   0.15844768  0.41024637]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 2190 is [True, False, False, False, True, False]
Human Feedback received at timestep 2190 of 1
Current timestep = 2191. State = [[-0.23199452  0.02880769]]. Action = [[-0.0404551  -0.07166427 -0.14228603  0.00917351]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 2191 is [True, False, False, False, True, False]
Current timestep = 2192. State = [[-0.23208556  0.03267093]]. Action = [[ 0.09188259  0.24598312 -0.21040852 -0.7808145 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 2192 is [True, False, False, False, True, False]
Current timestep = 2193. State = [[-0.23182546  0.03634477]]. Action = [[-0.00412214  0.1468187   0.16120464  0.86235   ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 2193 is [True, False, False, False, True, False]
Scene graph at timestep 2193 is [True, False, False, False, True, False]
State prediction error at timestep 2193 is tensor(2.9142e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2194. State = [[-0.23186447  0.04064466]]. Action = [[ 0.11874276 -0.07379174 -0.12539728 -0.71457744]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 2194 is [True, False, False, False, True, False]
Current timestep = 2195. State = [[-0.2309671   0.04233369]]. Action = [[-0.0297226   0.13544303  0.122455   -0.24486923]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 2195 is [True, False, False, False, True, False]
Current timestep = 2196. State = [[-0.23060377  0.04524992]]. Action = [[ 0.198421    0.16729495  0.2302689  -0.5647085 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 2196 is [True, False, False, False, True, False]
Current timestep = 2197. State = [[-0.22904672  0.04991201]]. Action = [[-0.08800933  0.2224293  -0.12968989 -0.01120651]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 2197 is [True, False, False, False, True, False]
Current timestep = 2198. State = [[-0.22855157  0.0547774 ]]. Action = [[ 0.19485074  0.1777235  -0.10395801  0.43042183]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 2198 is [True, False, False, False, True, False]
Current timestep = 2199. State = [[-0.22669695  0.06030884]]. Action = [[-0.16401869  0.18157    -0.16033679 -0.73899305]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 2199 is [True, False, False, False, True, False]
Current timestep = 2200. State = [[-0.22706752  0.06709312]]. Action = [[ 0.21861339  0.01999941  0.16300535 -0.4432016 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 2200 is [True, False, False, False, True, False]
Current timestep = 2201. State = [[-0.22551243  0.07100414]]. Action = [[ 0.14502156  0.08545324 -0.0137538  -0.9568289 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 2201 is [True, False, False, False, True, False]
Scene graph at timestep 2201 is [True, False, False, False, True, False]
State prediction error at timestep 2201 is tensor(2.2239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2202. State = [[-0.22246802  0.07485051]]. Action = [[-0.11837551 -0.15501225 -0.10932899 -0.52740705]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 2202 is [True, False, False, False, True, False]
Human Feedback received at timestep 2202 of -1
Current timestep = 2203. State = [[-0.22008118  0.07556573]]. Action = [[ 0.24126482  0.07544872  0.18158829 -0.87265885]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 2203 is [True, False, False, False, True, False]
Current timestep = 2204. State = [[-0.21638629  0.07686126]]. Action = [[-0.1872969   0.09350589 -0.04062629  0.7484243 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 2204 is [True, False, False, False, True, False]
Current timestep = 2205. State = [[-0.21478775  0.07861368]]. Action = [[ 0.122747  -0.1573338  0.1274184 -0.8822899]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 2205 is [True, False, False, False, True, False]
Current timestep = 2206. State = [[-0.21334372  0.07868497]]. Action = [[ 0.11057791  0.02278697 -0.22226799 -0.7680329 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 2206 is [True, False, False, False, True, False]
Current timestep = 2207. State = [[-0.21179795  0.07876538]]. Action = [[-0.22886498 -0.11338297  0.22166437  0.11606169]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 2207 is [True, False, False, False, True, False]
Current timestep = 2208. State = [[-0.21188736  0.07852864]]. Action = [[-0.07208557 -0.0502241  -0.18424933 -0.97154886]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 2208 is [True, False, False, False, True, False]
Current timestep = 2209. State = [[-0.21193348  0.07829021]]. Action = [[-0.07079287  0.1688894   0.09584132  0.17615318]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 2209 is [True, False, False, False, True, False]
Current timestep = 2210. State = [[-0.21220532  0.0789296 ]]. Action = [[-0.16000742  0.23430455  0.06969094 -0.39318538]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 2210 is [True, False, False, False, True, False]
Scene graph at timestep 2210 is [True, False, False, False, True, False]
State prediction error at timestep 2210 is tensor(1.4022e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2211. State = [[-0.21398629  0.08259428]]. Action = [[ 0.1185998   0.02691072 -0.06958777  0.92474747]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 2211 is [True, False, False, False, True, False]
Current timestep = 2212. State = [[-0.21502385  0.08478185]]. Action = [[-0.13584052  0.05027184 -0.01548672  0.67525506]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 2212 is [True, False, False, False, True, False]
Current timestep = 2213. State = [[-0.21629585  0.08712566]]. Action = [[ 0.05347404  0.23544604 -0.10927263  0.75382304]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 2213 is [True, False, False, False, True, False]
Current timestep = 2214. State = [[-0.21823958  0.09141084]]. Action = [[-0.20376314 -0.06906925 -0.17067935  0.88835907]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 2214 is [True, False, False, False, True, False]
Scene graph at timestep 2214 is [True, False, False, False, True, False]
State prediction error at timestep 2214 is tensor(1.4379e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2215. State = [[-0.2196115   0.09394988]]. Action = [[ 4.4489801e-02 -1.8341658e-01  2.5376678e-04  6.1068177e-01]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 2215 is [True, False, False, False, True, False]
Scene graph at timestep 2215 is [True, False, False, False, True, False]
State prediction error at timestep 2215 is tensor(3.6942e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2216. State = [[-0.21982516  0.0939336 ]]. Action = [[-0.17016302  0.00896704  0.07378626 -0.34520072]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 2216 is [True, False, False, False, True, False]
Current timestep = 2217. State = [[-0.21997304  0.09390767]]. Action = [[-0.01182741 -0.08769929  0.09734091  0.490955  ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 2217 is [True, False, False, False, True, False]
Human Feedback received at timestep 2217 of -1
Current timestep = 2218. State = [[-0.22018382  0.09315887]]. Action = [[ 0.1375727  -0.13634537  0.11981311  0.19649851]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 2218 is [True, False, False, False, True, False]
Current timestep = 2219. State = [[-0.21998744  0.09100543]]. Action = [[ 0.0923295   0.13535422  0.19430813 -0.5534918 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 2219 is [True, False, False, False, True, False]
Current timestep = 2220. State = [[-0.22009464  0.09069811]]. Action = [[ 0.02656361 -0.22356865 -0.05475026  0.9907458 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 2220 is [True, False, False, False, True, False]
Scene graph at timestep 2220 is [True, False, False, False, True, False]
State prediction error at timestep 2220 is tensor(1.7889e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2221. State = [[-0.21976149  0.08831452]]. Action = [[-0.08391097  0.09901181  0.22943678 -0.8426556 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 2221 is [True, False, False, False, True, False]
Scene graph at timestep 2221 is [True, False, False, False, True, False]
State prediction error at timestep 2221 is tensor(3.9174e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2222. State = [[-0.21972169  0.08765445]]. Action = [[ 0.12501076 -0.10085604 -0.06424141 -0.57143515]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 2222 is [True, False, False, False, True, False]
Current timestep = 2223. State = [[-0.21946707  0.0861233 ]]. Action = [[-0.14286475 -0.16355245 -0.22404596 -0.88970107]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 2223 is [True, False, False, False, True, False]
Current timestep = 2224. State = [[-0.21936287  0.08347987]]. Action = [[-0.23053388 -0.01357219 -0.17249134 -0.26485527]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 2224 is [True, False, False, False, True, False]
Current timestep = 2225. State = [[-0.22031143  0.08221273]]. Action = [[-0.07168946  0.22307062 -0.15395954  0.00617182]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 2225 is [True, False, False, False, True, False]
Scene graph at timestep 2225 is [True, False, False, False, True, False]
State prediction error at timestep 2225 is tensor(6.1785e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2226. State = [[-0.2220527   0.08377638]]. Action = [[0.06588387 0.21935534 0.13776332 0.8504627 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 2226 is [True, False, False, False, True, False]
Scene graph at timestep 2226 is [True, False, False, False, True, False]
State prediction error at timestep 2226 is tensor(2.2123e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2227. State = [[-0.22350453  0.08677811]]. Action = [[ 0.2024832   0.02848822 -0.17147212  0.1439693 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 2227 is [True, False, False, False, True, False]
Current timestep = 2228. State = [[-0.22402784  0.08820011]]. Action = [[ 0.23429358  0.1609264  -0.13766266 -0.3623221 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 2228 is [True, False, False, False, True, False]
Current timestep = 2229. State = [[-0.22361983  0.09049813]]. Action = [[-0.18670525  0.23076296  0.1162149   0.20506835]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 2229 is [True, False, False, False, True, False]
Current timestep = 2230. State = [[-0.22496907  0.09508752]]. Action = [[ 0.21950316  0.20847785 -0.10132195  0.8606026 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 2230 is [True, False, False, False, True, False]
Scene graph at timestep 2230 is [True, False, False, False, True, False]
State prediction error at timestep 2230 is tensor(5.6759e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2231. State = [[-0.22454926  0.10007197]]. Action = [[ 0.21364874 -0.20412365 -0.21949048  0.1520909 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 2231 is [True, False, False, False, True, False]
Current timestep = 2232. State = [[-0.22267193  0.10084329]]. Action = [[-0.21567138 -0.05039266 -0.19396663 -0.5076354 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 2232 is [True, False, False, False, True, False]
Scene graph at timestep 2232 is [True, False, False, False, True, False]
State prediction error at timestep 2232 is tensor(3.4107e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2233. State = [[-0.22258583  0.10105608]]. Action = [[ 0.05487439 -0.11777237  0.02044091  0.32373118]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 2233 is [True, False, False, False, True, False]
Current timestep = 2234. State = [[-0.22255431  0.10086689]]. Action = [[ 0.03606355  0.18957269  0.09801003 -0.7741549 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 2234 is [True, False, False, False, True, False]
Current timestep = 2235. State = [[-0.22222656  0.10148964]]. Action = [[ 0.00153986 -0.09113219  0.06224841  0.12540615]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 2235 is [True, False, False, False, True, False]
Current timestep = 2236. State = [[-0.22195926  0.10139173]]. Action = [[-0.06307456 -0.21770665  0.09537506 -0.5983105 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 2236 is [True, False, False, False, True, False]
Current timestep = 2237. State = [[-0.22143538  0.09962389]]. Action = [[ 0.03414118 -0.16148259 -0.06871381  0.7506453 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 2237 is [True, False, False, False, True, False]
Current timestep = 2238. State = [[-0.22083277  0.09715566]]. Action = [[-0.1740918   0.16120398  0.08054626 -0.20569658]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 2238 is [True, False, False, False, True, False]
Scene graph at timestep 2238 is [True, False, False, False, True, False]
State prediction error at timestep 2238 is tensor(8.1789e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2239. State = [[-0.22089489  0.09695244]]. Action = [[ 0.1628632  -0.18068479  0.09060717 -0.6191444 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 2239 is [True, False, False, False, True, False]
Current timestep = 2240. State = [[-0.22048058  0.09536471]]. Action = [[ 0.06918243  0.2346031   0.14548782 -0.24615425]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 2240 is [True, False, False, False, True, False]
Current timestep = 2241. State = [[-0.22049145  0.09549674]]. Action = [[-0.2216451  -0.20703143  0.13464314 -0.5928786 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 2241 is [True, False, False, False, True, False]
Current timestep = 2242. State = [[-0.22028944  0.09469812]]. Action = [[ 0.10367423 -0.21837133 -0.15340246 -0.47731614]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 2242 is [True, False, False, False, True, False]
Current timestep = 2243. State = [[-0.21936813  0.09115481]]. Action = [[-0.04876578 -0.21697862 -0.03963451  0.81382704]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 2243 is [True, False, False, False, True, False]
Scene graph at timestep 2243 is [True, False, False, False, True, False]
State prediction error at timestep 2243 is tensor(3.2777e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2244. State = [[-0.21862386  0.08670594]]. Action = [[-0.095337    0.24523067 -0.07996278  0.7889552 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 2244 is [True, False, False, False, True, False]
Current timestep = 2245. State = [[-0.21879622  0.08633021]]. Action = [[-0.24716553 -0.00168487  0.21292967 -0.08667058]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 2245 is [True, False, False, False, True, False]
Scene graph at timestep 2245 is [True, False, False, False, True, False]
State prediction error at timestep 2245 is tensor(6.1151e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2246. State = [[-0.22042271  0.08592036]]. Action = [[ 0.00612408 -0.18664533 -0.01330954 -0.33672523]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 2246 is [True, False, False, False, True, False]
Current timestep = 2247. State = [[-0.22090979  0.08392546]]. Action = [[ 0.03508219  0.1091105  -0.19133504  0.62996435]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 2247 is [True, False, False, False, True, False]
Current timestep = 2248. State = [[-0.22096713  0.08353337]]. Action = [[-0.15390615  0.10174286 -0.1722218  -0.2932949 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 2248 is [True, False, False, False, True, False]
Scene graph at timestep 2248 is [True, False, False, False, True, False]
State prediction error at timestep 2248 is tensor(4.0262e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2249. State = [[-0.22211944  0.08453463]]. Action = [[ 0.24623218  0.05353659 -0.16802232  0.6549194 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 2249 is [True, False, False, False, True, False]
Current timestep = 2250. State = [[-0.22224075  0.08473673]]. Action = [[-0.05889821 -0.17152518  0.00309718  0.7027972 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 2250 is [True, False, False, False, True, False]
Current timestep = 2251. State = [[-0.22211705  0.08384469]]. Action = [[-0.08930747 -0.00378887  0.17907861  0.6959777 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 2251 is [True, False, False, False, True, False]
Current timestep = 2252. State = [[-0.22210617  0.08344004]]. Action = [[-0.01382017 -0.00301571  0.22380066 -0.942488  ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 2252 is [True, False, False, False, True, False]
Scene graph at timestep 2252 is [True, False, False, False, True, False]
State prediction error at timestep 2252 is tensor(2.2512e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2253. State = [[-0.2221401   0.08298955]]. Action = [[ 0.0852288  -0.1795481   0.08098853  0.49875975]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 2253 is [True, False, False, False, True, False]
Current timestep = 2254. State = [[-0.22181186  0.0806094 ]]. Action = [[-0.15742873 -0.10507527  0.01120412 -0.8502718 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 2254 is [True, False, False, False, True, False]
Scene graph at timestep 2254 is [True, False, False, False, True, False]
State prediction error at timestep 2254 is tensor(2.9974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2255. State = [[-0.22263905  0.07812315]]. Action = [[-0.20157573  0.11036065 -0.01713678  0.89044476]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 2255 is [True, False, False, False, True, False]
Current timestep = 2256. State = [[-0.22419456  0.07789126]]. Action = [[ 0.23183647 -0.01507767 -0.23641574 -0.9572885 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 2256 is [True, False, False, False, True, False]
Current timestep = 2257. State = [[-0.22426039  0.07781908]]. Action = [[-0.09557389  0.21970022  0.05052945  0.6357974 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 2257 is [True, False, False, False, True, False]
Scene graph at timestep 2257 is [True, False, False, False, True, False]
State prediction error at timestep 2257 is tensor(3.8901e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2258. State = [[-0.22486465  0.0792063 ]]. Action = [[ 0.1612956  -0.18137233 -0.02551675  0.11995804]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 2258 is [True, False, False, False, True, False]
Scene graph at timestep 2258 is [True, False, False, False, True, False]
State prediction error at timestep 2258 is tensor(1.0738e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2259. State = [[-0.22461244  0.07837806]]. Action = [[ 0.23905778 -0.18666066 -0.00843933 -0.6732767 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 2259 is [True, False, False, False, True, False]
Current timestep = 2260. State = [[-0.22340505  0.07579741]]. Action = [[0.1564092  0.09386307 0.19204399 0.6801107 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 2260 is [True, False, False, False, True, False]
Current timestep = 2261. State = [[-0.22267243  0.07536114]]. Action = [[ 0.0102483   0.23272759 -0.08698085 -0.3784573 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 2261 is [True, False, False, False, True, False]
Current timestep = 2262. State = [[-0.22225738  0.07643986]]. Action = [[-0.195115    0.19360527  0.09507322 -0.5723061 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 2262 is [True, False, False, False, True, False]
Current timestep = 2263. State = [[-0.22325344  0.0793686 ]]. Action = [[-0.09611833 -0.10528567  0.15411639 -0.68770397]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 2263 is [True, False, False, False, True, False]
Current timestep = 2264. State = [[-0.22370149  0.08017766]]. Action = [[-0.0394612   0.19697383  0.01991013 -0.2529688 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 2264 is [True, False, False, False, True, False]
Current timestep = 2265. State = [[-0.2247982   0.08314875]]. Action = [[ 0.07776162  0.07894906  0.16437331 -0.88664055]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 2265 is [True, False, False, False, True, False]
Current timestep = 2266. State = [[-0.22553729  0.08543482]]. Action = [[ 0.21621078  0.16528171 -0.04249592  0.3833847 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 2266 is [True, False, False, False, True, False]
Current timestep = 2267. State = [[-0.22485007  0.08869645]]. Action = [[-0.14682356  0.15478492  0.13061064  0.3662554 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 2267 is [True, False, False, False, True, False]
Current timestep = 2268. State = [[-0.22596854  0.09290686]]. Action = [[-0.20099026  0.17063034 -0.07014641  0.47676218]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 2268 is [True, False, False, False, True, False]
Current timestep = 2269. State = [[-0.22795624  0.09794833]]. Action = [[ 0.04387683  0.04898274  0.0917719  -0.46469867]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 2269 is [True, False, False, False, True, False]
Scene graph at timestep 2269 is [True, False, False, False, True, False]
State prediction error at timestep 2269 is tensor(9.5433e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2270. State = [[-0.22976993  0.1016549 ]]. Action = [[ 0.03831205 -0.16091833 -0.03100774 -0.65391535]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 2270 is [True, False, False, False, True, False]
Current timestep = 2271. State = [[-0.2299798   0.10218607]]. Action = [[-0.18432383 -0.15949655 -0.06474513  0.8224838 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 2271 is [True, False, False, False, True, False]
Current timestep = 2272. State = [[-0.22983581  0.10167428]]. Action = [[ 0.16471219 -0.22422126  0.02907813  0.844656  ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 2272 is [True, False, False, False, True, False]
Scene graph at timestep 2272 is [True, False, False, False, True, False]
State prediction error at timestep 2272 is tensor(1.7689e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2273. State = [[-0.22901411  0.09875372]]. Action = [[ 0.18775076 -0.01428454  0.05058792  0.33703542]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 2273 is [True, False, False, False, True, False]
Current timestep = 2274. State = [[-0.22843117  0.09700096]]. Action = [[-0.13695057  0.07051986 -0.13238478  0.36469877]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 2274 is [True, False, False, False, True, False]
Current timestep = 2275. State = [[-0.22857875  0.09710549]]. Action = [[ 0.04617259  0.21808356  0.13997686 -0.4385631 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 2275 is [True, False, False, False, True, False]
Current timestep = 2276. State = [[-0.2287425   0.09787118]]. Action = [[0.18089259 0.00758225 0.11545008 0.10246634]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 2276 is [True, False, False, False, True, False]
Current timestep = 2277. State = [[-0.22793175  0.09838138]]. Action = [[ 0.1994378   0.10519803 -0.05062287 -0.91838706]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 2277 is [True, False, False, False, True, False]
Current timestep = 2278. State = [[-0.22545415  0.09986064]]. Action = [[-0.23708448 -0.144452   -0.04395735 -0.07624775]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 2278 is [True, False, False, False, True, False]
Current timestep = 2279. State = [[-0.22510746  0.09982304]]. Action = [[ 0.06970647 -0.1393887  -0.12933455  0.41452587]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 2279 is [True, False, False, False, True, False]
Current timestep = 2280. State = [[-0.22430457  0.09907017]]. Action = [[-0.10719243  0.02993649 -0.1892889  -0.3702643 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 2280 is [True, False, False, False, True, False]
Scene graph at timestep 2280 is [True, False, False, False, True, False]
State prediction error at timestep 2280 is tensor(4.3730e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2281. State = [[-0.22431308  0.09902864]]. Action = [[ 0.20823944  0.07169577  0.00799561 -0.03507555]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 2281 is [True, False, False, False, True, False]
Current timestep = 2282. State = [[-0.22382976  0.09920438]]. Action = [[-0.08998802 -0.14530225 -0.05492437 -0.9714522 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 2282 is [True, False, False, False, True, False]
Current timestep = 2283. State = [[-0.22335637  0.09864414]]. Action = [[-0.21196029  0.19013429 -0.1902002  -0.27871048]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 2283 is [True, False, False, False, True, False]
Current timestep = 2284. State = [[-0.22393024  0.09962545]]. Action = [[-0.23653953  0.0121187   0.00457346  0.82022166]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 2284 is [True, False, False, False, True, False]
Scene graph at timestep 2284 is [True, False, False, False, True, False]
State prediction error at timestep 2284 is tensor(6.3757e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2285. State = [[-0.2250642   0.10092499]]. Action = [[ 0.09613952  0.23906028  0.19670534 -0.38042784]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 2285 is [True, False, False, False, True, False]
Current timestep = 2286. State = [[-0.22627802  0.10346895]]. Action = [[-0.03520311 -0.19088417  0.0367603  -0.52163374]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 2286 is [True, False, False, False, True, False]
Current timestep = 2287. State = [[-0.22650929  0.10378951]]. Action = [[-0.10507672 -0.0622815  -0.00964056  0.971866  ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 2287 is [True, False, False, False, True, False]
Current timestep = 2288. State = [[-0.22704649  0.10334288]]. Action = [[-0.2185235  -0.24031213 -0.02981795 -0.6939183 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 2288 is [True, False, False, False, True, False]
Current timestep = 2289. State = [[-0.22849251  0.10108174]]. Action = [[-0.06463251  0.10494667  0.19651878 -0.9330014 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 2289 is [True, False, False, False, True, False]
Current timestep = 2290. State = [[-0.22994831  0.10116054]]. Action = [[ 0.01199704  0.14849299 -0.23057091 -0.73844737]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 2290 is [True, False, False, False, True, False]
Current timestep = 2291. State = [[-0.23082271  0.1022815 ]]. Action = [[-0.16689062  0.10204849 -0.1258074  -0.56763417]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 2291 is [True, False, False, False, True, False]
Current timestep = 2292. State = [[-0.23309985  0.10503213]]. Action = [[ 0.24330783  0.18778002 -0.08041298  0.87754965]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 2292 is [True, False, False, False, True, False]
Current timestep = 2293. State = [[-0.23441373  0.1076169 ]]. Action = [[-0.23959786 -0.11231428 -0.14197402  0.7173264 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 2293 is [True, False, False, False, True, False]
Current timestep = 2294. State = [[-0.23551425  0.10908875]]. Action = [[ 0.0423061  -0.00710207  0.01410446 -0.36271638]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 2294 is [True, False, False, False, True, False]
Current timestep = 2295. State = [[-0.23599651  0.10998303]]. Action = [[-0.18167478  0.19667107  0.14991292  0.44406235]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 2295 is [True, False, False, False, True, False]
Current timestep = 2296. State = [[-0.23810901  0.11333891]]. Action = [[-0.05862471  0.08541703 -0.03359771  0.6210568 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 2296 is [True, False, False, False, True, False]
Current timestep = 2297. State = [[-0.24042964  0.11688138]]. Action = [[0.18470234 0.0238049  0.22456616 0.15053391]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 2297 is [True, False, False, False, True, False]
Current timestep = 2298. State = [[-0.24131891  0.11845641]]. Action = [[ 0.05458215  0.13801098 -0.07197346  0.89978695]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 2298 is [True, False, False, False, True, False]
Current timestep = 2299. State = [[-0.24238025  0.12028876]]. Action = [[-0.00667417 -0.0731791   0.11789584  0.48406148]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 2299 is [True, False, False, False, True, False]
Current timestep = 2300. State = [[-0.24276978  0.12095859]]. Action = [[ 0.12613714  0.11529824 -0.11231491 -0.6516867 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 2300 is [True, False, False, False, True, False]
Current timestep = 2301. State = [[-0.24303724  0.12163901]]. Action = [[-0.11323047 -0.14000337 -0.23161362  0.50249076]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 2301 is [True, False, False, False, True, False]
Scene graph at timestep 2301 is [True, False, False, False, True, False]
State prediction error at timestep 2301 is tensor(1.9261e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2302. State = [[-0.24309291  0.12173427]]. Action = [[ 0.09514737  0.0716559  -0.2103243  -0.91484594]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 2302 is [True, False, False, False, True, False]
Current timestep = 2303. State = [[-0.24305627  0.12180158]]. Action = [[ 0.14396673  0.14608312 -0.10601355  0.49007547]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 2303 is [True, False, False, False, True, False]
Human Feedback received at timestep 2303 of -1
Current timestep = 2304. State = [[-0.24252175  0.12312824]]. Action = [[ 0.1479241   0.17848915  0.02382877 -0.20713997]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 2304 is [True, False, False, False, True, False]
Current timestep = 2305. State = [[-0.2415757  0.1259075]]. Action = [[-0.14835738  0.07419223 -0.07497825  0.53993106]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 2305 is [True, False, False, False, True, False]
Scene graph at timestep 2305 is [True, False, False, False, False, True]
State prediction error at timestep 2305 is tensor(1.2924e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2306. State = [[-0.24236694  0.12825677]]. Action = [[ 0.24195617 -0.17252924 -0.01708274 -0.267466  ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 2306 is [True, False, False, False, False, True]
Current timestep = 2307. State = [[-0.24131927  0.12863418]]. Action = [[-0.11739737 -0.06758603  0.09249178 -0.8398601 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 2307 is [True, False, False, False, False, True]
Current timestep = 2308. State = [[-0.24083796  0.12877111]]. Action = [[ 0.15457883 -0.05856282  0.20197928  0.06786835]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 2308 is [True, False, False, False, False, True]
Current timestep = 2309. State = [[-0.23995498  0.1282117 ]]. Action = [[-0.07330367 -0.11738327 -0.10352087 -0.37587333]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 2309 is [True, False, False, False, False, True]
Current timestep = 2310. State = [[-0.23935005  0.12723011]]. Action = [[-0.17268531  0.22997198 -0.03473333 -0.34839368]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 2310 is [True, False, False, False, False, True]
Current timestep = 2311. State = [[-0.23990637  0.12806079]]. Action = [[ 0.12271744 -0.03347579 -0.22471139  0.15074492]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 2311 is [True, False, False, False, False, True]
Current timestep = 2312. State = [[-0.24000448  0.12831147]]. Action = [[-0.011245    0.21533781 -0.20306678 -0.47080588]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 2312 is [True, False, False, False, False, True]
Current timestep = 2313. State = [[-0.24023598  0.12985988]]. Action = [[-0.09625989  0.13645855  0.04058075 -0.75189793]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 2313 is [True, False, False, False, False, True]
Current timestep = 2314. State = [[-0.24138172  0.13258842]]. Action = [[ 0.19818515 -0.21540737  0.12889153 -0.4782598 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 2314 is [True, False, False, False, False, True]
Current timestep = 2315. State = [[-0.24082273  0.13252658]]. Action = [[ 0.23471195 -0.0474643   0.13715851  0.84556746]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 2315 is [True, False, False, False, False, True]
Current timestep = 2316. State = [[-0.2381527   0.13193166]]. Action = [[ 0.04526871 -0.0355182  -0.08436286 -0.8964025 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 2316 is [True, False, False, False, False, True]
Current timestep = 2317. State = [[-0.23551887  0.13214517]]. Action = [[-0.01037994 -0.15212238  0.09639868  0.53780913]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 2317 is [True, False, False, False, False, True]
Current timestep = 2318. State = [[-0.23346812  0.1303069 ]]. Action = [[0.1992262  0.01703018 0.10820544 0.4257908 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 2318 is [True, False, False, False, False, True]
Scene graph at timestep 2318 is [True, False, False, False, False, True]
State prediction error at timestep 2318 is tensor(1.4729e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2319. State = [[-0.2301771   0.13012393]]. Action = [[-0.11062473 -0.21375284  0.23548564 -0.21021634]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 2319 is [True, False, False, False, False, True]
Current timestep = 2320. State = [[-0.22844115  0.1276976 ]]. Action = [[ 0.05570245  0.22801775  0.03185922 -0.13475174]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 2320 is [True, False, False, False, False, True]
Current timestep = 2321. State = [[-0.2274804   0.12830085]]. Action = [[-0.16319017  0.23499712 -0.03390722 -0.19008559]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 2321 is [True, False, False, False, False, True]
Current timestep = 2322. State = [[-0.22808696  0.13034174]]. Action = [[-0.15334295  0.10388091  0.04291439  0.13184655]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 2322 is [True, False, False, False, False, True]
Current timestep = 2323. State = [[-0.2294225   0.13286059]]. Action = [[ 0.15502572  0.1824339  -0.1550079  -0.45607173]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 2323 is [True, False, False, False, False, True]
Scene graph at timestep 2323 is [True, False, False, False, False, True]
State prediction error at timestep 2323 is tensor(2.3860e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2324. State = [[-0.23023298  0.13614002]]. Action = [[-0.11959267  0.02347964  0.12702689 -0.23575544]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 2324 is [True, False, False, False, False, True]
Current timestep = 2325. State = [[-0.23123372  0.13846475]]. Action = [[-0.14980955  0.05757776 -0.08052623 -0.99014217]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 2325 is [True, False, False, False, False, True]
Current timestep = 2326. State = [[-0.23278712  0.14144567]]. Action = [[-0.19453666 -0.06728664  0.1234076  -0.41434824]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 2326 is [True, False, False, False, False, True]
Current timestep = 2327. State = [[-0.23431878  0.14388365]]. Action = [[-0.2167846   0.23255217 -0.12345131  0.30185318]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 2327 is [True, False, False, False, False, True]
Current timestep = 2328. State = [[-0.23711623  0.1482613 ]]. Action = [[-0.0884884  -0.2306856  -0.13471685  0.21870542]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 2328 is [True, False, False, False, False, True]
Current timestep = 2329. State = [[-0.23874947  0.14951867]]. Action = [[-0.16299863  0.16564828 -0.08819346  0.31054544]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 2329 is [True, False, False, False, False, True]
Current timestep = 2330. State = [[-0.24147946  0.15232271]]. Action = [[0.05424115 0.04404077 0.1941486  0.30967724]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 2330 is [True, False, False, False, False, True]
Scene graph at timestep 2330 is [True, False, False, False, False, True]
State prediction error at timestep 2330 is tensor(5.5811e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2331. State = [[-0.24306971  0.15454327]]. Action = [[ 0.0963375   0.12460548 -0.08428434  0.00034857]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 2331 is [True, False, False, False, False, True]
Current timestep = 2332. State = [[-0.24439938  0.15645589]]. Action = [[-0.07467905 -0.1177583   0.12868741 -0.6038258 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 2332 is [True, False, False, False, False, True]
Current timestep = 2333. State = [[-0.244679    0.15682061]]. Action = [[-0.19797319 -0.09975675 -0.02984165 -0.83761334]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 2333 is [True, False, False, False, False, True]
Current timestep = 2334. State = [[-0.24592032  0.15628454]]. Action = [[ 0.20829743 -0.01359034 -0.06784138  0.98015404]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 2334 is [True, False, False, False, False, True]
Current timestep = 2335. State = [[-0.24564761  0.15577462]]. Action = [[ 0.1737768  -0.051039   -0.13303183  0.66105115]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 2335 is [True, False, False, False, False, True]
Current timestep = 2336. State = [[-0.2453906   0.15460612]]. Action = [[-0.06002122 -0.14463063  0.04828012 -0.1327064 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 2336 is [True, False, False, False, False, True]
Current timestep = 2337. State = [[-0.24476898  0.152214  ]]. Action = [[-0.1356692  -0.13285244 -0.08205259  0.06083441]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 2337 is [True, False, False, False, False, True]
Current timestep = 2338. State = [[-0.24428605  0.14937256]]. Action = [[-0.20772275  0.10538608  0.15827769  0.11214471]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 2338 is [True, False, False, False, False, True]
Current timestep = 2339. State = [[-0.2455492   0.14831436]]. Action = [[ 0.23599309 -0.18718877 -0.13699129 -0.5826467 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 2339 is [True, False, False, False, False, True]
Current timestep = 2340. State = [[-0.24516581  0.14552902]]. Action = [[-0.16986133 -0.08132276 -0.12460302 -0.9304321 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 2340 is [True, False, False, False, False, True]
Current timestep = 2341. State = [[-0.24581213  0.14285192]]. Action = [[ 0.24687022 -0.21801801 -0.14788534 -0.37691814]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 2341 is [True, False, False, False, False, True]
Current timestep = 2342. State = [[-0.24465437  0.13789956]]. Action = [[ 0.13902259 -0.12975022 -0.04945415 -0.2611019 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 2342 is [True, False, False, False, False, True]
Scene graph at timestep 2342 is [True, False, False, False, False, True]
State prediction error at timestep 2342 is tensor(1.4869e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2343. State = [[-0.24337868  0.13282128]]. Action = [[ 0.16198125  0.17268533  0.11734578 -0.93622077]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 2343 is [True, False, False, False, False, True]
Scene graph at timestep 2343 is [True, False, False, False, False, True]
State prediction error at timestep 2343 is tensor(1.2812e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2344. State = [[-0.24268845  0.13155554]]. Action = [[-0.16060722 -0.14558901  0.10779905  0.9815042 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 2344 is [True, False, False, False, False, True]
Current timestep = 2345. State = [[-0.24237312  0.12982963]]. Action = [[0.10280469 0.19825223 0.14015085 0.30920267]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 2345 is [True, False, False, False, False, True]
Current timestep = 2346. State = [[-0.24231058  0.13028218]]. Action = [[ 0.00291616 -0.19608559 -0.08866718  0.01973844]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 2346 is [True, False, False, False, False, True]
Scene graph at timestep 2346 is [True, False, False, False, False, True]
State prediction error at timestep 2346 is tensor(1.8459e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2347. State = [[-0.24158233  0.12887952]]. Action = [[ 0.1811099   0.03123596 -0.08168074 -0.02068424]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 2347 is [True, False, False, False, False, True]
Current timestep = 2348. State = [[-0.24052727  0.12805548]]. Action = [[ 0.11022353 -0.19956197 -0.21362127  0.28496122]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 2348 is [True, False, False, False, False, True]
Scene graph at timestep 2348 is [True, False, False, False, False, True]
State prediction error at timestep 2348 is tensor(4.5156e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2349. State = [[-0.23863539  0.12445947]]. Action = [[-0.22499    -0.00528681 -0.04115191  0.89672005]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 2349 is [True, False, False, False, False, True]
Current timestep = 2350. State = [[-0.23819369  0.12318981]]. Action = [[ 0.1947627   0.15047869  0.04282528 -0.59302324]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 2350 is [True, False, False, False, True, False]
Current timestep = 2351. State = [[-0.23805565  0.12337217]]. Action = [[-0.06849553  0.16145974 -0.04254521 -0.7637334 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 2351 is [True, False, False, False, True, False]
Scene graph at timestep 2351 is [True, False, False, False, True, False]
State prediction error at timestep 2351 is tensor(1.0480e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2352. State = [[-0.23817372  0.12371125]]. Action = [[-0.21956573 -0.20407955  0.02617696 -0.0978604 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 2352 is [True, False, False, False, True, False]
Current timestep = 2353. State = [[-0.23825178  0.12356404]]. Action = [[-0.09173232  0.18063009  0.17405736 -0.76400137]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 2353 is [True, False, False, False, True, False]
Current timestep = 2354. State = [[-0.23898652  0.12478153]]. Action = [[ 0.13208139 -0.00562014  0.22077662 -0.7927337 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 2354 is [True, False, False, False, True, False]
Current timestep = 2355. State = [[-0.23934114  0.12537394]]. Action = [[ 0.10379034  0.21366587 -0.1406891   0.7541578 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 2355 is [True, False, False, False, True, False]
Current timestep = 2356. State = [[-0.23981722  0.12698549]]. Action = [[ 0.21243346  0.05513561 -0.17364538 -0.06773442]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 2356 is [True, False, False, False, False, True]
Current timestep = 2357. State = [[-0.23939967  0.12846856]]. Action = [[-0.12463345  0.21257782  0.17649049 -0.2630267 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 2357 is [True, False, False, False, False, True]
Current timestep = 2358. State = [[-0.24025422  0.13222347]]. Action = [[0.2026546  0.23041111 0.08943859 0.7694645 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 2358 is [True, False, False, False, False, True]
Current timestep = 2359. State = [[-0.23887612  0.13713878]]. Action = [[-0.02229312  0.16340673  0.04683667 -0.66586906]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 2359 is [True, False, False, False, False, True]
Scene graph at timestep 2359 is [True, False, False, False, False, True]
State prediction error at timestep 2359 is tensor(1.1631e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2360. State = [[-0.2367581   0.14217854]]. Action = [[ 0.2249983  -0.16552682 -0.23643097  0.36811268]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 2360 is [True, False, False, False, False, True]
Current timestep = 2361. State = [[-0.23370889  0.14378425]]. Action = [[-0.22860165  0.05711526  0.11401376  0.83974123]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 2361 is [True, False, False, False, False, True]
Current timestep = 2362. State = [[-0.23242575  0.14550912]]. Action = [[-0.10118043  0.05355528  0.11965242  0.3566103 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 2362 is [True, False, False, False, False, True]
Current timestep = 2363. State = [[-0.2328838   0.14791842]]. Action = [[-0.17503348  0.14737403  0.03211114 -0.41120553]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 2363 is [True, False, False, False, False, True]
Scene graph at timestep 2363 is [True, False, False, False, False, True]
State prediction error at timestep 2363 is tensor(1.3684e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2364. State = [[-0.23519516  0.15154822]]. Action = [[ 9.2670500e-02 -5.3155184e-02  5.6710839e-04  8.7427092e-01]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 2364 is [True, False, False, False, False, True]
Current timestep = 2365. State = [[-0.2357702   0.15267843]]. Action = [[-0.017565   -0.06963985 -0.08460897 -0.78856105]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 2365 is [True, False, False, False, False, True]
Scene graph at timestep 2365 is [True, False, False, False, False, True]
State prediction error at timestep 2365 is tensor(1.6823e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2366. State = [[-0.23592775  0.1529404 ]]. Action = [[-0.07432017  0.18075591 -0.06084783 -0.5596415 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 2366 is [True, False, False, False, False, True]
Scene graph at timestep 2366 is [True, False, False, False, False, True]
State prediction error at timestep 2366 is tensor(1.8197e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2367. State = [[-0.23730938  0.15527721]]. Action = [[-0.22848931  0.13706714  0.16837049  0.7666794 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 2367 is [True, False, False, False, False, True]
Scene graph at timestep 2367 is [True, False, False, False, False, True]
State prediction error at timestep 2367 is tensor(2.6569e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2368. State = [[-0.24032883  0.15960546]]. Action = [[-0.02600272 -0.16611825 -0.15982062 -0.8151005 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 2368 is [True, False, False, False, False, True]
Current timestep = 2369. State = [[-0.2411819  0.1603807]]. Action = [[ 0.07989591 -0.2286744   0.12425631  0.8973893 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 2369 is [True, False, False, False, False, True]
Current timestep = 2370. State = [[-0.24072842  0.15852985]]. Action = [[-0.08931227 -0.15561874  0.23419756  0.48955488]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 2370 is [True, False, False, False, False, True]
Scene graph at timestep 2370 is [True, False, False, False, False, True]
State prediction error at timestep 2370 is tensor(3.5053e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2371. State = [[-0.24037151  0.15585634]]. Action = [[-0.17634352 -0.07361475  0.22719151  0.49778473]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 2371 is [True, False, False, False, False, True]
Current timestep = 2372. State = [[-0.24089648  0.15363663]]. Action = [[ 0.09341791 -0.200995    0.19474262  0.32647526]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 2372 is [True, False, False, False, False, True]
Scene graph at timestep 2372 is [True, False, False, False, False, True]
State prediction error at timestep 2372 is tensor(5.9980e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2373. State = [[-0.2407392   0.15004086]]. Action = [[-0.24582012 -0.13660711  0.03285488  0.97537017]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 2373 is [True, False, False, False, False, True]
Current timestep = 2374. State = [[-0.24207585  0.14663544]]. Action = [[-0.18484372  0.13805503  0.04375315 -0.24461442]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 2374 is [True, False, False, False, False, True]
Current timestep = 2375. State = [[-0.24589984  0.14557181]]. Action = [[ 0.10130471  0.21954143 -0.2413239   0.8762512 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 2375 is [True, False, False, False, False, True]
Scene graph at timestep 2375 is [True, False, False, False, False, True]
State prediction error at timestep 2375 is tensor(1.8450e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2376. State = [[-0.24781944  0.14704363]]. Action = [[ 0.08106917 -0.02040014 -0.18693466  0.28311205]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 2376 is [True, False, False, False, False, True]
Current timestep = 2377. State = [[-0.2481484   0.14746569]]. Action = [[ 0.22477853  0.00829709  0.07485518 -0.2480551 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 2377 is [True, False, False, False, False, True]
Scene graph at timestep 2377 is [True, False, False, False, False, True]
State prediction error at timestep 2377 is tensor(1.5930e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2378. State = [[-0.24812938  0.14748357]]. Action = [[-0.09336926 -0.12962636 -0.15872066  0.6526581 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 2378 is [True, False, False, False, False, True]
Current timestep = 2379. State = [[-0.24791548  0.14654335]]. Action = [[-0.08223036 -0.23087078 -0.19221738  0.602108  ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 2379 is [True, False, False, False, False, True]
Current timestep = 2380. State = [[-0.24760604  0.14350465]]. Action = [[-0.21861486  0.15300459 -0.04149422 -0.73255575]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 2380 is [True, False, False, False, False, True]
Current timestep = 2381. State = [[-0.24921182  0.14340444]]. Action = [[-0.23850103 -0.15286382  0.03953061 -0.8840549 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 2381 is [True, False, False, False, False, True]
Scene graph at timestep 2381 is [True, False, False, False, False, True]
State prediction error at timestep 2381 is tensor(2.2850e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2382. State = [[-0.25261542  0.14121822]]. Action = [[-0.02436298 -0.16856521  0.18840617  0.4195062 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 2382 is [True, False, False, False, False, True]
Current timestep = 2383. State = [[-0.25583556  0.1382603 ]]. Action = [[ 0.00201076  0.20851383  0.15211907 -0.8941971 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 2383 is [True, False, False, False, False, True]
Current timestep = 2384. State = [[-0.25838274  0.13806039]]. Action = [[ 0.08118206 -0.18107562  0.0108009  -0.8564818 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 2384 is [True, False, False, False, False, True]
Scene graph at timestep 2384 is [True, False, False, False, False, True]
State prediction error at timestep 2384 is tensor(2.1012e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2385. State = [[-0.2594222   0.13626722]]. Action = [[-0.09470987 -0.02440435 -0.24457005  0.81245184]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 2385 is [True, False, False, False, False, True]
Scene graph at timestep 2385 is [True, False, False, False, False, True]
State prediction error at timestep 2385 is tensor(3.8814e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2386. State = [[-0.2603855   0.13476388]]. Action = [[ 0.08883971 -0.00836642 -0.00466211 -0.00672895]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 2386 is [True, False, False, False, False, True]
Current timestep = 2387. State = [[-0.26033068  0.13353258]]. Action = [[-0.02321014 -0.14040631 -0.14606765  0.82506895]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 2387 is [True, False, False, False, False, True]
Scene graph at timestep 2387 is [True, False, False, False, False, True]
State prediction error at timestep 2387 is tensor(3.7803e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2388. State = [[-0.26017338  0.13088033]]. Action = [[ 0.15539405 -0.13089095  0.18970072  0.90946853]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 2388 is [True, False, False, False, False, True]
Current timestep = 2389. State = [[-0.25932172  0.12726629]]. Action = [[ 0.06889981  0.16303799  0.14746279 -0.6056528 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 2389 is [True, False, False, False, False, True]
Current timestep = 2390. State = [[-0.25920168  0.12665243]]. Action = [[ 0.10207975 -0.12073392 -0.22236475  0.7861903 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 2390 is [True, False, False, False, False, True]
Current timestep = 2391. State = [[-0.25867155  0.12508552]]. Action = [[-0.22302826 -0.09760354 -0.01329434  0.0673883 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 2391 is [True, False, False, False, False, True]
Current timestep = 2392. State = [[-0.25857267  0.12339157]]. Action = [[-0.05820319 -0.1649279  -0.23110494  0.09192002]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 2392 is [True, False, False, False, False, True]
Current timestep = 2393. State = [[-0.25882372  0.12056763]]. Action = [[ 0.10568202 -0.1216644  -0.20288959 -0.32954407]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 2393 is [True, False, False, False, True, False]
Current timestep = 2394. State = [[-0.2582359   0.11686891]]. Action = [[-0.10268092 -0.19897233 -0.02867118  0.80406034]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 2394 is [True, False, False, False, True, False]
Current timestep = 2395. State = [[-0.25835058  0.11172734]]. Action = [[ 0.22173855 -0.1473541  -0.12317055 -0.49499696]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 2395 is [True, False, False, False, True, False]
Current timestep = 2396. State = [[-0.2571195   0.10704497]]. Action = [[-0.20282333  0.15295872 -0.11904433 -0.667862  ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 2396 is [True, False, False, False, True, False]
Current timestep = 2397. State = [[-0.25763905  0.10599172]]. Action = [[ 0.02279347  0.12896574  0.23935938 -0.05716723]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 2397 is [True, False, False, False, True, False]
Current timestep = 2398. State = [[-0.2579205   0.10671187]]. Action = [[-0.05307308 -0.107144    0.12290066 -0.22155309]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 2398 is [True, False, False, False, True, False]
Current timestep = 2399. State = [[-0.25810954  0.10610368]]. Action = [[ 0.16977185  0.1331864  -0.04193702  0.26060688]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 2399 is [True, False, False, False, True, False]
Current timestep = 2400. State = [[-0.2581213   0.10605507]]. Action = [[ 0.03978902 -0.10353109  0.10501012  0.39842033]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 2400 is [True, False, False, False, True, False]
Current timestep = 2401. State = [[-0.2576169   0.10573214]]. Action = [[-0.04178986 -0.20974623  0.09030876  0.10254228]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 2401 is [True, False, False, False, True, False]
Current timestep = 2402. State = [[-0.25676167  0.1030935 ]]. Action = [[-0.06058258 -0.2345041  -0.21804672  0.8843248 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 2402 is [True, False, False, False, True, False]
Current timestep = 2403. State = [[-0.25661543  0.0984059 ]]. Action = [[-0.2046359   0.23196274 -0.17107517 -0.5106469 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 2403 is [True, False, False, False, True, False]
Current timestep = 2404. State = [[-0.2580797   0.09761187]]. Action = [[ 0.17938352 -0.14617817  0.23628205 -0.53009415]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 2404 is [True, False, False, False, True, False]
Scene graph at timestep 2404 is [True, False, False, False, True, False]
State prediction error at timestep 2404 is tensor(3.3383e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2405. State = [[-0.25871184  0.09532723]]. Action = [[-0.1089986  -0.00250868  0.11713713  0.04514444]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 2405 is [True, False, False, False, True, False]
Scene graph at timestep 2405 is [True, False, False, False, True, False]
State prediction error at timestep 2405 is tensor(5.1622e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2406. State = [[-0.25986198  0.09421977]]. Action = [[0.03876245 0.21627283 0.19391507 0.71789837]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 2406 is [True, False, False, False, True, False]
Scene graph at timestep 2406 is [True, False, False, False, True, False]
State prediction error at timestep 2406 is tensor(5.4469e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2407. State = [[-0.26056424  0.09459074]]. Action = [[-0.13233772 -0.00298609 -0.13152242 -0.11291254]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 2407 is [True, False, False, False, True, False]
Current timestep = 2408. State = [[-0.26205423  0.09473725]]. Action = [[-0.04828055 -0.01002468 -0.1820883  -0.44137406]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 2408 is [True, False, False, False, True, False]
Scene graph at timestep 2408 is [True, False, False, False, True, False]
State prediction error at timestep 2408 is tensor(7.0830e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2409. State = [[-0.26364636  0.09461259]]. Action = [[-0.21742164 -0.04507791 -0.22235192 -0.30839795]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 2409 is [True, False, False, False, True, False]
Current timestep = 2410. State = [[-0.26606432  0.09444365]]. Action = [[ 0.02978674 -0.13327552  0.06416327 -0.26507545]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 2410 is [True, False, False, False, True, False]
Current timestep = 2411. State = [[-0.26810426  0.09350939]]. Action = [[ 0.11569896  0.07932431  0.194637   -0.7668955 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 2411 is [True, False, False, False, True, False]
Current timestep = 2412. State = [[-0.2683713   0.09352269]]. Action = [[-0.00282228  0.22434565  0.02139133  0.04441321]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 2412 is [True, False, False, False, True, False]
Scene graph at timestep 2412 is [True, False, False, False, True, False]
State prediction error at timestep 2412 is tensor(1.9799e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2413. State = [[-0.26901147  0.09494084]]. Action = [[-0.10092951 -0.01913333 -0.07458478  0.32658446]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 2413 is [True, False, False, False, True, False]
Current timestep = 2414. State = [[-0.26969737  0.09595639]]. Action = [[-0.08137506 -0.20115758  0.02251682  0.01661539]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 2414 is [True, False, False, False, True, False]
Scene graph at timestep 2414 is [True, False, False, False, True, False]
State prediction error at timestep 2414 is tensor(2.4870e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2415. State = [[-0.2708507   0.09533425]]. Action = [[-0.18259072  0.13473463 -0.08241889  0.3137871 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 2415 is [True, False, False, False, True, False]
Human Feedback received at timestep 2415 of -1
Current timestep = 2416. State = [[-0.27200818  0.09479059]]. Action = [[-0.14417411 -0.12135068  0.21948612 -0.03311622]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 2416 is [True, False, False, False, True, False]
Current timestep = 2417. State = [[-0.27438104  0.09303956]]. Action = [[-0.01040366 -0.24282826  0.17829251 -0.63318026]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 2417 is [True, False, False, False, True, False]
Current timestep = 2418. State = [[-0.2760361  0.0894141]]. Action = [[ 0.00122464 -0.22417772 -0.11188862  0.73972106]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 2418 is [True, False, False, False, True, False]
Current timestep = 2419. State = [[-0.27677315  0.08398463]]. Action = [[-0.12055469  0.19516975 -0.11784272  0.27706218]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 2419 is [True, False, False, False, True, False]
Current timestep = 2420. State = [[-0.27731547  0.08021292]]. Action = [[-0.14943399  0.08693036 -0.0525858  -0.06238925]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 2420 is [True, False, False, False, True, False]
Current timestep = 2421. State = [[-0.27765685  0.07793744]]. Action = [[-0.1986108   0.0210906   0.06414866 -0.65977025]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 2421 is [True, False, False, False, True, False]
Scene graph at timestep 2421 is [True, False, False, False, True, False]
State prediction error at timestep 2421 is tensor(1.0522e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2421 of -1
Current timestep = 2422. State = [[-0.27805293  0.07679818]]. Action = [[-0.052642    0.15017363 -0.02256581  0.82789207]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 2422 is [True, False, False, False, True, False]
Current timestep = 2423. State = [[-0.278265    0.07610938]]. Action = [[ 0.20319957 -0.07482146  0.20098418  0.864362  ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 2423 is [True, False, False, False, True, False]
Current timestep = 2424. State = [[-0.27714008  0.07502782]]. Action = [[0.20914209 0.19625038 0.07931706 0.15800667]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 2424 is [True, False, False, False, True, False]
Scene graph at timestep 2424 is [True, False, False, False, True, False]
State prediction error at timestep 2424 is tensor(4.0055e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2425. State = [[-0.27610308  0.07552218]]. Action = [[ 0.00780967 -0.23817375  0.19422153 -0.37150156]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 2425 is [True, False, False, False, True, False]
Human Feedback received at timestep 2425 of -1
Current timestep = 2426. State = [[-0.2744357   0.07331393]]. Action = [[-0.08056693 -0.13619725 -0.03535424 -0.57982725]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 2426 is [True, False, False, False, True, False]
Scene graph at timestep 2426 is [True, False, False, False, True, False]
State prediction error at timestep 2426 is tensor(1.4612e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2427. State = [[-0.27369857  0.07206928]]. Action = [[-0.12932687  0.0187889   0.10358003  0.12841511]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 2427 is [True, False, False, False, True, False]
Scene graph at timestep 2427 is [True, False, False, False, True, False]
State prediction error at timestep 2427 is tensor(1.1035e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2428. State = [[-0.2731663   0.07107192]]. Action = [[-0.01660666  0.17064446  0.03382468 -0.78488374]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 2428 is [True, False, False, False, True, False]
Current timestep = 2429. State = [[-0.27320573  0.07129531]]. Action = [[-0.24196614 -0.21909222  0.23234355 -0.82872456]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 2429 is [True, False, False, False, True, False]
Current timestep = 2430. State = [[-0.27320573  0.07129531]]. Action = [[-0.03106003 -0.10926202 -0.19301243  0.78541636]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 2430 is [True, False, False, False, True, False]
Current timestep = 2431. State = [[-0.27316067  0.07106452]]. Action = [[-0.15487844 -0.18563727  0.18339965  0.12016761]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 2431 is [True, False, False, False, True, False]
Current timestep = 2432. State = [[-0.2731913   0.07080217]]. Action = [[ 0.23415017  0.19674021 -0.20004827 -0.56959325]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 2432 is [True, False, False, False, True, False]
Current timestep = 2433. State = [[-0.27260312  0.07091369]]. Action = [[-0.20128937  0.06547695  0.15188852 -0.21453112]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 2433 is [True, False, False, False, True, False]
Current timestep = 2434. State = [[-0.27190864  0.07133539]]. Action = [[ 0.2156722  -0.03555648 -0.02322775  0.7896284 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 2434 is [True, False, False, False, True, False]
Current timestep = 2435. State = [[-0.26990932  0.07204679]]. Action = [[-0.02495033  0.15616879 -0.04163127 -0.39426482]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 2435 is [True, False, False, False, True, False]
Current timestep = 2436. State = [[-0.26822057  0.0731813 ]]. Action = [[-0.24550292  0.21440685  0.21107036  0.58268523]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 2436 is [True, False, False, False, True, False]
Scene graph at timestep 2436 is [True, False, False, False, True, False]
State prediction error at timestep 2436 is tensor(1.8837e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2437. State = [[-0.26729214  0.07379477]]. Action = [[ 0.12754685  0.02083433 -0.11797953 -0.02437907]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 2437 is [True, False, False, False, True, False]
Scene graph at timestep 2437 is [True, False, False, False, True, False]
State prediction error at timestep 2437 is tensor(3.7057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2438. State = [[-0.26616973  0.07495194]]. Action = [[ 0.05913132  0.23886535 -0.13706513  0.10089421]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 2438 is [True, False, False, False, True, False]
Current timestep = 2439. State = [[-0.26497915  0.07774871]]. Action = [[-0.13598326 -0.14075951 -0.09719004  0.6065633 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 2439 is [True, False, False, False, True, False]
Current timestep = 2440. State = [[-0.2650957   0.07805276]]. Action = [[ 0.04418319 -0.20825863  0.15897676  0.32067394]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 2440 is [True, False, False, False, True, False]
Current timestep = 2441. State = [[-0.26448238  0.07729934]]. Action = [[ 0.19125038  0.21540219 -0.24489163 -0.58551633]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 2441 is [True, False, False, False, True, False]
Current timestep = 2442. State = [[-0.26293275  0.07776621]]. Action = [[ 0.21968853 -0.20309278 -0.10008186 -0.951088  ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 2442 is [True, False, False, False, True, False]
Scene graph at timestep 2442 is [True, False, False, False, True, False]
State prediction error at timestep 2442 is tensor(1.1757e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2443. State = [[-0.25963235  0.07653932]]. Action = [[ 0.16794538 -0.18631639  0.12537375 -0.32310688]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 2443 is [True, False, False, False, True, False]
Current timestep = 2444. State = [[-0.2559385  0.0736929]]. Action = [[-0.21828431 -0.11499965 -0.19682495  0.9298841 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 2444 is [True, False, False, False, True, False]
Current timestep = 2445. State = [[-0.25394797  0.07069119]]. Action = [[ 0.2051689  -0.21332812 -0.16859896  0.5809374 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 2445 is [True, False, False, False, True, False]
Current timestep = 2446. State = [[-0.25102368  0.06591835]]. Action = [[-0.06757271  0.18310708  0.06871623  0.7322593 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 2446 is [True, False, False, False, True, False]
Current timestep = 2447. State = [[-0.24997886  0.06472162]]. Action = [[ 0.20345107 -0.02759738 -0.18259066  0.6658144 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 2447 is [True, False, False, False, True, False]
Scene graph at timestep 2447 is [True, False, False, False, True, False]
State prediction error at timestep 2447 is tensor(1.1742e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2448. State = [[-0.24762525  0.06429975]]. Action = [[-0.17114748  0.10421115 -0.01635212 -0.22971666]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 2448 is [True, False, False, False, True, False]
Scene graph at timestep 2448 is [True, False, False, False, True, False]
State prediction error at timestep 2448 is tensor(9.7318e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2449. State = [[-0.24697405  0.0643997 ]]. Action = [[-0.10480464 -0.19757062  0.06859517  0.4451748 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 2449 is [True, False, False, False, True, False]
Current timestep = 2450. State = [[-0.24661483  0.06365465]]. Action = [[-0.20203575  0.1323868   0.14205524  0.94207287]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 2450 is [True, False, False, False, True, False]
Current timestep = 2451. State = [[-0.24671784  0.06384256]]. Action = [[0.07235253 0.13854182 0.01863325 0.3094623 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 2451 is [True, False, False, False, True, False]
Current timestep = 2452. State = [[-0.24713069  0.06457444]]. Action = [[ 0.09943348 -0.1655054  -0.16293667  0.0175488 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 2452 is [True, False, False, False, True, False]
Current timestep = 2453. State = [[-0.24696921  0.06424083]]. Action = [[ 0.02019131 -0.14024356  0.09444714  0.66722727]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 2453 is [True, False, False, False, True, False]
Scene graph at timestep 2453 is [True, False, False, False, True, False]
State prediction error at timestep 2453 is tensor(1.6766e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2454. State = [[-0.24641432  0.06292848]]. Action = [[-0.05479513  0.03532046  0.20927429  0.95631075]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 2454 is [True, False, False, False, True, False]
Current timestep = 2455. State = [[-0.24642546  0.06235966]]. Action = [[-0.09223829 -0.13691129 -0.19717488 -0.05764514]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 2455 is [True, False, False, False, True, False]
Current timestep = 2456. State = [[-0.24627063  0.06077011]]. Action = [[-0.06772049  0.09859908 -0.22401659 -0.14372885]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 2456 is [True, False, False, False, True, False]
Current timestep = 2457. State = [[-0.24636526  0.06086051]]. Action = [[ 0.22635779  0.21495867 -0.17967461 -0.00992286]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 2457 is [True, False, False, False, True, False]
Scene graph at timestep 2457 is [True, False, False, False, True, False]
State prediction error at timestep 2457 is tensor(2.2656e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2458. State = [[-0.24643396  0.06129181]]. Action = [[ 0.10941875 -0.15908256  0.05977204  0.8983495 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 2458 is [True, False, False, False, True, False]
Current timestep = 2459. State = [[-0.2462897   0.06111445]]. Action = [[ 0.08583128  0.19520578  0.1728087  -0.18157375]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 2459 is [True, False, False, False, True, False]
Current timestep = 2460. State = [[-0.24552521  0.06168478]]. Action = [[-0.21329866 -0.21991745 -0.02644293  0.26556325]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 2460 is [True, False, False, False, True, False]
Current timestep = 2461. State = [[-0.2456622   0.06151814]]. Action = [[-0.15606824  0.01299831  0.03992698 -0.12637937]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 2461 is [True, False, False, False, True, False]
Current timestep = 2462. State = [[-0.24574482  0.06142585]]. Action = [[0.12216592 0.01049981 0.13294637 0.25670242]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 2462 is [True, False, False, False, True, False]
Current timestep = 2463. State = [[-0.24574098  0.06136271]]. Action = [[-0.15640873 -0.07722265  0.0202921  -0.8219915 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 2463 is [True, False, False, False, True, False]
Current timestep = 2464. State = [[-0.24578433  0.06089791]]. Action = [[-0.02445994 -0.01423994 -0.04805255  0.5853398 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 2464 is [True, False, False, False, True, False]
Current timestep = 2465. State = [[-0.24594447  0.0606057 ]]. Action = [[ 0.13695186 -0.00796683 -0.05544756 -0.8684816 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 2465 is [True, False, False, False, True, False]
Current timestep = 2466. State = [[-0.24575487  0.059925  ]]. Action = [[-0.01284029 -0.20430824 -0.19933408  0.42030537]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 2466 is [True, False, False, False, True, False]
Current timestep = 2467. State = [[-0.24544114  0.0578588 ]]. Action = [[-0.2370873   0.22982568  0.14991051 -0.59337837]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 2467 is [True, False, False, False, True, False]
Current timestep = 2468. State = [[-0.24648382  0.05814216]]. Action = [[ 0.01171657 -0.02130133 -0.23390995 -0.02171135]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 2468 is [True, False, False, False, True, False]
Scene graph at timestep 2468 is [True, False, False, False, True, False]
State prediction error at timestep 2468 is tensor(3.1383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2469. State = [[-0.24686952  0.05816277]]. Action = [[ 0.00822711 -0.05533144  0.22372389  0.9276719 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 2469 is [True, False, False, False, True, False]
Current timestep = 2470. State = [[-0.24703152  0.05793106]]. Action = [[-0.177078   -0.1471598  -0.23944911 -0.09423786]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 2470 is [True, False, False, False, True, False]
Current timestep = 2471. State = [[-0.24849027  0.0562126 ]]. Action = [[-0.02292041 -0.05542782 -0.13070685 -0.6393896 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 2471 is [True, False, False, False, True, False]
Current timestep = 2472. State = [[-0.25017458  0.05483253]]. Action = [[ 0.11335889  0.19411916  0.01950505 -0.77728665]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 2472 is [True, False, False, False, True, False]
Current timestep = 2473. State = [[-0.25080797  0.05520646]]. Action = [[-0.14031944  0.16808474  0.13212708  0.16659164]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 2473 is [True, False, False, False, True, False]
Current timestep = 2474. State = [[-0.2524687   0.05657864]]. Action = [[-0.18746154 -0.16445336 -0.21357425 -0.11393869]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 2474 is [True, False, False, False, True, False]
Scene graph at timestep 2474 is [True, False, False, False, True, False]
State prediction error at timestep 2474 is tensor(6.5169e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2475. State = [[-0.25477692  0.05662902]]. Action = [[-0.1972684   0.2431525  -0.08397156  0.77197444]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 2475 is [True, False, False, False, True, False]
Scene graph at timestep 2475 is [True, False, False, False, True, False]
State prediction error at timestep 2475 is tensor(4.9611e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2476. State = [[-0.25799048  0.05946676]]. Action = [[ 0.21184048  0.14043954 -0.1865214  -0.77506834]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 2476 is [True, False, False, False, True, False]
Scene graph at timestep 2476 is [True, False, False, False, True, False]
State prediction error at timestep 2476 is tensor(2.1707e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2477. State = [[-0.2595313  0.0626313]]. Action = [[-0.20521288  0.05676866 -0.03027703 -0.5694761 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 2477 is [True, False, False, False, True, False]
Scene graph at timestep 2477 is [True, False, False, False, True, False]
State prediction error at timestep 2477 is tensor(9.0520e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2478. State = [[-0.2617155   0.06610094]]. Action = [[ 0.1743772   0.12945807 -0.21872175 -0.6169419 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 2478 is [True, False, False, False, True, False]
Scene graph at timestep 2478 is [True, False, False, False, True, False]
State prediction error at timestep 2478 is tensor(5.6009e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2479. State = [[-0.26313737  0.06904441]]. Action = [[-0.18086442 -0.03328449 -0.18107796  0.6018604 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 2479 is [True, False, False, False, True, False]
Current timestep = 2480. State = [[-0.2642876   0.07078474]]. Action = [[-0.13364325 -0.12234482  0.23946428  0.49693167]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 2480 is [True, False, False, False, True, False]
Current timestep = 2481. State = [[-0.26553723  0.07099557]]. Action = [[-0.04961488 -0.10929248  0.05394521 -0.24900424]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 2481 is [True, False, False, False, True, False]
Current timestep = 2482. State = [[-0.26657218  0.07039621]]. Action = [[-0.02514683  0.11843401  0.05018458  0.8832538 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 2482 is [True, False, False, False, True, False]
Current timestep = 2483. State = [[-0.26780188  0.07109359]]. Action = [[ 0.05523935  0.23850006 -0.20768446 -0.58639055]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 2483 is [True, False, False, False, True, False]
Current timestep = 2484. State = [[-0.26903298  0.07369293]]. Action = [[ 0.09593529  0.09427109  0.09636131 -0.83945036]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 2484 is [True, False, False, False, True, False]
Current timestep = 2485. State = [[-0.2701674   0.07619207]]. Action = [[-0.24555671  0.11471474 -0.00587493  0.21177495]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 2485 is [True, False, False, False, True, False]
Current timestep = 2486. State = [[-0.27097896  0.07780018]]. Action = [[ 0.06814975 -0.10792905  0.06641251  0.83751345]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 2486 is [True, False, False, False, True, False]
Current timestep = 2487. State = [[-0.2709306   0.07775846]]. Action = [[-0.03188057 -0.21734208  0.16180503  0.44126236]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 2487 is [True, False, False, False, True, False]
Current timestep = 2488. State = [[-0.2706264   0.07658911]]. Action = [[-0.14374918 -0.03923111 -0.14583538 -0.51846063]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 2488 is [True, False, False, False, True, False]
Current timestep = 2489. State = [[-0.2705215  0.0755507]]. Action = [[-0.21997271  0.15695453 -0.13385458 -0.10859871]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 2489 is [True, False, False, False, True, False]
Scene graph at timestep 2489 is [True, False, False, False, True, False]
State prediction error at timestep 2489 is tensor(2.1480e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2490. State = [[-0.2705151   0.07500835]]. Action = [[ 0.10766    -0.08952242  0.19847846  0.8266728 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 2490 is [True, False, False, False, True, False]
Current timestep = 2491. State = [[-0.27018678  0.07334673]]. Action = [[ 0.19394255  0.08427164 -0.10912704 -0.61945796]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 2491 is [True, False, False, False, True, False]
Current timestep = 2492. State = [[-0.2698599   0.07277599]]. Action = [[ 0.03039789 -0.24075206 -0.20101887 -0.08366507]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 2492 is [True, False, False, False, True, False]
Current timestep = 2493. State = [[-0.268952    0.06977087]]. Action = [[ 0.04095268  0.01428279 -0.14651394  0.540066  ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 2493 is [True, False, False, False, True, False]
Current timestep = 2494. State = [[-0.26860288  0.0681638 ]]. Action = [[-0.07468039  0.18065476  0.12970057  0.83916235]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 2494 is [True, False, False, False, True, False]
Current timestep = 2495. State = [[-0.26870522  0.06829945]]. Action = [[-0.07816747 -0.09958097 -0.03846446 -0.08726531]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 2495 is [True, False, False, False, True, False]
Scene graph at timestep 2495 is [True, False, False, False, True, False]
State prediction error at timestep 2495 is tensor(1.7284e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2496. State = [[-0.26865956  0.06774418]]. Action = [[-0.22094364 -0.06832816  0.06560361 -0.42344534]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 2496 is [True, False, False, False, True, False]
Current timestep = 2497. State = [[-0.26861456  0.06718855]]. Action = [[-0.14221236 -0.06514344  0.00444615  0.9358809 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 2497 is [True, False, False, False, True, False]
Scene graph at timestep 2497 is [True, False, False, False, True, False]
State prediction error at timestep 2497 is tensor(7.0085e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2498. State = [[-0.26853412  0.06584333]]. Action = [[ 0.2317718  -0.15375145  0.01249185  0.6800213 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 2498 is [True, False, False, False, True, False]
Current timestep = 2499. State = [[-0.26815405  0.06347289]]. Action = [[ 0.14257377  0.06813601 -0.06231488 -0.13831067]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 2499 is [True, False, False, False, True, False]
Current timestep = 2500. State = [[-0.26789823  0.06272005]]. Action = [[-0.17327097  0.12879544  0.22452009  0.5438522 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 2500 is [True, False, False, False, True, False]
Current timestep = 2501. State = [[-0.2680185   0.06288324]]. Action = [[-0.03858587  0.09195003  0.05388215  0.268551  ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 2501 is [True, False, False, False, True, False]
Current timestep = 2502. State = [[-0.26835713  0.06358289]]. Action = [[-0.2244771   0.0126743   0.24880433 -0.07215595]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 2502 is [True, False, False, False, True, False]
Current timestep = 2503. State = [[-0.26854205  0.06416582]]. Action = [[ 0.23082298  0.12275395  0.10164946 -0.7545101 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 2503 is [True, False, False, False, True, False]
Current timestep = 2504. State = [[-0.2682543  0.0652553]]. Action = [[0.04561958 0.11002526 0.13636747 0.27414858]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 2504 is [True, False, False, False, True, False]
Current timestep = 2505. State = [[-0.2674517   0.06689759]]. Action = [[ 0.23894745  0.22157553  0.07701403 -0.9039295 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 2505 is [True, False, False, False, True, False]
Current timestep = 2506. State = [[-0.26507372  0.07095379]]. Action = [[ 0.01064813  0.21406993 -0.00471687  0.82777524]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 2506 is [True, False, False, False, True, False]
Current timestep = 2507. State = [[-0.26334074  0.07612681]]. Action = [[-0.22830515  0.11768806 -0.16861622 -0.04296613]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 2507 is [True, False, False, False, True, False]
Current timestep = 2508. State = [[-0.2642939   0.08131615]]. Action = [[-0.16843148  0.13445091 -0.01734175  0.6168518 ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 2508 is [True, False, False, False, True, False]
Current timestep = 2509. State = [[-0.25029334 -0.0727159 ]]. Action = [[-0.23557514  0.17146003 -0.16754869 -0.6444926 ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 2509 is [True, False, False, False, True, False]
Scene graph at timestep 2509 is [True, False, False, False, True, False]
State prediction error at timestep 2509 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Current timestep = 2510. State = [[-0.24951324 -0.07710724]]. Action = [[-0.1982865   0.0871065  -0.04721218  0.91339684]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 2510 is [True, False, False, False, True, False]
Current timestep = 2511. State = [[-0.24952076 -0.07705982]]. Action = [[-0.1348713   0.11251488  0.17112434  0.01008129]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 2511 is [True, False, False, False, True, False]
Current timestep = 2512. State = [[-0.24971294 -0.07685899]]. Action = [[ 0.00445336  0.03666714  0.12198108 -0.97395676]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 2512 is [True, False, False, False, True, False]
Current timestep = 2513. State = [[-0.24974215 -0.07637376]]. Action = [[ 0.14471066 -0.14544138  0.24431795  0.3759886 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 2513 is [True, False, False, False, True, False]
Current timestep = 2514. State = [[-0.24974255 -0.07652163]]. Action = [[-0.16028981  0.0109264   0.23168653  0.61704206]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 2514 is [True, False, False, False, True, False]
Current timestep = 2515. State = [[-0.24987738 -0.07674285]]. Action = [[-0.1262189  -0.07793115  0.10127449  0.01629496]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 2515 is [True, False, False, False, True, False]
Current timestep = 2516. State = [[-0.25101787 -0.07772017]]. Action = [[-0.11632875 -0.09120476 -0.11499643  0.8464327 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 2516 is [True, False, False, False, True, False]
Scene graph at timestep 2516 is [True, False, False, False, True, False]
State prediction error at timestep 2516 is tensor(4.9477e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2517. State = [[-0.25280538 -0.07976226]]. Action = [[-0.12439755 -0.19472176  0.21942317  0.748809  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 2517 is [True, False, False, False, True, False]
Current timestep = 2518. State = [[-0.25478345 -0.0830271 ]]. Action = [[-0.1551548  -0.20966487 -0.03494458 -0.8730119 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 2518 is [True, False, False, False, True, False]
Current timestep = 2519. State = [[-0.25757053 -0.08870021]]. Action = [[ 0.2038647  -0.23386894  0.03433776 -0.31095648]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 2519 is [True, False, False, False, True, False]
Current timestep = 2520. State = [[-0.25886008 -0.09461738]]. Action = [[-0.10722962  0.11953813  0.15039986 -0.22421539]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 2520 is [True, False, False, False, True, False]
Current timestep = 2521. State = [[-0.26004654 -0.09729207]]. Action = [[-0.1034019   0.1306659  -0.04318935  0.50514567]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 2521 is [True, False, False, False, True, False]
Current timestep = 2522. State = [[-0.26145676 -0.09822965]]. Action = [[ 0.06935802 -0.19661091  0.16955566 -0.7209934 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 2522 is [True, False, False, False, True, False]
Current timestep = 2523. State = [[-0.26234838 -0.10070283]]. Action = [[-0.04898691 -0.21435755 -0.23703037 -0.01843596]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 2523 is [True, False, False, False, True, False]
Scene graph at timestep 2523 is [True, False, False, False, True, False]
State prediction error at timestep 2523 is tensor(5.2333e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2524. State = [[-0.26384696 -0.10544195]]. Action = [[-0.11019459 -0.22812818  0.08084947  0.9500983 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 2524 is [True, False, False, False, True, False]
Current timestep = 2525. State = [[-0.26582077 -0.11068424]]. Action = [[ 0.04357731 -0.01949215  0.24516374 -0.83308166]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 2525 is [True, False, False, False, True, False]
Scene graph at timestep 2525 is [True, False, False, False, True, False]
State prediction error at timestep 2525 is tensor(4.5811e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2525 of -1
Current timestep = 2526. State = [[-0.26713428 -0.11470937]]. Action = [[ 0.08549169  0.13136137 -0.02450019 -0.0063619 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 2526 is [True, False, False, False, True, False]
Current timestep = 2527. State = [[-0.267342   -0.11554112]]. Action = [[ 0.01492667 -0.08572876  0.09717101  0.753747  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 2527 is [True, False, False, False, True, False]
Scene graph at timestep 2527 is [True, False, False, False, True, False]
State prediction error at timestep 2527 is tensor(7.7794e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2528. State = [[-0.26753125 -0.11668119]]. Action = [[-0.02298629  0.14137805 -0.21692428  0.35185504]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 2528 is [True, False, False, False, True, False]
Current timestep = 2529. State = [[-0.26754802 -0.11686312]]. Action = [[-0.16534457 -0.23890132 -0.13860002  0.2601261 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 2529 is [True, False, False, False, True, False]
Current timestep = 2530. State = [[-0.26882175 -0.11934399]]. Action = [[ 0.13504124 -0.20721433 -0.02762908 -0.6466293 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 2530 is [True, False, False, False, True, False]
Current timestep = 2531. State = [[-0.2691877  -0.12286253]]. Action = [[0.05341119 0.2138429  0.23762959 0.3225553 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 2531 is [True, False, False, False, True, False]
Current timestep = 2532. State = [[-0.26911932 -0.12311976]]. Action = [[ 0.00137344 -0.02769159 -0.22626549 -0.84826905]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 2532 is [True, False, False, False, True, False]
Current timestep = 2533. State = [[-0.26896888 -0.12325244]]. Action = [[-0.15100941 -0.14447725 -0.22565469 -0.5800842 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 2533 is [True, False, False, False, True, False]
Current timestep = 2534. State = [[-0.26982915 -0.12486664]]. Action = [[-0.14008047  0.17859113 -0.00571512 -0.80897754]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 2534 is [True, False, False, False, True, False]
Human Feedback received at timestep 2534 of -1
Current timestep = 2535. State = [[-0.2709958 -0.1246528]]. Action = [[-0.18168107  0.03778809 -0.09303865 -0.21819735]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 2535 is [True, False, False, False, True, False]
Current timestep = 2536. State = [[-0.27209315 -0.1245126 ]]. Action = [[ 0.00913736 -0.09823701 -0.21076033  0.6733763 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 2536 is [True, False, False, False, True, False]
Scene graph at timestep 2536 is [True, False, False, False, True, False]
State prediction error at timestep 2536 is tensor(4.1752e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2537. State = [[-0.27283752 -0.12531804]]. Action = [[ 0.05820972  0.14364839 -0.05903944 -0.7439696 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 2537 is [True, False, False, False, True, False]
Scene graph at timestep 2537 is [True, False, False, True, False, False]
State prediction error at timestep 2537 is tensor(2.7218e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2538. State = [[-0.27288902 -0.12508546]]. Action = [[ 0.03792098  0.18422103 -0.1358634  -0.1782065 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 2538 is [True, False, False, True, False, False]
Current timestep = 2539. State = [[-0.27290124 -0.1238135 ]]. Action = [[-0.04077403 -0.21886578  0.20418331  0.71291006]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 2539 is [True, False, False, True, False, False]
Current timestep = 2540. State = [[-0.27290702 -0.12413242]]. Action = [[ 0.03922281 -0.05179498  0.00262672 -0.01889259]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 2540 is [True, False, False, False, True, False]
Scene graph at timestep 2540 is [True, False, False, False, True, False]
State prediction error at timestep 2540 is tensor(1.9210e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2541. State = [[-0.27290505 -0.12458511]]. Action = [[ 0.18695515  0.03337798 -0.00082868 -0.7281309 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 2541 is [True, False, False, False, True, False]
Current timestep = 2542. State = [[-0.27259856 -0.1244524 ]]. Action = [[-0.13352287 -0.04164486  0.02278447  0.50284517]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 2542 is [True, False, False, False, True, False]
Current timestep = 2543. State = [[-0.27231893 -0.12436841]]. Action = [[-0.07172307  0.20919049  0.14711076  0.09644568]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 2543 is [True, False, False, False, True, False]
Scene graph at timestep 2543 is [True, False, False, False, True, False]
State prediction error at timestep 2543 is tensor(3.8798e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2544. State = [[-0.27225283 -0.12359103]]. Action = [[-0.01818885 -0.18079121 -0.07651895 -0.56971276]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 2544 is [True, False, False, False, True, False]
Current timestep = 2545. State = [[-0.27224207 -0.12377044]]. Action = [[ 0.16314352  0.07476184 -0.06037664  0.92278945]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 2545 is [True, False, False, False, True, False]
Scene graph at timestep 2545 is [True, False, False, False, True, False]
State prediction error at timestep 2545 is tensor(2.3313e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2546. State = [[-0.27198255 -0.12347886]]. Action = [[ 0.04744837  0.20165455 -0.14803064  0.5819552 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 2546 is [True, False, False, False, True, False]
Current timestep = 2547. State = [[-0.27154574 -0.12171474]]. Action = [[ 0.07986972 -0.03671503 -0.23387337 -0.37990427]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 2547 is [True, False, False, False, True, False]
Scene graph at timestep 2547 is [True, False, False, False, True, False]
State prediction error at timestep 2547 is tensor(3.4135e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2548. State = [[-0.27094823 -0.12103914]]. Action = [[ 0.24056655 -0.23136158  0.2374739   0.33373606]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 2548 is [True, False, False, False, True, False]
Current timestep = 2549. State = [[-0.2681196  -0.12205365]]. Action = [[ 0.10831749 -0.04578342  0.19586837  0.29794478]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 2549 is [True, False, False, False, True, False]
Scene graph at timestep 2549 is [True, False, False, False, True, False]
State prediction error at timestep 2549 is tensor(2.4382e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2549 of -1
Current timestep = 2550. State = [[-0.2643683  -0.12284722]]. Action = [[ 0.14719135 -0.24325778  0.09952173  0.905406  ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 2550 is [True, False, False, False, True, False]
Current timestep = 2551. State = [[-0.26078093 -0.12581976]]. Action = [[-0.10634844  0.10211423  0.22830826 -0.68143106]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 2551 is [True, False, False, False, True, False]
Current timestep = 2552. State = [[-0.2590417  -0.12678522]]. Action = [[ 0.20711896 -0.2079139   0.15143684 -0.15356535]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 2552 is [True, False, False, True, False, False]
Current timestep = 2553. State = [[-0.25593904 -0.12961473]]. Action = [[-0.07791913 -0.17277622  0.19890445  0.33191764]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 2553 is [True, False, False, True, False, False]
Current timestep = 2554. State = [[-0.25422785 -0.13381632]]. Action = [[-0.07147563  0.22339952 -0.19642477  0.13883781]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 2554 is [True, False, False, True, False, False]
Current timestep = 2555. State = [[-0.2541871  -0.13406561]]. Action = [[ 0.12069577  0.22287124 -0.21634305 -0.24096215]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 2555 is [True, False, False, True, False, False]
Current timestep = 2556. State = [[-0.25393283 -0.1327396 ]]. Action = [[-0.23271614  0.07392853  0.01960588 -0.803464  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 2556 is [True, False, False, True, False, False]
Scene graph at timestep 2556 is [True, False, False, True, False, False]
State prediction error at timestep 2556 is tensor(6.3363e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2557. State = [[-0.25402197 -0.13145489]]. Action = [[-0.18032157  0.11519659 -0.07608984 -0.7381433 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 2557 is [True, False, False, True, False, False]
Current timestep = 2558. State = [[-0.25405633 -0.1298814 ]]. Action = [[-0.11937685  0.00698751 -0.06845826 -0.11915869]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 2558 is [True, False, False, True, False, False]
Human Feedback received at timestep 2558 of -1
Current timestep = 2559. State = [[-0.25479785 -0.12880227]]. Action = [[0.19613403 0.1336261  0.07858124 0.9889667 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 2559 is [True, False, False, True, False, False]
Current timestep = 2560. State = [[-0.25488344 -0.1266972 ]]. Action = [[ 0.13702914 -0.17825212 -0.23983777 -0.6316044 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 2560 is [True, False, False, True, False, False]
Scene graph at timestep 2560 is [True, False, False, True, False, False]
State prediction error at timestep 2560 is tensor(1.5482e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2561. State = [[-0.2548361  -0.12657142]]. Action = [[ 0.1562702   0.06187615 -0.05400848  0.64214504]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 2561 is [True, False, False, True, False, False]
Current timestep = 2562. State = [[-0.25449568 -0.12617815]]. Action = [[ 0.1322419  -0.22017099  0.00506225 -0.393748  ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 2562 is [True, False, False, True, False, False]
Current timestep = 2563. State = [[-0.25282338 -0.12728879]]. Action = [[ 0.14472151 -0.14380316 -0.1388341  -0.06288725]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 2563 is [True, False, False, True, False, False]
Scene graph at timestep 2563 is [True, False, False, True, False, False]
State prediction error at timestep 2563 is tensor(1.8974e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2564. State = [[-0.24993387 -0.12917429]]. Action = [[ 0.12817633  0.13863087 -0.20558666  0.70352376]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 2564 is [True, False, False, True, False, False]
Current timestep = 2565. State = [[-0.24695256 -0.13009582]]. Action = [[ 0.0560711  -0.21571842  0.14397973 -0.71023846]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 2565 is [True, False, False, True, False, False]
Scene graph at timestep 2565 is [True, False, False, True, False, False]
State prediction error at timestep 2565 is tensor(1.4355e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2566. State = [[-0.24280855 -0.13169266]]. Action = [[-0.14257629  0.20632541 -0.14733274 -0.16339433]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 2566 is [True, False, False, True, False, False]
Scene graph at timestep 2566 is [True, False, False, True, False, False]
State prediction error at timestep 2566 is tensor(8.7032e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2567. State = [[-0.24137452 -0.13185938]]. Action = [[-0.22856149  0.01320019  0.19478428  0.9245827 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 2567 is [True, False, False, True, False, False]
Current timestep = 2568. State = [[-0.24153449 -0.13184065]]. Action = [[-0.12401137 -0.15029985  0.10284156  0.6783252 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 2568 is [True, False, False, True, False, False]
Scene graph at timestep 2568 is [True, False, False, True, False, False]
State prediction error at timestep 2568 is tensor(9.1031e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2569. State = [[-0.24175176 -0.13218151]]. Action = [[-0.0230711   0.18579862  0.18129003 -0.266101  ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 2569 is [True, False, False, True, False, False]
Current timestep = 2570. State = [[-0.24218772 -0.13164914]]. Action = [[ 0.11712366  0.07443818  0.23318565 -0.58113605]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 2570 is [True, False, False, True, False, False]
Current timestep = 2571. State = [[-0.24228364 -0.1309437 ]]. Action = [[ 0.04242381 -0.22610319  0.14914942  0.3478552 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 2571 is [True, False, False, True, False, False]
Current timestep = 2572. State = [[-0.2422557  -0.13135612]]. Action = [[0.12906158 0.10940188 0.20462918 0.02963018]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 2572 is [True, False, False, True, False, False]
Current timestep = 2573. State = [[-0.24218586 -0.13151725]]. Action = [[ 0.0202063  -0.22210799  0.14645854 -0.01623845]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 2573 is [True, False, False, True, False, False]
Scene graph at timestep 2573 is [True, False, False, True, False, False]
State prediction error at timestep 2573 is tensor(6.3529e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2574. State = [[-0.24203636 -0.13278697]]. Action = [[ 0.03303233  0.01479843  0.10656002 -0.17423826]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 2574 is [True, False, False, True, False, False]
Scene graph at timestep 2574 is [True, False, False, True, False, False]
State prediction error at timestep 2574 is tensor(1.3275e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2575. State = [[-0.2415082  -0.13300957]]. Action = [[ 0.23989883 -0.1185966   0.16334552  0.47966242]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 2575 is [True, False, False, True, False, False]
Current timestep = 2576. State = [[-0.23943698 -0.13485597]]. Action = [[-0.23416263 -0.16975151  0.1587739   0.38059044]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 2576 is [True, False, False, True, False, False]
Current timestep = 2577. State = [[-0.23913513 -0.1380457 ]]. Action = [[ 0.1759271   0.15832376 -0.09932634  0.6318487 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 2577 is [True, False, False, True, False, False]
Scene graph at timestep 2577 is [True, False, False, True, False, False]
State prediction error at timestep 2577 is tensor(9.5880e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2578. State = [[-0.23859768 -0.13834617]]. Action = [[ 0.10880923 -0.04919481 -0.03219908 -0.5324297 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 2578 is [True, False, False, True, False, False]
Current timestep = 2579. State = [[-0.23703253 -0.13895795]]. Action = [[-0.23554051 -0.23721516  0.2341451   0.7175958 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 2579 is [True, False, False, True, False, False]
Current timestep = 2580. State = [[-0.23669289 -0.14241648]]. Action = [[ 0.22337317 -0.14306246  0.11539376 -0.0582068 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 2580 is [True, False, False, True, False, False]
Scene graph at timestep 2580 is [True, False, False, True, False, False]
State prediction error at timestep 2580 is tensor(6.1996e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2581. State = [[-0.23584038 -0.14664622]]. Action = [[-0.21592599  0.14484575 -0.00200997  0.2660998 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 2581 is [True, False, False, True, False, False]
Current timestep = 2582. State = [[-0.23574352 -0.1477291 ]]. Action = [[-0.03514726  0.05064872  0.09373495  0.30721426]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 2582 is [True, False, False, True, False, False]
Current timestep = 2583. State = [[-0.23572695 -0.14767064]]. Action = [[ 0.04580143  0.14069891 -0.23222636  0.10945153]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 2583 is [True, False, False, True, False, False]
Scene graph at timestep 2583 is [True, False, False, True, False, False]
State prediction error at timestep 2583 is tensor(3.8729e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2584. State = [[-0.23579061 -0.14755456]]. Action = [[-0.19154643 -0.18807863  0.22013515  0.23216116]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 2584 is [True, False, False, True, False, False]
Current timestep = 2585. State = [[-0.2360199  -0.14830369]]. Action = [[ 0.20432371 -0.21924515 -0.18014671 -0.8018937 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 2585 is [True, False, False, True, False, False]
Scene graph at timestep 2585 is [True, False, False, True, False, False]
State prediction error at timestep 2585 is tensor(3.1157e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2586. State = [[-0.23592661 -0.1511046 ]]. Action = [[ 0.00468692  0.18060806 -0.01184885 -0.06615847]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 2586 is [True, False, False, True, False, False]
Current timestep = 2587. State = [[-0.23587756 -0.15116045]]. Action = [[ 0.05194479  0.22814643 -0.20275116 -0.8179666 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 2587 is [True, False, False, True, False, False]
Scene graph at timestep 2587 is [True, False, False, True, False, False]
State prediction error at timestep 2587 is tensor(1.2096e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2588. State = [[-0.23567452 -0.14990316]]. Action = [[-0.02782011  0.19248593  0.04951873 -0.9622895 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 2588 is [True, False, False, True, False, False]
Current timestep = 2589. State = [[-0.23540595 -0.14726879]]. Action = [[-0.07790808  0.13937956  0.07294822 -0.8449022 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 2589 is [True, False, False, True, False, False]
Current timestep = 2590. State = [[-0.23529416 -0.144305  ]]. Action = [[-0.03563935 -0.0690625  -0.2271822   0.74621105]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 2590 is [True, False, False, True, False, False]
Current timestep = 2591. State = [[-0.23524025 -0.14359452]]. Action = [[ 0.06499439  0.0699642  -0.08260992  0.5076393 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 2591 is [True, False, False, True, False, False]
Current timestep = 2592. State = [[-0.23511833 -0.14205511]]. Action = [[-0.21443795  0.12263638  0.11283323  0.47164345]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 2592 is [True, False, False, True, False, False]
Scene graph at timestep 2592 is [True, False, False, True, False, False]
State prediction error at timestep 2592 is tensor(2.8271e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2593. State = [[-0.23622915 -0.13931185]]. Action = [[-0.15802327  0.18644029 -0.09427699 -0.60409206]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 2593 is [True, False, False, True, False, False]
Current timestep = 2594. State = [[-0.23775041 -0.13492942]]. Action = [[ 0.13406122 -0.24152802 -0.05612694  0.1294291 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 2594 is [True, False, False, True, False, False]
Current timestep = 2595. State = [[-0.23799318 -0.13479082]]. Action = [[-0.1569552 -0.0645842 -0.2152476  0.4516585]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 2595 is [True, False, False, True, False, False]
Current timestep = 2596. State = [[-0.23887482 -0.13465786]]. Action = [[-0.19293341 -0.01317796  0.20624772 -0.36161464]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 2596 is [True, False, False, True, False, False]
Scene graph at timestep 2596 is [True, False, False, True, False, False]
State prediction error at timestep 2596 is tensor(5.2852e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2596 of 1
Current timestep = 2597. State = [[-0.24075088 -0.13465953]]. Action = [[ 0.0513826   0.20875841 -0.11243799 -0.7200898 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 2597 is [True, False, False, True, False, False]
Scene graph at timestep 2597 is [True, False, False, True, False, False]
State prediction error at timestep 2597 is tensor(4.6850e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2598. State = [[-0.24189122 -0.13322783]]. Action = [[-0.10603863  0.15929967 -0.10812984 -0.88543934]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 2598 is [True, False, False, True, False, False]
Current timestep = 2599. State = [[-0.2428502  -0.13035102]]. Action = [[ 0.23261261  0.06097582 -0.12124974 -0.4719664 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 2599 is [True, False, False, True, False, False]
Current timestep = 2600. State = [[-0.24291459 -0.12726621]]. Action = [[0.23735562 0.19566125 0.01595661 0.55184555]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 2600 is [True, False, False, True, False, False]
Current timestep = 2601. State = [[-0.24266843 -0.12334564]]. Action = [[ 0.01889527 -0.23779349  0.06204852 -0.81946   ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 2601 is [True, False, False, True, False, False]
Current timestep = 2602. State = [[-0.2423354  -0.12299866]]. Action = [[ 0.22799015  0.20913267  0.14980292 -0.52782214]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 2602 is [True, False, False, False, True, False]
Current timestep = 2603. State = [[-0.24022041 -0.12086851]]. Action = [[-0.21470354 -0.05363712 -0.20745724  0.03813577]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 2603 is [True, False, False, False, True, False]
Scene graph at timestep 2603 is [True, False, False, False, True, False]
State prediction error at timestep 2603 is tensor(3.1475e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2604. State = [[-0.24026358 -0.1203595 ]]. Action = [[-0.24534507 -0.05647185 -0.00974305  0.84049606]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 2604 is [True, False, False, False, True, False]
Current timestep = 2605. State = [[-0.2404901  -0.12017305]]. Action = [[-0.0447464   0.18374223  0.14623323  0.3766284 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 2605 is [True, False, False, False, True, False]
Scene graph at timestep 2605 is [True, False, False, False, True, False]
State prediction error at timestep 2605 is tensor(1.4940e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2605 of 1
Current timestep = 2606. State = [[-0.24105133 -0.11766312]]. Action = [[-0.1548184   0.20972723 -0.1542778  -0.66323924]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 2606 is [True, False, False, False, True, False]
Current timestep = 2607. State = [[-0.24252185 -0.11313203]]. Action = [[0.17229575 0.22169521 0.19456899 0.79751945]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 2607 is [True, False, False, False, True, False]
Current timestep = 2608. State = [[-0.24292713 -0.10760616]]. Action = [[ 0.10960966  0.17318207  0.23252559 -0.14091247]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 2608 is [True, False, False, False, True, False]
Current timestep = 2609. State = [[-0.2434537  -0.10149399]]. Action = [[ 0.1247831  -0.15276259  0.0984574  -0.7712501 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 2609 is [True, False, False, False, True, False]
Current timestep = 2610. State = [[-0.24334638 -0.0997228 ]]. Action = [[-0.19216491  0.07840142 -0.09479687 -0.7822099 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 2610 is [True, False, False, False, True, False]
Current timestep = 2611. State = [[-0.24355942 -0.097832  ]]. Action = [[ 0.0465599  -0.09057246  0.14488721 -0.5954622 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 2611 is [True, False, False, False, True, False]
Current timestep = 2612. State = [[-0.24352136 -0.0976062 ]]. Action = [[ 0.04560727 -0.06200889 -0.02280352 -0.20584208]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 2612 is [True, False, False, False, True, False]
Current timestep = 2613. State = [[-0.2434576 -0.0976839]]. Action = [[ 0.00095975 -0.00080946 -0.15043835  0.16334689]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 2613 is [True, False, False, False, True, False]
Current timestep = 2614. State = [[-0.2434683  -0.09758871]]. Action = [[-0.04566033  0.21158952  0.09660113  0.1971308 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 2614 is [True, False, False, False, True, False]
Current timestep = 2615. State = [[-0.24348979 -0.09609418]]. Action = [[ 0.1399597  -0.21318007 -0.0942204  -0.14098966]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 2615 is [True, False, False, False, True, False]
Current timestep = 2616. State = [[-0.24335971 -0.09623353]]. Action = [[ 0.23736122  0.02779353 -0.18264717 -0.6435161 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 2616 is [True, False, False, False, True, False]
Current timestep = 2617. State = [[-0.24130341 -0.09648174]]. Action = [[ 0.16206962 -0.05252601 -0.16800418 -0.9069129 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 2617 is [True, False, False, False, True, False]
Current timestep = 2618. State = [[-0.23885512 -0.09674188]]. Action = [[-0.19447644  0.19991332 -0.01893617  0.22801685]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 2618 is [True, False, False, False, True, False]
Scene graph at timestep 2618 is [True, False, False, False, True, False]
State prediction error at timestep 2618 is tensor(6.3702e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2619. State = [[-0.238427   -0.09540261]]. Action = [[ 0.16015649  0.22288305 -0.20885746 -0.7127958 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 2619 is [True, False, False, False, True, False]
Current timestep = 2620. State = [[-0.23730607 -0.09188583]]. Action = [[ 0.13369113 -0.07811698 -0.08218232 -0.18369359]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 2620 is [True, False, False, False, True, False]
Current timestep = 2621. State = [[-0.23527645 -0.08989245]]. Action = [[ 0.21706036  0.08980766  0.11652324 -0.82394475]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 2621 is [True, False, False, False, True, False]
Current timestep = 2622. State = [[-0.23206177 -0.08785257]]. Action = [[-0.08384706 -0.218743   -0.15430029  0.0160737 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 2622 is [True, False, False, False, True, False]
Scene graph at timestep 2622 is [True, False, False, False, True, False]
State prediction error at timestep 2622 is tensor(6.1439e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2623. State = [[-0.23038568 -0.08829723]]. Action = [[0.14585584 0.08440492 0.15314823 0.706697  ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 2623 is [True, False, False, False, True, False]
Human Feedback received at timestep 2623 of 1
Current timestep = 2624. State = [[-0.22760683 -0.08837242]]. Action = [[ 0.03038037  0.06714401  0.1690653  -0.9895074 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 2624 is [True, False, False, False, True, False]
Current timestep = 2625. State = [[-0.2244326  -0.08783432]]. Action = [[ 0.14744616  0.17448425  0.03571883 -0.06252897]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 2625 is [True, False, False, False, True, False]
Current timestep = 2626. State = [[-0.22077163 -0.08592512]]. Action = [[-0.19062005 -0.08374608 -0.11826418 -0.8981879 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 2626 is [True, False, False, False, True, False]
Current timestep = 2627. State = [[-0.21921584 -0.08552632]]. Action = [[-0.20919406 -0.1382133  -0.11097917  0.85683346]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 2627 is [True, False, False, False, True, False]
Current timestep = 2628. State = [[-0.21925847 -0.08565918]]. Action = [[ 0.24103051  0.22268319  0.12314489 -0.86003155]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 2628 is [True, False, False, False, True, False]
Current timestep = 2629. State = [[-0.21898514 -0.08452525]]. Action = [[0.17163497 0.009866   0.11436564 0.3298037 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 2629 is [True, False, False, False, True, False]
Scene graph at timestep 2629 is [True, False, False, False, True, False]
State prediction error at timestep 2629 is tensor(2.0635e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2629 of 1
Current timestep = 2630. State = [[-0.21686698 -0.08382223]]. Action = [[ 0.22806671 -0.17141868  0.23645097 -0.7754268 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 2630 is [True, False, False, False, True, False]
Current timestep = 2631. State = [[-0.21331657 -0.08422936]]. Action = [[-0.00785734  0.18903461 -0.02403878 -0.18110228]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 2631 is [True, False, False, False, True, False]
Current timestep = 2632. State = [[-0.2101947  -0.08317005]]. Action = [[ 0.20817238 -0.1553507  -0.13142733 -0.56738305]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 2632 is [True, False, False, False, True, False]
Current timestep = 2633. State = [[-0.20614715 -0.08356892]]. Action = [[ 0.07793403 -0.07635605 -0.15591882  0.8325684 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 2633 is [True, False, False, False, True, False]
Current timestep = 2634. State = [[-0.20249088 -0.08394578]]. Action = [[-0.12038994  0.00415716  0.16235092 -0.74711955]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 2634 is [True, False, False, False, True, False]
Current timestep = 2635. State = [[-0.20087461 -0.08413125]]. Action = [[0.20076159 0.12812772 0.08592984 0.8644519 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 2635 is [True, False, False, False, True, False]
Human Feedback received at timestep 2635 of 1
Current timestep = 2636. State = [[-0.19882716 -0.08410904]]. Action = [[-0.04034312 -0.2308233   0.12724143 -0.9599707 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 2636 is [True, False, False, False, True, False]
Current timestep = 2637. State = [[-0.19726168 -0.0849741 ]]. Action = [[ 0.17572463 -0.01386312  0.0637491  -0.6911874 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 2637 is [True, False, False, False, True, False]
Scene graph at timestep 2637 is [True, False, False, False, True, False]
State prediction error at timestep 2637 is tensor(1.4934e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2638. State = [[-0.1941639  -0.08595084]]. Action = [[ 0.20650005 -0.16404936  0.14941323 -0.41478622]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 2638 is [True, False, False, False, True, False]
Scene graph at timestep 2638 is [True, False, False, False, True, False]
State prediction error at timestep 2638 is tensor(1.4078e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2639. State = [[-0.18930598 -0.08761193]]. Action = [[-0.10731004  0.03729981 -0.24683389  0.8599087 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 2639 is [True, False, False, False, True, False]
Current timestep = 2640. State = [[-0.18593892 -0.08861469]]. Action = [[ 0.13328066 -0.11694111  0.06201267  0.70824873]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 2640 is [True, False, False, False, True, False]
Scene graph at timestep 2640 is [True, False, False, False, True, False]
State prediction error at timestep 2640 is tensor(3.1944e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2641. State = [[-0.1825814  -0.09092486]]. Action = [[ 0.08163917 -0.22791038  0.24352902  0.38032722]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 2641 is [True, False, False, False, True, False]
Scene graph at timestep 2641 is [True, False, False, False, True, False]
State prediction error at timestep 2641 is tensor(2.5033e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2642. State = [[-0.17972457 -0.09497613]]. Action = [[ 0.10180444 -0.22867833  0.13068497  0.9074838 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 2642 is [True, False, False, False, True, False]
Current timestep = 2643. State = [[-0.17697504 -0.10007745]]. Action = [[ 0.15949467 -0.01144661 -0.13878676  0.21146333]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 2643 is [True, False, False, False, True, False]
Human Feedback received at timestep 2643 of 1
Current timestep = 2644. State = [[-0.1737004  -0.10467584]]. Action = [[ 0.2188102  -0.23297313  0.10751224 -0.97814065]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 2644 is [True, False, False, False, True, False]
Current timestep = 2645. State = [[-0.16932139 -0.11044162]]. Action = [[-0.24089958 -0.13629465 -0.23672397  0.81271124]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 2645 is [True, False, False, False, True, False]
Current timestep = 2646. State = [[-0.16795513 -0.11533417]]. Action = [[-0.07552075  0.0910669   0.05911124  0.78417516]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 2646 is [True, False, False, False, True, False]
Current timestep = 2647. State = [[-0.1679702  -0.11728254]]. Action = [[ 0.16465133 -0.00586507  0.04229555  0.8509481 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 2647 is [True, False, False, False, True, False]
Current timestep = 2648. State = [[-0.16664204 -0.1184414 ]]. Action = [[ 0.20738173 -0.02289    -0.21040615 -0.34061015]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 2648 is [True, False, False, False, True, False]
Scene graph at timestep 2648 is [True, False, False, False, True, False]
State prediction error at timestep 2648 is tensor(1.1892e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2649. State = [[-0.16315015 -0.11921714]]. Action = [[-0.12643056  0.19379449  0.2386018  -0.973475  ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 2649 is [True, False, False, False, True, False]
Current timestep = 2650. State = [[-0.1612921  -0.11923245]]. Action = [[ 0.01143676 -0.08033234 -0.16088042  0.00118113]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 2650 is [True, False, False, False, True, False]
Current timestep = 2651. State = [[-0.16060416 -0.11924758]]. Action = [[ 0.12857676 -0.00513791 -0.10435224 -0.39971733]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 2651 is [True, False, False, False, True, False]
Current timestep = 2652. State = [[-0.15949097 -0.11938262]]. Action = [[ 0.1146248   0.07406166 -0.0606882   0.8606708 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 2652 is [True, False, False, False, True, False]
Human Feedback received at timestep 2652 of 1
Current timestep = 2653. State = [[-0.1576021  -0.11904509]]. Action = [[ 0.10922676 -0.2068877   0.24412239 -0.61465675]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 2653 is [True, False, False, False, True, False]
Current timestep = 2654. State = [[-0.1552571  -0.12009943]]. Action = [[-0.00678992  0.19238508  0.00948596 -0.9420257 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 2654 is [True, False, False, False, True, False]
Scene graph at timestep 2654 is [True, False, False, False, True, False]
State prediction error at timestep 2654 is tensor(6.4045e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2655. State = [[-0.1518979  -0.11994968]]. Action = [[ 0.09034002  0.09327662 -0.00293609  0.8446417 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 2655 is [True, False, False, False, True, False]
Current timestep = 2656. State = [[-0.1490037  -0.11901183]]. Action = [[ 1.3754299e-01 -2.3916364e-05 -2.2879659e-01 -8.2475334e-01]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 2656 is [True, False, False, False, True, False]
Current timestep = 2657. State = [[-0.14546436 -0.1183674 ]]. Action = [[ 0.02392906 -0.00723626  0.03590596 -0.51186043]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 2657 is [True, False, False, False, True, False]
Current timestep = 2658. State = [[-0.14255455 -0.1182966 ]]. Action = [[ 0.14949465  0.03279576 -0.07266766  0.52686334]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 2658 is [True, False, False, False, True, False]
Current timestep = 2659. State = [[-0.13899171 -0.11813718]]. Action = [[-0.18100637  0.06225455 -0.11664091 -0.94181746]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 2659 is [True, False, False, False, True, False]
Human Feedback received at timestep 2659 of 1
Current timestep = 2660. State = [[-0.13814457 -0.11758289]]. Action = [[-0.07359484 -0.05672899  0.1496095  -0.7149256 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 2660 is [True, False, False, False, True, False]
Current timestep = 2661. State = [[-0.13834533 -0.1175478 ]]. Action = [[ 0.01278287 -0.08974382  0.03962627 -0.642342  ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 2661 is [True, False, False, False, True, False]
Scene graph at timestep 2661 is [True, False, False, False, True, False]
State prediction error at timestep 2661 is tensor(4.4805e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2662. State = [[-0.13847776 -0.1176458 ]]. Action = [[0.01789781 0.21713811 0.0928967  0.21482694]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 2662 is [True, False, False, False, True, False]
Current timestep = 2663. State = [[-0.13853309 -0.1162753 ]]. Action = [[ 0.09279704 -0.11437821  0.21425015  0.51123774]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 2663 is [True, False, False, False, True, False]
Current timestep = 2664. State = [[-0.13846852 -0.11634373]]. Action = [[-0.13356031 -0.09166737  0.17660463  0.34114897]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 2664 is [True, False, False, False, True, False]
Current timestep = 2665. State = [[-0.13851526 -0.11661187]]. Action = [[ 0.01823905 -0.13505517 -0.08091828 -0.3000443 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 2665 is [True, False, False, False, True, False]
Scene graph at timestep 2665 is [True, False, False, False, True, False]
State prediction error at timestep 2665 is tensor(5.8766e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2666. State = [[-0.13856347 -0.11805037]]. Action = [[-0.01661438 -0.07416508 -0.2375372  -0.36534733]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 2666 is [True, False, False, False, True, False]
Scene graph at timestep 2666 is [True, False, False, False, True, False]
State prediction error at timestep 2666 is tensor(6.8304e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2667. State = [[-0.13872515 -0.11968921]]. Action = [[-0.12216851 -0.03899541 -0.02925757 -0.36526215]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 2667 is [True, False, False, False, True, False]
Current timestep = 2668. State = [[-0.13910455 -0.12125558]]. Action = [[-0.19946246  0.24910659  0.17186117 -0.10955232]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 2668 is [True, False, False, False, True, False]
Current timestep = 2669. State = [[-0.1393775  -0.12027822]]. Action = [[ 0.15036884 -0.1739716  -0.00879988  0.04665041]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 2669 is [True, False, False, False, True, False]
Current timestep = 2670. State = [[-0.13946821 -0.12077718]]. Action = [[ 0.05022961  0.09241009 -0.190217   -0.8476602 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 2670 is [True, False, False, False, True, False]
Current timestep = 2671. State = [[-0.13952687 -0.12067635]]. Action = [[0.23629725 0.2116656  0.21674284 0.44626665]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 2671 is [True, False, False, False, True, False]
Current timestep = 2672. State = [[-0.1391787  -0.11844092]]. Action = [[-0.15380843 -0.06548673  0.19404012  0.16654503]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 2672 is [True, False, False, False, True, False]
Current timestep = 2673. State = [[-0.13917603 -0.11808399]]. Action = [[ 0.13378185 -0.09915893 -0.1603278   0.4512105 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 2673 is [True, False, False, False, True, False]
Scene graph at timestep 2673 is [True, False, False, False, True, False]
State prediction error at timestep 2673 is tensor(1.4415e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2674. State = [[-0.1390951  -0.11844458]]. Action = [[ 0.12274024  0.11543417  0.15505353 -0.15294123]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 2674 is [True, False, False, False, True, False]
Current timestep = 2675. State = [[-0.13823608 -0.11761069]]. Action = [[-0.11812478 -0.08229522  0.19049147  0.4883622 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 2675 is [True, False, False, False, True, False]
Current timestep = 2676. State = [[-0.13825247 -0.11779519]]. Action = [[ 0.0792428  -0.11506124 -0.02181721  0.2858709 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 2676 is [True, False, False, False, True, False]
Scene graph at timestep 2676 is [True, False, False, False, True, False]
State prediction error at timestep 2676 is tensor(1.5709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2677. State = [[-0.13814971 -0.1185781 ]]. Action = [[-0.20240134 -0.20750923 -0.18990247  0.19167972]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 2677 is [True, False, False, False, True, False]
Current timestep = 2678. State = [[-0.13828489 -0.12190057]]. Action = [[-0.1395198  -0.07477188  0.235354   -0.3173666 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 2678 is [True, False, False, False, True, False]
Current timestep = 2679. State = [[-0.13872556 -0.12489741]]. Action = [[ 0.1990284  -0.23875985 -0.21672021 -0.78211373]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 2679 is [True, False, False, False, True, False]
Current timestep = 2680. State = [[-0.13867897 -0.12923956]]. Action = [[ 0.03021196 -0.12541714  0.19202536 -0.5591811 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 2680 is [True, False, False, False, True, False]
Current timestep = 2681. State = [[-0.13860284 -0.13351075]]. Action = [[-0.2367391   0.20614874  0.15927267 -0.69842476]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 2681 is [True, False, False, True, False, False]
Scene graph at timestep 2681 is [True, False, False, True, False, False]
State prediction error at timestep 2681 is tensor(7.4529e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2682. State = [[-0.13868308 -0.13410352]]. Action = [[ 0.04525694  0.00792673 -0.12892494  0.44183862]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 2682 is [True, False, False, True, False, False]
Scene graph at timestep 2682 is [True, False, False, True, False, False]
State prediction error at timestep 2682 is tensor(9.0466e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2683. State = [[-0.13867988 -0.1342202 ]]. Action = [[ 0.10358793 -0.08493614  0.16517833 -0.91770285]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 2683 is [True, False, False, True, False, False]
Current timestep = 2684. State = [[-0.1386053 -0.1349482]]. Action = [[ 0.17953265 -0.195119    0.0079703   0.4482888 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 2684 is [True, False, False, True, False, False]
Current timestep = 2685. State = [[-0.13829131 -0.13730183]]. Action = [[-0.21358284 -0.14748712  0.19451615  0.22360432]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 2685 is [True, False, False, True, False, False]
Current timestep = 2686. State = [[-0.13843392 -0.14224389]]. Action = [[-2.0259510e-01  4.1026473e-02 -6.0145557e-04  9.1050935e-01]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 2686 is [True, False, False, True, False, False]
Current timestep = 2687. State = [[-0.13866393 -0.1445983 ]]. Action = [[ 0.22646761 -0.17281774 -0.17485228 -0.11093706]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 2687 is [True, False, False, True, False, False]
Scene graph at timestep 2687 is [True, False, False, True, False, False]
State prediction error at timestep 2687 is tensor(3.2309e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2688. State = [[-0.1387839  -0.14779437]]. Action = [[-0.07797849  0.12090364  0.07768774 -0.9911779 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 2688 is [True, False, False, True, False, False]
Current timestep = 2689. State = [[-0.13885209 -0.14843602]]. Action = [[-0.05063407 -0.13133857 -0.04461557  0.75984037]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 2689 is [True, False, False, True, False, False]
Human Feedback received at timestep 2689 of -1
Current timestep = 2690. State = [[-0.13917053 -0.1504529 ]]. Action = [[-0.14721632  0.10230768 -0.01005852  0.03678775]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 2690 is [True, False, False, True, False, False]
Scene graph at timestep 2690 is [True, False, False, True, False, False]
State prediction error at timestep 2690 is tensor(1.1041e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2691. State = [[-0.13958183 -0.15083748]]. Action = [[-0.19481954  0.24359104  0.0519149  -0.74580526]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 2691 is [True, False, False, True, False, False]
Current timestep = 2692. State = [[-0.14060098 -0.1496254 ]]. Action = [[ 0.09677386 -0.12144628 -0.09200455  0.34310913]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 2692 is [True, False, False, True, False, False]
Current timestep = 2693. State = [[-0.14106265 -0.14990327]]. Action = [[-0.20879155 -0.23617813  0.02998504 -0.42436272]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 2693 is [True, False, False, True, False, False]
Scene graph at timestep 2693 is [True, False, False, True, False, False]
State prediction error at timestep 2693 is tensor(3.0671e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2693 of -1
Current timestep = 2694. State = [[-0.14340615 -0.15331061]]. Action = [[-0.1707475   0.00037929  0.1232464  -0.28943348]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 2694 is [True, False, False, True, False, False]
Current timestep = 2695. State = [[-0.14612043 -0.15616105]]. Action = [[-0.00082007 -0.05976388  0.10184991  0.48602962]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 2695 is [True, False, False, True, False, False]
Current timestep = 2696. State = [[-0.1477976 -0.15838  ]]. Action = [[ 0.07591209  0.12556767  0.12640238 -0.1935451 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 2696 is [True, False, False, True, False, False]
Current timestep = 2697. State = [[-0.14806478 -0.15795438]]. Action = [[ 0.14405644  0.20103368 -0.04582734  0.14973092]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 2697 is [True, False, False, True, False, False]
Current timestep = 2698. State = [[-0.14798294 -0.15609688]]. Action = [[-0.16257404  0.05015463  0.2411558  -0.2722931 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 2698 is [True, False, False, True, False, False]
Current timestep = 2699. State = [[-0.14823113 -0.15456387]]. Action = [[-0.01830369 -0.21140255 -0.15694138 -0.17193407]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 2699 is [True, False, False, True, False, False]
Current timestep = 2700. State = [[-0.14869483 -0.15541796]]. Action = [[ 0.18043703 -0.02322067 -0.11277562  0.76642036]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 2700 is [True, False, False, True, False, False]
Scene graph at timestep 2700 is [True, False, False, True, False, False]
State prediction error at timestep 2700 is tensor(3.1634e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2701. State = [[-0.14864905 -0.1556671 ]]. Action = [[ 0.24279222 -0.01490097  0.11001369  0.01589286]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 2701 is [True, False, False, True, False, False]
Current timestep = 2702. State = [[-0.148545  -0.1555727]]. Action = [[ 0.06808811  0.16623992 -0.110816   -0.68843454]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 2702 is [True, False, False, True, False, False]
Current timestep = 2703. State = [[-0.14820366 -0.15391871]]. Action = [[ 0.19411871  0.21938646 -0.06233408 -0.12294298]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 2703 is [True, False, False, True, False, False]
Scene graph at timestep 2703 is [True, False, False, True, False, False]
State prediction error at timestep 2703 is tensor(2.6153e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2704. State = [[-0.14606722 -0.15013692]]. Action = [[-0.23320387  0.13224977  0.15146929  0.9374939 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 2704 is [True, False, False, True, False, False]
Current timestep = 2705. State = [[-0.14566766 -0.14674641]]. Action = [[-0.22892655  0.23901084 -0.01795566  0.8323134 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 2705 is [True, False, False, True, False, False]
Scene graph at timestep 2705 is [True, False, False, True, False, False]
State prediction error at timestep 2705 is tensor(6.7373e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2706. State = [[-0.14572982 -0.14169604]]. Action = [[-0.23482937 -0.1309471   0.22205353  0.3270111 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 2706 is [True, False, False, True, False, False]
Current timestep = 2707. State = [[-0.1471533  -0.14026397]]. Action = [[ 0.18424755 -0.21125297  0.20652285 -0.45986563]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 2707 is [True, False, False, True, False, False]
Scene graph at timestep 2707 is [True, False, False, True, False, False]
State prediction error at timestep 2707 is tensor(1.9627e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2708. State = [[-0.14724278 -0.14094931]]. Action = [[ 0.1930672   0.03128135 -0.21740387  0.82981277]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 2708 is [True, False, False, True, False, False]
Current timestep = 2709. State = [[-0.14723557 -0.14112002]]. Action = [[-0.00162832 -0.05863234 -0.23617898 -0.50972223]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 2709 is [True, False, False, True, False, False]
Current timestep = 2710. State = [[-0.14721291 -0.14095838]]. Action = [[ 0.06037122  0.23168778 -0.12934174  0.73556006]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 2710 is [True, False, False, True, False, False]
Scene graph at timestep 2710 is [True, False, False, True, False, False]
State prediction error at timestep 2710 is tensor(6.8078e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2711. State = [[-0.14707032 -0.13921617]]. Action = [[-0.09648994  0.19387901  0.05628303  0.4451617 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 2711 is [True, False, False, True, False, False]
Current timestep = 2712. State = [[-0.14703271 -0.13620125]]. Action = [[ 0.22852308  0.00856727 -0.08836874  0.50645757]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 2712 is [True, False, False, True, False, False]
Current timestep = 2713. State = [[-0.14662047 -0.13371249]]. Action = [[0.23976427 0.02394611 0.19557542 0.69989717]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 2713 is [True, False, False, True, False, False]
Current timestep = 2714. State = [[-0.14432648 -0.13186762]]. Action = [[-0.1730967  -0.16772039  0.04921025  0.5821762 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 2714 is [True, False, False, True, False, False]
Current timestep = 2715. State = [[-0.14395098 -0.13216515]]. Action = [[-0.10772142  0.11001658 -0.04364491  0.626714  ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 2715 is [True, False, False, True, False, False]
Current timestep = 2716. State = [[-0.1440415  -0.13188149]]. Action = [[-0.1399542   0.10556611 -0.19417986 -0.72441   ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 2716 is [True, False, False, True, False, False]
Current timestep = 2717. State = [[-0.14405303 -0.13018896]]. Action = [[-0.08380312  0.14336789  0.01233438 -0.0894807 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 2717 is [True, False, False, True, False, False]
Current timestep = 2718. State = [[-0.14412682 -0.12757184]]. Action = [[ 0.12240428 -0.20371409  0.12195265 -0.02481872]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 2718 is [True, False, False, True, False, False]
Current timestep = 2719. State = [[-0.14413153 -0.12770998]]. Action = [[ 0.00883162  0.1792835  -0.08596823  0.5264015 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 2719 is [True, False, False, True, False, False]
Human Feedback received at timestep 2719 of 1
Current timestep = 2720. State = [[-0.14408912 -0.12598385]]. Action = [[ 0.23424798  0.20765868 -0.01080501  0.384053  ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 2720 is [True, False, False, True, False, False]
Current timestep = 2721. State = [[-0.14368775 -0.12172507]]. Action = [[-0.20767239  0.16059518 -0.14620315  0.4119171 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 2721 is [True, False, False, True, False, False]
Scene graph at timestep 2721 is [True, False, False, False, True, False]
State prediction error at timestep 2721 is tensor(1.0169e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2722. State = [[-0.14375547 -0.11728428]]. Action = [[-0.21411973 -0.05720508 -0.07875393 -0.86028177]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 2722 is [True, False, False, False, True, False]
Current timestep = 2723. State = [[-0.14404666 -0.11475848]]. Action = [[ 0.02145857  0.21192801 -0.17843634  0.5002328 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 2723 is [True, False, False, False, True, False]
Current timestep = 2724. State = [[-0.1441932  -0.11054835]]. Action = [[-0.18963887  0.21464908 -0.24444605  0.6815387 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 2724 is [True, False, False, False, True, False]
Current timestep = 2725. State = [[-0.14598829 -0.10485882]]. Action = [[-0.08400723 -0.12758334  0.08459431  0.4132687 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 2725 is [True, False, False, False, True, False]
Scene graph at timestep 2725 is [True, False, False, False, True, False]
State prediction error at timestep 2725 is tensor(6.4745e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2726. State = [[-0.14740987 -0.10324208]]. Action = [[-0.03363174 -0.16432329  0.14155963 -0.05414128]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 2726 is [True, False, False, False, True, False]
Current timestep = 2727. State = [[-0.14801842 -0.1039393 ]]. Action = [[ 0.2446503   0.1452429   0.06702423 -0.42099714]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 2727 is [True, False, False, False, True, False]
Human Feedback received at timestep 2727 of 1
Current timestep = 2728. State = [[-0.14810441 -0.10301789]]. Action = [[-0.18312716  0.10064006 -0.1905216   0.4221592 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 2728 is [True, False, False, False, True, False]
Current timestep = 2729. State = [[-0.14831416 -0.10105286]]. Action = [[0.00792134 0.22123957 0.03943777 0.6124742 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 2729 is [True, False, False, False, True, False]
Scene graph at timestep 2729 is [True, False, False, False, True, False]
State prediction error at timestep 2729 is tensor(3.2944e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2730. State = [[-0.14884631 -0.09633321]]. Action = [[-0.20441003  0.20432657 -0.10046852  0.07024837]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 2730 is [True, False, False, False, True, False]
Current timestep = 2731. State = [[-0.1504827  -0.09050216]]. Action = [[-0.02013047  0.04590923  0.05888867  0.07227111]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 2731 is [True, False, False, False, True, False]
Current timestep = 2732. State = [[-0.1514053  -0.08633952]]. Action = [[ 0.07782495  0.14783424 -0.1299     -0.9626907 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 2732 is [True, False, False, False, True, False]
Current timestep = 2733. State = [[-0.15193523 -0.08181783]]. Action = [[-0.09289414  0.14079434  0.05858767 -0.39003313]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 2733 is [True, False, False, False, True, False]
Current timestep = 2734. State = [[-0.15294354 -0.07658504]]. Action = [[-0.11187661  0.18965977  0.05922025 -0.85270965]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 2734 is [True, False, False, False, True, False]
Current timestep = 2735. State = [[-0.1546334  -0.07109766]]. Action = [[ 0.21868545  0.08656463 -0.19473079 -0.8847844 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 2735 is [True, False, False, False, True, False]
Scene graph at timestep 2735 is [True, False, False, False, True, False]
State prediction error at timestep 2735 is tensor(3.1298e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2736. State = [[-0.15534669 -0.06614367]]. Action = [[ 0.04738176 -0.1979525  -0.0191282   0.04255974]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 2736 is [True, False, False, False, True, False]
Scene graph at timestep 2736 is [True, False, False, False, True, False]
State prediction error at timestep 2736 is tensor(1.7991e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2737. State = [[-0.15541962 -0.06614534]]. Action = [[-0.08591816 -0.21979274  0.19274753  0.6247375 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 2737 is [True, False, False, False, True, False]
Current timestep = 2738. State = [[-0.15554316 -0.06869024]]. Action = [[ 0.05094871 -0.20268257  0.11419472  0.00504339]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 2738 is [True, False, False, False, True, False]
Current timestep = 2739. State = [[-0.15614678 -0.07218532]]. Action = [[-0.13152614  0.23876867  0.22809887 -0.8535567 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 2739 is [True, False, False, False, True, False]
Current timestep = 2740. State = [[-0.15655737 -0.07175969]]. Action = [[ 0.02084124 -0.0445087  -0.13754793  0.38733268]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 2740 is [True, False, False, False, True, False]
Current timestep = 2741. State = [[-0.1566703  -0.07234243]]. Action = [[-0.10351099 -0.08977485  0.11081514 -0.3303957 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 2741 is [True, False, False, False, True, False]
Current timestep = 2742. State = [[-0.15698083 -0.07350828]]. Action = [[ 0.08974189 -0.08784823  0.04710364 -0.37075984]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 2742 is [True, False, False, False, True, False]
Current timestep = 2743. State = [[-0.15721515 -0.07478753]]. Action = [[-0.10900983  0.15588349 -0.12416202  0.1339196 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 2743 is [True, False, False, False, True, False]
Current timestep = 2744. State = [[-0.15764333 -0.07445167]]. Action = [[-0.03214142  0.16855758 -0.23081128 -0.13449931]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 2744 is [True, False, False, False, True, False]
Scene graph at timestep 2744 is [True, False, False, False, True, False]
State prediction error at timestep 2744 is tensor(2.5585e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2745. State = [[-0.15818213 -0.07265195]]. Action = [[-0.08035746 -0.07682241 -0.02298857 -0.39836848]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 2745 is [True, False, False, False, True, False]
Current timestep = 2746. State = [[-0.15879837 -0.07229922]]. Action = [[ 0.16413021  0.18551868 -0.14244877 -0.4511745 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 2746 is [True, False, False, False, True, False]
Current timestep = 2747. State = [[-0.15899237 -0.07015751]]. Action = [[ 0.10174835 -0.14517958  0.17372432 -0.9571752 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 2747 is [True, False, False, False, True, False]
Scene graph at timestep 2747 is [True, False, False, False, True, False]
State prediction error at timestep 2747 is tensor(2.5492e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2748. State = [[-0.15909432 -0.06995922]]. Action = [[-0.17867018  0.1422919   0.08431602  0.93056154]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 2748 is [True, False, False, False, True, False]
Scene graph at timestep 2748 is [True, False, False, False, True, False]
State prediction error at timestep 2748 is tensor(2.2255e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2749. State = [[-0.15920489 -0.06857558]]. Action = [[ 0.23894292 -0.04612195 -0.12734394  0.7684779 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 2749 is [True, False, False, False, True, False]
Current timestep = 2750. State = [[-0.15918878 -0.06831443]]. Action = [[-0.10865778  0.19630992  0.01349807 -0.02107894]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 2750 is [True, False, False, False, True, False]
Scene graph at timestep 2750 is [True, False, False, False, True, False]
State prediction error at timestep 2750 is tensor(1.0504e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2751. State = [[-0.15947606 -0.06574114]]. Action = [[-0.14591084  0.02730459  0.12686247  0.4571637 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 2751 is [True, False, False, False, True, False]
Current timestep = 2752. State = [[-0.15998216 -0.06303982]]. Action = [[-0.10819407  0.15451103 -0.2034937   0.6615193 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 2752 is [True, False, False, False, True, False]
Current timestep = 2753. State = [[-0.16086365 -0.05950001]]. Action = [[-0.12811004 -0.10543896  0.23981982  0.5138881 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 2753 is [True, False, False, False, True, False]
Current timestep = 2754. State = [[-0.16178097 -0.05846656]]. Action = [[-0.11189389 -0.18780293 -0.12398919  0.8434514 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 2754 is [True, False, False, False, True, False]
Current timestep = 2755. State = [[-0.16405174 -0.06007071]]. Action = [[ 0.20292765 -0.16260982 -0.23196325 -0.619856  ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 2755 is [True, False, False, False, True, False]
Scene graph at timestep 2755 is [True, False, False, False, True, False]
State prediction error at timestep 2755 is tensor(5.5164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2756. State = [[-0.16466907 -0.06248941]]. Action = [[0.15934992 0.12576959 0.13041997 0.47715402]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 2756 is [True, False, False, False, True, False]
Current timestep = 2757. State = [[-0.16482008 -0.06228106]]. Action = [[-0.22927868  0.13962597  0.0442349  -0.90663594]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 2757 is [True, False, False, False, True, False]
Current timestep = 2758. State = [[-0.1657155  -0.06132972]]. Action = [[-0.02708215  0.18638945 -0.02803497 -0.6523213 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 2758 is [True, False, False, False, True, False]
Current timestep = 2759. State = [[-0.16694137 -0.0584562 ]]. Action = [[-0.00532496 -0.1130169  -0.20679612 -0.75849295]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 2759 is [True, False, False, False, True, False]
Scene graph at timestep 2759 is [True, False, False, False, True, False]
State prediction error at timestep 2759 is tensor(1.9956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2760. State = [[-0.1677827  -0.05800895]]. Action = [[ 0.17603141 -0.19480734  0.19599682  0.92238736]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 2760 is [True, False, False, False, True, False]
Current timestep = 2761. State = [[-0.16757052 -0.05937355]]. Action = [[ 0.1863445  -0.13133594 -0.15742846  0.37184727]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 2761 is [True, False, False, False, True, False]
Scene graph at timestep 2761 is [True, False, False, False, True, False]
State prediction error at timestep 2761 is tensor(8.1760e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2762. State = [[-0.16715643 -0.06134663]]. Action = [[ 0.23964512  0.0053179  -0.1360768  -0.3513201 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 2762 is [True, False, False, False, True, False]
Current timestep = 2763. State = [[-0.16585971 -0.06188326]]. Action = [[ 0.13146839 -0.02446124  0.23779804  0.74150586]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 2763 is [True, False, False, False, True, False]
Scene graph at timestep 2763 is [True, False, False, False, True, False]
State prediction error at timestep 2763 is tensor(9.7517e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2764. State = [[-0.16416572 -0.06234856]]. Action = [[-0.12140581  0.22601092 -0.13882832  0.3867836 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 2764 is [True, False, False, False, True, False]
Current timestep = 2765. State = [[-0.16383535 -0.06159674]]. Action = [[-0.08999467 -0.15844204 -0.14387958  0.78789806]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 2765 is [True, False, False, False, True, False]
Current timestep = 2766. State = [[-0.16380784 -0.0621204 ]]. Action = [[ 0.19605839 -0.07048796 -0.02198665  0.2759416 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 2766 is [True, False, False, False, True, False]
Scene graph at timestep 2766 is [True, False, False, False, True, False]
State prediction error at timestep 2766 is tensor(1.0336e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2767. State = [[-0.16282041 -0.06299932]]. Action = [[ 0.05656621  0.01877141 -0.04876043  0.6777046 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 2767 is [True, False, False, False, True, False]
Current timestep = 2768. State = [[-0.16191228 -0.06325385]]. Action = [[-0.06812227 -0.14861411 -0.17052765 -0.8559283 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 2768 is [True, False, False, False, True, False]
Current timestep = 2769. State = [[-0.16137634 -0.06485917]]. Action = [[0.24248505 0.16756421 0.21243191 0.8437302 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 2769 is [True, False, False, False, True, False]
Current timestep = 2770. State = [[-0.15940414 -0.06482709]]. Action = [[-0.08966325 -0.20092182  0.21053916  0.11383212]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 2770 is [True, False, False, False, True, False]
Current timestep = 2771. State = [[-0.15809222 -0.06664417]]. Action = [[-0.14920591 -0.19105281 -0.13473643 -0.54695976]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 2771 is [True, False, False, False, True, False]
Current timestep = 2772. State = [[-0.1581158  -0.07023797]]. Action = [[ 0.05213812 -0.06939402 -0.1530879  -0.01215833]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 2772 is [True, False, False, False, True, False]
Current timestep = 2773. State = [[-0.15809415 -0.0730103 ]]. Action = [[0.12079033 0.02683511 0.00548163 0.40716076]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 2773 is [True, False, False, False, True, False]
Scene graph at timestep 2773 is [True, False, False, False, True, False]
State prediction error at timestep 2773 is tensor(1.4680e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2774. State = [[-0.15764692 -0.07445469]]. Action = [[-0.16689034  0.03796217  0.2385653   0.7845018 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 2774 is [True, False, False, False, True, False]
Scene graph at timestep 2774 is [True, False, False, False, True, False]
State prediction error at timestep 2774 is tensor(1.3175e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2775. State = [[-0.15758511 -0.07492594]]. Action = [[ 0.2265026   0.13197601 -0.02566028 -0.7811205 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 2775 is [True, False, False, False, True, False]
Scene graph at timestep 2775 is [True, False, False, False, True, False]
State prediction error at timestep 2775 is tensor(1.2154e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2776. State = [[-0.15684184 -0.07491101]]. Action = [[ 0.24368286 -0.02246051  0.22671258 -0.9392205 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 2776 is [True, False, False, False, True, False]
Current timestep = 2777. State = [[-0.15385011 -0.07478377]]. Action = [[ 0.2220928   0.17875981  0.14986587 -0.93466973]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 2777 is [True, False, False, False, True, False]
Scene graph at timestep 2777 is [True, False, False, False, True, False]
State prediction error at timestep 2777 is tensor(5.8024e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2778. State = [[-0.14954107 -0.07308532]]. Action = [[ 0.10225257  0.22652897 -0.06593664  0.68218195]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 2778 is [True, False, False, False, True, False]
Current timestep = 2779. State = [[-0.14516136 -0.06974322]]. Action = [[ 0.21000373 -0.09071907  0.1241034  -0.67073804]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 2779 is [True, False, False, False, True, False]
Current timestep = 2780. State = [[-0.13982211 -0.06918489]]. Action = [[-0.21693599 -0.19091147  0.02867723  0.66443515]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 2780 is [True, False, False, False, True, False]
Current timestep = 2781. State = [[-0.13749962 -0.06973042]]. Action = [[ 0.05805874 -0.02042419 -0.19073674  0.8457093 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 2781 is [True, False, False, False, True, False]
Scene graph at timestep 2781 is [True, False, False, False, True, False]
State prediction error at timestep 2781 is tensor(2.1373e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2782. State = [[-0.13687143 -0.0704307 ]]. Action = [[ 0.06214595 -0.22235796 -0.23554361 -0.24785143]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 2782 is [True, False, False, False, True, False]
Human Feedback received at timestep 2782 of 1
Current timestep = 2783. State = [[-0.13602449 -0.07311874]]. Action = [[-0.10412718  0.14892355 -0.0563924  -0.6720975 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 2783 is [True, False, False, False, True, False]
Current timestep = 2784. State = [[-0.13590091 -0.07348508]]. Action = [[-0.15521447 -0.09758152 -0.1879325   0.45475018]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 2784 is [True, False, False, False, True, False]
Current timestep = 2785. State = [[-0.13604157 -0.07443435]]. Action = [[ 0.22383094 -0.24258845 -0.04956809 -0.6851168 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 2785 is [True, False, False, False, True, False]
Current timestep = 2786. State = [[-0.13528281 -0.07747719]]. Action = [[0.08601862 0.02223262 0.00164744 0.5799817 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 2786 is [True, False, False, False, True, False]
Current timestep = 2787. State = [[-0.13421613 -0.07980169]]. Action = [[ 0.20179296  0.13610536  0.1718908  -0.7767879 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 2787 is [True, False, False, False, True, False]
Current timestep = 2788. State = [[-0.13141234 -0.07987824]]. Action = [[ 0.02616632 -0.02637336 -0.02142745 -0.34199977]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 2788 is [True, False, False, False, True, False]
Current timestep = 2789. State = [[-0.1279634  -0.08020649]]. Action = [[0.18032336 0.10709894 0.22264451 0.08835495]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 2789 is [True, False, False, False, True, False]
Scene graph at timestep 2789 is [True, False, False, False, True, False]
State prediction error at timestep 2789 is tensor(6.0098e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2790. State = [[-0.12373962 -0.07982084]]. Action = [[ 0.1965732   0.16480967  0.07612914 -0.3926943 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 2790 is [True, False, False, False, True, False]
Scene graph at timestep 2790 is [True, False, False, False, True, False]
State prediction error at timestep 2790 is tensor(4.2854e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2791. State = [[-0.11880916 -0.07777424]]. Action = [[ 0.00235999 -0.05116072  0.12487352  0.92634416]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 2791 is [True, False, False, False, True, False]
Current timestep = 2792. State = [[-0.11418264 -0.07716221]]. Action = [[-0.16239613 -0.05512047  0.07090685  0.35553086]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 2792 is [True, False, False, False, True, False]
Current timestep = 2793. State = [[-0.11184916 -0.07739708]]. Action = [[0.08553606 0.09977949 0.23697513 0.02614415]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 2793 is [True, False, False, False, True, False]
Scene graph at timestep 2793 is [True, False, False, False, True, False]
State prediction error at timestep 2793 is tensor(5.8660e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2793 of 1
Current timestep = 2794. State = [[-0.11033556 -0.07678408]]. Action = [[ 0.14911571 -0.07789937  0.20825952 -0.7841519 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 2794 is [True, False, False, False, True, False]
Scene graph at timestep 2794 is [True, False, False, False, True, False]
State prediction error at timestep 2794 is tensor(9.2260e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2795. State = [[-0.10855413 -0.07654889]]. Action = [[-0.1664466   0.13796112  0.07194918 -0.10321361]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 2795 is [True, False, False, False, True, False]
Current timestep = 2796. State = [[-0.10863542 -0.07519974]]. Action = [[-0.17969444  0.19092488  0.22582495 -0.20136112]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 2796 is [True, False, False, False, True, False]
Current timestep = 2797. State = [[-0.10913991 -0.07211541]]. Action = [[ 0.20791209 -0.21146476 -0.13087672  0.62587786]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 2797 is [True, False, False, False, True, False]
Current timestep = 2798. State = [[-0.10881437 -0.07262035]]. Action = [[ 0.22714162 -0.17292652 -0.10092367 -0.8832385 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 2798 is [True, False, False, False, True, False]
Current timestep = 2799. State = [[-0.10746768 -0.07451086]]. Action = [[-0.20965028 -0.23478785 -0.09483542 -0.8922776 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 2799 is [True, False, False, False, True, False]
Scene graph at timestep 2799 is [True, False, False, False, True, False]
State prediction error at timestep 2799 is tensor(3.0362e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2800. State = [[-0.10732862 -0.07821245]]. Action = [[ 0.17111397  0.12115437 -0.1940883  -0.9596328 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 2800 is [True, False, False, False, True, False]
Current timestep = 2801. State = [[-0.10723683 -0.07851838]]. Action = [[-0.12291817  0.1735309  -0.11226177  0.6668123 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 2801 is [True, False, False, False, True, False]
Current timestep = 2802. State = [[-0.10725346 -0.07803263]]. Action = [[-0.17731032  0.1975565   0.23878264  0.69117427]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 2802 is [True, False, False, False, True, False]
Scene graph at timestep 2802 is [True, False, False, False, True, False]
State prediction error at timestep 2802 is tensor(6.8948e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2803. State = [[-0.10757854 -0.075305  ]]. Action = [[ 0.05304831  0.09558013 -0.18546903 -0.91543484]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 2803 is [True, False, False, False, True, False]
Scene graph at timestep 2803 is [True, False, False, False, True, False]
State prediction error at timestep 2803 is tensor(1.5201e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2804. State = [[-0.10797667 -0.07263291]]. Action = [[-0.2066916  -0.17207788 -0.08581817  0.4199828 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 2804 is [True, False, False, False, True, False]
Current timestep = 2805. State = [[-0.1079997  -0.07281418]]. Action = [[ 0.23909575 -0.18166907  0.06055805  0.2737484 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 2805 is [True, False, False, False, True, False]
Current timestep = 2806. State = [[-0.10785685 -0.07475593]]. Action = [[ 0.18823579 -0.21820615  0.17971146  0.38838983]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 2806 is [True, False, False, False, True, False]
Current timestep = 2807. State = [[-0.10698108 -0.07845034]]. Action = [[ 0.12929964 -0.02745658  0.10300764  0.8888943 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 2807 is [True, False, False, False, True, False]
Scene graph at timestep 2807 is [True, False, False, False, True, False]
State prediction error at timestep 2807 is tensor(5.5457e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2808. State = [[-0.10600448 -0.08065999]]. Action = [[ 0.22985658  0.22777998 -0.02810611 -0.9084791 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 2808 is [True, False, False, False, True, False]
Current timestep = 2809. State = [[-0.10289887 -0.07986552]]. Action = [[ 0.11707059  0.0344182  -0.22055209  0.8522918 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 2809 is [True, False, False, False, True, False]
Current timestep = 2810. State = [[-0.09805804 -0.07891433]]. Action = [[ 0.17795396 -0.06305942 -0.19265504  0.49385655]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 2810 is [True, False, False, False, True, False]
Scene graph at timestep 2810 is [True, False, False, False, True, False]
State prediction error at timestep 2810 is tensor(4.9177e-07, grad_fn=<MseLossBackward0>)
Current timestep = 2811. State = [[-0.09207911 -0.07910402]]. Action = [[ 0.22310609 -0.22913754  0.0739693  -0.43965626]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 2811 is [True, False, False, False, True, False]
Scene graph at timestep 2811 is [True, False, False, False, True, False]
State prediction error at timestep 2811 is tensor(3.0682e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2812. State = [[-0.08447639 -0.08180081]]. Action = [[-0.10000521 -0.05940562 -0.16158143 -0.7008732 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 2812 is [True, False, False, False, True, False]
Human Feedback received at timestep 2812 of 1
Current timestep = 2813. State = [[-0.0805896  -0.08365846]]. Action = [[-0.13918471 -0.18621442  0.05018792 -0.87349427]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 2813 is [True, False, False, False, True, False]
Current timestep = 2814. State = [[-0.07985725 -0.08713299]]. Action = [[0.20950532 0.20736516 0.22715318 0.54911816]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 2814 is [True, False, False, False, True, False]
Current timestep = 2815. State = [[-0.07767107 -0.08706188]]. Action = [[ 0.14068103  0.03420103 -0.22527307 -0.91795856]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 2815 is [True, False, False, False, True, False]
Current timestep = 2816. State = [[-0.0747255  -0.08685997]]. Action = [[-0.2157226  -0.14373784  0.2020264   0.52843904]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 2816 is [True, False, False, False, True, False]
Current timestep = 2817. State = [[-0.07377016 -0.08823894]]. Action = [[ 0.09320801 -0.13786936  0.00367674  0.69308317]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 2817 is [True, False, False, False, True, False]
Scene graph at timestep 2817 is [True, False, False, False, True, False]
State prediction error at timestep 2817 is tensor(1.2375e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2818. State = [[-0.07340381 -0.09077995]]. Action = [[-0.04016872 -0.09471045 -0.15860365  0.00572681]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 2818 is [True, False, False, False, True, False]
Current timestep = 2819. State = [[-0.07323614 -0.09386646]]. Action = [[-0.20127557 -0.09635638 -0.07157868  0.8057821 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 2819 is [True, False, False, False, True, False]
Current timestep = 2820. State = [[-0.07341542 -0.0968731 ]]. Action = [[-0.02399066 -0.23641838 -0.20587124  0.63247883]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 2820 is [True, False, False, False, True, False]
Current timestep = 2821. State = [[-0.07347377 -0.10193335]]. Action = [[ 0.23490384 -0.05468932 -0.07554075  0.76455045]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 2821 is [True, False, False, False, True, False]
Scene graph at timestep 2821 is [True, False, False, False, True, False]
State prediction error at timestep 2821 is tensor(4.1449e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2822. State = [[-0.07309274 -0.10659523]]. Action = [[ 0.19558612 -0.22455034  0.13110447 -0.2824728 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 2822 is [True, False, False, False, True, False]
Current timestep = 2823. State = [[-0.07127482 -0.11191817]]. Action = [[ 0.23231775 -0.15219967  0.10729584  0.64658713]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 2823 is [True, False, False, False, True, False]
Current timestep = 2824. State = [[-0.06695965 -0.11761404]]. Action = [[-0.03508362  0.13284063 -0.15669988  0.824577  ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 2824 is [True, False, False, False, True, False]
Scene graph at timestep 2824 is [True, False, False, False, True, False]
State prediction error at timestep 2824 is tensor(1.9995e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2825. State = [[-0.06376324 -0.11952219]]. Action = [[-0.23522532 -0.14936091  0.15552264  0.5764973 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 2825 is [True, False, False, False, True, False]
Scene graph at timestep 2825 is [True, False, False, False, True, False]
State prediction error at timestep 2825 is tensor(1.6170e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2826. State = [[-0.06378963 -0.12223426]]. Action = [[-0.10805768  0.24552202  0.21251437 -0.3823042 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 2826 is [True, False, False, False, True, False]
Current timestep = 2827. State = [[-0.06416431 -0.12128818]]. Action = [[-0.0655006  -0.02178715  0.1220715   0.50377584]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 2827 is [True, False, False, False, True, False]
Current timestep = 2828. State = [[-0.06461483 -0.12135804]]. Action = [[0.04169634 0.19393313 0.0281364  0.9586897 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 2828 is [True, False, False, False, True, False]
Scene graph at timestep 2828 is [True, False, False, False, True, False]
State prediction error at timestep 2828 is tensor(2.4661e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2829. State = [[-0.06476312 -0.11987966]]. Action = [[ 0.04084399 -0.07728598 -0.04844697  0.88307714]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 2829 is [True, False, False, False, True, False]
Current timestep = 2830. State = [[-0.06478255 -0.12002271]]. Action = [[ 0.2426675  -0.19392638  0.09527579 -0.44673944]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 2830 is [True, False, False, False, True, False]
Current timestep = 2831. State = [[-0.06458523 -0.12118503]]. Action = [[ 0.07795566 -0.12189597  0.21292508  0.5902724 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 2831 is [True, False, False, False, True, False]
Current timestep = 2832. State = [[-0.06408001 -0.12322778]]. Action = [[-0.08642191 -0.1849092  -0.02728511  0.8111913 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 2832 is [True, False, False, False, True, False]
Current timestep = 2833. State = [[-0.06354199 -0.1266624 ]]. Action = [[ 0.16639644  0.06853992 -0.15646283  0.47360873]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 2833 is [True, False, False, False, True, False]
Current timestep = 2834. State = [[-0.06181332 -0.12780543]]. Action = [[-0.22714926  0.04398167  0.21298862 -0.76156324]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 2834 is [True, False, False, True, False, False]
Current timestep = 2835. State = [[-0.06125935 -0.12811245]]. Action = [[ 0.13720906 -0.09948951 -0.05628231  0.5596719 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 2835 is [True, False, False, True, False, False]
Current timestep = 2836. State = [[-0.06012893 -0.12905702]]. Action = [[ 0.17195085  0.15840536 -0.22838567 -0.47172856]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 2836 is [True, False, False, True, False, False]
Scene graph at timestep 2836 is [True, False, False, True, False, False]
State prediction error at timestep 2836 is tensor(5.6451e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2837. State = [[-0.05748977 -0.12874822]]. Action = [[ 0.03963286 -0.19423127  0.08184582 -0.6836243 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 2837 is [True, False, False, True, False, False]
Scene graph at timestep 2837 is [True, False, False, True, False, False]
State prediction error at timestep 2837 is tensor(2.5328e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2838. State = [[-0.05463336 -0.13053915]]. Action = [[-0.00793031  0.0111559   0.19494036 -0.36215687]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 2838 is [True, False, False, True, False, False]
Current timestep = 2839. State = [[-0.05360559 -0.13136394]]. Action = [[-0.19288789 -0.01841101  0.02463701  0.659657  ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 2839 is [True, False, False, True, False, False]
Current timestep = 2840. State = [[-0.05372985 -0.13204958]]. Action = [[-0.13124266 -0.10054412 -0.09438717  0.2676574 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 2840 is [True, False, False, True, False, False]
Scene graph at timestep 2840 is [True, False, False, True, False, False]
State prediction error at timestep 2840 is tensor(5.5460e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2841. State = [[-0.05407055 -0.134317  ]]. Action = [[ 0.1395768  -0.14048816  0.11749977 -0.7507333 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 2841 is [True, False, False, True, False, False]
Current timestep = 2842. State = [[-0.05410677 -0.13722964]]. Action = [[0.13051173 0.14805853 0.06929263 0.73300505]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 2842 is [True, False, False, True, False, False]
Current timestep = 2843. State = [[-0.05378285 -0.1375652 ]]. Action = [[ 0.1849407  -0.11323008  0.15922171  0.10634208]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 2843 is [True, False, False, True, False, False]
Human Feedback received at timestep 2843 of 1
Current timestep = 2844. State = [[-0.05244818 -0.13867113]]. Action = [[-0.21274067  0.0141879  -0.09548503 -0.00097084]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 2844 is [True, False, False, True, False, False]
Scene graph at timestep 2844 is [True, False, False, True, False, False]
State prediction error at timestep 2844 is tensor(4.6617e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2845. State = [[-0.05207647 -0.13934061]]. Action = [[ 0.23437315 -0.09107393  0.23894233  0.09581816]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 2845 is [True, False, False, True, False, False]
Current timestep = 2846. State = [[-0.05031599 -0.14102359]]. Action = [[-0.05399188  0.01563391 -0.12262908 -0.4395665 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 2846 is [True, False, False, True, False, False]
Scene graph at timestep 2846 is [True, False, False, True, False, False]
State prediction error at timestep 2846 is tensor(1.2441e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2847. State = [[-0.0488238  -0.14187409]]. Action = [[-0.13524401 -0.0956721  -0.1212222  -0.8014965 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 2847 is [True, False, False, True, False, False]
Current timestep = 2848. State = [[-0.04871305 -0.14357924]]. Action = [[ 0.23485875  0.01657361 -0.08473551 -0.8773904 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 2848 is [False, True, False, True, False, False]
Current timestep = 2849. State = [[-0.04759684 -0.14459373]]. Action = [[-0.09444445 -0.24823779 -0.02464931 -0.3647406 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 2849 is [False, True, False, True, False, False]
Current timestep = 2850. State = [[-0.0474603  -0.14842242]]. Action = [[-0.10811502 -0.13710606 -0.23077694  0.25555634]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 2850 is [False, True, False, True, False, False]
Scene graph at timestep 2850 is [False, True, False, True, False, False]
State prediction error at timestep 2850 is tensor(8.1594e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2851. State = [[-0.04751204 -0.15321583]]. Action = [[0.19678998 0.21664986 0.1582205  0.8618045 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 2851 is [False, True, False, True, False, False]
Current timestep = 2852. State = [[-0.04711694 -0.1535893 ]]. Action = [[-0.00764713 -0.07673718 -0.21884702 -0.7514066 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 2852 is [False, True, False, True, False, False]
Scene graph at timestep 2852 is [False, True, False, True, False, False]
State prediction error at timestep 2852 is tensor(1.3978e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2853. State = [[-0.04695956 -0.15433073]]. Action = [[-0.1984207  -0.11465529 -0.07034792 -0.1389944 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 2853 is [False, True, False, True, False, False]
Scene graph at timestep 2853 is [False, True, False, True, False, False]
State prediction error at timestep 2853 is tensor(3.1737e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2854. State = [[-0.04701486 -0.15634517]]. Action = [[ 0.03268626 -0.22941227 -0.09299543 -0.81492215]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 2854 is [False, True, False, True, False, False]
Scene graph at timestep 2854 is [False, True, False, True, False, False]
State prediction error at timestep 2854 is tensor(1.6125e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2855. State = [[-0.04723235 -0.16132313]]. Action = [[ 0.20906395 -0.22818652  0.06918231  0.9642458 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 2855 is [False, True, False, True, False, False]
Human Feedback received at timestep 2855 of -1
Current timestep = 2856. State = [[-0.04663467 -0.16725735]]. Action = [[ 0.21288648 -0.06002413  0.07040238  0.6306739 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 2856 is [False, True, False, True, False, False]
Current timestep = 2857. State = [[-0.04450161 -0.17153583]]. Action = [[ 0.1419878   0.10533577 -0.08672342  0.6893574 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 2857 is [False, True, False, True, False, False]
Scene graph at timestep 2857 is [False, True, False, True, False, False]
State prediction error at timestep 2857 is tensor(4.4654e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2858. State = [[-0.04046346 -0.17256247]]. Action = [[0.24283564 0.19429916 0.08250874 0.31375957]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 2858 is [False, True, False, True, False, False]
Current timestep = 2859. State = [[-0.03486075 -0.17171833]]. Action = [[-0.19666259 -0.07674862  0.22076538 -0.8393739 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 2859 is [False, True, False, True, False, False]
Scene graph at timestep 2859 is [False, True, False, True, False, False]
State prediction error at timestep 2859 is tensor(2.4833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2860. State = [[-0.03059738 -0.17189938]]. Action = [[-0.19169955 -0.2096172   0.06035328 -0.86857563]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 2860 is [False, True, False, True, False, False]
Current timestep = 2861. State = [[-0.03008491 -0.17515416]]. Action = [[-0.105735   -0.00324655 -0.21486054  0.2717855 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 2861 is [False, True, False, True, False, False]
Current timestep = 2862. State = [[-0.03038112 -0.17673023]]. Action = [[-0.20081304  0.15153962  0.06976283  0.38616633]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 2862 is [False, True, False, True, False, False]
Current timestep = 2863. State = [[-0.03055694 -0.17665137]]. Action = [[-0.12705737 -0.0284138  -0.1762196   0.30893493]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 2863 is [False, True, False, True, False, False]
Scene graph at timestep 2863 is [False, True, False, True, False, False]
State prediction error at timestep 2863 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2864. State = [[-0.03096641 -0.17686836]]. Action = [[-0.08668911  0.09349722 -0.07459354 -0.9328072 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 2864 is [False, True, False, True, False, False]
Current timestep = 2865. State = [[-0.03184727 -0.17682697]]. Action = [[ 0.21636379 -0.08049563 -0.19265273 -0.44394767]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 2865 is [False, True, False, True, False, False]
Scene graph at timestep 2865 is [False, True, False, True, False, False]
State prediction error at timestep 2865 is tensor(1.7882e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2866. State = [[-0.03181892 -0.1768228 ]]. Action = [[-0.09408316 -0.02554904 -0.11236086  0.13637495]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 2866 is [False, True, False, True, False, False]
Scene graph at timestep 2866 is [False, True, False, True, False, False]
State prediction error at timestep 2866 is tensor(4.0890e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2867. State = [[-0.03186887 -0.17728518]]. Action = [[ 0.15073612 -0.17432173 -0.24237104  0.98612225]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 2867 is [False, True, False, True, False, False]
Current timestep = 2868. State = [[-0.03215075 -0.17901386]]. Action = [[-0.09249389  0.03321421  0.22911382 -0.82681745]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 2868 is [False, True, False, True, False, False]
Current timestep = 2869. State = [[-0.03226931 -0.17945302]]. Action = [[-0.11525607  0.22717994 -0.10526252 -0.57234037]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 2869 is [False, True, False, True, False, False]
Current timestep = 2870. State = [[-0.03252855 -0.17887723]]. Action = [[ 0.20081693 -0.19072661 -0.13496418  0.38647223]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 2870 is [False, True, False, True, False, False]
Current timestep = 2871. State = [[-0.03261669 -0.17921492]]. Action = [[-0.11471348 -0.04187723 -0.09742954  0.52714086]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 2871 is [False, True, False, True, False, False]
Current timestep = 2872. State = [[-0.03281864 -0.18057127]]. Action = [[ 0.14789885 -0.19486444  0.18814987 -0.8178041 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 2872 is [False, True, False, True, False, False]
Current timestep = 2873. State = [[-0.03306711 -0.18312928]]. Action = [[0.07386351 0.2367864  0.03945389 0.14940798]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 2873 is [False, True, False, True, False, False]
Current timestep = 2874. State = [[-0.03307784 -0.18275617]]. Action = [[-0.04610318 -0.08750418 -0.071769   -0.7861081 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 2874 is [False, True, False, True, False, False]
Current timestep = 2875. State = [[-0.03310699 -0.18302178]]. Action = [[-0.02348398 -0.11920397  0.00609097 -0.5550854 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 2875 is [False, True, False, True, False, False]
Current timestep = 2876. State = [[-0.03326699 -0.18434402]]. Action = [[ 0.13308942  0.24131554 -0.23534642 -0.54310554]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 2876 is [False, True, False, True, False, False]
Current timestep = 2877. State = [[-0.03317985 -0.1830467 ]]. Action = [[ 0.0562036   0.18678263  0.198482   -0.7578646 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 2877 is [False, True, False, True, False, False]
Current timestep = 2878. State = [[-0.03292199 -0.1802828 ]]. Action = [[-0.23698823  0.06550673 -0.22830898  0.8909855 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 2878 is [False, True, False, True, False, False]
Current timestep = 2879. State = [[-0.03277541 -0.17875423]]. Action = [[ 0.08781734 -0.1804738  -0.05343822 -0.3100323 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 2879 is [False, True, False, True, False, False]
Current timestep = 2880. State = [[-0.03298124 -0.17905603]]. Action = [[ 0.1316531   0.23506412 -0.22770636  0.19273436]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 2880 is [False, True, False, True, False, False]
Scene graph at timestep 2880 is [False, True, False, True, False, False]
State prediction error at timestep 2880 is tensor(2.8523e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2881. State = [[-0.03290715 -0.17671001]]. Action = [[-0.17002633  0.189538   -0.01208092 -0.09001213]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 2881 is [False, True, False, True, False, False]
Current timestep = 2882. State = [[-0.03280291 -0.17309515]]. Action = [[ 0.12236264 -0.13195722 -0.21050112  0.26384473]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 2882 is [False, True, False, True, False, False]
Scene graph at timestep 2882 is [False, True, False, True, False, False]
State prediction error at timestep 2882 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2883. State = [[-0.03269207 -0.17246397]]. Action = [[-0.1860548  -0.14145555 -0.2158387  -0.06233358]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 2883 is [False, True, False, True, False, False]
Current timestep = 2884. State = [[-0.03276476 -0.17321813]]. Action = [[ 0.13702846 -0.15165344  0.03405282 -0.9010488 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 2884 is [False, True, False, True, False, False]
Current timestep = 2885. State = [[-0.03290043 -0.17466158]]. Action = [[ 0.02525505 -0.14267081 -0.0967366  -0.9523871 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 2885 is [False, True, False, True, False, False]
Scene graph at timestep 2885 is [False, True, False, True, False, False]
State prediction error at timestep 2885 is tensor(3.6825e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2886. State = [[-0.03297653 -0.17686242]]. Action = [[ 0.20453167 -0.00320354 -0.10579064 -0.7861646 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 2886 is [False, True, False, True, False, False]
Current timestep = 2887. State = [[-0.03246186 -0.17786884]]. Action = [[ 0.12684035 -0.07117674 -0.02924246 -0.77932245]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 2887 is [False, True, False, True, False, False]
Current timestep = 2888. State = [[-0.0317211  -0.17911083]]. Action = [[-0.03935097  0.05313414 -0.00569887  0.21860492]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 2888 is [False, True, False, True, False, False]
Current timestep = 2889. State = [[-0.03156463 -0.17951511]]. Action = [[-0.15156949 -0.10184008  0.07117829 -0.14039683]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 2889 is [False, True, False, True, False, False]
Current timestep = 2890. State = [[-0.03165567 -0.1813447 ]]. Action = [[-0.22625758 -0.04309635  0.09953713 -0.8244652 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 2890 is [False, True, False, True, False, False]
Current timestep = 2891. State = [[-0.03186822 -0.18299189]]. Action = [[-0.21690603  0.20683223 -0.16944422 -0.33874375]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 2891 is [False, True, False, True, False, False]
Scene graph at timestep 2891 is [False, True, False, True, False, False]
State prediction error at timestep 2891 is tensor(5.8418e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2892. State = [[-0.03224012 -0.18289293]]. Action = [[-0.07590412 -0.22535233 -0.09424773 -0.20254284]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 2892 is [False, True, False, True, False, False]
Current timestep = 2893. State = [[-0.03331225 -0.18573098]]. Action = [[ 0.14965686 -0.04427537 -0.22560267  0.6750672 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 2893 is [False, True, False, True, False, False]
Current timestep = 2894. State = [[-0.03374921 -0.18735936]]. Action = [[ 0.23182082 -0.06035101 -0.21206048 -0.14632583]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 2894 is [False, True, False, True, False, False]
Current timestep = 2895. State = [[-0.03377956 -0.18837745]]. Action = [[ 0.18529546 -0.05683219 -0.09468932  0.37429392]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 2895 is [False, True, False, True, False, False]
Current timestep = 2896. State = [[-0.03361835 -0.18974014]]. Action = [[ 0.03349644 -0.22362408 -0.16814992 -0.34402758]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 2896 is [False, True, False, True, False, False]
Scene graph at timestep 2896 is [False, True, False, True, False, False]
State prediction error at timestep 2896 is tensor(1.6024e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2897. State = [[-0.03348755 -0.19369966]]. Action = [[ 0.22005105 -0.06353815  0.17027748  0.56560564]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 2897 is [False, True, False, True, False, False]
Current timestep = 2898. State = [[-0.03180118 -0.19659156]]. Action = [[ 0.247567    0.02184436 -0.06455719  0.32692647]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 2898 is [False, True, False, True, False, False]
Scene graph at timestep 2898 is [False, True, False, True, False, False]
State prediction error at timestep 2898 is tensor(9.7654e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2899. State = [[-0.02757577 -0.19817287]]. Action = [[ 0.24635309 -0.24003245  0.05590072 -0.78587186]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 2899 is [False, True, False, True, False, False]
Current timestep = 2900. State = [[-0.02011456 -0.20278671]]. Action = [[ 0.199548    0.08189669 -0.24011248  0.72445965]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 2900 is [False, True, False, True, False, False]
Current timestep = 2901. State = [[-0.01352809 -0.20435867]]. Action = [[-0.2243522  -0.20260471  0.03048843 -0.59750926]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 2901 is [False, True, False, True, False, False]
Current timestep = 2902. State = [[-0.00928448 -0.20850685]]. Action = [[ 0.0193032   0.0657098  -0.15543695  0.55961037]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 2902 is [False, True, False, True, False, False]
Current timestep = 2903. State = [[-0.00660762 -0.21018179]]. Action = [[0.01262513 0.09341338 0.04322129 0.71726525]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 2903 is [False, True, False, True, False, False]
Current timestep = 2904. State = [[-0.00440618 -0.21018897]]. Action = [[-0.13400966  0.11443049  0.11830327  0.5817976 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 2904 is [False, True, False, True, False, False]
Scene graph at timestep 2904 is [False, True, False, True, False, False]
State prediction error at timestep 2904 is tensor(4.2494e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2904 of 1
Current timestep = 2905. State = [[-0.00385786 -0.21002059]]. Action = [[-0.06464255 -0.08244425 -0.15030184 -0.90159386]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 2905 is [False, True, False, True, False, False]
Current timestep = 2906. State = [[-0.00390699 -0.21000737]]. Action = [[0.1139673  0.11540729 0.20576233 0.91304207]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 2906 is [False, True, False, True, False, False]
Current timestep = 2907. State = [[-0.00374003 -0.20962626]]. Action = [[-0.10660394 -0.21861777  0.1415562   0.62624824]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 2907 is [False, True, False, True, False, False]
Current timestep = 2908. State = [[-0.00396822 -0.21155384]]. Action = [[ 0.07650426 -0.06929022 -0.05269957  0.6937306 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 2908 is [False, True, False, True, False, False]
Current timestep = 2909. State = [[-0.00404264 -0.21273485]]. Action = [[ 0.23429942  0.17640293 -0.01270768 -0.5455862 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 2909 is [False, True, False, True, False, False]
Scene graph at timestep 2909 is [False, True, False, True, False, False]
State prediction error at timestep 2909 is tensor(4.0891e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2910. State = [[-0.00195881 -0.21240915]]. Action = [[-0.06234755 -0.1667972  -0.23399296  0.60638726]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 2910 is [False, True, False, True, False, False]
Current timestep = 2911. State = [[-0.00126753 -0.21336935]]. Action = [[-0.16346951 -0.15662971  0.01480895 -0.09096861]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 2911 is [False, True, False, True, False, False]
Current timestep = 2912. State = [[-0.00172729 -0.21628967]]. Action = [[ 0.02114218 -0.01994854 -0.19904205  0.12498581]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 2912 is [False, True, False, True, False, False]
Current timestep = 2913. State = [[-0.00187882 -0.2182749 ]]. Action = [[ 0.20830342 -0.23700514  0.22900867 -0.3271134 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 2913 is [False, True, False, True, False, False]
Current timestep = 2914. State = [[-0.00149178 -0.2219762 ]]. Action = [[-0.06133117  0.0783608  -0.12846586 -0.69543   ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 2914 is [False, True, False, True, False, False]
Scene graph at timestep 2914 is [False, True, False, True, False, False]
State prediction error at timestep 2914 is tensor(2.2351e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2914 of -1
Current timestep = 2915. State = [[-0.00147516 -0.22357357]]. Action = [[ 0.23291266 -0.24425709 -0.19900559 -0.54794484]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 2915 is [False, True, False, True, False, False]
Current timestep = 2916. State = [[-0.0004392 -0.227906 ]]. Action = [[-0.09995045 -0.05052942  0.2186681   0.76315963]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 2916 is [False, True, False, True, False, False]
Current timestep = 2917. State = [[-0.00048484 -0.23133646]]. Action = [[ 0.03017473  0.02786508  0.19411528 -0.71858263]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 2917 is [False, True, False, True, False, False]
Current timestep = 2918. State = [[-0.0005238  -0.23300709]]. Action = [[ 0.17820582 -0.22949766 -0.06830241  0.41603315]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 2918 is [False, True, False, True, False, False]
Current timestep = 2919. State = [[ 0.00096935 -0.23687823]]. Action = [[-0.05116332  0.13113141 -0.24089648  0.56410336]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 2919 is [False, True, False, True, False, False]
Human Feedback received at timestep 2919 of -1
Current timestep = 2920. State = [[ 0.00413253 -0.23781525]]. Action = [[-0.13664074  0.0552429  -0.04722483 -0.9000539 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 2920 is [False, True, False, True, False, False]
Scene graph at timestep 2920 is [False, True, False, True, False, False]
State prediction error at timestep 2920 is tensor(4.3757e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2921. State = [[ 0.00659315 -0.23811135]]. Action = [[-0.0513268   0.21758568  0.14506784  0.22514355]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 2921 is [False, True, False, True, False, False]
Scene graph at timestep 2921 is [False, True, False, True, False, False]
State prediction error at timestep 2921 is tensor(2.7157e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2922. State = [[ 0.00725681 -0.23694333]]. Action = [[ 0.02462429 -0.13417184 -0.07510495 -0.59079355]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 2922 is [False, True, False, True, False, False]
Scene graph at timestep 2922 is [False, True, False, True, False, False]
State prediction error at timestep 2922 is tensor(5.6665e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2923. State = [[ 0.00716022 -0.23716182]]. Action = [[-0.13190266  0.1190055  -0.22769062  0.646536  ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 2923 is [False, True, False, True, False, False]
Current timestep = 2924. State = [[ 0.00703612 -0.23696917]]. Action = [[ 0.23710024 -0.06742859 -0.07648875 -0.9743928 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 2924 is [False, True, False, True, False, False]
Scene graph at timestep 2924 is [False, True, False, True, False, False]
State prediction error at timestep 2924 is tensor(1.1972e-06, grad_fn=<MseLossBackward0>)
Current timestep = 2925. State = [[ 0.00805695 -0.23701994]]. Action = [[ 0.18053216 -0.12826534 -0.22209345  0.61486053]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 2925 is [False, True, False, True, False, False]
Current timestep = 2926. State = [[ 0.01008394 -0.23734722]]. Action = [[ 0.19456002  0.1746105  -0.24373142  0.785063  ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 2926 is [False, True, False, True, False, False]
Scene graph at timestep 2926 is [False, True, False, True, False, False]
State prediction error at timestep 2926 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2927. State = [[ 0.01339945 -0.23652592]]. Action = [[-0.1046634   0.22681624  0.05598417 -0.7886827 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 2927 is [False, True, False, True, False, False]
Scene graph at timestep 2927 is [False, True, False, True, False, False]
State prediction error at timestep 2927 is tensor(7.9034e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2928. State = [[ 0.01543855 -0.2332693 ]]. Action = [[-0.02262782  0.23051691 -0.22092386  0.7813058 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 2928 is [False, True, False, True, False, False]
Current timestep = 2929. State = [[ 0.01718899 -0.22867544]]. Action = [[ 0.04663938  0.14610362 -0.22777353  0.3137461 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 2929 is [False, True, False, True, False, False]
Current timestep = 2930. State = [[ 0.01906835 -0.22442153]]. Action = [[-0.19696428 -0.08792666 -0.17887406  0.02444804]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 2930 is [False, True, False, True, False, False]
Current timestep = 2931. State = [[ 0.01904243 -0.22344035]]. Action = [[-0.08519746 -0.09137094  0.08038217 -0.6546574 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 2931 is [False, True, False, True, False, False]
Current timestep = 2932. State = [[ 0.01892196 -0.22376546]]. Action = [[-0.14961834 -0.02729115 -0.23167445  0.38439322]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 2932 is [False, True, False, True, False, False]
Scene graph at timestep 2932 is [False, True, False, True, False, False]
State prediction error at timestep 2932 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2933. State = [[ 0.01877129 -0.22420278]]. Action = [[ 0.12511623 -0.2168514  -0.10420069  0.9604893 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 2933 is [False, True, False, True, False, False]
Current timestep = 2934. State = [[ 0.01854968 -0.22579943]]. Action = [[ 0.19198304 -0.05002958  0.24564049 -0.64078593]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 2934 is [False, True, False, True, False, False]
Current timestep = 2935. State = [[ 0.01865691 -0.22666135]]. Action = [[ 0.24716526  0.02387959 -0.23054661 -0.1974042 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 2935 is [False, True, False, True, False, False]
Current timestep = 2936. State = [[ 0.0206733  -0.22703369]]. Action = [[ 0.02839208 -0.21628837 -0.12953989  0.6237788 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 2936 is [False, True, False, True, False, False]
Current timestep = 2937. State = [[ 0.02198483 -0.22977568]]. Action = [[ 0.02277061  0.13452402  0.01747236 -0.8103763 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 2937 is [False, True, False, True, False, False]
Current timestep = 2938. State = [[ 0.02314413 -0.22976364]]. Action = [[-0.05888131  0.04694757 -0.1916995   0.03312564]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 2938 is [False, True, False, True, False, False]
Current timestep = 2939. State = [[ 0.02338558 -0.22968014]]. Action = [[ 0.16611695  0.05550295 -0.19037084  0.14379394]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 2939 is [False, True, False, True, False, False]
Current timestep = 2940. State = [[ 0.02466805 -0.2295233 ]]. Action = [[ 0.01964161 -0.19823298 -0.22967413  0.26826048]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 2940 is [False, True, False, True, False, False]
Current timestep = 2941. State = [[ 0.02662589 -0.2309277 ]]. Action = [[ 0.24209905  0.08350283 -0.24394092  0.9428657 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 2941 is [False, True, False, True, False, False]
Current timestep = 2942. State = [[ 0.03058349 -0.23065099]]. Action = [[ 0.10321429 -0.02548835  0.16906154 -0.77719957]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 2942 is [False, True, False, True, False, False]
Scene graph at timestep 2942 is [False, True, False, True, False, False]
State prediction error at timestep 2942 is tensor(7.3494e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2943. State = [[ 0.03522864 -0.23090334]]. Action = [[-0.21494584 -0.20118615 -0.10519785 -0.08172905]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 2943 is [False, True, False, True, False, False]
Current timestep = 2944. State = [[ 0.03686199 -0.23403539]]. Action = [[-0.17632215 -0.191553    0.19326782 -0.8496689 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 2944 is [False, True, False, True, False, False]
Scene graph at timestep 2944 is [False, True, False, True, False, False]
State prediction error at timestep 2944 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2945. State = [[ 0.03616462 -0.23870656]]. Action = [[ 0.07963458  0.22380313 -0.05214931 -0.14495295]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 2945 is [False, True, False, True, False, False]
Scene graph at timestep 2945 is [False, True, False, True, False, False]
State prediction error at timestep 2945 is tensor(1.4833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2946. State = [[ 0.0362778 -0.2385704]]. Action = [[ 0.19182509 -0.00746486  0.22010773  0.3477564 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 2946 is [False, True, False, True, False, False]
Human Feedback received at timestep 2946 of -1
Current timestep = 2947. State = [[ 0.03745474 -0.23860101]]. Action = [[ 0.02778774  0.11257124 -0.19757378 -0.4873128 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 2947 is [False, True, False, True, False, False]
Current timestep = 2948. State = [[ 0.03921691 -0.23816775]]. Action = [[-0.2329221   0.1265066   0.14324015 -0.53602827]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 2948 is [False, True, False, True, False, False]
Current timestep = 2949. State = [[ 0.03977792 -0.23707399]]. Action = [[ 0.12426209 -0.11958426 -0.0398643  -0.608841  ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 2949 is [False, True, False, True, False, False]
Current timestep = 2950. State = [[ 0.04038313 -0.2372791 ]]. Action = [[ 0.07959303 -0.01144621 -0.12268513  0.58532596]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 2950 is [False, True, False, True, False, False]
Scene graph at timestep 2950 is [False, True, False, True, False, False]
State prediction error at timestep 2950 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Current timestep = 2951. State = [[ 0.04105886 -0.23726168]]. Action = [[-0.23309968 -0.1801465   0.07959789  0.6795969 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 2951 is [False, True, False, True, False, False]
Current timestep = 2952. State = [[ 0.04060588 -0.23934782]]. Action = [[-0.20556761  0.02132335 -0.22620553  0.8737569 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 2952 is [False, True, False, True, False, False]
Current timestep = 2953. State = [[ 0.04027535 -0.24036543]]. Action = [[ 0.08333927  0.23741454 -0.19871292 -0.921559  ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 2953 is [False, True, False, True, False, False]
Current timestep = 2954. State = [[ 0.04038607 -0.23842281]]. Action = [[ 0.04085344  0.16907316 -0.22230941  0.55083704]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 2954 is [False, True, False, True, False, False]
Current timestep = 2955. State = [[ 0.04052379 -0.23576641]]. Action = [[ 0.2116521   0.09147525 -0.1616167   0.418252  ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 2955 is [False, True, False, True, False, False]
Scene graph at timestep 2955 is [False, True, False, True, False, False]
State prediction error at timestep 2955 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 2956. State = [[ 0.0409232 -0.2330528]]. Action = [[ 0.16724616 -0.16976425 -0.08347216  0.7190006 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 2956 is [False, True, False, True, False, False]
Current timestep = 2957. State = [[ 0.04172122 -0.23296131]]. Action = [[ 0.24020296  0.16726857 -0.0016983  -0.28658253]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 2957 is [False, True, False, True, False, False]
Scene graph at timestep 2957 is [False, True, False, True, False, False]
State prediction error at timestep 2957 is tensor(1.3403e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2958. State = [[ 0.04411865 -0.22999136]]. Action = [[ 0.01527327  0.17424858 -0.17821182  0.10869682]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 2958 is [False, True, False, True, False, False]
Current timestep = 2959. State = [[ 0.04608807 -0.22616462]]. Action = [[ 0.14983463  0.16762617  0.13597083 -0.9163917 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 2959 is [False, True, False, True, False, False]
Current timestep = 2960. State = [[ 0.04897382 -0.22148693]]. Action = [[ 0.05137986  0.21898448 -0.18514855  0.6707928 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 2960 is [False, True, False, True, False, False]
Scene graph at timestep 2960 is [False, True, False, True, False, False]
State prediction error at timestep 2960 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 2961. State = [[ 0.0532965  -0.21586363]]. Action = [[ 0.08728504  0.19450897  0.02544206 -0.14962709]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 2961 is [False, True, False, True, False, False]
Current timestep = 2962. State = [[ 0.05833084 -0.20993008]]. Action = [[ 0.19594067 -0.08475441  0.24405807  0.3655839 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 2962 is [False, False, True, True, False, False]
Current timestep = 2963. State = [[ 0.0626342  -0.20741501]]. Action = [[ 0.00738284  0.03087071 -0.05079544  0.8280512 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 2963 is [False, False, True, True, False, False]
Current timestep = 2964. State = [[ 0.06631248 -0.20586488]]. Action = [[0.17420447 0.10725471 0.19887555 0.13689637]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 2964 is [False, False, True, True, False, False]
Current timestep = 2965. State = [[ 0.06897167 -0.20479308]]. Action = [[-0.22134109 -0.05810219 -0.07698235  0.63472986]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 2965 is [False, False, True, True, False, False]
Current timestep = 2966. State = [[ 0.06895997 -0.20462689]]. Action = [[-0.19460908 -0.01184891  0.19686633  0.11120009]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 2966 is [False, False, True, True, False, False]
Current timestep = 2967. State = [[ 0.06845417 -0.20451407]]. Action = [[-0.21492772  0.06272787 -0.19893946 -0.721816  ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 2967 is [False, False, True, True, False, False]
Scene graph at timestep 2967 is [False, False, True, True, False, False]
State prediction error at timestep 2967 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 2968. State = [[ 0.06805466 -0.2044689 ]]. Action = [[ 0.18396986 -0.18360233 -0.03719112  0.22919798]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 2968 is [False, False, True, True, False, False]
Current timestep = 2969. State = [[ 0.06777684 -0.20457348]]. Action = [[0.22851938 0.01387772 0.21394956 0.19059157]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 2969 is [False, False, True, True, False, False]
Current timestep = 2970. State = [[ 0.06772208 -0.20475137]]. Action = [[-0.00288565 -0.23072344 -0.10925946  0.23302507]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 2970 is [False, False, True, True, False, False]
Human Feedback received at timestep 2970 of 1
Current timestep = 2971. State = [[ 0.06772208 -0.20475137]]. Action = [[ 0.05774847 -0.02527802  0.1306113  -0.8817539 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 2971 is [False, False, True, True, False, False]
Scene graph at timestep 2971 is [False, False, True, True, False, False]
State prediction error at timestep 2971 is tensor(7.5539e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2972. State = [[ 0.06772208 -0.20475137]]. Action = [[ 0.23922569  0.08768752  0.24025309 -0.53489435]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 2972 is [False, False, True, True, False, False]
Current timestep = 2973. State = [[ 0.06774461 -0.20443262]]. Action = [[ 0.13208738  0.15796256  0.155595   -0.9094822 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 2973 is [False, False, True, True, False, False]
Current timestep = 2974. State = [[ 0.06774461 -0.20443262]]. Action = [[ 0.17433321 -0.23214042 -0.10697801 -0.8898664 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 2974 is [False, False, True, True, False, False]
Scene graph at timestep 2974 is [False, False, True, True, False, False]
State prediction error at timestep 2974 is tensor(9.9144e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2975. State = [[ 0.06770825 -0.2045508 ]]. Action = [[ 0.218342    0.18882221  0.0976955  -0.09301966]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 2975 is [False, False, True, True, False, False]
Current timestep = 2976. State = [[ 0.06764528 -0.2047799 ]]. Action = [[-0.22265793 -0.24592912  0.11639413 -0.83154005]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 2976 is [False, False, True, True, False, False]
Scene graph at timestep 2976 is [False, False, True, True, False, False]
State prediction error at timestep 2976 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2977. State = [[ 0.06669103 -0.2080077 ]]. Action = [[ 0.22872603  0.04716751  0.20945054 -0.01948643]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 2977 is [False, False, True, True, False, False]
Current timestep = 2978. State = [[ 0.06595153 -0.20949902]]. Action = [[-0.16651648  0.1266926   0.01928884 -0.8798761 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 2978 is [False, False, True, True, False, False]
Scene graph at timestep 2978 is [False, False, True, True, False, False]
State prediction error at timestep 2978 is tensor(8.3942e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2979. State = [[ 0.06549522 -0.2088986 ]]. Action = [[-0.06507464  0.08631516 -0.01035325 -0.3125955 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 2979 is [False, False, True, True, False, False]
Current timestep = 2980. State = [[ 0.06501196 -0.20821483]]. Action = [[-0.17862633  0.21758527  0.21134192  0.32509434]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 2980 is [False, False, True, True, False, False]
Scene graph at timestep 2980 is [False, False, True, True, False, False]
State prediction error at timestep 2980 is tensor(1.0979e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2981. State = [[ 0.06344874 -0.20522153]]. Action = [[ 0.18257901 -0.02569391  0.19429019  0.801092  ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 2981 is [False, False, True, True, False, False]
Current timestep = 2982. State = [[ 0.0618754 -0.2030894]]. Action = [[-0.12010065  0.00275719  0.21842524 -0.7906962 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 2982 is [False, False, True, True, False, False]
Current timestep = 2983. State = [[ 0.05994642 -0.20228022]]. Action = [[0.20021695 0.24447364 0.0121268  0.93398297]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 2983 is [False, False, True, True, False, False]
Current timestep = 2984. State = [[ 0.05878365 -0.20151262]]. Action = [[ 0.03932843  0.11819705 -0.1046401  -0.7241715 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 2984 is [False, False, True, True, False, False]
Current timestep = 2985. State = [[ 0.0582387  -0.19928579]]. Action = [[-0.21781382  0.17061704 -0.1793783   0.56096566]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 2985 is [False, False, True, True, False, False]
Current timestep = 2986. State = [[ 0.05634402 -0.19602525]]. Action = [[ 0.14503235  0.02442092 -0.09460509 -0.94936496]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 2986 is [False, False, True, True, False, False]
Scene graph at timestep 2986 is [False, False, True, True, False, False]
State prediction error at timestep 2986 is tensor(7.5383e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2987. State = [[ 0.05407458 -0.192458  ]]. Action = [[-0.0088665   0.11794889 -0.0886023   0.735255  ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 2987 is [False, False, True, True, False, False]
Scene graph at timestep 2987 is [False, False, True, True, False, False]
State prediction error at timestep 2987 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 2988. State = [[ 0.05292211 -0.18861929]]. Action = [[-0.04620448  0.22791666  0.17800719  0.94443905]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 2988 is [False, False, True, True, False, False]
Current timestep = 2989. State = [[ 0.05260326 -0.18309474]]. Action = [[0.02825174 0.00620127 0.01658383 0.11925054]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 2989 is [False, False, True, True, False, False]
Scene graph at timestep 2989 is [False, False, True, True, False, False]
State prediction error at timestep 2989 is tensor(2.0024e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2990. State = [[ 0.05263266 -0.17922553]]. Action = [[ 0.09021786  0.16986376 -0.1994143   0.8539791 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 2990 is [False, False, True, True, False, False]
Scene graph at timestep 2990 is [False, False, True, True, False, False]
State prediction error at timestep 2990 is tensor(5.7473e-05, grad_fn=<MseLossBackward0>)
Current timestep = 2991. State = [[ 0.05281498 -0.17474298]]. Action = [[0.15780675 0.21479142 0.21665424 0.48321497]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 2991 is [False, False, True, True, False, False]
Current timestep = 2992. State = [[ 0.05303056 -0.16876528]]. Action = [[ 0.06066149 -0.11510053  0.19778565  0.4712373 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 2992 is [False, False, True, True, False, False]
Current timestep = 2993. State = [[ 0.05313685 -0.16660582]]. Action = [[ 0.0468812  -0.00223354 -0.04405174 -0.9247151 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 2993 is [False, False, True, True, False, False]
Current timestep = 2994. State = [[ 0.05317742 -0.16558646]]. Action = [[ 0.1496383  -0.1366489  -0.10789815 -0.72831285]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 2994 is [False, False, True, True, False, False]
Current timestep = 2995. State = [[ 0.05306587 -0.16580482]]. Action = [[ 0.05912945 -0.06032741 -0.1994898   0.6640589 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 2995 is [False, False, True, True, False, False]
Human Feedback received at timestep 2995 of 1
Current timestep = 2996. State = [[ 0.0531478  -0.16602392]]. Action = [[-0.08407342 -0.08581603 -0.04377037 -0.58796984]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 2996 is [False, False, True, True, False, False]
Current timestep = 2997. State = [[ 0.05310237 -0.16677181]]. Action = [[ 0.0784626   0.18735161 -0.21916936 -0.9160224 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 2997 is [False, False, True, True, False, False]
Current timestep = 2998. State = [[ 0.05324499 -0.16546899]]. Action = [[ 0.18835163  0.1550346  -0.15691167  0.553462  ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 2998 is [False, False, True, True, False, False]
Scene graph at timestep 2998 is [False, False, True, True, False, False]
State prediction error at timestep 2998 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Current timestep = 2999. State = [[ 0.05381358 -0.16335268]]. Action = [[-0.22528738  0.12044761  0.16132683 -0.55194426]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 2999 is [False, False, True, True, False, False]
Current timestep = 3000. State = [[ 0.05386191 -0.16066042]]. Action = [[ 0.16216862 -0.0204421   0.21167421  0.07561076]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 3000 is [False, False, True, True, False, False]
Current timestep = 3001. State = [[ 0.05392618 -0.15873738]]. Action = [[-0.09203638  0.20652464  0.02037913  0.8366704 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 3001 is [False, False, True, True, False, False]
Current timestep = 3002. State = [[ 0.05404398 -0.15516002]]. Action = [[ 0.22815895  0.08853608  0.00890872 -0.35371637]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 3002 is [False, False, True, True, False, False]
Current timestep = 3003. State = [[ 0.05471674 -0.15186106]]. Action = [[ 0.24210066 -0.10300374 -0.16219422  0.3452083 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 3003 is [False, False, True, True, False, False]
Scene graph at timestep 3003 is [False, False, True, True, False, False]
State prediction error at timestep 3003 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Current timestep = 3004. State = [[ 0.05652501 -0.15072826]]. Action = [[ 0.08032578 -0.15359712  0.23195079 -0.698583  ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 3004 is [False, False, True, True, False, False]
Scene graph at timestep 3004 is [False, False, True, True, False, False]
State prediction error at timestep 3004 is tensor(6.5467e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3005. State = [[ 0.05805703 -0.15055758]]. Action = [[-0.13248459  0.04537743  0.18894058  0.3422544 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 3005 is [False, False, True, True, False, False]
Current timestep = 3006. State = [[ 0.05838009 -0.15026069]]. Action = [[ 0.10322541  0.2416588  -0.13335104 -0.6116027 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 3006 is [False, False, True, True, False, False]
Current timestep = 3007. State = [[ 0.05932793 -0.1476567 ]]. Action = [[-0.18975805  0.12497753 -0.22353321  0.63345504]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 3007 is [False, False, True, True, False, False]
Current timestep = 3008. State = [[ 0.05947162 -0.14464067]]. Action = [[-0.2464653   0.07549965 -0.08584443 -0.5799587 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 3008 is [False, False, True, True, False, False]
Current timestep = 3009. State = [[ 0.0594266  -0.14168414]]. Action = [[-0.1860404   0.23015875  0.2060234   0.39607346]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 3009 is [False, False, True, True, False, False]
Scene graph at timestep 3009 is [False, False, True, True, False, False]
State prediction error at timestep 3009 is tensor(3.8596e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3010. State = [[ 0.05928395 -0.13590166]]. Action = [[ 0.16114402  0.10574371 -0.17431106  0.77739596]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 3010 is [False, False, True, True, False, False]
Scene graph at timestep 3010 is [False, False, True, True, False, False]
State prediction error at timestep 3010 is tensor(4.3562e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3011. State = [[-0.25138733 -0.06286942]]. Action = [[0.12252897 0.2043941  0.0097875  0.93730235]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 3011 is [False, False, True, True, False, False]
Human Feedback received at timestep 3011 of 1
Current timestep = 3012. State = [[-0.2492002  -0.06762961]]. Action = [[ 0.15635824  0.13433397 -0.20447126  0.16273022]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 3012 is [True, False, False, False, True, False]
Current timestep = 3013. State = [[-0.24825472 -0.06767173]]. Action = [[-0.06707752  0.1168806  -0.15112753  0.22670221]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 3013 is [True, False, False, False, True, False]
Current timestep = 3014. State = [[-0.2479068  -0.06659219]]. Action = [[ 0.16323513  0.20909637 -0.01688212  0.01640534]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 3014 is [True, False, False, False, True, False]
Current timestep = 3015. State = [[-0.24631642 -0.06424247]]. Action = [[-0.1060212  -0.24722132  0.04334643 -0.621114  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3015 is [True, False, False, False, True, False]
Scene graph at timestep 3015 is [True, False, False, False, True, False]
State prediction error at timestep 3015 is tensor(3.9338e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3016. State = [[-0.24574569 -0.06436254]]. Action = [[0.20503461 0.15154728 0.14241844 0.802935  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 3016 is [True, False, False, False, True, False]
Current timestep = 3017. State = [[-0.24345468 -0.06415216]]. Action = [[-0.07064864 -0.13689163 -0.18996687  0.87217355]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 3017 is [True, False, False, False, True, False]
Current timestep = 3018. State = [[-0.24174169 -0.06434206]]. Action = [[-0.05882233  0.16795507  0.2336027   0.5481243 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 3018 is [True, False, False, False, True, False]
Current timestep = 3019. State = [[-0.24108587 -0.06376996]]. Action = [[ 0.22245109 -0.11290333 -0.01598406 -0.69835955]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 3019 is [True, False, False, False, True, False]
Scene graph at timestep 3019 is [True, False, False, False, True, False]
State prediction error at timestep 3019 is tensor(7.5070e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3020. State = [[-0.23880213 -0.06406431]]. Action = [[-0.15807943 -0.225269    0.14304522 -0.56435734]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 3020 is [True, False, False, False, True, False]
Current timestep = 3021. State = [[-0.23832181 -0.06567112]]. Action = [[-0.12622432 -0.10375056 -0.1024811  -0.9627535 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 3021 is [True, False, False, False, True, False]
Scene graph at timestep 3021 is [True, False, False, False, True, False]
State prediction error at timestep 3021 is tensor(1.8350e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3022. State = [[-0.23855166 -0.06776553]]. Action = [[-0.20906322 -0.05735163  0.12605533  0.88089406]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 3022 is [True, False, False, False, True, False]
Current timestep = 3023. State = [[-0.23928767 -0.06982386]]. Action = [[ 0.16447878  0.1821813  -0.20322041  0.61220944]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 3023 is [True, False, False, False, True, False]
Scene graph at timestep 3023 is [True, False, False, False, True, False]
State prediction error at timestep 3023 is tensor(2.0518e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3024. State = [[-0.23940201 -0.06965597]]. Action = [[ 0.17045444 -0.05937679  0.10047853  0.4218719 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 3024 is [True, False, False, False, True, False]
Scene graph at timestep 3024 is [True, False, False, False, True, False]
State prediction error at timestep 3024 is tensor(2.1307e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3025. State = [[-0.23922834 -0.06961127]]. Action = [[ 0.18682253  0.19999039  0.14029875 -0.33529496]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 3025 is [True, False, False, False, True, False]
Scene graph at timestep 3025 is [True, False, False, False, True, False]
State prediction error at timestep 3025 is tensor(9.0572e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3026. State = [[-0.2375858  -0.06797998]]. Action = [[ 0.13579047  0.23880821 -0.13436645  0.30486   ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 3026 is [True, False, False, False, True, False]
Scene graph at timestep 3026 is [True, False, False, False, True, False]
State prediction error at timestep 3026 is tensor(2.7127e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3027. State = [[-0.23549901 -0.0646851 ]]. Action = [[-0.02754019 -0.16583253  0.157422    0.6123209 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 3027 is [True, False, False, False, True, False]
Current timestep = 3028. State = [[-0.23456125 -0.06456099]]. Action = [[-0.07905999 -0.02422464  0.20883793  0.69306004]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 3028 is [True, False, False, False, True, False]
Current timestep = 3029. State = [[-0.23449704 -0.06466606]]. Action = [[ 0.21380493 -0.23593037  0.18723196 -0.4002233 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 3029 is [True, False, False, False, True, False]
Current timestep = 3030. State = [[-0.23293121 -0.06617109]]. Action = [[-0.16719408  0.06998336  0.03981906 -0.6768512 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 3030 is [True, False, False, False, True, False]
Current timestep = 3031. State = [[-0.23284934 -0.06646869]]. Action = [[-0.16521874 -0.01791808  0.08597463  0.01476574]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 3031 is [True, False, False, False, True, False]
Current timestep = 3032. State = [[-0.23288567 -0.06669316]]. Action = [[ 0.13363379 -0.18005748 -0.08998224  0.5247412 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 3032 is [True, False, False, False, True, False]
Scene graph at timestep 3032 is [True, False, False, False, True, False]
State prediction error at timestep 3032 is tensor(6.4174e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3033. State = [[-0.23251519 -0.06869472]]. Action = [[0.18330112 0.07120755 0.14319491 0.5160527 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 3033 is [True, False, False, False, True, False]
Scene graph at timestep 3033 is [True, False, False, False, True, False]
State prediction error at timestep 3033 is tensor(6.4320e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3034. State = [[-0.23104143 -0.06928384]]. Action = [[-0.20842212 -0.22611701 -0.20054612 -0.00523621]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 3034 is [True, False, False, False, True, False]
Current timestep = 3035. State = [[-0.23103772 -0.07245344]]. Action = [[-0.03671592  0.23914701  0.18672892 -0.48463607]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 3035 is [True, False, False, False, True, False]
Current timestep = 3036. State = [[-0.23107596 -0.07228757]]. Action = [[ 0.16945109 -0.17499591  0.01065797 -0.737711  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 3036 is [True, False, False, False, True, False]
Scene graph at timestep 3036 is [True, False, False, False, True, False]
State prediction error at timestep 3036 is tensor(2.6715e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3037. State = [[-0.23011419 -0.07352453]]. Action = [[ 0.24264252  0.11126411 -0.04253896 -0.20247394]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 3037 is [True, False, False, False, True, False]
Current timestep = 3038. State = [[-0.22703385 -0.07375387]]. Action = [[-0.04573357  0.23508209  0.21452743  0.8758521 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 3038 is [True, False, False, False, True, False]
Current timestep = 3039. State = [[-0.22432865 -0.07241882]]. Action = [[ 0.14190525  0.22272113 -0.0462251   0.5543672 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 3039 is [True, False, False, False, True, False]
Current timestep = 3040. State = [[-0.22139296 -0.06897542]]. Action = [[ 0.03281069 -0.15208231  0.09573144  0.06141877]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 3040 is [True, False, False, False, True, False]
Current timestep = 3041. State = [[-0.21913198 -0.06853098]]. Action = [[ 0.04902393 -0.19602525 -0.19800693 -0.41721082]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 3041 is [True, False, False, False, True, False]
Current timestep = 3042. State = [[-0.21711677 -0.06990916]]. Action = [[-0.16331829 -0.23061441 -0.099277    0.28816915]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 3042 is [True, False, False, False, True, False]
Scene graph at timestep 3042 is [True, False, False, False, True, False]
State prediction error at timestep 3042 is tensor(2.8286e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3043. State = [[-0.21693411 -0.07257636]]. Action = [[-0.17583191  0.20432961  0.2031222   0.6210114 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 3043 is [True, False, False, False, True, False]
Scene graph at timestep 3043 is [True, False, False, False, True, False]
State prediction error at timestep 3043 is tensor(1.7341e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3044. State = [[-0.21687533 -0.07259679]]. Action = [[-0.15257518  0.00141722 -0.14763257 -0.5911384 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 3044 is [True, False, False, False, True, False]
Scene graph at timestep 3044 is [True, False, False, False, True, False]
State prediction error at timestep 3044 is tensor(4.0697e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3045. State = [[-0.21686491 -0.07264989]]. Action = [[ 0.10371208  0.07051256 -0.08141783  0.3776797 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 3045 is [True, False, False, False, True, False]
Current timestep = 3046. State = [[-0.21694662 -0.07210563]]. Action = [[-0.21599163  0.2229976   0.12177187  0.20485699]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 3046 is [True, False, False, False, True, False]
Scene graph at timestep 3046 is [True, False, False, False, True, False]
State prediction error at timestep 3046 is tensor(1.1164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3047. State = [[-0.21777675 -0.06899174]]. Action = [[-0.2263591   0.22191966  0.09049648  0.7778108 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 3047 is [True, False, False, False, True, False]
Current timestep = 3048. State = [[-0.22041559 -0.06452745]]. Action = [[-0.05652547  0.00796449 -0.02801755 -0.6962057 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 3048 is [True, False, False, False, True, False]
Current timestep = 3049. State = [[-0.22261424 -0.06145615]]. Action = [[-0.05204187  0.23012501  0.15415564 -0.6309549 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 3049 is [True, False, False, False, True, False]
Current timestep = 3050. State = [[-0.22531047 -0.05672212]]. Action = [[-0.20748803 -0.17156723 -0.1562455   0.23432314]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 3050 is [True, False, False, False, True, False]
Current timestep = 3051. State = [[-0.22921672 -0.05569227]]. Action = [[-0.18491828 -0.17728981 -0.08709928 -0.975216  ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 3051 is [True, False, False, False, True, False]
Current timestep = 3052. State = [[-0.23398285 -0.05672803]]. Action = [[-0.11508466  0.10565871 -0.15650569 -0.1614989 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 3052 is [True, False, False, False, True, False]
Scene graph at timestep 3052 is [True, False, False, False, True, False]
State prediction error at timestep 3052 is tensor(2.7161e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3053. State = [[-0.23874518 -0.05596297]]. Action = [[-0.20080492  0.05558109 -0.07381581 -0.9015597 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 3053 is [True, False, False, False, True, False]
Human Feedback received at timestep 3053 of 1
Current timestep = 3054. State = [[-0.24324054 -0.0553948 ]]. Action = [[ 0.01066455 -0.19326483 -0.19670826 -0.17059636]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 3054 is [True, False, False, False, True, False]
Scene graph at timestep 3054 is [True, False, False, False, True, False]
State prediction error at timestep 3054 is tensor(1.9151e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3055. State = [[-0.24596919 -0.05678187]]. Action = [[ 0.1879335   0.17649543 -0.20175032 -0.5312974 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 3055 is [True, False, False, False, True, False]
Current timestep = 3056. State = [[-0.24574628 -0.05628002]]. Action = [[-0.1657299  -0.04410261 -0.08260919 -0.9556921 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 3056 is [True, False, False, False, True, False]
Current timestep = 3057. State = [[-0.2465425  -0.05620764]]. Action = [[ 0.1593875  -0.15584497  0.21051085  0.4718666 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 3057 is [True, False, False, False, True, False]
Human Feedback received at timestep 3057 of -1
Current timestep = 3058. State = [[-0.24671812 -0.05738292]]. Action = [[-0.20205541 -0.20213792  0.18239194  0.8059988 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 3058 is [True, False, False, False, True, False]
Current timestep = 3059. State = [[-0.24807811 -0.06049143]]. Action = [[ 0.14287812 -0.19475591 -0.21948984 -0.07661343]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 3059 is [True, False, False, False, True, False]
Current timestep = 3060. State = [[-0.24857277 -0.06496695]]. Action = [[-0.094578    0.01872665  0.07668021 -0.00597894]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 3060 is [True, False, False, False, True, False]
Scene graph at timestep 3060 is [True, False, False, False, True, False]
State prediction error at timestep 3060 is tensor(1.2417e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3061. State = [[-0.24946268 -0.06818814]]. Action = [[ 0.1539635  -0.15076233 -0.12393352  0.01574481]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 3061 is [True, False, False, False, True, False]
Current timestep = 3062. State = [[-0.25015566 -0.07154349]]. Action = [[-0.22303818 -0.17379428 -0.07441548 -0.25121087]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 3062 is [True, False, False, False, True, False]
Scene graph at timestep 3062 is [True, False, False, False, True, False]
State prediction error at timestep 3062 is tensor(1.1960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3063. State = [[-0.25139558 -0.07678465]]. Action = [[-0.09641838 -0.19929999  0.19191274 -0.04240984]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 3063 is [True, False, False, False, True, False]
Current timestep = 3064. State = [[-0.25300843 -0.08210332]]. Action = [[0.15432447 0.19668573 0.10485366 0.54875624]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 3064 is [True, False, False, False, True, False]
Current timestep = 3065. State = [[-0.25354022 -0.08357739]]. Action = [[-0.14953181  0.03042921 -0.21178289  0.20487332]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 3065 is [True, False, False, False, True, False]
Scene graph at timestep 3065 is [True, False, False, False, True, False]
State prediction error at timestep 3065 is tensor(1.7012e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3065 of -1
Current timestep = 3066. State = [[-0.25419974 -0.08389736]]. Action = [[-0.03360821 -0.03366575 -0.20617531 -0.39174628]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 3066 is [True, False, False, False, True, False]
Current timestep = 3067. State = [[-0.25482893 -0.08418825]]. Action = [[-0.1100232   0.21445811 -0.03813359  0.95964515]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 3067 is [True, False, False, False, True, False]
Current timestep = 3068. State = [[-0.25604331 -0.08306663]]. Action = [[ 0.0394899  -0.11415119  0.24180079 -0.8588875 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 3068 is [True, False, False, False, True, False]
Current timestep = 3069. State = [[-0.25674897 -0.08298307]]. Action = [[-0.04800731  0.09817392  0.09930372  0.278692  ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 3069 is [True, False, False, False, True, False]
Current timestep = 3070. State = [[-0.25773838 -0.08262423]]. Action = [[ 0.11477053  0.03477433 -0.17139159 -0.7077297 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 3070 is [True, False, False, False, True, False]
Current timestep = 3071. State = [[-0.25826705 -0.08220343]]. Action = [[0.23384011 0.07136288 0.08222145 0.75227094]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 3071 is [True, False, False, False, True, False]
Current timestep = 3072. State = [[-0.25807884 -0.08090431]]. Action = [[-0.04615471  0.1893242  -0.05957007  0.9120946 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 3072 is [True, False, False, False, True, False]
Current timestep = 3073. State = [[-0.258002   -0.07815783]]. Action = [[-0.19940896 -0.17823128  0.12589613  0.5453992 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 3073 is [True, False, False, False, True, False]
Current timestep = 3074. State = [[-0.25844336 -0.07797007]]. Action = [[ 0.14229631 -0.11527202 -0.16081765  0.11452913]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 3074 is [True, False, False, False, True, False]
Current timestep = 3075. State = [[-0.25837448 -0.07856653]]. Action = [[-0.03174949 -0.19166423  0.03876072  0.9934044 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 3075 is [True, False, False, False, True, False]
Current timestep = 3076. State = [[-0.25854212 -0.08036795]]. Action = [[ 0.02879861  0.01559046  0.01928717 -0.32442307]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 3076 is [True, False, False, False, True, False]
Current timestep = 3077. State = [[-0.25872263 -0.08146446]]. Action = [[-0.2001396  -0.2236786  -0.15988027 -0.7869983 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 3077 is [True, False, False, False, True, False]
Current timestep = 3078. State = [[-0.2595913  -0.08507645]]. Action = [[-0.17879385 -0.12520391 -0.20003535  0.54550004]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 3078 is [True, False, False, False, True, False]
Current timestep = 3079. State = [[-0.26196855 -0.08937655]]. Action = [[0.19264713 0.19098538 0.02297819 0.77876127]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 3079 is [True, False, False, False, True, False]
Scene graph at timestep 3079 is [True, False, False, False, True, False]
State prediction error at timestep 3079 is tensor(3.3938e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3080. State = [[-0.26244    -0.09021915]]. Action = [[-0.11353789 -0.11056355 -0.17057303 -0.05575216]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 3080 is [True, False, False, False, True, False]
Current timestep = 3081. State = [[-0.26352176 -0.09169529]]. Action = [[ 0.05311176 -0.21556771  0.1599378   0.24087858]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 3081 is [True, False, False, False, True, False]
Current timestep = 3082. State = [[-0.2642582  -0.09479426]]. Action = [[-0.11343575  0.1459344  -0.23801363  0.75517344]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 3082 is [True, False, False, False, True, False]
Current timestep = 3083. State = [[-0.26559594 -0.09561275]]. Action = [[-0.05592971  0.01054022 -0.12804739  0.26016068]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 3083 is [True, False, False, False, True, False]
Current timestep = 3084. State = [[-0.26699826 -0.09598005]]. Action = [[-0.04013051  0.111462    0.23064113 -0.7228708 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 3084 is [True, False, False, False, True, False]
Current timestep = 3085. State = [[-0.26824448 -0.09536636]]. Action = [[ 0.17536333  0.22504735 -0.19679004  0.55537117]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 3085 is [True, False, False, False, True, False]
Current timestep = 3086. State = [[-0.2683807  -0.09282345]]. Action = [[ 0.17566067  0.20682201 -0.04179245  0.5577198 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 3086 is [True, False, False, False, True, False]
Human Feedback received at timestep 3086 of -1
Current timestep = 3087. State = [[-0.26816347 -0.08880681]]. Action = [[ 0.16677183 -0.09309831 -0.09963398 -0.0733977 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 3087 is [True, False, False, False, True, False]
Scene graph at timestep 3087 is [True, False, False, False, True, False]
State prediction error at timestep 3087 is tensor(2.6299e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3088. State = [[-0.26758263 -0.08719773]]. Action = [[-0.17295414  0.08369493  0.07668054 -0.7109665 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 3088 is [True, False, False, False, True, False]
Current timestep = 3089. State = [[-0.267271   -0.08590163]]. Action = [[ 0.16269872 -0.15409288 -0.05512743 -0.26297724]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 3089 is [True, False, False, False, True, False]
Scene graph at timestep 3089 is [True, False, False, False, True, False]
State prediction error at timestep 3089 is tensor(4.0420e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3090. State = [[-0.2671307  -0.08588762]]. Action = [[-0.02902697 -0.18991344  0.00577024  0.49090958]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 3090 is [True, False, False, False, True, False]
Current timestep = 3091. State = [[-0.26646623 -0.0869751 ]]. Action = [[-0.22159526 -0.16655374  0.11712897  0.2844478 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 3091 is [True, False, False, False, True, False]
Current timestep = 3092. State = [[-0.26659364 -0.08930423]]. Action = [[0.16050273 0.21870321 0.14633274 0.89229584]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 3092 is [True, False, False, False, True, False]
Current timestep = 3093. State = [[-0.2665105  -0.08897544]]. Action = [[ 0.1831013   0.24374443 -0.17447673  0.5692476 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 3093 is [True, False, False, False, True, False]
Scene graph at timestep 3093 is [True, False, False, False, True, False]
State prediction error at timestep 3093 is tensor(1.8324e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3094. State = [[-0.265553   -0.08642489]]. Action = [[ 0.18509978  0.18449831  0.17537725 -0.4040916 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 3094 is [True, False, False, False, True, False]
Current timestep = 3095. State = [[-0.2631129  -0.08291597]]. Action = [[ 0.03319189  0.12200299  0.204045   -0.8970583 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 3095 is [True, False, False, False, True, False]
Current timestep = 3096. State = [[-0.26104224 -0.07967967]]. Action = [[ 0.2449596  -0.07309949 -0.20685045 -0.5402318 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 3096 is [True, False, False, False, True, False]
Current timestep = 3097. State = [[-0.25730452 -0.07813028]]. Action = [[-0.20466319 -0.17019215  0.01216486 -0.1484034 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 3097 is [True, False, False, False, True, False]
Current timestep = 3098. State = [[-0.25585106 -0.07828511]]. Action = [[ 0.14537579  0.13185123 -0.08680345  0.34422266]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 3098 is [True, False, False, False, True, False]
Current timestep = 3099. State = [[-0.25400057 -0.07706544]]. Action = [[-0.11304782  0.17217216 -0.12373465 -0.05477697]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 3099 is [True, False, False, False, True, False]
Current timestep = 3100. State = [[-0.2533871 -0.0741895]]. Action = [[0.1475732  0.15905583 0.10211694 0.7986833 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 3100 is [True, False, False, False, True, False]
Current timestep = 3101. State = [[-0.25193557 -0.07074347]]. Action = [[-0.16037464  0.22247177  0.14738756  0.3796898 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 3101 is [True, False, False, False, True, False]
Current timestep = 3102. State = [[-0.2517869  -0.06617965]]. Action = [[-0.02516907 -0.22894482 -0.01297058 -0.40488636]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 3102 is [True, False, False, False, True, False]
Current timestep = 3103. State = [[-0.25185096 -0.06580784]]. Action = [[ 0.00158411 -0.0105761  -0.07384196  0.18321347]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 3103 is [True, False, False, False, True, False]
Scene graph at timestep 3103 is [True, False, False, False, True, False]
State prediction error at timestep 3103 is tensor(8.3235e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3104. State = [[-0.25182837 -0.06544116]]. Action = [[0.01280358 0.01192442 0.10985628 0.6581435 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 3104 is [True, False, False, False, True, False]
Current timestep = 3105. State = [[-0.2518172  -0.06523682]]. Action = [[-0.18513036  0.10142675  0.05629066 -0.5329416 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 3105 is [True, False, False, False, True, False]
Current timestep = 3106. State = [[-0.2519905  -0.06408064]]. Action = [[-0.18585512  0.01963046 -0.13270627 -0.37253368]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 3106 is [True, False, False, False, True, False]
Current timestep = 3107. State = [[-0.25264752 -0.06293491]]. Action = [[ 0.1262232  -0.2306606   0.16276398  0.03580511]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 3107 is [True, False, False, False, True, False]
Current timestep = 3108. State = [[-0.25282243 -0.06377634]]. Action = [[-0.22976059  0.06106305 -0.13956366  0.58852243]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 3108 is [True, False, False, False, True, False]
Current timestep = 3109. State = [[-0.25383532 -0.06382418]]. Action = [[-0.23299827  0.01552343  0.09115911 -0.663229  ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 3109 is [True, False, False, False, True, False]
Current timestep = 3110. State = [[-0.2565264 -0.0639999]]. Action = [[ 0.13288641 -0.0054909  -0.12100413 -0.5688565 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 3110 is [True, False, False, False, True, False]
Current timestep = 3111. State = [[-0.25779927 -0.0641572 ]]. Action = [[ 0.19133145 -0.0633426  -0.21337631 -0.9263937 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 3111 is [True, False, False, False, True, False]
Current timestep = 3112. State = [[-0.25786388 -0.06446826]]. Action = [[-0.1077247  -0.10513307  0.08772185  0.23810542]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 3112 is [True, False, False, False, True, False]
Current timestep = 3113. State = [[-0.2580739  -0.06582686]]. Action = [[ 0.00656199 -0.00462766 -0.2181493   0.057917  ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 3113 is [True, False, False, False, True, False]
Current timestep = 3114. State = [[-0.25845766 -0.06677219]]. Action = [[-0.21868064  0.00695598 -0.04786798 -0.45432985]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 3114 is [True, False, False, False, True, False]
Current timestep = 3115. State = [[-0.2600709  -0.06741443]]. Action = [[-0.16796347  0.07059732  0.20537269  0.9938929 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 3115 is [True, False, False, False, True, False]
Scene graph at timestep 3115 is [True, False, False, False, True, False]
State prediction error at timestep 3115 is tensor(7.8164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3116. State = [[-0.2634945  -0.06728202]]. Action = [[ 0.04471016 -0.19583113 -0.23847693  0.9923215 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 3116 is [True, False, False, False, True, False]
Current timestep = 3117. State = [[-0.2653551  -0.06861608]]. Action = [[ 0.09682485  0.2142295   0.18558773 -0.4558671 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 3117 is [True, False, False, False, True, False]
Current timestep = 3118. State = [[-0.26617792 -0.06812817]]. Action = [[ 0.08634746 -0.1428529   0.09243405  0.19942188]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 3118 is [True, False, False, False, True, False]
Current timestep = 3119. State = [[-0.2663011  -0.06821995]]. Action = [[ 0.10493201  0.084692   -0.02395326 -0.3403793 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 3119 is [True, False, False, False, True, False]
Current timestep = 3120. State = [[-0.26629272 -0.0682652 ]]. Action = [[-0.15819785 -0.1427726  -0.0306388   0.27197814]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 3120 is [True, False, False, False, True, False]
Current timestep = 3121. State = [[-0.2664691  -0.06906191]]. Action = [[ 0.17473882 -0.02800027  0.11383188 -0.64995015]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 3121 is [True, False, False, False, True, False]
Scene graph at timestep 3121 is [True, False, False, False, True, False]
State prediction error at timestep 3121 is tensor(4.4728e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3122. State = [[-0.26639232 -0.06934059]]. Action = [[ 0.03076583  0.24859762 -0.08202733  0.32971478]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 3122 is [True, False, False, False, True, False]
Human Feedback received at timestep 3122 of -1
Current timestep = 3123. State = [[-0.26646033 -0.06844199]]. Action = [[ 0.15212935  0.11709794 -0.16682354  0.84606206]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 3123 is [True, False, False, False, True, False]
Current timestep = 3124. State = [[-0.2662648  -0.06692602]]. Action = [[-0.1918831  -0.08825701 -0.15111865  0.46349895]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 3124 is [True, False, False, False, True, False]
Current timestep = 3125. State = [[-0.26627317 -0.06662212]]. Action = [[-0.1685831   0.03930593 -0.03530292 -0.08946747]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 3125 is [True, False, False, False, True, False]
Current timestep = 3126. State = [[-0.26641294 -0.06629676]]. Action = [[ 0.22674197  0.13174236 -0.14865306 -0.22916079]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 3126 is [True, False, False, False, True, False]
Current timestep = 3127. State = [[-0.2664113  -0.06486735]]. Action = [[ 0.12775692 -0.20919228  0.05188024 -0.46889496]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 3127 is [True, False, False, False, True, False]
Current timestep = 3128. State = [[-0.26619235 -0.06503016]]. Action = [[ 0.1124227  -0.21691287  0.14044744  0.83951783]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 3128 is [True, False, False, False, True, False]
Current timestep = 3129. State = [[-0.2648313  -0.06698775]]. Action = [[ 0.18961212 -0.03137985 -0.23705013  0.65149474]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 3129 is [True, False, False, False, True, False]
Scene graph at timestep 3129 is [True, False, False, False, True, False]
State prediction error at timestep 3129 is tensor(4.9665e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3130. State = [[-0.26217985 -0.06818987]]. Action = [[ 0.05331054  0.20253003  0.13511255 -0.3939289 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 3130 is [True, False, False, False, True, False]
Current timestep = 3131. State = [[-0.25912136 -0.06834441]]. Action = [[ 0.23383856 -0.18669812  0.01669565  0.66942096]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 3131 is [True, False, False, False, True, False]
Scene graph at timestep 3131 is [True, False, False, False, True, False]
State prediction error at timestep 3131 is tensor(2.6213e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3132. State = [[-0.2549214  -0.06924697]]. Action = [[-0.0938102  -0.05827285  0.12900698 -0.89185333]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 3132 is [True, False, False, False, True, False]
Current timestep = 3133. State = [[-0.25274315 -0.06989931]]. Action = [[-0.02819237  0.14195564  0.00809041  0.22939074]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 3133 is [True, False, False, False, True, False]
Current timestep = 3134. State = [[-0.25206232 -0.06984327]]. Action = [[ 0.17375076 -0.15009667 -0.13508143  0.41847873]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 3134 is [True, False, False, False, True, False]
Scene graph at timestep 3134 is [True, False, False, False, True, False]
State prediction error at timestep 3134 is tensor(1.8723e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3135. State = [[-0.24977091 -0.07088162]]. Action = [[-0.09442472 -0.2008047   0.09279859  0.25429535]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 3135 is [True, False, False, False, True, False]
Current timestep = 3136. State = [[-0.24898344 -0.07326191]]. Action = [[-0.09535745  0.17611647  0.02838492 -0.5880747 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 3136 is [True, False, False, False, True, False]
Current timestep = 3137. State = [[-0.24884555 -0.07347435]]. Action = [[ 0.19367552 -0.12753774  0.21964574 -0.5937093 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 3137 is [True, False, False, False, True, False]
Scene graph at timestep 3137 is [True, False, False, False, True, False]
State prediction error at timestep 3137 is tensor(3.7375e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3138. State = [[-0.24799067 -0.0746119 ]]. Action = [[-0.04840079  0.13838848  0.24453723 -0.28860068]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 3138 is [True, False, False, False, True, False]
Current timestep = 3139. State = [[-0.2479577  -0.07428289]]. Action = [[ 0.00853381  0.21487868  0.1133191  -0.5187163 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 3139 is [True, False, False, False, True, False]
Current timestep = 3140. State = [[-0.24759372 -0.07286313]]. Action = [[-0.19862372 -0.18714322  0.15406391  0.530365  ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 3140 is [True, False, False, False, True, False]
Current timestep = 3141. State = [[-0.24747322 -0.07315628]]. Action = [[-0.02274783 -0.23647164  0.22664505 -0.8186178 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 3141 is [True, False, False, False, True, False]
Scene graph at timestep 3141 is [True, False, False, False, True, False]
State prediction error at timestep 3141 is tensor(1.6147e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3142. State = [[-0.24766992 -0.07594929]]. Action = [[ 0.15719098  0.23407483 -0.21767546 -0.823606  ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 3142 is [True, False, False, False, True, False]
Current timestep = 3143. State = [[-0.24747309 -0.07564672]]. Action = [[ 0.15090394 -0.08733673  0.19238114 -0.7928264 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 3143 is [True, False, False, False, True, False]
Current timestep = 3144. State = [[-0.2468594  -0.07565366]]. Action = [[-0.01373586  0.18547899 -0.00809243 -0.1425888 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 3144 is [True, False, False, False, True, False]
Current timestep = 3145. State = [[-0.24627212 -0.07463352]]. Action = [[-0.14083734 -0.10176972  0.22750473 -0.65190953]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 3145 is [True, False, False, False, True, False]
Current timestep = 3146. State = [[-0.24635294 -0.07480815]]. Action = [[ 0.136518   -0.17781653  0.21293443 -0.19311017]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 3146 is [True, False, False, False, True, False]
Current timestep = 3147. State = [[-0.2454633  -0.07596296]]. Action = [[ 0.14020777 -0.12862156  0.00162867 -0.6803098 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 3147 is [True, False, False, False, True, False]
Scene graph at timestep 3147 is [True, False, False, False, True, False]
State prediction error at timestep 3147 is tensor(4.7059e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3148. State = [[-0.24292463 -0.07825051]]. Action = [[ 0.19102529 -0.17817353  0.1569055  -0.94864887]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 3148 is [True, False, False, False, True, False]
Current timestep = 3149. State = [[-0.23932211 -0.08191001]]. Action = [[-0.06620795  0.21772885  0.10233405 -0.6192914 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 3149 is [True, False, False, False, True, False]
Scene graph at timestep 3149 is [True, False, False, False, True, False]
State prediction error at timestep 3149 is tensor(1.4468e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3150. State = [[-0.23803832 -0.0822632 ]]. Action = [[ 0.03439206 -0.0905463   0.24699348  0.20700157]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 3150 is [True, False, False, False, True, False]
Current timestep = 3151. State = [[-0.23755158 -0.08291432]]. Action = [[ 0.21959588  0.07358855 -0.19010678  0.9420967 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 3151 is [True, False, False, False, True, False]
Current timestep = 3152. State = [[-0.2351402  -0.08308118]]. Action = [[ 0.05398241 -0.14642817  0.10377547 -0.25007224]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 3152 is [True, False, False, False, True, False]
Current timestep = 3153. State = [[-0.23277505 -0.08443353]]. Action = [[ 0.08895189  0.20912361  0.11626446 -0.8458611 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 3153 is [True, False, False, False, True, False]
Current timestep = 3154. State = [[-0.22969884 -0.08402164]]. Action = [[ 0.16901425  0.08459434 -0.17985502  0.76831007]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 3154 is [True, False, False, False, True, False]
Human Feedback received at timestep 3154 of 1
Current timestep = 3155. State = [[-0.22541174 -0.08309389]]. Action = [[0.12880003 0.19558    0.06658292 0.7073426 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 3155 is [True, False, False, False, True, False]
Current timestep = 3156. State = [[-0.22057271 -0.08065645]]. Action = [[-0.23530278 -0.14136598 -0.03091982 -0.7374957 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 3156 is [True, False, False, False, True, False]
Scene graph at timestep 3156 is [True, False, False, False, True, False]
State prediction error at timestep 3156 is tensor(7.2047e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3157. State = [[-0.2185844  -0.08053637]]. Action = [[0.21901327 0.13887239 0.01307327 0.3688624 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 3157 is [True, False, False, False, True, False]
Scene graph at timestep 3157 is [True, False, False, False, True, False]
State prediction error at timestep 3157 is tensor(2.1950e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3158. State = [[-0.21590477 -0.07932339]]. Action = [[ 0.11815077 -0.2161481   0.06119469 -0.968071  ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 3158 is [True, False, False, False, True, False]
Current timestep = 3159. State = [[-0.2133861  -0.07994767]]. Action = [[-0.09419614  0.00885975 -0.07075837  0.5095123 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 3159 is [True, False, False, False, True, False]
Current timestep = 3160. State = [[-0.21220762 -0.08040364]]. Action = [[-0.19103248 -0.18420517  0.073724   -0.87303424]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 3160 is [True, False, False, False, True, False]
Current timestep = 3161. State = [[-0.2122673  -0.08192459]]. Action = [[-0.12687044  0.07459769  0.1394589  -0.7202577 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 3161 is [True, False, False, False, True, False]
Current timestep = 3162. State = [[-0.21246593 -0.08233894]]. Action = [[ 0.17288935 -0.03463104 -0.09104171 -0.795027  ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 3162 is [True, False, False, False, True, False]
Current timestep = 3163. State = [[-0.2125637  -0.08283908]]. Action = [[-0.20453212 -0.13466538 -0.16213536 -0.7267751 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 3163 is [True, False, False, False, True, False]
Human Feedback received at timestep 3163 of 1
Current timestep = 3164. State = [[-0.21308044 -0.08488147]]. Action = [[ 0.10745662  0.14784175 -0.23004013 -0.0192548 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 3164 is [True, False, False, False, True, False]
Scene graph at timestep 3164 is [True, False, False, False, True, False]
State prediction error at timestep 3164 is tensor(3.3976e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3165. State = [[-0.21313918 -0.08490577]]. Action = [[ 0.16560858 -0.20427237  0.15797877 -0.47910047]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 3165 is [True, False, False, False, True, False]
Current timestep = 3166. State = [[-0.21294111 -0.08645263]]. Action = [[ 0.13156494 -0.10810685  0.19364259  0.01665175]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 3166 is [True, False, False, False, True, False]
Current timestep = 3167. State = [[-0.2121665  -0.08871343]]. Action = [[-0.01792109 -0.04495288  0.09579772 -0.48777825]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 3167 is [True, False, False, False, True, False]
Scene graph at timestep 3167 is [True, False, False, False, True, False]
State prediction error at timestep 3167 is tensor(5.2808e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3168. State = [[-0.21168584 -0.09068611]]. Action = [[-0.11730114 -0.1283661  -0.16839309  0.39567816]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 3168 is [True, False, False, False, True, False]
Current timestep = 3169. State = [[-0.2116324  -0.09390073]]. Action = [[-0.19896907 -0.12069055 -0.10039683 -0.49179512]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 3169 is [True, False, False, False, True, False]
Current timestep = 3170. State = [[-0.21176808 -0.09770189]]. Action = [[-0.02422959 -0.23692702  0.10722652  0.63613594]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 3170 is [True, False, False, False, True, False]
Scene graph at timestep 3170 is [True, False, False, False, True, False]
State prediction error at timestep 3170 is tensor(3.7636e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3171. State = [[-0.21205416 -0.10354242]]. Action = [[ 0.07524881  0.03316674 -0.16798456  0.24321723]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 3171 is [True, False, False, False, True, False]
Scene graph at timestep 3171 is [True, False, False, False, True, False]
State prediction error at timestep 3171 is tensor(1.9909e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3172. State = [[-0.21185882 -0.10669902]]. Action = [[ 0.06059909 -0.20547925 -0.1556111   0.1609298 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 3172 is [True, False, False, False, True, False]
Current timestep = 3173. State = [[-0.21169688 -0.11135624]]. Action = [[-0.20461178 -0.12797827 -0.21889158 -0.08100039]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 3173 is [True, False, False, False, True, False]
Current timestep = 3174. State = [[-0.21208245 -0.11592304]]. Action = [[-0.21901028  0.06852904  0.21446872  0.67396224]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 3174 is [True, False, False, False, True, False]
Scene graph at timestep 3174 is [True, False, False, False, True, False]
State prediction error at timestep 3174 is tensor(2.5453e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3175. State = [[-0.21454085 -0.11878517]]. Action = [[ 0.22295609 -0.19571337 -0.19919328 -0.74713504]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 3175 is [True, False, False, False, True, False]
Current timestep = 3176. State = [[-0.2152451  -0.12260712]]. Action = [[-0.01431365 -0.20788008 -0.17012745 -0.24972534]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 3176 is [True, False, False, False, True, False]
Human Feedback received at timestep 3176 of -1
Current timestep = 3177. State = [[-0.21575779 -0.1306699 ]]. Action = [[ 0.20785972  0.1200698   0.2034353  -0.39775014]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 3177 is [True, False, False, False, True, False]
Current timestep = 3178. State = [[-0.21561496 -0.13124399]]. Action = [[-0.23080732  0.00603184 -0.08046408 -0.7890111 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 3178 is [True, False, False, True, False, False]
Current timestep = 3179. State = [[-0.21577342 -0.13130961]]. Action = [[-0.12698874  0.07020655 -0.04681171 -0.02133816]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 3179 is [True, False, False, True, False, False]
Current timestep = 3180. State = [[-0.21648233 -0.13131037]]. Action = [[-0.06286229 -0.12662919 -0.23555857 -0.5283028 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 3180 is [True, False, False, True, False, False]
Current timestep = 3181. State = [[-0.21745513 -0.13234834]]. Action = [[0.18225873 0.21301356 0.12906703 0.69822466]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 3181 is [True, False, False, True, False, False]
Current timestep = 3182. State = [[-0.21735272 -0.13171135]]. Action = [[ 0.06744599  0.07460427  0.1573788  -0.46742117]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 3182 is [True, False, False, True, False, False]
Scene graph at timestep 3182 is [True, False, False, True, False, False]
State prediction error at timestep 3182 is tensor(2.5095e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3183. State = [[-0.21695988 -0.13041095]]. Action = [[ 0.14531487  0.20032874 -0.04926889 -0.557342  ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 3183 is [True, False, False, True, False, False]
Scene graph at timestep 3183 is [True, False, False, True, False, False]
State prediction error at timestep 3183 is tensor(5.9313e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3184. State = [[-0.21613188 -0.12727119]]. Action = [[ 0.22955203 -0.04693738  0.169456    0.67886174]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 3184 is [True, False, False, True, False, False]
Current timestep = 3185. State = [[-0.21392192 -0.12519774]]. Action = [[0.05337629 0.17509174 0.22463745 0.2279079 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 3185 is [True, False, False, True, False, False]
Current timestep = 3186. State = [[-0.21251592 -0.12196929]]. Action = [[ 0.0353907  -0.015606   -0.18407975  0.81817484]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 3186 is [True, False, False, True, False, False]
Scene graph at timestep 3186 is [True, False, False, False, True, False]
State prediction error at timestep 3186 is tensor(6.7090e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3187. State = [[-0.21112557 -0.11993813]]. Action = [[-0.11607382  0.20139062  0.07205755  0.552349  ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 3187 is [True, False, False, False, True, False]
Current timestep = 3188. State = [[-0.21052006 -0.11700079]]. Action = [[ 0.06926629 -0.02382615  0.07485899 -0.00788683]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 3188 is [True, False, False, False, True, False]
Current timestep = 3189. State = [[-0.20993242 -0.11524162]]. Action = [[-0.22431219 -0.19960189 -0.09117979 -0.69637644]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 3189 is [True, False, False, False, True, False]
Current timestep = 3190. State = [[-0.20984043 -0.11589359]]. Action = [[ 0.19181594  0.10369867  0.03857774 -0.7077852 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 3190 is [True, False, False, False, True, False]
Scene graph at timestep 3190 is [True, False, False, False, True, False]
State prediction error at timestep 3190 is tensor(4.6079e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3191. State = [[-0.20973234 -0.11548629]]. Action = [[-0.02499969  0.1808036  -0.13429093 -0.732118  ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 3191 is [True, False, False, False, True, False]
Current timestep = 3192. State = [[-0.20954782 -0.11344241]]. Action = [[ 0.09239784 -0.06452766 -0.1326838   0.833995  ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 3192 is [True, False, False, False, True, False]
Current timestep = 3193. State = [[-0.20932749 -0.11299022]]. Action = [[-0.15615302  0.01197508  0.2094813   0.9236016 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 3193 is [True, False, False, False, True, False]
Human Feedback received at timestep 3193 of 1
Current timestep = 3194. State = [[-0.20917583 -0.11242115]]. Action = [[-0.2187897  -0.23293786  0.10837209  0.39303112]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 3194 is [True, False, False, False, True, False]
Current timestep = 3195. State = [[-0.20929082 -0.11376559]]. Action = [[ 0.02765521  0.06324819 -0.01866052  0.14647138]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 3195 is [True, False, False, False, True, False]
Current timestep = 3196. State = [[-0.20932159 -0.11414555]]. Action = [[-0.21376693 -0.11479688 -0.23218958  0.84389293]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 3196 is [True, False, False, False, True, False]
Scene graph at timestep 3196 is [True, False, False, False, True, False]
State prediction error at timestep 3196 is tensor(2.7283e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3197. State = [[-0.21121018 -0.11546259]]. Action = [[ 0.1371823   0.19218016 -0.08792758 -0.17066145]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 3197 is [True, False, False, False, True, False]
Current timestep = 3198. State = [[-0.21123117 -0.11471853]]. Action = [[0.03674424 0.11036283 0.17822987 0.93852735]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 3198 is [True, False, False, False, True, False]
Current timestep = 3199. State = [[-0.21125586 -0.11300211]]. Action = [[ 0.05913088  0.15717036 -0.2382739  -0.47981423]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 3199 is [True, False, False, False, True, False]
Current timestep = 3200. State = [[-0.21118332 -0.11029661]]. Action = [[ 0.18009347  0.12715843  0.16915777 -0.73457736]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 3200 is [True, False, False, False, True, False]
Current timestep = 3201. State = [[-0.21082953 -0.10695716]]. Action = [[ 0.03748178  0.04543364 -0.11117564 -0.30893493]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 3201 is [True, False, False, False, True, False]
Current timestep = 3202. State = [[-0.21057646 -0.10486408]]. Action = [[-0.12922578 -0.19074233 -0.03342314 -0.08826953]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 3202 is [True, False, False, False, True, False]
Current timestep = 3203. State = [[-0.21054672 -0.10487263]]. Action = [[ 0.04199573 -0.02478574 -0.19997248 -0.84411836]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 3203 is [True, False, False, False, True, False]
Current timestep = 3204. State = [[-0.21048675 -0.10483401]]. Action = [[-0.06976819  0.13964313 -0.15517378 -0.92518544]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 3204 is [True, False, False, False, True, False]
Scene graph at timestep 3204 is [True, False, False, False, True, False]
State prediction error at timestep 3204 is tensor(4.4544e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3205. State = [[-0.21051447 -0.10415418]]. Action = [[ 9.6819997e-02  1.4307100e-01 -4.5029819e-04 -8.0137885e-01]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 3205 is [True, False, False, False, True, False]
Current timestep = 3206. State = [[-0.21051845 -0.10183751]]. Action = [[ 0.04591227  0.06193608 -0.12024893 -0.46734238]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 3206 is [True, False, False, False, True, False]
Current timestep = 3207. State = [[-0.21048827 -0.09925196]]. Action = [[ 0.15396136  0.21512663 -0.03838609 -0.9078503 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 3207 is [True, False, False, False, True, False]
Scene graph at timestep 3207 is [True, False, False, False, True, False]
State prediction error at timestep 3207 is tensor(5.0986e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3208. State = [[-0.20954692 -0.09443108]]. Action = [[ 0.11687472  0.22806665 -0.05627009 -0.84574205]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 3208 is [True, False, False, False, True, False]
Current timestep = 3209. State = [[-0.20799983 -0.0880997 ]]. Action = [[0.01086804 0.1692602  0.12398961 0.8133626 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 3209 is [True, False, False, False, True, False]
Human Feedback received at timestep 3209 of 1
Current timestep = 3210. State = [[-0.20647651 -0.08181518]]. Action = [[ 0.1996854   0.09471476 -0.0088332  -0.19183844]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 3210 is [True, False, False, False, True, False]
Current timestep = 3211. State = [[-0.20328487 -0.07698931]]. Action = [[ 0.01334658 -0.18947345 -0.12033996 -0.25395095]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 3211 is [True, False, False, False, True, False]
Scene graph at timestep 3211 is [True, False, False, False, True, False]
State prediction error at timestep 3211 is tensor(1.5196e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3212. State = [[-0.20039721 -0.07567141]]. Action = [[ 0.01354644 -0.18886119 -0.20295483  0.8385315 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 3212 is [True, False, False, False, True, False]
Current timestep = 3213. State = [[-0.19832231 -0.07649311]]. Action = [[-0.02032521  0.18423852  0.05896258 -0.8078489 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 3213 is [True, False, False, False, True, False]
Current timestep = 3214. State = [[-0.19737864 -0.07581628]]. Action = [[ 0.07763815 -0.08995274 -0.10972087 -0.31145686]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 3214 is [True, False, False, False, True, False]
Current timestep = 3215. State = [[-0.19623357 -0.0756883 ]]. Action = [[-0.11642572  0.13398984 -0.14652036 -0.91154563]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 3215 is [True, False, False, False, True, False]
Current timestep = 3216. State = [[-0.19615364 -0.07485523]]. Action = [[-0.20201537 -0.12819692 -0.2239986   0.46031344]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 3216 is [True, False, False, False, True, False]
Current timestep = 3217. State = [[-0.19636609 -0.07498072]]. Action = [[-0.06842251  0.03056872  0.1842896   0.73793364]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 3217 is [True, False, False, False, True, False]
Current timestep = 3218. State = [[-0.19654238 -0.07500459]]. Action = [[-0.01058343  0.06093329 -0.08299349  0.11144567]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 3218 is [True, False, False, False, True, False]
Current timestep = 3219. State = [[-0.19683623 -0.07488675]]. Action = [[ 0.08939195 -0.17882444  0.00519952 -0.22632074]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 3219 is [True, False, False, False, True, False]
Current timestep = 3220. State = [[-0.19685477 -0.07526045]]. Action = [[ 0.0646185   0.19267416 -0.13934705 -0.9782959 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 3220 is [True, False, False, False, True, False]
Current timestep = 3221. State = [[-0.19682159 -0.07458115]]. Action = [[-0.02363257  0.22112921 -0.03909808 -0.88896585]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 3221 is [True, False, False, False, True, False]
Current timestep = 3222. State = [[-0.19696991 -0.07212861]]. Action = [[ 0.12338975 -0.18532038  0.22585669 -0.4199127 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 3222 is [True, False, False, False, True, False]
Current timestep = 3223. State = [[-0.19679333 -0.07193738]]. Action = [[ 0.176552    0.15856123 -0.20222247 -0.8361425 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 3223 is [True, False, False, False, True, False]
Current timestep = 3224. State = [[-0.19523406 -0.07037766]]. Action = [[ 0.1379211   0.02005851  0.1742611  -0.81931466]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 3224 is [True, False, False, False, True, False]
Current timestep = 3225. State = [[-0.19262017 -0.06857117]]. Action = [[ 0.08631343 -0.12676927  0.00788435  0.27596295]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 3225 is [True, False, False, False, True, False]
Scene graph at timestep 3225 is [True, False, False, False, True, False]
State prediction error at timestep 3225 is tensor(3.9822e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3226. State = [[-0.18993622 -0.06849074]]. Action = [[ 0.11311534  0.03692761 -0.02477038 -0.6245127 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 3226 is [True, False, False, False, True, False]
Current timestep = 3227. State = [[-0.18645047 -0.06871215]]. Action = [[-0.24922459 -0.22606811  0.06533313 -0.1934346 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 3227 is [True, False, False, False, True, False]
Current timestep = 3228. State = [[-0.18558107 -0.06997427]]. Action = [[-0.03396854  0.01243138 -0.1568797   0.06166971]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 3228 is [True, False, False, False, True, False]
Current timestep = 3229. State = [[-0.18574286 -0.07050636]]. Action = [[ 0.2177769   0.05575794  0.0890502  -0.72489905]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 3229 is [True, False, False, False, True, False]
Current timestep = 3230. State = [[-0.18417904 -0.07090992]]. Action = [[-0.2022469  -0.22889817  0.00169888  0.52104783]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 3230 is [True, False, False, False, True, False]
Current timestep = 3231. State = [[-0.18407097 -0.07319802]]. Action = [[-0.01556137 -0.23784429 -0.22440907  0.9804933 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 3231 is [True, False, False, False, True, False]
Human Feedback received at timestep 3231 of 1
Current timestep = 3232. State = [[-0.18387826 -0.07767905]]. Action = [[-0.15593626  0.1594902   0.02672875  0.34860075]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 3232 is [True, False, False, False, True, False]
Current timestep = 3233. State = [[-0.18415302 -0.07873406]]. Action = [[ 0.14595193 -0.17595118  0.14450616  0.87643874]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 3233 is [True, False, False, False, True, False]
Scene graph at timestep 3233 is [True, False, False, False, True, False]
State prediction error at timestep 3233 is tensor(2.5716e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3234. State = [[-0.1843619  -0.08155794]]. Action = [[-0.11786604 -0.09571826  0.07687157  0.27848518]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 3234 is [True, False, False, False, True, False]
Scene graph at timestep 3234 is [True, False, False, False, True, False]
State prediction error at timestep 3234 is tensor(1.4572e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3235. State = [[-0.18468885 -0.08465347]]. Action = [[0.22536486 0.14031374 0.01511994 0.00822794]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 3235 is [True, False, False, False, True, False]
Current timestep = 3236. State = [[-0.18452026 -0.08455788]]. Action = [[ 0.23441142  0.1999895  -0.17442901  0.8070996 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 3236 is [True, False, False, False, True, False]
Current timestep = 3237. State = [[-0.18315811 -0.08409467]]. Action = [[ 0.04372346 -0.19100386 -0.135211   -0.940763  ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 3237 is [True, False, False, False, True, False]
Current timestep = 3238. State = [[-0.18140437 -0.08444946]]. Action = [[ 0.17023635  0.1537537  -0.19652885  0.2751974 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 3238 is [True, False, False, False, True, False]
Current timestep = 3239. State = [[-0.17843592 -0.08416727]]. Action = [[-0.16438147  0.18276438  0.1634726   0.36654758]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 3239 is [True, False, False, False, True, False]
Current timestep = 3240. State = [[-0.17654999 -0.08206366]]. Action = [[ 0.240857   -0.1449144  -0.02884658  0.7151046 ]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 3240 is [True, False, False, False, True, False]
Current timestep = 3241. State = [[-0.17286868 -0.08233865]]. Action = [[ 0.19096643 -0.17005002  0.18517664 -0.4200837 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 3241 is [True, False, False, False, True, False]
Current timestep = 3242. State = [[-0.16778807 -0.08382858]]. Action = [[ 0.09714872 -0.10886441 -0.09423617  0.30889285]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 3242 is [True, False, False, False, True, False]
Scene graph at timestep 3242 is [True, False, False, False, True, False]
State prediction error at timestep 3242 is tensor(1.8150e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3243. State = [[-0.16243276 -0.08551966]]. Action = [[ 0.22527134 -0.11165722  0.21643832  0.4587798 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 3243 is [True, False, False, False, True, False]
Current timestep = 3244. State = [[-0.15737508 -0.08738896]]. Action = [[ 0.04629916  0.16576484  0.0706304  -0.3349964 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 3244 is [True, False, False, False, True, False]
Human Feedback received at timestep 3244 of 1
Current timestep = 3245. State = [[-0.15314806 -0.08762553]]. Action = [[ 0.09286469 -0.16673088  0.24856341  0.02029157]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 3245 is [True, False, False, False, True, False]
Current timestep = 3246. State = [[-0.14901261 -0.08935666]]. Action = [[ 0.11654648 -0.24266548 -0.15039784 -0.43788928]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 3246 is [True, False, False, False, True, False]
Current timestep = 3247. State = [[-0.14462611 -0.09313969]]. Action = [[-0.00605999  0.22642231 -0.06112956 -0.5162068 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 3247 is [True, False, False, False, True, False]
Current timestep = 3248. State = [[-0.1409975  -0.09346505]]. Action = [[ 0.04888484 -0.04963765 -0.12381193  0.8984169 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 3248 is [True, False, False, False, True, False]
Current timestep = 3249. State = [[-0.13884242 -0.09371088]]. Action = [[ 0.19282609 -0.13749665 -0.22949839 -0.7596703 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 3249 is [True, False, False, False, True, False]
Current timestep = 3250. State = [[-0.1366591  -0.09543808]]. Action = [[ 0.18500388  0.02251464 -0.04028615  0.17908418]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 3250 is [True, False, False, False, True, False]
Current timestep = 3251. State = [[-0.1330209  -0.09615305]]. Action = [[ 0.20771354  0.10385126 -0.2037665   0.56058407]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 3251 is [True, False, False, False, True, False]
Current timestep = 3252. State = [[-0.12755786 -0.09640642]]. Action = [[-0.11373247  0.1765247   0.04782566  0.35608065]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 3252 is [True, False, False, False, True, False]
Current timestep = 3253. State = [[-0.12340058 -0.09522074]]. Action = [[ 0.17225161 -0.1630814  -0.01500669  0.23703825]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 3253 is [True, False, False, False, True, False]
Current timestep = 3254. State = [[-0.11920957 -0.09592779]]. Action = [[ 0.23379308  0.19257647 -0.14875822 -0.48196286]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 3254 is [True, False, False, False, True, False]
Current timestep = 3255. State = [[-0.11466026 -0.09490785]]. Action = [[ 0.10240743 -0.17219129  0.12597904 -0.8503042 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 3255 is [True, False, False, False, True, False]
Current timestep = 3256. State = [[-0.1093296  -0.09535351]]. Action = [[-0.02837236  0.13160771  0.13958332 -0.70640576]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 3256 is [True, False, False, False, True, False]
Scene graph at timestep 3256 is [True, False, False, False, True, False]
State prediction error at timestep 3256 is tensor(4.7601e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3257. State = [[-0.10579311 -0.09491215]]. Action = [[ 0.07857528  0.22941723  0.12897879 -0.53171176]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 3257 is [True, False, False, False, True, False]
Current timestep = 3258. State = [[-0.10211692 -0.09211827]]. Action = [[-0.21201746 -0.20159304 -0.03783154 -0.34796923]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 3258 is [True, False, False, False, True, False]
Human Feedback received at timestep 3258 of 1
Current timestep = 3259. State = [[-0.10133698 -0.0922085 ]]. Action = [[ 0.1839813  -0.24834062  0.17193967  0.8055918 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 3259 is [True, False, False, False, True, False]
Scene graph at timestep 3259 is [True, False, False, False, True, False]
State prediction error at timestep 3259 is tensor(2.5610e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3260. State = [[-0.10067771 -0.09525738]]. Action = [[-0.10221539 -0.01977578 -0.05018732  0.7078967 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 3260 is [True, False, False, False, True, False]
Current timestep = 3261. State = [[-0.1005942  -0.09649789]]. Action = [[ 0.04028475  0.01016861  0.13939905 -0.26350653]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 3261 is [True, False, False, False, True, False]
Scene graph at timestep 3261 is [True, False, False, False, True, False]
State prediction error at timestep 3261 is tensor(5.6907e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3262. State = [[-0.10057735 -0.09712972]]. Action = [[ 0.21193019 -0.0133137   0.09057063  0.8126447 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 3262 is [True, False, False, False, True, False]
Current timestep = 3263. State = [[-0.09926869 -0.09722359]]. Action = [[ 0.20997101 -0.11103168 -0.19984744  0.4406637 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 3263 is [True, False, False, False, True, False]
Current timestep = 3264. State = [[-0.0965128  -0.09823614]]. Action = [[-0.19402005 -0.04516587 -0.20468718  0.9075322 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 3264 is [True, False, False, False, True, False]
Current timestep = 3265. State = [[-0.09509633 -0.09960052]]. Action = [[ 0.05844641 -0.05130598  0.2036708  -0.4810413 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 3265 is [True, False, False, False, True, False]
Current timestep = 3266. State = [[-0.09355833 -0.10103814]]. Action = [[-0.09299794  0.14626074  0.15544382 -0.7154816 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 3266 is [True, False, False, False, True, False]
Current timestep = 3267. State = [[-0.09220311 -0.10103719]]. Action = [[ 0.24296498 -0.17797583  0.0881362  -0.00318909]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 3267 is [True, False, False, False, True, False]
Current timestep = 3268. State = [[-0.0893529  -0.10285852]]. Action = [[-0.24276403 -0.09844181 -0.10836969  0.8321922 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 3268 is [True, False, False, False, True, False]
Current timestep = 3269. State = [[-0.0892923  -0.10494346]]. Action = [[ 0.05213276  0.22395232 -0.01834045  0.7623838 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 3269 is [True, False, False, False, True, False]
Current timestep = 3270. State = [[-0.08920947 -0.10443883]]. Action = [[ 0.20092914  0.22557643 -0.04812989 -0.27047586]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 3270 is [True, False, False, False, True, False]
Scene graph at timestep 3270 is [True, False, False, False, True, False]
State prediction error at timestep 3270 is tensor(9.9819e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3271. State = [[-0.08807084 -0.10180554]]. Action = [[0.04147562 0.13112321 0.12925494 0.4929576 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 3271 is [True, False, False, False, True, False]
Scene graph at timestep 3271 is [True, False, False, False, True, False]
State prediction error at timestep 3271 is tensor(5.5333e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3272. State = [[-0.08633403 -0.09885024]]. Action = [[-0.24334966  0.21420646  0.16979435 -0.5541563 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 3272 is [True, False, False, False, True, False]
Scene graph at timestep 3272 is [True, False, False, False, True, False]
State prediction error at timestep 3272 is tensor(2.5776e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3273. State = [[-0.08655325 -0.09467719]]. Action = [[ 0.13952577 -0.12085304  0.11704904  0.87625504]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 3273 is [True, False, False, False, True, False]
Current timestep = 3274. State = [[-0.08608148 -0.09362127]]. Action = [[-0.209617    0.13078594 -0.21120423  0.01165378]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 3274 is [True, False, False, False, True, False]
Current timestep = 3275. State = [[-0.08646573 -0.09125589]]. Action = [[ 0.24533883  0.21400094  0.12057087 -0.42381382]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 3275 is [True, False, False, False, True, False]
Current timestep = 3276. State = [[-0.08609067 -0.0865329 ]]. Action = [[-0.16883269  0.19631687 -0.20322229 -0.6764209 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 3276 is [True, False, False, False, True, False]
Scene graph at timestep 3276 is [True, False, False, False, True, False]
State prediction error at timestep 3276 is tensor(4.7669e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3277. State = [[-0.08657889 -0.081013  ]]. Action = [[-0.21310945  0.19180441 -0.21179473 -0.7744438 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 3277 is [True, False, False, False, True, False]
Current timestep = 3278. State = [[-0.0875007  -0.07494919]]. Action = [[-0.03506997 -0.12555732 -0.07611851  0.9927757 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 3278 is [True, False, False, False, True, False]
Current timestep = 3279. State = [[-0.08786616 -0.0726479 ]]. Action = [[-0.03985518  0.04748303  0.05544046 -0.01501501]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 3279 is [True, False, False, False, True, False]
Scene graph at timestep 3279 is [True, False, False, False, True, False]
State prediction error at timestep 3279 is tensor(1.0798e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3280. State = [[-0.08823777 -0.07095119]]. Action = [[-0.06593019 -0.07237799 -0.1628933  -0.81609535]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 3280 is [True, False, False, False, True, False]
Current timestep = 3281. State = [[-0.08857357 -0.07083096]]. Action = [[ 0.07583374  0.09871092 -0.13991505  0.80914366]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 3281 is [True, False, False, False, True, False]
Current timestep = 3282. State = [[-0.08877167 -0.06972159]]. Action = [[ 0.20408148 -0.14488329  0.16183895 -0.79126793]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 3282 is [True, False, False, False, True, False]
Human Feedback received at timestep 3282 of 1
Current timestep = 3283. State = [[-0.08876763 -0.06991726]]. Action = [[ 0.03233743  0.09151524 -0.09035668  0.5381541 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 3283 is [True, False, False, False, True, False]
Current timestep = 3284. State = [[-0.08855934 -0.06942189]]. Action = [[ 0.05612633  0.07806295 -0.06964585  0.8415265 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 3284 is [True, False, False, False, True, False]
Current timestep = 3285. State = [[-0.08813956 -0.06815798]]. Action = [[-0.00307192  0.15435684  0.21090478  0.87243557]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 3285 is [True, False, False, False, True, False]
Current timestep = 3286. State = [[-0.08796132 -0.06540488]]. Action = [[-0.21470167  0.1398629   0.13236654 -0.9035564 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 3286 is [True, False, False, False, True, False]
Current timestep = 3287. State = [[-0.08839949 -0.06210219]]. Action = [[ 0.05468148 -0.16453883  0.06497788  0.53140986]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 3287 is [True, False, False, False, True, False]
Current timestep = 3288. State = [[-0.08849837 -0.06199384]]. Action = [[-0.112661   -0.14012764 -0.09317581 -0.97298837]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 3288 is [True, False, False, False, True, False]
Current timestep = 3289. State = [[-0.08858966 -0.06271986]]. Action = [[ 0.06621435 -0.21947794 -0.21742427 -0.79562545]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 3289 is [True, False, False, False, True, False]
Current timestep = 3290. State = [[-0.08840846 -0.06512489]]. Action = [[ 0.23674133 -0.073127    0.14027274  0.28294468]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 3290 is [True, False, False, False, True, False]
Current timestep = 3291. State = [[-0.08774383 -0.0672921 ]]. Action = [[-0.09043968  0.12923968  0.23091674  0.7034793 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 3291 is [True, False, False, False, True, False]
Current timestep = 3292. State = [[-0.08770153 -0.06704387]]. Action = [[ 0.13092619  0.20887733 -0.12852746  0.96817017]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 3292 is [True, False, False, False, True, False]
Scene graph at timestep 3292 is [True, False, False, False, True, False]
State prediction error at timestep 3292 is tensor(1.6816e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3293. State = [[-0.08740229 -0.06540582]]. Action = [[ 0.23743832 -0.16010624  0.10379449 -0.96180993]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 3293 is [True, False, False, False, True, False]
Current timestep = 3294. State = [[-0.08507979 -0.06647663]]. Action = [[ 0.07456633 -0.20624892 -0.06693773 -0.743166  ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 3294 is [True, False, False, False, True, False]
Current timestep = 3295. State = [[-0.08306907 -0.06893361]]. Action = [[-0.19989097 -0.20901582  0.21286833  0.80931616]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 3295 is [True, False, False, False, True, False]
Current timestep = 3296. State = [[-0.0825308  -0.07274956]]. Action = [[ 0.21408951  0.20501053 -0.17283864 -0.4828552 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 3296 is [True, False, False, False, True, False]
Current timestep = 3297. State = [[-0.08197697 -0.07280037]]. Action = [[-0.12724932  0.05659294 -0.16040522  0.9090861 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 3297 is [True, False, False, False, True, False]
Current timestep = 3298. State = [[-0.08200767 -0.07226602]]. Action = [[ 0.16567141 -0.10432005 -0.03197578 -0.31136227]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 3298 is [True, False, False, False, True, False]
Scene graph at timestep 3298 is [True, False, False, False, True, False]
State prediction error at timestep 3298 is tensor(6.2741e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3299. State = [[-0.08133352 -0.07312632]]. Action = [[ 0.02127001  0.21164101  0.19793475 -0.9887147 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 3299 is [True, False, False, False, True, False]
Current timestep = 3300. State = [[-0.08026669 -0.07235505]]. Action = [[-0.0713042  -0.22505572 -0.2032408   0.9384861 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 3300 is [True, False, False, False, True, False]
Scene graph at timestep 3300 is [True, False, False, False, True, False]
State prediction error at timestep 3300 is tensor(1.9990e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3301. State = [[-0.07995915 -0.07327848]]. Action = [[-0.03701662 -0.04734585 -0.21638387 -0.7821273 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 3301 is [True, False, False, False, True, False]
Scene graph at timestep 3301 is [True, False, False, False, True, False]
State prediction error at timestep 3301 is tensor(4.2463e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3302. State = [[-0.07974499 -0.07446969]]. Action = [[-0.09853071 -0.2106265   0.14461535 -0.9564341 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 3302 is [True, False, False, False, True, False]
Current timestep = 3303. State = [[-0.07957474 -0.07793427]]. Action = [[ 0.24356735 -0.06842518  0.24027553 -0.46330607]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 3303 is [True, False, False, False, True, False]
Human Feedback received at timestep 3303 of 1
Current timestep = 3304. State = [[-0.07846669 -0.08094327]]. Action = [[-0.18600205  0.13390934 -0.00719208  0.8925977 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 3304 is [True, False, False, False, True, False]
Current timestep = 3305. State = [[-0.07827977 -0.08132834]]. Action = [[-0.14540292 -0.18989104 -0.18356043  0.3812126 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 3305 is [True, False, False, False, True, False]
Current timestep = 3306. State = [[-0.07843652 -0.08354922]]. Action = [[0.14378601 0.04447374 0.03394589 0.08173227]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 3306 is [True, False, False, False, True, False]
Current timestep = 3307. State = [[-0.07840739 -0.08439303]]. Action = [[-0.15808041 -0.09085312 -0.04733753  0.98485637]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 3307 is [True, False, False, False, True, False]
Current timestep = 3308. State = [[-0.07864759 -0.08603923]]. Action = [[-0.03224647  0.1806212  -0.00173616 -0.69488907]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 3308 is [True, False, False, False, True, False]
Current timestep = 3309. State = [[-0.07881691 -0.08574394]]. Action = [[-0.05907823 -0.20018409  0.12585348  0.6701242 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 3309 is [True, False, False, False, True, False]
Current timestep = 3310. State = [[-0.07905478 -0.08803327]]. Action = [[ 0.15002522 -0.19859953 -0.20467444  0.90586686]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 3310 is [True, False, False, False, True, False]
Current timestep = 3311. State = [[-0.07928015 -0.09146355]]. Action = [[-0.23393969  0.23821068 -0.2059312  -0.10352534]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 3311 is [True, False, False, False, True, False]
Scene graph at timestep 3311 is [True, False, False, False, True, False]
State prediction error at timestep 3311 is tensor(1.2575e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3312. State = [[-0.07956786 -0.09124135]]. Action = [[-0.2384313   0.21489221  0.06022602  0.18226969]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 3312 is [True, False, False, False, True, False]
Current timestep = 3313. State = [[-0.08027077 -0.08919974]]. Action = [[ 0.00116825 -0.18412861  0.12871844 -0.31320333]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 3313 is [True, False, False, False, True, False]
Scene graph at timestep 3313 is [True, False, False, False, True, False]
State prediction error at timestep 3313 is tensor(9.6484e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3314. State = [[-0.08076318 -0.0901242 ]]. Action = [[ 0.09897709  0.08457637 -0.18304415 -0.9792619 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 3314 is [True, False, False, False, True, False]
Current timestep = 3315. State = [[-0.08107786 -0.09000338]]. Action = [[ 0.1753639  0.1719887 -0.0320597  0.7453656]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 3315 is [True, False, False, False, True, False]
Current timestep = 3316. State = [[-0.08124118 -0.08824676]]. Action = [[-0.1873637  -0.22405191 -0.07863544  0.8308655 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 3316 is [True, False, False, False, True, False]
Scene graph at timestep 3316 is [True, False, False, False, True, False]
State prediction error at timestep 3316 is tensor(1.2114e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3317. State = [[-0.0815127  -0.08890747]]. Action = [[ 0.00355214  0.04830554 -0.09500077  0.0303601 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 3317 is [True, False, False, False, True, False]
Current timestep = 3318. State = [[-0.08153003 -0.08917736]]. Action = [[-0.07898961 -0.23529607  0.07494831  0.95214415]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 3318 is [True, False, False, False, True, False]
Current timestep = 3319. State = [[-0.08184431 -0.09155659]]. Action = [[-0.07929564  0.15047976 -0.0627408   0.4718393 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 3319 is [True, False, False, False, True, False]
Current timestep = 3320. State = [[-0.08221874 -0.09136158]]. Action = [[0.13921356 0.18631104 0.04816422 0.657537  ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 3320 is [True, False, False, False, True, False]
Scene graph at timestep 3320 is [True, False, False, False, True, False]
State prediction error at timestep 3320 is tensor(1.8250e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3321. State = [[-0.08245168 -0.08992225]]. Action = [[ 0.02038094 -0.23708749  0.1418997   0.19178534]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 3321 is [True, False, False, False, True, False]
Current timestep = 3322. State = [[-0.08244646 -0.09064125]]. Action = [[ 0.21344182  0.10165483 -0.06802289  0.5533353 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 3322 is [True, False, False, False, True, False]
Current timestep = 3323. State = [[-0.08248278 -0.090473  ]]. Action = [[-0.08562355  0.2325964  -0.22835973  0.49778247]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 3323 is [True, False, False, False, True, False]
Current timestep = 3324. State = [[-0.08265553 -0.08795443]]. Action = [[-0.15333596  0.08568549  0.1779046   0.00919569]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 3324 is [True, False, False, False, True, False]
Current timestep = 3325. State = [[-0.08288604 -0.08536173]]. Action = [[0.20109639 0.16832417 0.06673819 0.21617174]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 3325 is [True, False, False, False, True, False]
Current timestep = 3326. State = [[-0.0830609  -0.08198886]]. Action = [[-0.1214859  -0.13953865 -0.16729073 -0.27717352]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 3326 is [True, False, False, False, True, False]
Scene graph at timestep 3326 is [True, False, False, False, True, False]
State prediction error at timestep 3326 is tensor(2.3368e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3327. State = [[-0.0831817  -0.08169392]]. Action = [[-0.22612381 -0.20279768 -0.23631899 -0.49343967]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 3327 is [True, False, False, False, True, False]
Scene graph at timestep 3327 is [True, False, False, False, True, False]
State prediction error at timestep 3327 is tensor(1.7604e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3328. State = [[-0.08342133 -0.08276414]]. Action = [[ 0.01427525 -0.00379835 -0.02997492  0.0411756 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 3328 is [True, False, False, False, True, False]
Current timestep = 3329. State = [[-0.08352903 -0.08333136]]. Action = [[ 0.22167909  0.21739435 -0.01778516  0.8251984 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 3329 is [True, False, False, False, True, False]
Scene graph at timestep 3329 is [True, False, False, False, True, False]
State prediction error at timestep 3329 is tensor(1.0835e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3330. State = [[-0.08358209 -0.08208378]]. Action = [[-0.19789398 -0.23708951 -0.09056267 -0.21604979]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 3330 is [True, False, False, False, True, False]
Current timestep = 3331. State = [[-0.08352694 -0.08310375]]. Action = [[ 0.19282475 -0.14046812 -0.03184199  0.06930423]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 3331 is [True, False, False, False, True, False]
Current timestep = 3332. State = [[-0.08347128 -0.08492071]]. Action = [[-0.22799931  0.19149429  0.16470143 -0.30736232]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 3332 is [True, False, False, False, True, False]
Scene graph at timestep 3332 is [True, False, False, False, True, False]
State prediction error at timestep 3332 is tensor(3.1656e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3333. State = [[-0.08352321 -0.08440649]]. Action = [[ 0.22186255 -0.17197028 -0.15746161 -0.66527027]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 3333 is [True, False, False, False, True, False]
Current timestep = 3334. State = [[-0.08346445 -0.08537818]]. Action = [[-0.06921536 -0.19034176  0.11371624 -0.2712651 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 3334 is [True, False, False, False, True, False]
Current timestep = 3335. State = [[-0.08362766 -0.08855102]]. Action = [[-0.24107195 -0.04456535 -0.24709761  0.7060535 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 3335 is [True, False, False, False, True, False]
Scene graph at timestep 3335 is [True, False, False, False, True, False]
State prediction error at timestep 3335 is tensor(2.9138e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3336. State = [[-0.0843944  -0.09191375]]. Action = [[-0.00639121 -0.14399996  0.05385691  0.24185348]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 3336 is [True, False, False, False, True, False]
Current timestep = 3337. State = [[-0.08515429 -0.095681  ]]. Action = [[ 0.11611372 -0.18798572  0.19276181 -0.624004  ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 3337 is [True, False, False, False, True, False]
Human Feedback received at timestep 3337 of 1
Current timestep = 3338. State = [[-0.085969   -0.10043977]]. Action = [[ 0.16920218 -0.02240235  0.16384777 -0.83131456]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 3338 is [True, False, False, False, True, False]
Scene graph at timestep 3338 is [True, False, False, False, True, False]
State prediction error at timestep 3338 is tensor(2.8135e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3339. State = [[-0.08610644 -0.10282619]]. Action = [[0.01599365 0.23163977 0.15842944 0.79695725]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 3339 is [True, False, False, False, True, False]
Current timestep = 3340. State = [[-0.0860481  -0.10243606]]. Action = [[-0.23853599 -0.06128021 -0.15835588 -0.631967  ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 3340 is [True, False, False, False, True, False]
Human Feedback received at timestep 3340 of 0
Current timestep = 3341. State = [[-0.08628871 -0.10300732]]. Action = [[-0.00922692 -0.17649214 -0.10238139 -0.19235659]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 3341 is [True, False, False, False, True, False]
Scene graph at timestep 3341 is [True, False, False, False, True, False]
State prediction error at timestep 3341 is tensor(1.0103e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3342. State = [[-0.08667316 -0.10537828]]. Action = [[ 0.00332564 -0.06441766  0.11240965 -0.8801771 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 3342 is [True, False, False, False, True, False]
Current timestep = 3343. State = [[-0.0869574  -0.10730859]]. Action = [[-0.16877298 -0.12432244 -0.09167629 -0.29301536]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 3343 is [True, False, False, False, True, False]
Current timestep = 3344. State = [[-0.08837075 -0.11045524]]. Action = [[ 0.22661039 -0.17781681  0.14344198 -0.87622076]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 3344 is [True, False, False, False, True, False]
Current timestep = 3345. State = [[-0.08814906 -0.11434761]]. Action = [[ 0.09836704  0.14932472  0.08372116 -0.06137353]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 3345 is [True, False, False, False, True, False]
Current timestep = 3346. State = [[-0.08815999 -0.11490899]]. Action = [[ 0.0158174   0.22731042  0.00224113 -0.35118437]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 3346 is [True, False, False, False, True, False]
Current timestep = 3347. State = [[-0.08812454 -0.11341123]]. Action = [[ 0.20890778  0.10389519  0.2016038  -0.16323459]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 3347 is [True, False, False, False, True, False]
Current timestep = 3348. State = [[-0.08772876 -0.11124626]]. Action = [[ 0.19907981  0.15668407  0.02300248 -0.57397336]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 3348 is [True, False, False, False, True, False]
Current timestep = 3349. State = [[-0.08606829 -0.10785103]]. Action = [[-0.05382864  0.05852178 -0.1902259   0.6585264 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 3349 is [True, False, False, False, True, False]
Current timestep = 3350. State = [[-0.08515908 -0.10545127]]. Action = [[ 0.13534814 -0.05534238  0.1457122  -0.50600296]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 3350 is [True, False, False, False, True, False]
Current timestep = 3351. State = [[-0.08370197 -0.10491474]]. Action = [[-0.19359963 -0.11203235 -0.09670101  0.6266866 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 3351 is [True, False, False, False, True, False]
Current timestep = 3352. State = [[-0.08361226 -0.10516644]]. Action = [[-0.03231886 -0.0951203  -0.01893608  0.67242074]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 3352 is [True, False, False, False, True, False]
Current timestep = 3353. State = [[-0.08367348 -0.10590979]]. Action = [[ 0.1664992   0.24405634 -0.18601328  0.15071917]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 3353 is [True, False, False, False, True, False]
Scene graph at timestep 3353 is [True, False, False, False, True, False]
State prediction error at timestep 3353 is tensor(4.7194e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3354. State = [[-0.0831196  -0.10427074]]. Action = [[-0.03822003 -0.18857251 -0.10214031 -0.8029522 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 3354 is [True, False, False, False, True, False]
Current timestep = 3355. State = [[-0.08299039 -0.10493003]]. Action = [[ 0.20100152  0.03368574  0.02644742 -0.7054046 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 3355 is [True, False, False, False, True, False]
Human Feedback received at timestep 3355 of -1
Current timestep = 3356. State = [[-0.08090969 -0.10484131]]. Action = [[-0.11697319  0.22870314  0.05583501 -0.45087457]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 3356 is [True, False, False, False, True, False]
Current timestep = 3357. State = [[-0.07985201 -0.10275175]]. Action = [[ 0.10455832  0.10389426  0.18609321 -0.729811  ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 3357 is [True, False, False, False, True, False]
Scene graph at timestep 3357 is [True, False, False, False, True, False]
State prediction error at timestep 3357 is tensor(8.0360e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3358. State = [[-0.0786955  -0.10041642]]. Action = [[ 0.05069822  0.07126385  0.16925514 -0.13077474]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 3358 is [True, False, False, False, True, False]
Current timestep = 3359. State = [[-0.07759574 -0.09785014]]. Action = [[-0.12628911  0.2358284   0.21978721 -0.87316674]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 3359 is [True, False, False, False, True, False]
Scene graph at timestep 3359 is [True, False, False, False, True, False]
State prediction error at timestep 3359 is tensor(1.1957e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3360. State = [[-0.07749063 -0.09368898]]. Action = [[ 0.04286408 -0.20055565 -0.17202325  0.1965586 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 3360 is [True, False, False, False, True, False]
Current timestep = 3361. State = [[-0.0774181  -0.09335782]]. Action = [[-0.21201055 -0.08029222 -0.04680517  0.39978063]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 3361 is [True, False, False, False, True, False]
Current timestep = 3362. State = [[-0.07752528 -0.0939225 ]]. Action = [[ 0.08820939 -0.15632622 -0.20616837  0.4859743 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 3362 is [True, False, False, False, True, False]
Scene graph at timestep 3362 is [True, False, False, False, True, False]
State prediction error at timestep 3362 is tensor(1.0857e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3363. State = [[-0.07751087 -0.09490456]]. Action = [[-0.14209291 -0.08012134 -0.13092485 -0.7379759 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 3363 is [True, False, False, False, True, False]
Current timestep = 3364. State = [[-0.0777327  -0.09626545]]. Action = [[-0.23203903  0.21970853  0.04223078 -0.30573046]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 3364 is [True, False, False, False, True, False]
Current timestep = 3365. State = [[-0.07821666 -0.09553266]]. Action = [[-0.1685439   0.14984035 -0.10919973  0.39017034]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 3365 is [True, False, False, False, True, False]
Scene graph at timestep 3365 is [True, False, False, False, True, False]
State prediction error at timestep 3365 is tensor(1.6333e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3366. State = [[-0.07960522 -0.09386721]]. Action = [[ 0.17865908  0.04880041  0.02298251 -0.7232913 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 3366 is [True, False, False, False, True, False]
Current timestep = 3367. State = [[-0.07981329 -0.09264547]]. Action = [[-0.19967122 -0.17255569  0.07455757 -0.19568264]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 3367 is [True, False, False, False, True, False]
Current timestep = 3368. State = [[-0.08100879 -0.09291054]]. Action = [[-0.19844918  0.22764012  0.20527947 -0.9254492 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 3368 is [True, False, False, False, True, False]
Current timestep = 3369. State = [[-0.08303903 -0.09082313]]. Action = [[0.05645531 0.0843454  0.05467457 0.6155344 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 3369 is [True, False, False, False, True, False]
Scene graph at timestep 3369 is [True, False, False, False, True, False]
State prediction error at timestep 3369 is tensor(2.2905e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3370. State = [[-0.08425313 -0.08856492]]. Action = [[-0.19154975 -0.18877283 -0.13670805 -0.74122006]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 3370 is [True, False, False, False, True, False]
Current timestep = 3371. State = [[-0.0860691  -0.08954059]]. Action = [[ 0.16573715 -0.18003061 -0.03177841  0.02645695]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 3371 is [True, False, False, False, True, False]
Current timestep = 3372. State = [[-0.08674078 -0.09188256]]. Action = [[-0.15178156  0.145697   -0.18292154 -0.21362871]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 3372 is [True, False, False, False, True, False]
Current timestep = 3373. State = [[-0.08783978 -0.09177944]]. Action = [[ 0.21915898 -0.0127679  -0.01034752 -0.20213556]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 3373 is [True, False, False, False, True, False]
Scene graph at timestep 3373 is [True, False, False, False, True, False]
State prediction error at timestep 3373 is tensor(7.0508e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3374. State = [[-0.08783423 -0.09133639]]. Action = [[0.18102425 0.19060677 0.23511493 0.38963103]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 3374 is [True, False, False, False, True, False]
Current timestep = 3375. State = [[-0.08794311 -0.08959061]]. Action = [[-0.0266162   0.10463512 -0.11950299 -0.31071675]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 3375 is [True, False, False, False, True, False]
Current timestep = 3376. State = [[-0.08804639 -0.08734434]]. Action = [[ 0.18754673 -0.15402292 -0.10826057  0.9077381 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 3376 is [True, False, False, False, True, False]
Current timestep = 3377. State = [[-0.0878441  -0.08718778]]. Action = [[ 0.1962739   0.08790439  0.12264398 -0.20824414]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 3377 is [True, False, False, False, True, False]
Current timestep = 3378. State = [[-0.08660542 -0.08604982]]. Action = [[ 0.1336681   0.16728109 -0.03340118  0.2077874 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 3378 is [True, False, False, False, True, False]
Current timestep = 3379. State = [[-0.08484171 -0.08323285]]. Action = [[ 0.2080999   0.1704346   0.18741864 -0.4315372 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 3379 is [True, False, False, False, True, False]
Scene graph at timestep 3379 is [True, False, False, False, True, False]
State prediction error at timestep 3379 is tensor(4.3855e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3380. State = [[-0.08128233 -0.07897364]]. Action = [[-0.0585175   0.17572263  0.16574013  0.41225064]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 3380 is [True, False, False, False, True, False]
Current timestep = 3381. State = [[-0.07927131 -0.07415532]]. Action = [[ 0.05267656  0.09490389  0.04615492 -0.4105276 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 3381 is [True, False, False, False, True, False]
Current timestep = 3382. State = [[-0.07776692 -0.06943586]]. Action = [[ 0.21198365 -0.09897584 -0.2095105   0.96313   ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 3382 is [True, False, False, False, True, False]
Current timestep = 3383. State = [[-0.07532299 -0.06712934]]. Action = [[ 0.0743348   0.22464663 -0.13201645 -0.69748807]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 3383 is [True, False, False, False, True, False]
Current timestep = 3384. State = [[-0.07212388 -0.06335153]]. Action = [[ 0.17251456  0.14354566  0.05623516 -0.49026185]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 3384 is [True, False, False, False, True, False]
Current timestep = 3385. State = [[-0.06742558 -0.05958794]]. Action = [[-0.17184912 -0.16677696  0.06783032  0.04698598]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 3385 is [True, False, False, False, True, False]
Current timestep = 3386. State = [[-0.06564578 -0.05920904]]. Action = [[-0.19776769 -0.08714499  0.00435781  0.37618947]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 3386 is [True, False, False, False, True, False]
Current timestep = 3387. State = [[-0.06582854 -0.05941005]]. Action = [[ 0.08186585  0.16331482 -0.19421571  0.28151488]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 3387 is [True, False, False, False, True, False]
Scene graph at timestep 3387 is [True, False, False, False, True, False]
State prediction error at timestep 3387 is tensor(1.0439e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3388. State = [[-0.06616189 -0.05807162]]. Action = [[-0.03887488 -0.19665767 -0.13474174 -0.69679207]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 3388 is [True, False, False, False, True, False]
Current timestep = 3389. State = [[-0.06606501 -0.05841883]]. Action = [[ 0.1526252  -0.17456976  0.07837939  0.79864   ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 3389 is [True, False, False, False, True, False]
Scene graph at timestep 3389 is [True, False, False, False, True, False]
State prediction error at timestep 3389 is tensor(8.7050e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3389 of 1
Current timestep = 3390. State = [[-0.06566197 -0.06057108]]. Action = [[0.23039246 0.1293518  0.2042768  0.6176418 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 3390 is [True, False, False, False, True, False]
Current timestep = 3391. State = [[-0.06368422 -0.06054861]]. Action = [[-0.18742472 -0.16074222  0.03937486  0.2192074 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 3391 is [True, False, False, False, True, False]
Scene graph at timestep 3391 is [True, False, False, False, True, False]
State prediction error at timestep 3391 is tensor(4.4041e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3392. State = [[-0.06335611 -0.06190737]]. Action = [[ 0.24773371 -0.19203758 -0.16521034 -0.07717592]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 3392 is [True, False, False, False, True, False]
Scene graph at timestep 3392 is [True, False, False, False, True, False]
State prediction error at timestep 3392 is tensor(9.1946e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3393. State = [[-0.06088956 -0.06528514]]. Action = [[-0.18943694 -0.09864645 -0.08696255 -0.19047737]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 3393 is [True, False, False, False, True, False]
Scene graph at timestep 3393 is [True, False, False, False, True, False]
State prediction error at timestep 3393 is tensor(1.2257e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3394. State = [[-0.06055301 -0.06905374]]. Action = [[ 0.10110706 -0.09857893  0.07076275 -0.5389534 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 3394 is [True, False, False, False, True, False]
Scene graph at timestep 3394 is [True, False, False, False, True, False]
State prediction error at timestep 3394 is tensor(7.9026e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3395. State = [[-0.05965644 -0.07253228]]. Action = [[0.04365599 0.05242482 0.18706441 0.59984136]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 3395 is [True, False, False, False, True, False]
Scene graph at timestep 3395 is [True, False, False, False, True, False]
State prediction error at timestep 3395 is tensor(3.4468e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3396. State = [[-0.05855356 -0.07373903]]. Action = [[-0.01788138  0.10218289  0.02090839  0.88341784]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 3396 is [True, False, False, False, True, False]
Current timestep = 3397. State = [[-0.0577415  -0.07387681]]. Action = [[-0.1437697  -0.20217682  0.07550925 -0.27352077]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 3397 is [True, False, False, False, True, False]
Current timestep = 3398. State = [[-0.05792394 -0.07627656]]. Action = [[ 0.18339756 -0.13776161 -0.21533099  0.8591149 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 3398 is [True, False, False, False, True, False]
Scene graph at timestep 3398 is [True, False, False, False, True, False]
State prediction error at timestep 3398 is tensor(3.4981e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3399. State = [[-0.05732867 -0.07926551]]. Action = [[-0.08895665  0.1666646   0.1460577  -0.43784857]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 3399 is [True, False, False, False, True, False]
Current timestep = 3400. State = [[-0.05731316 -0.07924271]]. Action = [[ 0.14539939  0.09073007  0.07817745 -0.6677268 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 3400 is [True, False, False, False, True, False]
Current timestep = 3401. State = [[-0.05647945 -0.07926074]]. Action = [[-0.24588144 -0.16282235 -0.20113313  0.68460035]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 3401 is [True, False, False, False, True, False]
Scene graph at timestep 3401 is [True, False, False, False, True, False]
State prediction error at timestep 3401 is tensor(2.2073e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3402. State = [[-0.05656853 -0.07998484]]. Action = [[ 0.23764563  0.24605525 -0.04418075 -0.9874678 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 3402 is [True, False, False, False, True, False]
Current timestep = 3403. State = [[-0.05609652 -0.07867706]]. Action = [[-0.07448685 -0.12457314 -0.16839154  0.25537145]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 3403 is [True, False, False, False, True, False]
Scene graph at timestep 3403 is [True, False, False, False, True, False]
State prediction error at timestep 3403 is tensor(1.4166e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3404. State = [[-0.05598777 -0.0792178 ]]. Action = [[ 0.01465315 -0.11923535  0.01336709  0.5104451 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 3404 is [True, False, False, False, True, False]
Current timestep = 3405. State = [[-0.05572001 -0.08047127]]. Action = [[ 0.24435025 -0.2451024   0.03388554  0.05626047]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 3405 is [True, False, False, False, True, False]
Current timestep = 3406. State = [[-0.05386545 -0.08365431]]. Action = [[ 0.14565635  0.0526     -0.08115466  0.5294131 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 3406 is [True, False, False, False, True, False]
Scene graph at timestep 3406 is [True, False, False, False, True, False]
State prediction error at timestep 3406 is tensor(1.5736e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3407. State = [[-0.05015532 -0.08491777]]. Action = [[ 0.22105825  0.24488932 -0.0269469   0.37027466]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 3407 is [True, False, False, False, True, False]
Current timestep = 3408. State = [[-0.04520919 -0.08372086]]. Action = [[-0.21389703  0.08651364 -0.03619461  0.5077269 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 3408 is [True, False, False, False, True, False]
Current timestep = 3409. State = [[-0.04317584 -0.08251008]]. Action = [[-0.03666198  0.06210461  0.04663646  0.6669421 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 3409 is [False, True, False, False, True, False]
Current timestep = 3410. State = [[-0.0422619  -0.08167926]]. Action = [[ 0.12585285 -0.01140644  0.1461244  -0.22883493]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 3410 is [False, True, False, False, True, False]
Current timestep = 3411. State = [[-0.03982349 -0.08090174]]. Action = [[0.08250162 0.20526063 0.00362587 0.8363342 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 3411 is [False, True, False, False, True, False]
Scene graph at timestep 3411 is [False, True, False, False, True, False]
State prediction error at timestep 3411 is tensor(5.2889e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3412. State = [[-0.03704796 -0.0780053 ]]. Action = [[-0.03115854  0.20530832 -0.11662936  0.98901606]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 3412 is [False, True, False, False, True, False]
Scene graph at timestep 3412 is [False, True, False, False, True, False]
State prediction error at timestep 3412 is tensor(1.0372e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3413. State = [[-0.2542055   0.09953803]]. Action = [[ 3.5920739e-04 -6.4396098e-02  1.9992289e-01  5.1038873e-01]]. Reward = [100.]
Curr episode timestep = 401
Scene graph at timestep 3413 is [False, True, False, False, True, False]
Human Feedback received at timestep 3413 of 1
Current timestep = 3414. State = [[-0.2508608   0.10852512]]. Action = [[-0.15396984  0.18486959  0.0604572  -0.987615  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 3414 is [True, False, False, False, True, False]
Current timestep = 3415. State = [[-0.25173572  0.11040267]]. Action = [[ 0.24599567 -0.01702197  0.17133045 -0.9013312 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 3415 is [True, False, False, False, True, False]
Scene graph at timestep 3415 is [True, False, False, False, True, False]
State prediction error at timestep 3415 is tensor(2.8693e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3416. State = [[-0.2506595   0.11097444]]. Action = [[-0.11418536 -0.1501271   0.0887942   0.21200395]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 3416 is [True, False, False, False, True, False]
Current timestep = 3417. State = [[-0.250587    0.11082381]]. Action = [[ 0.20927143 -0.16347365 -0.01474316 -0.9626066 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3417 is [True, False, False, False, True, False]
Scene graph at timestep 3417 is [True, False, False, False, True, False]
State prediction error at timestep 3417 is tensor(2.0220e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3418. State = [[-0.24980791  0.10949852]]. Action = [[-0.21951585  0.18762007 -0.10425186  0.8660681 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 3418 is [True, False, False, False, True, False]
Current timestep = 3419. State = [[-0.25015244  0.11022925]]. Action = [[-0.1629487   0.233931    0.04949823 -0.0588479 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 3419 is [True, False, False, False, True, False]
Current timestep = 3420. State = [[-0.25168085  0.11322228]]. Action = [[ 0.17020103 -0.21647069 -0.02397187 -0.8396573 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 3420 is [True, False, False, False, True, False]
Current timestep = 3421. State = [[-0.25157976  0.11316042]]. Action = [[ 0.21435717  0.15696299 -0.21140698 -0.817948  ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 3421 is [True, False, False, False, True, False]
Scene graph at timestep 3421 is [True, False, False, False, True, False]
State prediction error at timestep 3421 is tensor(1.1804e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3422. State = [[-0.25004554  0.11388697]]. Action = [[-0.10937136  0.04589537  0.08454153  0.5600532 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 3422 is [True, False, False, False, True, False]
Scene graph at timestep 3422 is [True, False, False, False, True, False]
State prediction error at timestep 3422 is tensor(1.7690e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3423. State = [[-0.25016186  0.11502546]]. Action = [[-0.07513055  0.21788114 -0.13935766 -0.7471718 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 3423 is [True, False, False, False, True, False]
Scene graph at timestep 3423 is [True, False, False, False, True, False]
State prediction error at timestep 3423 is tensor(3.2049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3424. State = [[-0.2513875   0.11826374]]. Action = [[ 0.0220679  -0.17849183  0.14006191  0.9139173 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 3424 is [True, False, False, False, True, False]
Current timestep = 3425. State = [[-0.25147018  0.11838017]]. Action = [[ 0.21884608 -0.05482814  0.19880766  0.71704054]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 3425 is [True, False, False, False, True, False]
Current timestep = 3426. State = [[-0.2501247   0.11825351]]. Action = [[-0.08080006 -0.1867596   0.10001099 -0.25899696]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 3426 is [True, False, False, False, True, False]
Current timestep = 3427. State = [[-0.24952053  0.11668666]]. Action = [[-0.22320181 -0.06515449  0.11350751 -0.5567344 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 3427 is [True, False, False, False, True, False]
Current timestep = 3428. State = [[-0.24950847  0.11558566]]. Action = [[-0.12807249  0.10045779  0.12916091 -0.73949575]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 3428 is [True, False, False, False, True, False]
Scene graph at timestep 3428 is [True, False, False, False, True, False]
State prediction error at timestep 3428 is tensor(2.0795e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3429. State = [[-0.24999966  0.11581088]]. Action = [[ 0.06658834 -0.0207531   0.20084798 -0.46366358]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 3429 is [True, False, False, False, True, False]
Current timestep = 3430. State = [[-0.2502443  0.1157411]]. Action = [[ 0.02988467 -0.00561091  0.1688481  -0.40746623]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 3430 is [True, False, False, False, True, False]
Scene graph at timestep 3430 is [True, False, False, False, True, False]
State prediction error at timestep 3430 is tensor(1.8462e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3431. State = [[-0.2504433   0.11583298]]. Action = [[-0.12232527  0.2012228  -0.04903071 -0.711379  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 3431 is [True, False, False, False, True, False]
Current timestep = 3432. State = [[-0.25142905  0.11729899]]. Action = [[-0.18509202 -0.06306259  0.10936782 -0.900554  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 3432 is [True, False, False, False, True, False]
Current timestep = 3433. State = [[-0.25244406  0.11818461]]. Action = [[ 0.01891124 -0.17232867 -0.1485525   0.6370467 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 3433 is [True, False, False, False, True, False]
Scene graph at timestep 3433 is [True, False, False, False, True, False]
State prediction error at timestep 3433 is tensor(6.0766e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3434. State = [[-0.25292233  0.11741965]]. Action = [[0.10866699 0.1085715  0.13969344 0.95399094]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 3434 is [True, False, False, False, True, False]
Current timestep = 3435. State = [[-0.25298613  0.11741732]]. Action = [[ 0.18486017  0.20643955  0.0985221  -0.8225286 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 3435 is [True, False, False, False, True, False]
Current timestep = 3436. State = [[-0.25325426  0.11830185]]. Action = [[-0.06464773 -0.20555648  0.02957669 -0.8559268 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 3436 is [True, False, False, False, True, False]
Current timestep = 3437. State = [[-0.2531792   0.11796127]]. Action = [[-0.08326313 -0.14473176 -0.1627425  -0.5379472 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 3437 is [True, False, False, False, True, False]
Current timestep = 3438. State = [[-0.2529467   0.11678737]]. Action = [[-0.23221283 -0.16787267  0.08599591 -0.06866992]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 3438 is [True, False, False, False, True, False]
Scene graph at timestep 3438 is [True, False, False, False, True, False]
State prediction error at timestep 3438 is tensor(1.1424e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3439. State = [[-0.2540272   0.11403998]]. Action = [[-0.1202921  -0.03675179  0.17609611 -0.8607463 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 3439 is [True, False, False, False, True, False]
Current timestep = 3440. State = [[-0.25549558  0.11213426]]. Action = [[-0.12213817  0.03368655  0.22872156  0.12180972]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 3440 is [True, False, False, False, True, False]
Scene graph at timestep 3440 is [True, False, False, False, True, False]
State prediction error at timestep 3440 is tensor(4.8095e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3441. State = [[-0.25722992  0.11104888]]. Action = [[-0.05138063 -0.00617106  0.14695185  0.51431847]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 3441 is [True, False, False, False, True, False]
Current timestep = 3442. State = [[-0.258677   0.1103607]]. Action = [[ 0.03108898 -0.06688428 -0.00587431  0.90707767]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 3442 is [True, False, False, False, True, False]
Current timestep = 3443. State = [[-0.259186    0.10928894]]. Action = [[-0.18319198  0.20221949 -0.02899982 -0.7521578 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 3443 is [True, False, False, False, True, False]
Current timestep = 3444. State = [[-0.26144746  0.11076206]]. Action = [[ 0.00812647 -0.16336209 -0.0067331  -0.23099095]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 3444 is [True, False, False, False, True, False]
Current timestep = 3445. State = [[-0.26237333  0.10973309]]. Action = [[ 0.03769523  0.02782553  0.01094636 -0.23107553]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 3445 is [True, False, False, False, True, False]
Current timestep = 3446. State = [[-0.26269278  0.1094298 ]]. Action = [[-0.10910386 -0.1319291  -0.20822358  0.05798662]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 3446 is [True, False, False, False, True, False]
Current timestep = 3447. State = [[-0.26348865  0.10792586]]. Action = [[ 0.19384429  0.0843046   0.20265037 -0.8834889 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 3447 is [True, False, False, False, True, False]
Current timestep = 3448. State = [[-0.26348197  0.10772226]]. Action = [[-0.09481785 -0.14616337  0.19910568  0.68616414]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 3448 is [True, False, False, False, True, False]
Current timestep = 3449. State = [[-0.26323742  0.10606759]]. Action = [[ 0.05142188  0.05347019 -0.14483826  0.5315025 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 3449 is [True, False, False, False, True, False]
Scene graph at timestep 3449 is [True, False, False, False, True, False]
State prediction error at timestep 3449 is tensor(7.5240e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3450. State = [[-0.26316652  0.10551988]]. Action = [[-0.13886742  0.07236719  0.0671854  -0.01866144]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 3450 is [True, False, False, False, True, False]
Current timestep = 3451. State = [[-0.26377022  0.10588788]]. Action = [[0.1675629  0.11772263 0.0572632  0.89186716]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 3451 is [True, False, False, False, True, False]
Scene graph at timestep 3451 is [True, False, False, False, True, False]
State prediction error at timestep 3451 is tensor(5.4653e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3452. State = [[-0.26405132  0.1064016 ]]. Action = [[ 0.1842668   0.15117085 -0.00818497 -0.14535928]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 3452 is [True, False, False, False, True, False]
Current timestep = 3453. State = [[-0.2643424   0.10734617]]. Action = [[ 0.20394292  0.13064235 -0.11897534  0.58111644]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 3453 is [True, False, False, False, True, False]
Scene graph at timestep 3453 is [True, False, False, False, True, False]
State prediction error at timestep 3453 is tensor(1.4541e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3454. State = [[-0.2629045   0.10929976]]. Action = [[ 0.1633845  -0.05624494 -0.20502186  0.51682305]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 3454 is [True, False, False, False, True, False]
Current timestep = 3455. State = [[-0.26085952  0.11014523]]. Action = [[-0.04098371 -0.12615596 -0.06986952 -0.0917356 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 3455 is [True, False, False, False, True, False]
Current timestep = 3456. State = [[-0.26044148  0.11021423]]. Action = [[-0.2307109   0.02574441  0.01132193 -0.6456721 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 3456 is [True, False, False, False, True, False]
Current timestep = 3457. State = [[-0.26043007  0.11045899]]. Action = [[-0.1180139  -0.11533743 -0.0175575  -0.9423384 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 3457 is [True, False, False, False, True, False]
Current timestep = 3458. State = [[-0.2605923   0.11056339]]. Action = [[ 0.14666188  0.18965751 -0.01744054  0.305107  ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 3458 is [True, False, False, False, True, False]
Scene graph at timestep 3458 is [True, False, False, False, True, False]
State prediction error at timestep 3458 is tensor(3.1132e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3459. State = [[-0.26097795  0.11124363]]. Action = [[-0.16919875  0.22878826 -0.06729981  0.7725924 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 3459 is [True, False, False, False, True, False]
Current timestep = 3460. State = [[-0.2629717  0.1149015]]. Action = [[-0.10289073  0.13124132  0.04677883  0.13966656]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 3460 is [True, False, False, False, True, False]
Current timestep = 3461. State = [[-0.26560983  0.11947334]]. Action = [[ 0.06277582 -0.18907237 -0.09530187  0.04054594]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 3461 is [True, False, False, False, True, False]
Scene graph at timestep 3461 is [True, False, False, False, True, False]
State prediction error at timestep 3461 is tensor(4.8345e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3462. State = [[-0.26603487  0.12016721]]. Action = [[-0.13572061  0.01588154 -0.02359261 -0.8074523 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 3462 is [True, False, False, False, True, False]
Current timestep = 3463. State = [[-0.26671046  0.12111881]]. Action = [[ 0.0313203   0.16917107 -0.19574103 -0.60856026]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 3463 is [True, False, False, False, True, False]
Current timestep = 3464. State = [[-0.26799804  0.12326463]]. Action = [[-0.08301169  0.14743793  0.17527765  0.38615298]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 3464 is [True, False, False, False, True, False]
Human Feedback received at timestep 3464 of -1
Current timestep = 3465. State = [[-0.2703278   0.12701823]]. Action = [[-0.13278651 -0.01371357 -0.18690364 -0.52530533]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 3465 is [True, False, False, False, True, False]
Scene graph at timestep 3465 is [True, False, False, False, False, True]
State prediction error at timestep 3465 is tensor(1.0840e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3466. State = [[-0.27235687  0.1300352 ]]. Action = [[ 0.22228563 -0.12427843  0.04850018 -0.47710246]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 3466 is [True, False, False, False, False, True]
Current timestep = 3467. State = [[-0.27233365  0.1299498 ]]. Action = [[-0.19078237 -0.1047712  -0.07624277  0.93769264]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 3467 is [True, False, False, False, False, True]
Scene graph at timestep 3467 is [True, False, False, False, False, True]
State prediction error at timestep 3467 is tensor(1.9896e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3468. State = [[-0.27223277  0.1299482 ]]. Action = [[ 0.13949123  0.23205364 -0.11722612  0.19060087]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 3468 is [True, False, False, False, False, True]
Current timestep = 3469. State = [[-0.27249253  0.13090865]]. Action = [[-0.07780308 -0.00929579  0.20416713 -0.73220026]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 3469 is [True, False, False, False, False, True]
Scene graph at timestep 3469 is [True, False, False, False, False, True]
State prediction error at timestep 3469 is tensor(4.8595e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3470. State = [[-0.2729427   0.13177183]]. Action = [[-0.17784154  0.03287369  0.11346191 -0.82063824]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 3470 is [True, False, False, False, False, True]
Current timestep = 3471. State = [[-0.27308652  0.13219303]]. Action = [[ 0.13660726  0.05911809  0.02092686 -0.725767  ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 3471 is [True, False, False, False, False, True]
Current timestep = 3472. State = [[-0.27227813  0.1327449 ]]. Action = [[ 0.2316128   0.18200523 -0.18909258 -0.5414639 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 3472 is [True, False, False, False, False, True]
Current timestep = 3473. State = [[-0.26988465  0.13454263]]. Action = [[-0.19382986  0.18411428  0.09723979 -0.3492397 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 3473 is [True, False, False, False, False, True]
Current timestep = 3474. State = [[-0.26835567  0.13566838]]. Action = [[-0.16387805 -0.08597967  0.19540715  0.8553834 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 3474 is [True, False, False, False, False, True]
Current timestep = 3475. State = [[-0.26850384  0.1362015 ]]. Action = [[-0.1244203   0.21192408  0.14011794 -0.60912305]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 3475 is [True, False, False, False, False, True]
Current timestep = 3476. State = [[-0.27010867  0.13893175]]. Action = [[ 0.1993314  -0.10108784  0.01080048 -0.42731696]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 3476 is [True, False, False, False, False, True]
Current timestep = 3477. State = [[-0.2699531   0.13939573]]. Action = [[-0.13389312 -0.11388989  0.20392716  0.92583394]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 3477 is [True, False, False, False, False, True]
Current timestep = 3478. State = [[-0.26991677  0.13939668]]. Action = [[ 0.1586093   0.20458272 -0.16219625  0.20482063]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 3478 is [True, False, False, False, False, True]
Current timestep = 3479. State = [[-0.26927844  0.14032146]]. Action = [[0.21648529 0.06691763 0.01278445 0.6955174 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 3479 is [True, False, False, False, False, True]
Current timestep = 3480. State = [[-0.26691562  0.14154963]]. Action = [[-0.22154321  0.06936985 -0.04258116 -0.12416118]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 3480 is [True, False, False, False, False, True]
Current timestep = 3481. State = [[-0.2637433   0.14295682]]. Action = [[ 0.1015442  0.0572578  0.0674352 -0.4345169]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 3481 is [True, False, False, False, False, True]
Current timestep = 3482. State = [[-0.26065734  0.14483152]]. Action = [[-0.08511126  0.19176292 -0.1300378  -0.53914   ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 3482 is [True, False, False, False, False, True]
Scene graph at timestep 3482 is [True, False, False, False, False, True]
State prediction error at timestep 3482 is tensor(1.0903e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3483. State = [[-0.2592856  0.1494176]]. Action = [[-0.2400548   0.09638023  0.21065226  0.03181517]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 3483 is [True, False, False, False, False, True]
Current timestep = 3484. State = [[-0.26089847  0.15390328]]. Action = [[-0.0332122  -0.18384044 -0.05770096  0.92981124]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 3484 is [True, False, False, False, False, True]
Scene graph at timestep 3484 is [True, False, False, False, False, True]
State prediction error at timestep 3484 is tensor(8.5454e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3485. State = [[-0.26162636  0.15513772]]. Action = [[ 0.11954641  0.09251785 -0.08144149 -0.06109428]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 3485 is [True, False, False, False, False, True]
Current timestep = 3486. State = [[-0.26188377  0.15611637]]. Action = [[-0.06461337  0.10786167 -0.11481689  0.9329959 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 3486 is [True, False, False, False, False, True]
Current timestep = 3487. State = [[-0.26275024  0.15805204]]. Action = [[ 0.1258558  -0.11614847 -0.02209754  0.65623   ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 3487 is [True, False, False, False, False, True]
Current timestep = 3488. State = [[-0.26195043  0.15832876]]. Action = [[-0.11472306 -0.11791265  0.24449587 -0.00802433]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 3488 is [True, False, False, False, False, True]
Human Feedback received at timestep 3488 of -1
Current timestep = 3489. State = [[-0.26193798  0.1581826 ]]. Action = [[ 0.03006682  0.15074608 -0.04141797  0.06321657]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 3489 is [True, False, False, False, False, True]
Current timestep = 3490. State = [[-0.26214802  0.15847258]]. Action = [[ 0.07228267 -0.01310797 -0.20097621  0.40536964]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 3490 is [True, False, False, False, False, True]
Current timestep = 3491. State = [[-0.2619406   0.15843475]]. Action = [[ 0.12428042 -0.11654918  0.11640102  0.1895585 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 3491 is [True, False, False, False, False, True]
Scene graph at timestep 3491 is [True, False, False, False, False, True]
State prediction error at timestep 3491 is tensor(1.1709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3492. State = [[-0.26053154  0.15827197]]. Action = [[ 0.17509308 -0.07407925 -0.1775206  -0.69045615]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 3492 is [True, False, False, False, False, True]
Current timestep = 3493. State = [[-0.25783592  0.15715162]]. Action = [[0.06563962 0.07763866 0.23263785 0.5281191 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 3493 is [True, False, False, False, False, True]
Scene graph at timestep 3493 is [True, False, False, False, False, True]
State prediction error at timestep 3493 is tensor(7.2319e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3494. State = [[-0.25480387  0.15765782]]. Action = [[-0.0342907  -0.00240338  0.20350724  0.510211  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 3494 is [True, False, False, False, False, True]
Current timestep = 3495. State = [[-0.25416568  0.1579245 ]]. Action = [[ 0.11516178  0.14787436 -0.02467637 -0.23135722]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 3495 is [True, False, False, False, False, True]
Current timestep = 3496. State = [[-0.25298324  0.15906015]]. Action = [[-0.10319753  0.09883946 -0.09690607  0.2975942 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 3496 is [True, False, False, False, False, True]
Current timestep = 3497. State = [[-0.2527579   0.16004431]]. Action = [[-0.0436817  -0.1278624   0.13018012 -0.28659868]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 3497 is [True, False, False, False, False, True]
Scene graph at timestep 3497 is [True, False, False, False, False, True]
State prediction error at timestep 3497 is tensor(2.7190e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3498. State = [[-0.252762    0.15998723]]. Action = [[ 0.05769032 -0.06470752  0.187159   -0.21987367]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 3498 is [True, False, False, False, False, True]
Current timestep = 3499. State = [[-0.25241187  0.15973558]]. Action = [[ 0.03680927 -0.03772338  0.15989435 -0.6435055 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 3499 is [True, False, False, False, False, True]
Scene graph at timestep 3499 is [True, False, False, False, False, True]
State prediction error at timestep 3499 is tensor(1.5755e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3500. State = [[-0.25217348  0.15917334]]. Action = [[-0.18922216 -0.08975098  0.16136241  0.74297357]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 3500 is [True, False, False, False, False, True]
Current timestep = 3501. State = [[-0.25208813  0.15854743]]. Action = [[0.15088838 0.12624902 0.07617855 0.4433639 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 3501 is [True, False, False, False, False, True]
Scene graph at timestep 3501 is [True, False, False, False, False, True]
State prediction error at timestep 3501 is tensor(1.0193e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3502. State = [[-0.25212005  0.15859458]]. Action = [[-0.07342052 -0.02714396 -0.15395851  0.17607844]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 3502 is [True, False, False, False, False, True]
Scene graph at timestep 3502 is [True, False, False, False, False, True]
State prediction error at timestep 3502 is tensor(1.6201e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3503. State = [[-0.25211734  0.15846387]]. Action = [[-0.22141622 -0.15242717  0.10031104 -0.93579227]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 3503 is [True, False, False, False, False, True]
Current timestep = 3504. State = [[-0.25246277  0.15782411]]. Action = [[-0.10785386  0.16265371  0.14120388  0.79664516]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 3504 is [True, False, False, False, False, True]
Current timestep = 3505. State = [[-0.25335124  0.15894514]]. Action = [[-0.01541506  0.02712405  0.15856877 -0.04324758]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 3505 is [True, False, False, False, False, True]
Scene graph at timestep 3505 is [True, False, False, False, False, True]
State prediction error at timestep 3505 is tensor(5.6994e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3506. State = [[-0.2538644   0.15973492]]. Action = [[-0.08657998  0.22667861  0.20487493  0.43378413]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 3506 is [True, False, False, False, False, True]
Current timestep = 3507. State = [[-0.25624964  0.16313349]]. Action = [[-0.15776826  0.238734    0.19647136 -0.16154683]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 3507 is [True, False, False, False, False, True]
Current timestep = 3508. State = [[-0.25993904  0.16831762]]. Action = [[ 0.16600269 -0.10971795 -0.24012066  0.26358247]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 3508 is [True, False, False, False, False, True]
Current timestep = 3509. State = [[-0.26129034  0.17011775]]. Action = [[-0.2290622  -0.08578338 -0.06675333 -0.97523826]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 3509 is [True, False, False, False, False, True]
Current timestep = 3510. State = [[-0.26211455  0.17111073]]. Action = [[ 1.3427764e-01  1.2046015e-01  2.9876828e-04 -5.7664591e-01]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 3510 is [True, False, False, False, False, True]
Current timestep = 3511. State = [[-0.26289028  0.17211151]]. Action = [[-0.1380997  -0.13131371  0.07017049  0.8863939 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 3511 is [True, False, False, False, False, True]
Scene graph at timestep 3511 is [True, False, False, False, False, True]
State prediction error at timestep 3511 is tensor(1.5303e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3512. State = [[-0.26312515  0.17232738]]. Action = [[ 0.10241538  0.09378982  0.16290331 -0.403701  ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 3512 is [True, False, False, False, False, True]
Human Feedback received at timestep 3512 of -1
Current timestep = 3513. State = [[-0.2634081   0.17251675]]. Action = [[ 0.18981937 -0.11533006 -0.10753983  0.9568751 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 3513 is [True, False, False, False, False, True]
Current timestep = 3514. State = [[-0.26325718  0.1721373 ]]. Action = [[-0.18953745  0.23799127 -0.02826877 -0.48982334]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 3514 is [True, False, False, False, False, True]
Scene graph at timestep 3514 is [True, False, False, False, False, True]
State prediction error at timestep 3514 is tensor(4.3863e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3515. State = [[-0.26450306  0.17386483]]. Action = [[-0.07679853 -0.09109434  0.14515191 -0.62023306]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 3515 is [True, False, False, False, False, True]
Current timestep = 3516. State = [[-0.26490766  0.17437054]]. Action = [[-0.18512449 -0.08870396 -0.01714061 -0.7139357 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 3516 is [True, False, False, False, False, True]
Current timestep = 3517. State = [[-0.2655715   0.17451373]]. Action = [[ 0.17910731  0.10421535 -0.21907412 -0.14781463]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 3517 is [True, False, False, False, False, True]
Scene graph at timestep 3517 is [True, False, False, False, False, True]
State prediction error at timestep 3517 is tensor(3.2325e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3518. State = [[-0.26576558  0.17467377]]. Action = [[ 0.17992139 -0.04676266 -0.11047618 -0.36692882]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 3518 is [True, False, False, False, False, True]
Scene graph at timestep 3518 is [True, False, False, False, False, True]
State prediction error at timestep 3518 is tensor(8.7854e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3519. State = [[-0.2656546   0.17458896]]. Action = [[ 0.01364395  0.0841642   0.13735914 -0.36091793]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 3519 is [True, False, False, False, False, True]
Current timestep = 3520. State = [[-0.2656394   0.17460951]]. Action = [[ 0.14721769 -0.06392652 -0.0196991   0.97740364]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 3520 is [True, False, False, False, False, True]
Current timestep = 3521. State = [[-0.26533225  0.17437977]]. Action = [[ 0.1341658  -0.11660972 -0.17263018 -0.97293013]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 3521 is [True, False, False, False, False, True]
Current timestep = 3522. State = [[-0.26402372  0.17307864]]. Action = [[-0.12781246  0.19103995  0.14347413  0.7630422 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 3522 is [True, False, False, False, False, True]
Current timestep = 3523. State = [[-0.26433355  0.17350632]]. Action = [[-0.22005142  0.18896896  0.21741277 -0.32566786]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 3523 is [True, False, False, False, False, True]
Scene graph at timestep 3523 is [True, False, False, False, False, True]
State prediction error at timestep 3523 is tensor(4.4778e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3524. State = [[-0.26604462  0.17624149]]. Action = [[ 0.1775127  -0.01531646 -0.1294922   0.49898684]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 3524 is [True, False, False, False, False, True]
Current timestep = 3525. State = [[-0.26588526  0.17711496]]. Action = [[-0.08393359  0.1557604  -0.02983758 -0.74772674]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 3525 is [True, False, False, False, False, True]
Current timestep = 3526. State = [[-0.26673734  0.17955567]]. Action = [[ 0.18290752  0.18170518 -0.01448172  0.5283253 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 3526 is [True, False, False, False, False, True]
Current timestep = 3527. State = [[-0.2658017   0.18270892]]. Action = [[-0.05938187 -0.12659058 -0.13046671 -0.76075214]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 3527 is [True, False, False, False, False, True]
Scene graph at timestep 3527 is [True, False, False, False, False, True]
State prediction error at timestep 3527 is tensor(5.3895e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3528. State = [[-0.26458755  0.18385698]]. Action = [[ 0.00765783 -0.21938105  0.23727149 -0.82335454]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 3528 is [True, False, False, False, False, True]
Scene graph at timestep 3528 is [True, False, False, False, False, True]
State prediction error at timestep 3528 is tensor(2.9301e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3529. State = [[-0.26412272  0.18320222]]. Action = [[ 0.10566157 -0.06462094 -0.05615325 -0.87121165]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 3529 is [True, False, False, False, False, True]
Current timestep = 3530. State = [[-0.26269948  0.18207829]]. Action = [[ 0.09512246 -0.20018928  0.10983613  0.25953603]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 3530 is [True, False, False, False, False, True]
Current timestep = 3531. State = [[-0.2608481   0.17886132]]. Action = [[ 0.00960174 -0.21690741  0.22488901 -0.28649175]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 3531 is [True, False, False, False, False, True]
Current timestep = 3532. State = [[-0.25909218  0.17386542]]. Action = [[ 0.00603718 -0.1937554  -0.11776569 -0.5778861 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 3532 is [True, False, False, False, False, True]
Current timestep = 3533. State = [[-0.2574316   0.16884686]]. Action = [[ 0.20063192  0.00101894  0.20332512 -0.1061098 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 3533 is [True, False, False, False, False, True]
Current timestep = 3534. State = [[-0.25508428  0.16539086]]. Action = [[-0.00792015 -0.10017991  0.17244875 -0.5883326 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 3534 is [True, False, False, False, False, True]
Current timestep = 3535. State = [[-0.2525885   0.16225019]]. Action = [[-0.01039924 -0.06846431  0.17282474  0.815511  ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 3535 is [True, False, False, False, False, True]
Scene graph at timestep 3535 is [True, False, False, False, False, True]
State prediction error at timestep 3535 is tensor(1.1057e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3536. State = [[-0.25004473  0.15884326]]. Action = [[ 0.16041839  0.05907708 -0.05621791  0.9144027 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 3536 is [True, False, False, False, False, True]
Current timestep = 3537. State = [[-0.2479652   0.15777728]]. Action = [[-0.07636824  0.13023135 -0.17984527 -0.8838777 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 3537 is [True, False, False, False, False, True]
Scene graph at timestep 3537 is [True, False, False, False, False, True]
State prediction error at timestep 3537 is tensor(5.5200e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3538. State = [[-0.24683848  0.15816407]]. Action = [[ 0.22785145 -0.1833527  -0.18997453 -0.09375501]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 3538 is [True, False, False, False, False, True]
Current timestep = 3539. State = [[-0.24446829  0.15672053]]. Action = [[-0.20622945 -0.04448578  0.16991812 -0.03943539]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 3539 is [True, False, False, False, False, True]
Human Feedback received at timestep 3539 of 1
Current timestep = 3540. State = [[-0.24415176  0.15536192]]. Action = [[-0.11010471  0.02161136  0.04143596 -0.82951283]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 3540 is [True, False, False, False, False, True]
Current timestep = 3541. State = [[-0.24420083  0.15534863]]. Action = [[-0.09386158  0.2065269   0.13826156 -0.7399903 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 3541 is [True, False, False, False, False, True]
Current timestep = 3542. State = [[-0.24493109  0.15652132]]. Action = [[ 0.13421655 -0.21783197 -0.11470219  0.22028005]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 3542 is [True, False, False, False, False, True]
Current timestep = 3543. State = [[-0.2446211  0.1554641]]. Action = [[ 0.0532833  -0.0125386   0.12662444 -0.46216452]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 3543 is [True, False, False, False, False, True]
Current timestep = 3544. State = [[-0.24432561  0.15486276]]. Action = [[0.08517128 0.2085675  0.20374519 0.94369245]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 3544 is [True, False, False, False, False, True]
Scene graph at timestep 3544 is [True, False, False, False, False, True]
State prediction error at timestep 3544 is tensor(3.7359e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3545. State = [[-0.24365808  0.15566638]]. Action = [[-0.02761285  0.18086198  0.00925714  0.62096024]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 3545 is [True, False, False, False, False, True]
Scene graph at timestep 3545 is [True, False, False, False, False, True]
State prediction error at timestep 3545 is tensor(1.6788e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3546. State = [[-0.243089    0.15802988]]. Action = [[ 0.05697444  0.15732491  0.0777376  -0.45811474]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 3546 is [True, False, False, False, False, True]
Scene graph at timestep 3546 is [True, False, False, False, False, True]
State prediction error at timestep 3546 is tensor(1.3881e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3547. State = [[-0.2420032   0.16179429]]. Action = [[6.0704350e-04 2.3065820e-01 1.3949090e-01 9.1521573e-01]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 3547 is [True, False, False, False, False, True]
Current timestep = 3548. State = [[-0.24209887  0.1664683 ]]. Action = [[-0.21693839 -0.09781921  0.11949533  0.01839161]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 3548 is [True, False, False, False, False, True]
Current timestep = 3549. State = [[-0.24365541  0.16856691]]. Action = [[-0.16092518 -0.10669687 -0.15154077 -0.7109028 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 3549 is [True, False, False, False, False, True]
Scene graph at timestep 3549 is [True, False, False, False, False, True]
State prediction error at timestep 3549 is tensor(2.1786e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3550. State = [[-0.24453953  0.16969329]]. Action = [[ 0.09162    -0.14304894  0.14041257 -0.32188386]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 3550 is [True, False, False, False, False, True]
Current timestep = 3551. State = [[-0.2444928   0.16919655]]. Action = [[-0.0051823  -0.12985477 -0.08341083  0.4303317 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 3551 is [True, False, False, False, False, True]
Scene graph at timestep 3551 is [True, False, False, False, False, True]
State prediction error at timestep 3551 is tensor(1.0179e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3552. State = [[-0.24405259  0.16771726]]. Action = [[-0.11856161 -0.11501278  0.03820956  0.81373763]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 3552 is [True, False, False, False, False, True]
Current timestep = 3553. State = [[-0.2437386   0.16574779]]. Action = [[ 0.19525513  0.08186901 -0.20142037 -0.45966578]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 3553 is [True, False, False, False, False, True]
Current timestep = 3554. State = [[-0.24358149  0.16495386]]. Action = [[-0.05550098  0.09555858 -0.15824567 -0.8736759 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 3554 is [True, False, False, False, False, True]
Current timestep = 3555. State = [[-0.24376889  0.16535053]]. Action = [[ 0.20346159  0.15406698 -0.08462538 -0.46474564]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 3555 is [True, False, False, False, False, True]
Current timestep = 3556. State = [[-0.24370517  0.16558443]]. Action = [[ 0.11144537 -0.06208977  0.21853215 -0.80378413]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 3556 is [True, False, False, False, False, True]
Current timestep = 3557. State = [[-0.24292253  0.16602404]]. Action = [[-0.21351749  0.220258    0.0486533   0.56830955]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 3557 is [True, False, False, False, False, True]
Scene graph at timestep 3557 is [True, False, False, False, False, True]
State prediction error at timestep 3557 is tensor(9.7006e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3558. State = [[-0.24403766  0.16803338]]. Action = [[ 0.2174336   0.10931841 -0.22968678 -0.79358506]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 3558 is [True, False, False, False, False, True]
Current timestep = 3559. State = [[-0.24292225  0.16999634]]. Action = [[-0.15321192 -0.0843464  -0.11881265  0.9721353 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 3559 is [True, False, False, False, False, True]
Current timestep = 3560. State = [[-0.24331878  0.170725  ]]. Action = [[-0.16013992  0.01727948  0.0220263   0.5930824 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 3560 is [True, False, False, False, False, True]
Current timestep = 3561. State = [[-0.2440512   0.17173932]]. Action = [[-0.10444473 -0.06657438 -0.04154798 -0.51482874]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 3561 is [True, False, False, False, False, True]
Current timestep = 3562. State = [[-0.24447353  0.17237127]]. Action = [[ 0.04394516  0.14332491  0.18118674 -0.4291451 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 3562 is [True, False, False, False, False, True]
Current timestep = 3563. State = [[-0.24554391  0.17403632]]. Action = [[-2.2703889e-01 -5.2481890e-05 -2.0775530e-01  7.6017570e-01]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 3563 is [True, False, False, False, False, True]
Current timestep = 3564. State = [[-0.24747108  0.17639974]]. Action = [[-0.15115838  0.13724622  0.12993264 -0.11963022]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 3564 is [True, False, False, False, False, True]
Current timestep = 3565. State = [[-0.25026146  0.18025018]]. Action = [[ 0.15000397 -0.06840879 -0.02964628  0.55285263]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 3565 is [True, False, False, False, False, True]
Current timestep = 3566. State = [[-0.25104648  0.18133208]]. Action = [[-0.09597203  0.1839593  -0.20941846 -0.96577066]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 3566 is [True, False, False, False, False, True]
Scene graph at timestep 3566 is [True, False, False, False, False, True]
State prediction error at timestep 3566 is tensor(2.1495e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3567. State = [[-0.25316194  0.18431787]]. Action = [[-0.0744838   0.17790377  0.23870128  0.5741768 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 3567 is [True, False, False, False, False, True]
Current timestep = 3568. State = [[-0.2558731   0.18813503]]. Action = [[-0.18043353 -0.06648302 -0.16177122 -0.27091992]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 3568 is [True, False, False, False, False, True]
Current timestep = 3569. State = [[-0.25788087  0.19077465]]. Action = [[ 0.03862315 -0.13523157 -0.16361356  0.33045518]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 3569 is [True, False, False, False, False, True]
Scene graph at timestep 3569 is [True, False, False, False, False, True]
State prediction error at timestep 3569 is tensor(4.2956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3570. State = [[-0.25833926  0.19099104]]. Action = [[ 0.12462962 -0.16486147 -0.20484883  0.9718168 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 3570 is [True, False, False, False, False, True]
Human Feedback received at timestep 3570 of -1
Current timestep = 3571. State = [[-0.25782    0.1891828]]. Action = [[ 0.15139571 -0.05549225 -0.09728006  0.59340644]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 3571 is [True, False, False, False, False, True]
Current timestep = 3572. State = [[-0.25697526  0.18693618]]. Action = [[-0.17744145 -0.18029256 -0.08621773 -0.48187757]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 3572 is [True, False, False, False, False, True]
Current timestep = 3573. State = [[-0.25621697  0.18390462]]. Action = [[ 0.156667   -0.07501899 -0.09061842 -0.80883646]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 3573 is [True, False, False, False, False, True]
Current timestep = 3574. State = [[-0.25531214  0.18038769]]. Action = [[-0.22793609  0.10002694  0.08771104  0.06986797]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 3574 is [True, False, False, False, False, True]
Current timestep = 3575. State = [[-0.2561168   0.18018416]]. Action = [[ 0.16429311  0.15133393 -0.14544724 -0.54942805]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 3575 is [True, False, False, False, False, True]
Scene graph at timestep 3575 is [True, False, False, False, False, True]
State prediction error at timestep 3575 is tensor(3.7716e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3576. State = [[-0.25646937  0.18074252]]. Action = [[-0.06230313  0.06856075 -0.10171886  0.23793745]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 3576 is [True, False, False, False, False, True]
Current timestep = 3577. State = [[-0.25675857  0.18128441]]. Action = [[-0.0513403   0.14995658 -0.20389456  0.2781589 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 3577 is [True, False, False, False, False, True]
Current timestep = 3578. State = [[-0.25811636  0.18321513]]. Action = [[-0.18020439 -0.19195004 -0.04593714  0.57841635]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 3578 is [True, False, False, False, False, True]
Scene graph at timestep 3578 is [True, False, False, False, False, True]
State prediction error at timestep 3578 is tensor(2.0695e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3579. State = [[-0.2590773   0.18398204]]. Action = [[-0.22422539  0.15936849  0.15912092 -0.38293326]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 3579 is [True, False, False, False, False, True]
Current timestep = 3580. State = [[-0.2623675   0.18686254]]. Action = [[-0.23467536  0.14612904 -0.04055694  0.79299235]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 3580 is [True, False, False, False, False, True]
Current timestep = 3581. State = [[-0.2673453   0.19110408]]. Action = [[-0.1495321  -0.03553158 -0.03005879 -0.5689835 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 3581 is [True, False, False, False, False, True]
Current timestep = 3582. State = [[-0.27176675  0.19363293]]. Action = [[ 0.17926717  0.09085643  0.04554617 -0.6272474 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 3582 is [True, False, False, False, False, True]
Current timestep = 3583. State = [[-0.27378568  0.19551544]]. Action = [[ 0.22078335  0.164285   -0.03702201  0.13829231]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 3583 is [True, False, False, False, False, True]
Current timestep = 3584. State = [[-0.2745901   0.19666268]]. Action = [[ 0.23291993 -0.2052848  -0.22389078  0.9142362 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 3584 is [True, False, False, False, False, True]
Current timestep = 3585. State = [[-0.27319145  0.19627574]]. Action = [[ 0.0014815   0.15684456 -0.20863432 -0.250588  ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 3585 is [True, False, False, False, False, True]
Current timestep = 3586. State = [[-0.27299973  0.1973568 ]]. Action = [[ 0.04915604 -0.11091048  0.00663543  0.17030787]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 3586 is [True, False, False, False, False, True]
Human Feedback received at timestep 3586 of -1
Current timestep = 3587. State = [[-0.27223477  0.19747576]]. Action = [[-0.16484928  0.03704748 -0.15670052 -0.53456765]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 3587 is [True, False, False, False, False, True]
Current timestep = 3588. State = [[-0.27179524  0.1975892 ]]. Action = [[-0.23110162  0.07529351 -0.05818452 -0.027215  ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 3588 is [True, False, False, False, False, True]
Current timestep = 3589. State = [[-0.27143914  0.19754998]]. Action = [[ 0.11146167 -0.08010903 -0.08947897 -0.8170999 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 3589 is [True, False, False, False, False, True]
Current timestep = 3590. State = [[-0.27027008  0.1960398 ]]. Action = [[ 0.1858499  -0.2258083   0.11579344  0.8375795 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 3590 is [True, False, False, False, False, True]
Current timestep = 3591. State = [[-0.26764354  0.19217569]]. Action = [[-0.00527315 -0.20222044 -0.12245476  0.10749328]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 3591 is [True, False, False, False, False, True]
Scene graph at timestep 3591 is [True, False, False, False, False, True]
State prediction error at timestep 3591 is tensor(1.0193e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3592. State = [[-0.26529542  0.18660791]]. Action = [[-0.02149294 -0.23774855  0.03367099 -0.5167963 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 3592 is [True, False, False, False, False, True]
Current timestep = 3593. State = [[-0.2630828   0.18094002]]. Action = [[-0.16731685 -0.03733709 -0.18915917  0.95081794]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 3593 is [True, False, False, False, False, True]
Current timestep = 3594. State = [[-0.26233467  0.17752425]]. Action = [[-0.02716014  0.22285384  0.16588381  0.69602597]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 3594 is [True, False, False, False, False, True]
Current timestep = 3595. State = [[-0.2628178   0.17755316]]. Action = [[ 0.06258917  0.01707157  0.11583987 -0.39365047]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 3595 is [True, False, False, False, False, True]
Current timestep = 3596. State = [[-0.26302603  0.17769553]]. Action = [[-0.22696893  0.05360386 -0.18967645 -0.9839279 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 3596 is [True, False, False, False, False, True]
Current timestep = 3597. State = [[-0.2634477   0.17824216]]. Action = [[ 0.13268796 -0.16347654  0.01062372 -0.69654375]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 3597 is [True, False, False, False, False, True]
Current timestep = 3598. State = [[-0.26278046  0.17673181]]. Action = [[-0.07107055 -0.18697394  0.08513114 -0.6781975 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 3598 is [True, False, False, False, False, True]
Current timestep = 3599. State = [[-0.26245406  0.17368315]]. Action = [[-0.05534154 -0.21401794  0.17682815 -0.12335718]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 3599 is [True, False, False, False, False, True]
Current timestep = 3600. State = [[-0.2627511  0.1693427]]. Action = [[-0.18126217  0.10192427  0.2286585  -0.91703975]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 3600 is [True, False, False, False, False, True]
Current timestep = 3601. State = [[-0.2640385   0.16829745]]. Action = [[0.23305345 0.05337796 0.10030171 0.39761758]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 3601 is [True, False, False, False, False, True]
Scene graph at timestep 3601 is [True, False, False, False, False, True]
State prediction error at timestep 3601 is tensor(2.3347e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3601 of 1
Current timestep = 3602. State = [[-0.26373208  0.16775863]]. Action = [[ 0.07817036 -0.16786736 -0.11637555 -0.22377622]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 3602 is [True, False, False, False, False, True]
Current timestep = 3603. State = [[-0.26307875  0.16597453]]. Action = [[-0.21128227  0.14862412  0.2216003   0.48834717]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 3603 is [True, False, False, False, False, True]
Current timestep = 3604. State = [[-0.2634391   0.16621526]]. Action = [[ 0.1436534  -0.09043637  0.01820406  0.55632734]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 3604 is [True, False, False, False, False, True]
Current timestep = 3605. State = [[-0.26318392  0.16549845]]. Action = [[-0.08837852  0.09256861 -0.03445278  0.5657785 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 3605 is [True, False, False, False, False, True]
Current timestep = 3606. State = [[-0.26352522  0.1657918 ]]. Action = [[-0.20922303  0.19080538  0.0417141   0.9501567 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 3606 is [True, False, False, False, False, True]
Current timestep = 3607. State = [[-0.265323    0.16800576]]. Action = [[ 0.02594164 -0.21277183  0.03844768 -0.8302932 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 3607 is [True, False, False, False, False, True]
Current timestep = 3608. State = [[-0.26615635  0.16733369]]. Action = [[-0.0179649  -0.16851814 -0.08714904  0.7236788 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 3608 is [True, False, False, False, False, True]
Current timestep = 3609. State = [[-0.26683462  0.16508497]]. Action = [[-0.2008028   0.00819665  0.17429906  0.33645177]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 3609 is [True, False, False, False, False, True]
Current timestep = 3610. State = [[-0.26814163  0.16353829]]. Action = [[ 0.22723722 -0.14826205 -0.17948502 -0.33240676]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 3610 is [True, False, False, False, False, True]
Current timestep = 3611. State = [[-0.26746127  0.1608775 ]]. Action = [[-0.15213053  0.10324311 -0.03557892  0.52884555]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 3611 is [True, False, False, False, False, True]
Current timestep = 3612. State = [[-0.26839423  0.1602246 ]]. Action = [[ 0.15009463  0.1271748   0.15088919 -0.7459009 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 3612 is [True, False, False, False, False, True]
Scene graph at timestep 3612 is [True, False, False, False, False, True]
State prediction error at timestep 3612 is tensor(1.9297e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3613. State = [[-0.26867822  0.160551  ]]. Action = [[ 0.07573673 -0.07727833  0.02867272 -0.86967736]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 3613 is [True, False, False, False, False, True]
Current timestep = 3614. State = [[-0.2684278  0.1599656]]. Action = [[ 0.10902351 -0.19080587  0.10134438 -0.471573  ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 3614 is [True, False, False, False, False, True]
Current timestep = 3615. State = [[-0.2672632   0.15776825]]. Action = [[0.0706065  0.09858391 0.12707457 0.84266305]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 3615 is [True, False, False, False, False, True]
Current timestep = 3616. State = [[-0.26679617  0.15727752]]. Action = [[ 0.06904846 -0.14742088  0.01815245 -0.07241619]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 3616 is [True, False, False, False, False, True]
Current timestep = 3617. State = [[-0.26576927  0.15531056]]. Action = [[ 0.2332241  -0.16378903  0.16049582 -0.37853348]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 3617 is [True, False, False, False, False, True]
Current timestep = 3618. State = [[-0.26311564  0.15176308]]. Action = [[ 0.11946926  0.02318969  0.10920048 -0.7777843 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 3618 is [True, False, False, False, False, True]
Current timestep = 3619. State = [[-0.26088944  0.14968587]]. Action = [[ 0.19849521  0.04203898 -0.17439888 -0.6423385 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 3619 is [True, False, False, False, False, True]
Current timestep = 3620. State = [[-0.25777674  0.14852501]]. Action = [[-0.04157923 -0.07486956 -0.19069983 -0.9574791 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 3620 is [True, False, False, False, False, True]
Current timestep = 3621. State = [[-0.25634772  0.14660496]]. Action = [[ 0.02669472 -0.09178738  0.11416918  0.47866273]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 3621 is [True, False, False, False, False, True]
Current timestep = 3622. State = [[-0.2552833   0.14450455]]. Action = [[ 0.06408858 -0.13421872 -0.06980705 -0.7960607 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 3622 is [True, False, False, False, False, True]
Human Feedback received at timestep 3622 of 1
Current timestep = 3623. State = [[-0.25361368  0.14085637]]. Action = [[-0.22790532 -0.13487479 -0.18573341  0.01297319]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 3623 is [True, False, False, False, False, True]
Current timestep = 3624. State = [[-0.25290173  0.13802877]]. Action = [[ 0.13532534 -0.04310223 -0.14243007  0.24393308]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 3624 is [True, False, False, False, False, True]
Scene graph at timestep 3624 is [True, False, False, False, False, True]
State prediction error at timestep 3624 is tensor(8.7375e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3625. State = [[-0.25202516  0.13541898]]. Action = [[ 0.11884582 -0.13435663 -0.06134565 -0.79539776]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 3625 is [True, False, False, False, False, True]
Scene graph at timestep 3625 is [True, False, False, False, False, True]
State prediction error at timestep 3625 is tensor(7.0071e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3626. State = [[-0.25051448  0.13154532]]. Action = [[-0.05177996  0.21112525 -0.15135615 -0.8947303 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 3626 is [True, False, False, False, False, True]
Current timestep = 3627. State = [[-0.2503146   0.13156867]]. Action = [[-0.00547586  0.18434423  0.24557698 -0.6082909 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 3627 is [True, False, False, False, False, True]
Scene graph at timestep 3627 is [True, False, False, False, False, True]
State prediction error at timestep 3627 is tensor(4.7318e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3628. State = [[-0.25055212  0.13198009]]. Action = [[ 0.18807903 -0.1657384   0.06363213  0.93959093]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 3628 is [True, False, False, False, False, True]
Current timestep = 3629. State = [[-0.24935888  0.13182549]]. Action = [[-0.1636091  -0.12721036  0.07687134 -0.01168823]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 3629 is [True, False, False, False, False, True]
Scene graph at timestep 3629 is [True, False, False, False, False, True]
State prediction error at timestep 3629 is tensor(1.5124e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3630. State = [[-0.24902086  0.13076982]]. Action = [[-0.11244714  0.22212583 -0.15981829 -0.8638346 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 3630 is [True, False, False, False, False, True]
Current timestep = 3631. State = [[-0.24939707  0.13148825]]. Action = [[-0.16154851 -0.14623933  0.02347013  0.11212432]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 3631 is [True, False, False, False, False, True]
Current timestep = 3632. State = [[-0.24953404  0.13170388]]. Action = [[ 0.04182857  0.15423304 -0.19251806 -0.8755676 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 3632 is [True, False, False, False, False, True]
Current timestep = 3633. State = [[-0.24973702  0.13203393]]. Action = [[-0.18732695 -0.06828906  0.03414673 -0.7007109 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 3633 is [True, False, False, False, False, True]
Current timestep = 3634. State = [[-0.25049788  0.13261014]]. Action = [[-0.15577772  0.1512199   0.02779531  0.49596584]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 3634 is [True, False, False, False, False, True]
Current timestep = 3635. State = [[-0.25258386  0.1349043 ]]. Action = [[-0.03864603  0.18227404 -0.08599657  0.36656177]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 3635 is [True, False, False, False, False, True]
Current timestep = 3636. State = [[-0.25504637  0.13853855]]. Action = [[-0.16282587 -0.17069343 -0.14374273  0.7274648 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 3636 is [True, False, False, False, False, True]
Scene graph at timestep 3636 is [True, False, False, False, False, True]
State prediction error at timestep 3636 is tensor(1.4823e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3637. State = [[-0.25709885  0.13941036]]. Action = [[ 0.13578269 -0.04217556  0.14054686  0.5653764 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 3637 is [True, False, False, False, False, True]
Current timestep = 3638. State = [[-0.25720313  0.13933985]]. Action = [[0.09922692 0.0725916  0.10198665 0.36519098]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 3638 is [True, False, False, False, False, True]
Scene graph at timestep 3638 is [True, False, False, False, False, True]
State prediction error at timestep 3638 is tensor(1.2105e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3639. State = [[-0.25724927  0.13936895]]. Action = [[0.04324028 0.03443539 0.00108668 0.7307174 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 3639 is [True, False, False, False, False, True]
Current timestep = 3640. State = [[-0.25741518  0.13956314]]. Action = [[-0.19466549  0.24807072 -0.11026353  0.39399981]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 3640 is [True, False, False, False, False, True]
Current timestep = 3641. State = [[-0.25937727  0.14284974]]. Action = [[ 0.08646631  0.01738319  0.05782324 -0.53233683]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 3641 is [True, False, False, False, False, True]
Current timestep = 3642. State = [[-0.26035807  0.14444749]]. Action = [[ 0.20101541  0.08854252  0.19330981 -0.36493635]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 3642 is [True, False, False, False, False, True]
Current timestep = 3643. State = [[-0.26048028  0.14570832]]. Action = [[-0.14701237  0.18220818 -0.03197882 -0.94597405]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 3643 is [True, False, False, False, False, True]
Current timestep = 3644. State = [[-0.2619833   0.14904599]]. Action = [[ 0.16575032 -0.12468144 -0.03137286 -0.94024885]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 3644 is [True, False, False, False, False, True]
Scene graph at timestep 3644 is [True, False, False, False, False, True]
State prediction error at timestep 3644 is tensor(9.6913e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3645. State = [[-0.26158148  0.1496236 ]]. Action = [[-0.1134021  -0.17547114  0.15787256  0.05824423]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 3645 is [True, False, False, False, False, True]
Current timestep = 3646. State = [[-0.26148975  0.14938822]]. Action = [[-0.01636994  0.11644205 -0.0872069   0.4633702 ]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 3646 is [True, False, False, False, False, True]
Current timestep = 3647. State = [[-0.26175186  0.1498286 ]]. Action = [[-0.19031087  0.14301413  0.14772004  0.19585848]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 3647 is [True, False, False, False, False, True]
Current timestep = 3648. State = [[-0.26302552  0.15195765]]. Action = [[ 0.15336186  0.19640025  0.20212358 -0.06775832]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 3648 is [True, False, False, False, False, True]
Current timestep = 3649. State = [[-0.26440713  0.15480393]]. Action = [[ 0.20187664  0.15212047 -0.00609215 -0.84221417]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 3649 is [True, False, False, False, False, True]
Current timestep = 3650. State = [[-0.26362437  0.15799844]]. Action = [[-0.12982029 -0.09097171 -0.01852882  0.57076514]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 3650 is [True, False, False, False, False, True]
Current timestep = 3651. State = [[-0.26374716  0.15955412]]. Action = [[ 0.11804071 -0.23847795  0.22688955  0.78943634]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 3651 is [True, False, False, False, False, True]
Current timestep = 3652. State = [[-0.26313093  0.15915415]]. Action = [[-0.17490882  0.13856673 -0.04388715  0.6501901 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 3652 is [True, False, False, False, False, True]
Current timestep = 3653. State = [[-0.2634512   0.15978901]]. Action = [[-0.23919943  0.07745758  0.20106572  0.06447768]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 3653 is [True, False, False, False, False, True]
Current timestep = 3654. State = [[-0.26487646  0.16218278]]. Action = [[-0.15520784  0.18671131  0.22079605  0.1165067 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 3654 is [True, False, False, False, False, True]
Current timestep = 3655. State = [[-0.2674757   0.16609858]]. Action = [[ 0.11540565 -0.2353095  -0.00630225 -0.2759183 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 3655 is [True, False, False, False, False, True]
Scene graph at timestep 3655 is [True, False, False, False, False, True]
State prediction error at timestep 3655 is tensor(2.7615e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3656. State = [[-0.2677497  0.1661752]]. Action = [[-0.14184926 -0.20105374  0.18158025 -0.4600743 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 3656 is [True, False, False, False, False, True]
Current timestep = 3657. State = [[-0.26874638  0.16475707]]. Action = [[-0.11371273  0.09312254 -0.15352112  0.8861897 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 3657 is [True, False, False, False, False, True]
Current timestep = 3658. State = [[-0.2697896   0.16453454]]. Action = [[ 0.10578087  0.01589644  0.1085532  -0.9329083 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 3658 is [True, False, False, False, False, True]
Human Feedback received at timestep 3658 of -1
Current timestep = 3659. State = [[-0.2698604   0.16457655]]. Action = [[-0.19738866 -0.22344616  0.05355263 -0.6826746 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 3659 is [True, False, False, False, False, True]
Current timestep = 3660. State = [[-0.2698604   0.16457655]]. Action = [[-0.17512535 -0.135528    0.0153099   0.36727834]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 3660 is [True, False, False, False, False, True]
Current timestep = 3661. State = [[-0.26981848  0.16448548]]. Action = [[ 0.18969747 -0.19562352 -0.14863785  0.24930072]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 3661 is [True, False, False, False, False, True]
Current timestep = 3662. State = [[-0.26912373  0.16264543]]. Action = [[-0.16965514 -0.04053959 -0.16530414 -0.16641247]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 3662 is [True, False, False, False, False, True]
Current timestep = 3663. State = [[-0.26903015  0.16113092]]. Action = [[-0.08663842 -0.0465371  -0.12525898  0.18193793]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 3663 is [True, False, False, False, False, True]
Current timestep = 3664. State = [[-0.26936835  0.1593106 ]]. Action = [[ 0.07983199 -0.05579731  0.02293247 -0.92204964]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 3664 is [True, False, False, False, False, True]
Current timestep = 3665. State = [[-0.26898965  0.15714784]]. Action = [[ 0.00407258 -0.19394498  0.04432619 -0.6269185 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 3665 is [True, False, False, False, False, True]
Current timestep = 3666. State = [[-0.26851928  0.15379515]]. Action = [[ 0.08331466  0.08061883 -0.13147025  0.568558  ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 3666 is [True, False, False, False, False, True]
Current timestep = 3667. State = [[-0.26815972  0.15238409]]. Action = [[-3.5437256e-02 -1.7406718e-01 -5.4539740e-04  6.1870396e-01]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 3667 is [True, False, False, False, False, True]
Current timestep = 3668. State = [[-0.26768097  0.15021741]]. Action = [[-0.20771606  0.11535072 -0.23555511 -0.35400665]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 3668 is [True, False, False, False, False, True]
Current timestep = 3669. State = [[-0.2689123   0.15000704]]. Action = [[-0.17715326  0.15826559 -0.2064428  -0.13042808]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 3669 is [True, False, False, False, False, True]
Current timestep = 3670. State = [[-0.2709807   0.15141301]]. Action = [[ 0.20758355  0.1554519   0.06424403 -0.49160218]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 3670 is [True, False, False, False, False, True]
Scene graph at timestep 3670 is [True, False, False, False, False, True]
State prediction error at timestep 3670 is tensor(2.1102e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3671. State = [[-0.27192113  0.1527924 ]]. Action = [[ 0.17340708 -0.20850717 -0.1142965   0.68911195]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 3671 is [True, False, False, False, False, True]
Human Feedback received at timestep 3671 of 1
Current timestep = 3672. State = [[-0.2716698   0.15235679]]. Action = [[-0.23400128  0.00327197  0.23683262  0.71011186]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 3672 is [True, False, False, False, False, True]
Scene graph at timestep 3672 is [True, False, False, False, False, True]
State prediction error at timestep 3672 is tensor(8.4238e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3673. State = [[-0.27148458  0.15217413]]. Action = [[-0.1466038   0.187141   -0.09748569 -0.78807485]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 3673 is [True, False, False, False, False, True]
Scene graph at timestep 3673 is [True, False, False, False, False, True]
State prediction error at timestep 3673 is tensor(1.1537e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3674. State = [[-0.27145633  0.15224193]]. Action = [[ 0.11932313  0.2293759   0.09161484 -0.74163806]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 3674 is [True, False, False, False, False, True]
Current timestep = 3675. State = [[-0.27147672  0.15261029]]. Action = [[0.0085336  0.15936041 0.01392865 0.339265  ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 3675 is [True, False, False, False, False, True]
Current timestep = 3676. State = [[-0.27176932  0.15438303]]. Action = [[-0.16740522 -0.09480885 -0.15597287 -0.02523094]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 3676 is [True, False, False, False, False, True]
Current timestep = 3677. State = [[-0.27184522  0.15556481]]. Action = [[ 0.00311345  0.20884603  0.19657168 -0.11906767]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 3677 is [True, False, False, False, False, True]
Current timestep = 3678. State = [[-0.27226976  0.15868321]]. Action = [[ 0.12688017 -0.07706204 -0.05129306 -0.17133236]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 3678 is [True, False, False, False, False, True]
Scene graph at timestep 3678 is [True, False, False, False, False, True]
State prediction error at timestep 3678 is tensor(3.7289e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3679. State = [[-0.27172565  0.15956338]]. Action = [[-0.00429177 -0.21478406  0.03137645  0.16130316]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 3679 is [True, False, False, False, False, True]
Current timestep = 3680. State = [[-0.2711068   0.15891945]]. Action = [[ 0.1551829  -0.00824398  0.19356358  0.9406589 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 3680 is [True, False, False, False, False, True]
Current timestep = 3681. State = [[-0.26963156  0.15913278]]. Action = [[0.20808107 0.09922913 0.03767976 0.00671685]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 3681 is [True, False, False, False, False, True]
Current timestep = 3682. State = [[-0.26721463  0.16022375]]. Action = [[ 0.06244075 -0.03130449 -0.1592639   0.7260983 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 3682 is [True, False, False, False, False, True]
Current timestep = 3683. State = [[-0.26522157  0.16064489]]. Action = [[-0.2243211  -0.0625113   0.22401053 -0.6790442 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 3683 is [True, False, False, False, False, True]
Scene graph at timestep 3683 is [True, False, False, False, False, True]
State prediction error at timestep 3683 is tensor(8.6335e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3684. State = [[-0.26498222  0.16031326]]. Action = [[-0.00480755 -0.20873095 -0.02910821  0.36206186]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 3684 is [True, False, False, False, False, True]
Scene graph at timestep 3684 is [True, False, False, False, False, True]
State prediction error at timestep 3684 is tensor(1.2329e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3685. State = [[-0.26397234  0.15759599]]. Action = [[ 0.06276658 -0.21208233  0.14588392  0.74440646]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 3685 is [True, False, False, False, False, True]
Current timestep = 3686. State = [[-0.26236445  0.15379861]]. Action = [[-0.14237727  0.00244167  0.17583781 -0.4978605 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 3686 is [True, False, False, False, False, True]
Scene graph at timestep 3686 is [True, False, False, False, False, True]
State prediction error at timestep 3686 is tensor(1.6874e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3687. State = [[-0.26195732  0.15189262]]. Action = [[ 0.09300005  0.15365946  0.10285926 -0.77174515]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 3687 is [True, False, False, False, False, True]
Current timestep = 3688. State = [[-0.26206282  0.15194221]]. Action = [[-0.18405195  0.12343758  0.05670816  0.38213933]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 3688 is [True, False, False, False, False, True]
Current timestep = 3689. State = [[-0.2626416   0.15272583]]. Action = [[-0.16738786  0.17801222 -0.13432541 -0.96139544]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 3689 is [True, False, False, False, False, True]
Current timestep = 3690. State = [[-0.26419654  0.15502606]]. Action = [[ 0.05396593 -0.14713545 -0.04511724 -0.31504744]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 3690 is [True, False, False, False, False, True]
Current timestep = 3691. State = [[-0.26456636  0.15535891]]. Action = [[-0.00997151  0.21469262 -0.17191759 -0.18856215]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 3691 is [True, False, False, False, False, True]
Current timestep = 3692. State = [[-0.2655499   0.15675902]]. Action = [[0.08123788 0.05488136 0.01528132 0.48207223]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 3692 is [True, False, False, False, False, True]
Current timestep = 3693. State = [[-0.26618168  0.15774861]]. Action = [[-0.21088421 -0.06011827  0.0746831  -0.17144918]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 3693 is [True, False, False, False, False, True]
Current timestep = 3694. State = [[-0.26717788  0.15881068]]. Action = [[ 0.04279768  0.1629473   0.08733726 -0.09280646]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 3694 is [True, False, False, False, False, True]
Current timestep = 3695. State = [[-0.26863542  0.16111961]]. Action = [[-0.07886288  0.1689919  -0.10256517  0.68089247]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 3695 is [True, False, False, False, False, True]
Scene graph at timestep 3695 is [True, False, False, False, False, True]
State prediction error at timestep 3695 is tensor(1.2893e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3696. State = [[-0.2709872   0.16480717]]. Action = [[-0.04494566  0.12279737  0.19088227 -0.64045805]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 3696 is [True, False, False, False, False, True]
Scene graph at timestep 3696 is [True, False, False, False, False, True]
State prediction error at timestep 3696 is tensor(5.2722e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3697. State = [[-0.27352077  0.16872177]]. Action = [[-0.04333568  0.14201725  0.11506203  0.67795396]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 3697 is [True, False, False, False, False, True]
Scene graph at timestep 3697 is [True, False, False, False, False, True]
State prediction error at timestep 3697 is tensor(2.2742e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3698. State = [[-0.27632955  0.17314926]]. Action = [[ 0.137209    0.13343179  0.09275639 -0.8618764 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 3698 is [True, False, False, False, False, True]
Current timestep = 3699. State = [[-0.27771285  0.17665602]]. Action = [[ 0.1728974   0.15776306 -0.00242618 -0.7662318 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 3699 is [True, False, False, False, False, True]
Current timestep = 3700. State = [[-0.27721485  0.18088801]]. Action = [[-0.00667658 -0.20846342 -0.17657539 -0.03516853]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 3700 is [True, False, False, False, False, True]
Current timestep = 3701. State = [[-0.27681231  0.18310858]]. Action = [[-0.05670522 -0.12907362  0.02259579  0.78941894]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 3701 is [True, False, False, False, False, True]
Scene graph at timestep 3701 is [True, False, False, False, False, True]
State prediction error at timestep 3701 is tensor(1.1527e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3702. State = [[-0.27600303  0.18451303]]. Action = [[ 0.23925266  0.08878696 -0.00339989  0.7206235 ]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 3702 is [True, False, False, False, False, True]
Scene graph at timestep 3702 is [True, False, False, False, False, True]
State prediction error at timestep 3702 is tensor(1.9254e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3703. State = [[-0.2726122   0.18665336]]. Action = [[-0.22263403  0.07849309 -0.03446791 -0.6197602 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 3703 is [True, False, False, False, False, True]
Scene graph at timestep 3703 is [True, False, False, False, False, True]
State prediction error at timestep 3703 is tensor(1.0937e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3704. State = [[-0.26891392  0.18878289]]. Action = [[ 0.03152746 -0.15318576 -0.01234052  0.48216414]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 3704 is [True, False, False, False, False, True]
Human Feedback received at timestep 3704 of -1
Current timestep = 3705. State = [[-0.2673138   0.18945038]]. Action = [[-0.1033712   0.18009168 -0.10059492  0.14944434]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 3705 is [True, False, False, False, False, True]
Current timestep = 3706. State = [[-0.26709682  0.19038557]]. Action = [[-0.16958115 -0.12411214 -0.14625603  0.07096481]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 3706 is [True, False, False, False, False, True]
Scene graph at timestep 3706 is [True, False, False, False, False, True]
State prediction error at timestep 3706 is tensor(4.1145e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3707. State = [[-0.26710647  0.1903691 ]]. Action = [[-0.17270891 -0.20223911  0.01367998  0.6281998 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 3707 is [True, False, False, False, False, True]
Current timestep = 3708. State = [[-0.26708475  0.19007896]]. Action = [[ 0.08534563  0.04428604 -0.02508417  0.00044715]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 3708 is [True, False, False, False, False, True]
Current timestep = 3709. State = [[-0.26710126  0.19005892]]. Action = [[-0.00561194 -0.02316914 -0.17834672 -0.6381    ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 3709 is [True, False, False, False, False, True]
Current timestep = 3710. State = [[-0.26708135  0.18977873]]. Action = [[-0.11230962 -0.08515283 -0.16285677  0.8252945 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 3710 is [True, False, False, False, False, True]
Current timestep = 3711. State = [[-0.26688945  0.18915886]]. Action = [[ 0.04529312 -0.1661229   0.10008705  0.959079  ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 3711 is [True, False, False, False, False, True]
Current timestep = 3712. State = [[-0.2662413   0.18680769]]. Action = [[0.19984746 0.0323042  0.22558534 0.4595474 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 3712 is [True, False, False, False, False, True]
Scene graph at timestep 3712 is [True, False, False, False, False, True]
State prediction error at timestep 3712 is tensor(2.6480e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3713. State = [[-0.26569724  0.18525997]]. Action = [[0.05260578 0.18956143 0.07132095 0.6682093 ]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 3713 is [True, False, False, False, False, True]
Current timestep = 3714. State = [[-0.26583803  0.18546651]]. Action = [[-0.02796556  0.10651457 -0.05635411 -0.9611134 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 3714 is [True, False, False, False, False, True]
Current timestep = 3715. State = [[-0.2660604   0.18589847]]. Action = [[ 0.12985551 -0.0482744   0.04407889 -0.41908753]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 3715 is [True, False, False, False, False, True]
Current timestep = 3716. State = [[-0.26586074  0.18588176]]. Action = [[ 0.2081415  -0.03887078 -0.05710323  0.08433676]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 3716 is [True, False, False, False, False, True]
Current timestep = 3717. State = [[-0.26498353  0.18537386]]. Action = [[-0.03829597  0.16276404 -0.0028505   0.6894934 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 3717 is [True, False, False, False, False, True]
Current timestep = 3718. State = [[-0.26459646  0.18621111]]. Action = [[ 0.17197588  0.0875839  -0.07530412  0.1938299 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 3718 is [True, False, False, False, False, True]
Current timestep = 3719. State = [[-0.26286194  0.18766762]]. Action = [[-0.21786645 -0.11075479  0.06564757  0.1936146 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 3719 is [True, False, False, False, False, True]
Current timestep = 3720. State = [[-0.26284695  0.18804306]]. Action = [[ 0.21995693 -0.04741138 -0.15500106 -0.04910636]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 3720 is [True, False, False, False, False, True]
Current timestep = 3721. State = [[-0.26176754  0.18797198]]. Action = [[-0.02833131 -0.03516169 -0.20538518  0.11437404]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 3721 is [True, False, False, False, False, True]
Current timestep = 3722. State = [[-0.26088193  0.18789162]]. Action = [[-0.08296709 -0.12785886 -0.19260055 -0.8400842 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 3722 is [True, False, False, False, False, True]
Current timestep = 3723. State = [[-0.26024148  0.18724613]]. Action = [[-0.17246042  0.08537772 -0.1771633  -0.65770024]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 3723 is [True, False, False, False, False, True]
Current timestep = 3724. State = [[-0.26060492  0.18764074]]. Action = [[ 0.03521198  0.14064386  0.08844087 -0.14278603]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 3724 is [True, False, False, False, False, True]
Current timestep = 3725. State = [[-0.26123467  0.18828556]]. Action = [[ 0.00825664 -0.22882813  0.08927637  0.91848874]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 3725 is [True, False, False, False, False, True]
Current timestep = 3726. State = [[-0.2606649   0.18754943]]. Action = [[-0.1262188   0.15978    -0.03740591  0.3177755 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 3726 is [True, False, False, False, False, True]
Current timestep = 3727. State = [[-0.2609856   0.18795857]]. Action = [[-0.10489669 -0.13095076  0.18095854  0.46560466]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 3727 is [True, False, False, False, False, True]
Current timestep = 3728. State = [[-0.26113495  0.18815462]]. Action = [[-0.14921255  0.05537394  0.09483024  0.6308942 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 3728 is [True, False, False, False, False, True]
Current timestep = 3729. State = [[-0.2616166   0.18886572]]. Action = [[ 0.01111686  0.19867209 -0.05528672 -0.5000402 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 3729 is [True, False, False, False, False, True]
Current timestep = 3730. State = [[-0.26301512  0.19080998]]. Action = [[-0.1216287   0.03843427 -0.14816833  0.7507417 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 3730 is [True, False, False, False, False, True]
Current timestep = 3731. State = [[-0.26481435  0.19321918]]. Action = [[ 0.04831317  0.0686405   0.02083769 -0.10534173]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 3731 is [True, False, False, False, False, True]
Scene graph at timestep 3731 is [True, False, False, False, False, True]
State prediction error at timestep 3731 is tensor(1.3746e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3732. State = [[-0.2663762   0.19519047]]. Action = [[-1.6586486e-01  6.4364076e-04  5.0828218e-02 -7.8203601e-01]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 3732 is [True, False, False, False, False, True]
Current timestep = 3733. State = [[-0.26789504  0.19710569]]. Action = [[-0.17028102 -0.20784527  0.04981861  0.13388467]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 3733 is [True, False, False, False, False, True]
Scene graph at timestep 3733 is [True, False, False, False, False, True]
State prediction error at timestep 3733 is tensor(6.7620e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3734. State = [[-0.26954103  0.19706847]]. Action = [[-0.22994837  0.1449548  -0.01933949 -0.06096721]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 3734 is [True, False, False, False, False, True]
Current timestep = 3735. State = [[-0.2705369  0.1967522]]. Action = [[ 0.07868803  0.0788779  -0.10146281  0.8161236 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 3735 is [True, False, False, False, False, True]
Current timestep = 3736. State = [[-0.27082235  0.19711189]]. Action = [[ 0.08436272 -0.0193166  -0.16781262 -0.5469149 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 3736 is [True, False, False, False, False, True]
Scene graph at timestep 3736 is [True, False, False, False, False, True]
State prediction error at timestep 3736 is tensor(3.6817e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3737. State = [[-0.27080125  0.19704898]]. Action = [[-0.1783382   0.01797107 -0.22584979  0.13878584]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 3737 is [True, False, False, False, False, True]
Current timestep = 3738. State = [[-0.27080125  0.19704898]]. Action = [[-0.07078376 -0.04506174 -0.09822679  0.19909573]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 3738 is [True, False, False, False, False, True]
Current timestep = 3739. State = [[-0.27078015  0.19698606]]. Action = [[-0.00951459  0.01057297 -0.00820981 -0.37468565]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 3739 is [True, False, False, False, False, True]
Scene graph at timestep 3739 is [True, False, False, False, False, True]
State prediction error at timestep 3739 is tensor(9.0560e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3740. State = [[-0.2708152   0.19689593]]. Action = [[ 0.20281947 -0.01277719  0.0492714   0.7159076 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 3740 is [True, False, False, False, False, True]
Scene graph at timestep 3740 is [True, False, False, False, False, True]
State prediction error at timestep 3740 is tensor(5.4487e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3741. State = [[-0.2706532   0.19647363]]. Action = [[0.10914475 0.0296182  0.10649627 0.6582451 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 3741 is [True, False, False, False, False, True]
Current timestep = 3742. State = [[-0.27056393  0.19614866]]. Action = [[-0.10800946 -0.07246596  0.12716234  0.70574903]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 3742 is [True, False, False, False, False, True]
Current timestep = 3743. State = [[-0.27033916  0.19566934]]. Action = [[-0.14929304 -0.07651201  0.21468705 -0.78460836]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 3743 is [True, False, False, False, False, True]
Current timestep = 3744. State = [[-0.2702075   0.19507606]]. Action = [[-0.18069495 -0.04179168  0.16242212  0.8974912 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 3744 is [True, False, False, False, False, True]
Current timestep = 3745. State = [[-0.27021256  0.19462226]]. Action = [[ 0.11439055  0.08296448  0.03407326 -0.16426575]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 3745 is [True, False, False, False, False, True]
Scene graph at timestep 3745 is [True, False, False, False, False, True]
State prediction error at timestep 3745 is tensor(2.1975e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3746. State = [[-0.27021256  0.19462226]]. Action = [[ 0.14180085  0.12959445 -0.16702887  0.6652739 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 3746 is [True, False, False, False, False, True]
Current timestep = 3747. State = [[-0.27021298  0.19475803]]. Action = [[ 0.04671344  0.17870402 -0.13015291  0.2892164 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 3747 is [True, False, False, False, False, True]
Current timestep = 3748. State = [[-0.27058527  0.19571114]]. Action = [[-0.2004199  -0.08263561 -0.14818911  0.97019565]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 3748 is [True, False, False, False, False, True]
Current timestep = 3749. State = [[-0.27085254  0.19645104]]. Action = [[-0.02946353  0.10633957  0.02310577 -0.31662095]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 3749 is [True, False, False, False, False, True]
Current timestep = 3750. State = [[-0.27107608  0.19768287]]. Action = [[0.23025355 0.15162513 0.04528883 0.6980593 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 3750 is [True, False, False, False, False, True]
Current timestep = 3751. State = [[-0.2700254   0.19999361]]. Action = [[ 0.20632872  0.09135509 -0.0165702  -0.93405855]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 3751 is [True, False, False, False, False, True]
Current timestep = 3752. State = [[-0.2675309   0.20213744]]. Action = [[-0.04325013 -0.03273198  0.13301042 -0.10474491]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 3752 is [True, False, False, False, False, True]
Current timestep = 3753. State = [[-0.26539156  0.20388515]]. Action = [[-0.23035717  0.00556871  0.14354128  0.93287826]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 3753 is [True, False, False, False, False, True]
Scene graph at timestep 3753 is [True, False, False, False, False, True]
State prediction error at timestep 3753 is tensor(3.6162e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3754. State = [[-0.26433733  0.205063  ]]. Action = [[-0.06661031  0.09509945 -0.06494854  0.15524304]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 3754 is [True, False, False, False, False, True]
Current timestep = 3755. State = [[-0.26461533  0.20631784]]. Action = [[-0.07810879  0.1758179   0.08942816  0.05291808]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 3755 is [True, False, False, False, False, True]
Current timestep = 3756. State = [[-0.26588008  0.2095439 ]]. Action = [[ 0.22336087 -0.05919763  0.15439332 -0.48761004]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 3756 is [True, False, False, False, False, True]
Scene graph at timestep 3756 is [True, False, False, False, False, True]
State prediction error at timestep 3756 is tensor(4.8308e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 3756 of -1
Current timestep = 3757. State = [[-0.26490363  0.21062228]]. Action = [[-0.03698048 -0.03542671 -0.12836368  0.5769489 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 3757 is [True, False, False, False, False, True]
Scene graph at timestep 3757 is [True, False, False, False, False, True]
State prediction error at timestep 3757 is tensor(1.8385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3758. State = [[-0.26471284  0.21117692]]. Action = [[-0.07669173  0.01549748  0.1077491  -0.8644677 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 3758 is [True, False, False, False, False, True]
Current timestep = 3759. State = [[-0.26487386  0.21184586]]. Action = [[ 1.1730194e-04  1.0661879e-01  1.1531758e-01 -8.1869912e-01]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 3759 is [True, False, False, False, False, True]
Current timestep = 3760. State = [[-0.26550394  0.21338648]]. Action = [[-0.12921283  0.14660099 -0.10592431 -0.8084439 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 3760 is [True, False, False, False, False, True]
Current timestep = 3761. State = [[-0.2669231  0.216252 ]]. Action = [[0.18086314 0.13100761 0.16650674 0.35742044]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 3761 is [True, False, False, False, False, True]
Scene graph at timestep 3761 is [True, False, False, False, False, True]
State prediction error at timestep 3761 is tensor(3.5228e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3762. State = [[-0.26675537  0.22016825]]. Action = [[-0.06879391  0.09729546  0.14167035  0.64487743]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 3762 is [True, False, False, False, False, True]
Current timestep = 3763. State = [[-0.26687747  0.2232207 ]]. Action = [[-0.240037    0.0534651   0.01197425 -0.7871589 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 3763 is [True, False, False, False, False, True]
Current timestep = 3764. State = [[-0.2663951   0.22484694]]. Action = [[0.16891533 0.07875758 0.13069186 0.12817621]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 3764 is [True, False, False, False, False, True]
Current timestep = 3765. State = [[-0.2644546   0.22692844]]. Action = [[-0.22859736  0.11806962 -0.16255324 -0.7852511 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 3765 is [True, False, False, False, False, True]
Current timestep = 3766. State = [[-0.26476145  0.23097858]]. Action = [[-0.15900126  0.08529484 -0.00123405 -0.27817595]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 3766 is [True, False, False, False, False, True]
Scene graph at timestep 3766 is [True, False, False, False, False, True]
State prediction error at timestep 3766 is tensor(2.9586e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3767. State = [[-0.266874    0.23579538]]. Action = [[-0.12998086 -0.03373559 -0.0791048   0.76760197]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 3767 is [True, False, False, False, False, True]
Current timestep = 3768. State = [[-0.26963896  0.23880017]]. Action = [[-0.1430316   0.18015596 -0.20474908 -0.47107446]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 3768 is [True, False, False, False, False, True]
Human Feedback received at timestep 3768 of -1
Current timestep = 3769. State = [[-0.27327546  0.2427563 ]]. Action = [[ 0.14571318 -0.11602749 -0.07881144  0.19756567]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 3769 is [True, False, False, False, False, True]
Current timestep = 3770. State = [[-0.2736786   0.24328016]]. Action = [[-0.22875349 -0.07333291  0.2188797  -0.9757403 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 3770 is [True, False, False, False, False, True]
Current timestep = 3771. State = [[-0.27375963  0.24331428]]. Action = [[-0.01965722 -0.09721625  0.05084485 -0.47811472]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 3771 is [True, False, False, False, False, True]
Current timestep = 3772. State = [[-0.27386594  0.24328326]]. Action = [[-0.17946623 -0.11870782 -0.15100478  0.6014023 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 3772 is [True, False, False, False, False, True]
Current timestep = 3773. State = [[-0.27386594  0.24328326]]. Action = [[ 0.02497479 -0.02401432  0.15375662 -0.90426826]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 3773 is [True, False, False, False, False, True]
Current timestep = 3774. State = [[-0.2738123   0.24315825]]. Action = [[ 0.05008447 -0.01564538  0.06350988  0.8088697 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 3774 is [True, False, False, False, False, True]
Scene graph at timestep 3774 is [True, False, False, False, False, True]
State prediction error at timestep 3774 is tensor(9.4113e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3775. State = [[-0.2736359   0.24280605]]. Action = [[ 0.11538115  0.0078119   0.12041208 -0.01811522]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 3775 is [True, False, False, False, False, True]
Current timestep = 3776. State = [[-0.27323517  0.24258307]]. Action = [[ 0.09384814  0.2359314  -0.21541834  0.5488601 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 3776 is [True, False, False, False, False, True]
Scene graph at timestep 3776 is [True, False, False, False, False, True]
State prediction error at timestep 3776 is tensor(2.6306e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3777. State = [[-0.27160445  0.24385138]]. Action = [[-0.02163063 -0.09421551 -0.05253148  0.01806927]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 3777 is [True, False, False, False, False, True]
Current timestep = 3778. State = [[-0.27012193  0.24455   ]]. Action = [[ 0.12172231 -0.20279236 -0.03062521 -0.03158188]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 3778 is [True, False, False, False, False, True]
Current timestep = 3779. State = [[-0.26806334  0.24330847]]. Action = [[-0.063664    0.20732453 -0.14036882 -0.64243263]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 3779 is [True, False, False, False, False, True]
Scene graph at timestep 3779 is [True, False, False, False, False, True]
State prediction error at timestep 3779 is tensor(1.8811e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3780. State = [[-0.26756984  0.2442422 ]]. Action = [[-0.2283908   0.01584059  0.19548279  0.73990464]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 3780 is [True, False, False, False, False, True]
Current timestep = 3781. State = [[-0.2673653  0.2445642]]. Action = [[ 0.09262565 -0.21049502  0.10297424 -0.603521  ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 3781 is [True, False, False, False, False, True]
Current timestep = 3782. State = [[-0.26628384  0.24308012]]. Action = [[-0.02612533 -0.10341784  0.17660788 -0.18641132]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 3782 is [True, False, False, False, False, True]
Current timestep = 3783. State = [[-0.26498163  0.2411286 ]]. Action = [[ 0.13586673 -0.07044759  0.06700844 -0.04955345]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 3783 is [True, False, False, False, False, True]
Current timestep = 3784. State = [[-0.26369825  0.23915997]]. Action = [[ 0.05835101 -0.24139942  0.11832941 -0.15091854]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 3784 is [True, False, False, False, False, True]
Current timestep = 3785. State = [[-0.2615029   0.23543741]]. Action = [[-0.15586631  0.15717751 -0.12358078  0.32146835]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 3785 is [True, False, False, False, False, True]
Current timestep = 3786. State = [[-0.26152292  0.2349476 ]]. Action = [[-0.16352338 -0.13782145 -0.17640305  0.60989046]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 3786 is [True, False, False, False, False, True]
Scene graph at timestep 3786 is [True, False, False, False, False, True]
State prediction error at timestep 3786 is tensor(4.3010e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3787. State = [[-0.26144004  0.23385096]]. Action = [[0.04427567 0.15280104 0.12181312 0.4584639 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 3787 is [True, False, False, False, False, True]
Current timestep = 3788. State = [[-0.26195964  0.23414744]]. Action = [[-0.18062836  0.16193813 -0.04927921  0.7431803 ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 3788 is [True, False, False, False, False, True]
Current timestep = 3789. State = [[-0.26353443  0.23588262]]. Action = [[-0.14167619 -0.06989932  0.23158592  0.00674522]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 3789 is [True, False, False, False, False, True]
Current timestep = 3790. State = [[-0.26467142  0.23692589]]. Action = [[0.0627909  0.07378072 0.06707275 0.07559097]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 3790 is [True, False, False, False, False, True]
Current timestep = 3791. State = [[-0.26551414  0.23788194]]. Action = [[-0.24112877  0.19397867  0.15334922  0.15536249]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 3791 is [True, False, False, False, False, True]
Scene graph at timestep 3791 is [True, False, False, False, False, True]
State prediction error at timestep 3791 is tensor(4.4730e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3792. State = [[-0.26902226  0.2415377 ]]. Action = [[-0.2415546   0.04426891 -0.0782603  -0.8047547 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 3792 is [True, False, False, False, False, True]
Current timestep = 3793. State = [[-0.2727886   0.24551298]]. Action = [[-0.06271398  0.10702991 -0.10308202  0.53027487]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 3793 is [True, False, False, False, False, True]
Current timestep = 3794. State = [[-0.2766372   0.24946545]]. Action = [[-0.19488963  0.13786197 -0.00858667  0.6500032 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 3794 is [True, False, False, False, False, True]
Current timestep = 3795. State = [[-0.27878   0.251766]]. Action = [[-0.06056941 -0.07145151 -0.00608455 -0.249192  ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 3795 is [True, False, False, False, False, True]
Human Feedback received at timestep 3795 of -1
Current timestep = 3796. State = [[-0.2800606   0.25312912]]. Action = [[-0.1793512   0.11189145  0.13727218 -0.9206884 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 3796 is [True, False, False, False, False, True]
Current timestep = 3797. State = [[-0.2807209   0.25372013]]. Action = [[ 0.07955104 -0.04345068 -0.19257066  0.75715125]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 3797 is [True, False, False, False, False, True]
Current timestep = 3798. State = [[-0.2806482   0.25351337]]. Action = [[ 0.15410733  0.0180769  -0.12640136 -0.4369912 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 3798 is [True, False, False, False, False, True]
Current timestep = 3799. State = [[-0.28053418  0.2534342 ]]. Action = [[ 0.19985142  0.16500002 -0.19566377 -0.16705298]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 3799 is [True, False, False, False, False, True]
Current timestep = 3800. State = [[-0.28030303  0.25374776]]. Action = [[-0.19715913 -0.03276932 -0.18248211 -0.48208547]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 3800 is [True, False, False, False, False, True]
Scene graph at timestep 3800 is [True, False, False, False, False, True]
State prediction error at timestep 3800 is tensor(2.5155e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3801. State = [[-0.28035942  0.25369352]]. Action = [[-0.02752502  0.13636935  0.13846573  0.9200084 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 3801 is [True, False, False, False, False, True]
Scene graph at timestep 3801 is [True, False, False, False, False, True]
State prediction error at timestep 3801 is tensor(6.3568e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3802. State = [[-0.28019074  0.25381416]]. Action = [[-0.14858612  0.127866   -0.07921842  0.44707537]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 3802 is [True, False, False, False, False, True]
Current timestep = 3803. State = [[-0.28019074  0.25381416]]. Action = [[-0.1164761  -0.13106541 -0.13948724  0.83782613]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 3803 is [True, False, False, False, False, True]
Current timestep = 3804. State = [[-0.28006    0.2538173]]. Action = [[-0.09490907  0.09145746  0.08134094  0.6225982 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 3804 is [True, False, False, False, False, True]
Current timestep = 3805. State = [[-0.28006    0.2538173]]. Action = [[-0.23332645 -0.2175157  -0.03125358 -0.4018209 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 3805 is [True, False, False, False, False, True]
Current timestep = 3806. State = [[-0.27995202  0.25382867]]. Action = [[ 0.08013406 -0.1094895   0.0878607  -0.5650335 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 3806 is [True, False, False, False, False, True]
Current timestep = 3807. State = [[-0.27941766  0.2533671 ]]. Action = [[-0.03504877 -0.10777578  0.14496118 -0.38212085]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 3807 is [True, False, False, False, False, True]
Current timestep = 3808. State = [[-0.27904332  0.25287282]]. Action = [[-0.01088291  0.21713257 -0.06307082  0.05735493]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 3808 is [True, False, False, False, False, True]
Current timestep = 3809. State = [[-0.2790044   0.25283137]]. Action = [[-0.05236925  0.17586935 -0.0816915  -0.70129097]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 3809 is [True, False, False, False, False, True]
Current timestep = 3810. State = [[-0.2789763   0.25276938]]. Action = [[-0.06356755  0.14251691  0.06242865  0.08678937]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 3810 is [True, False, False, False, False, True]
Current timestep = 3811. State = [[-0.2789763   0.25276938]]. Action = [[-0.15546387 -0.19097179 -0.06580286 -0.8429418 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 3811 is [True, False, False, False, False, True]
Current timestep = 3812. State = [[-0.2789763   0.25276938]]. Action = [[-0.13444017  0.04679334 -0.02881047  0.8429321 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 3812 is [True, False, False, False, False, True]
Scene graph at timestep 3812 is [True, False, False, False, False, True]
State prediction error at timestep 3812 is tensor(8.0364e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3813. State = [[-0.2789763   0.25276938]]. Action = [[-0.09679908  0.1660195  -0.03117627  0.5550698 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 3813 is [True, False, False, False, False, True]
Scene graph at timestep 3813 is [True, False, False, False, False, True]
State prediction error at timestep 3813 is tensor(1.4979e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3814. State = [[-0.2789763   0.25276938]]. Action = [[ 0.01070541  0.01799312 -0.09574616  0.95421743]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 3814 is [True, False, False, False, False, True]
Current timestep = 3815. State = [[-0.2789763   0.25276938]]. Action = [[-0.05726403  0.09656197  0.0738216   0.5260689 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 3815 is [True, False, False, False, False, True]
Scene graph at timestep 3815 is [True, False, False, False, False, True]
State prediction error at timestep 3815 is tensor(1.9257e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3816. State = [[-0.2789763   0.25276938]]. Action = [[-0.23501308 -0.22372946  0.18398213 -0.5033579 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 3816 is [True, False, False, False, False, True]
Scene graph at timestep 3816 is [True, False, False, False, False, True]
State prediction error at timestep 3816 is tensor(3.8407e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3817. State = [[-0.27892536  0.25280157]]. Action = [[0.13821644 0.1430541  0.07096669 0.9073322 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 3817 is [True, False, False, False, False, True]
Current timestep = 3818. State = [[-0.27804175  0.25341877]]. Action = [[-0.21196057  0.24090075  0.0164279  -0.34807706]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 3818 is [True, False, False, False, False, True]
Current timestep = 3819. State = [[-0.27737868  0.2539056 ]]. Action = [[-0.16275294 -0.09524044 -0.05931792 -0.7468337 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 3819 is [True, False, False, False, False, True]
Current timestep = 3820. State = [[-0.27723932  0.25396642]]. Action = [[-0.23410232 -0.10873374 -0.13145606 -0.4855249 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 3820 is [True, False, False, False, False, True]
Current timestep = 3821. State = [[-0.27695218  0.25409123]]. Action = [[-0.01169741 -0.18283254  0.20313823  0.53463733]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 3821 is [True, False, False, False, False, True]
Current timestep = 3822. State = [[-0.27690154  0.25409785]]. Action = [[-0.16764922 -0.02103384 -0.16681822  0.22903621]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 3822 is [True, False, False, False, False, True]
Current timestep = 3823. State = [[-0.27671885  0.2542151 ]]. Action = [[-0.06674735  0.0569548  -0.20965111 -0.4757049 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 3823 is [True, False, False, False, False, True]
Current timestep = 3824. State = [[-0.27657264  0.2543113 ]]. Action = [[-0.21758628 -0.14220856  0.12257683  0.32262218]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 3824 is [True, False, False, False, False, True]
Human Feedback received at timestep 3824 of -1
Current timestep = 3825. State = [[-0.27637258  0.2543779 ]]. Action = [[ 0.23356295 -0.03415926 -0.1356923   0.8282709 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 3825 is [True, False, False, False, False, True]
Scene graph at timestep 3825 is [True, False, False, False, False, True]
State prediction error at timestep 3825 is tensor(1.0804e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3826. State = [[-0.27396578  0.25432807]]. Action = [[-0.20663774  0.04831535  0.00291952  0.23709238]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 3826 is [True, False, False, False, False, True]
Scene graph at timestep 3826 is [True, False, False, False, False, True]
State prediction error at timestep 3826 is tensor(1.1946e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3827. State = [[-0.27199805  0.25480157]]. Action = [[-0.18494932 -0.04744937  0.03344864  0.0474391 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 3827 is [True, False, False, False, False, True]
Current timestep = 3828. State = [[-0.27017167  0.25495523]]. Action = [[ 0.08887577 -0.07033977 -0.036211   -0.3349067 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 3828 is [True, False, False, False, False, True]
Current timestep = 3829. State = [[-0.26801845  0.25397682]]. Action = [[-0.01883797 -0.22816688 -0.10310879 -0.3066312 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 3829 is [True, False, False, False, False, True]
Current timestep = 3830. State = [[-0.2660982  0.2508151]]. Action = [[-0.09277946 -0.15089856  0.16735065  0.45736098]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 3830 is [True, False, False, False, False, True]
Scene graph at timestep 3830 is [True, False, False, False, False, True]
State prediction error at timestep 3830 is tensor(4.4218e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3831. State = [[-0.26481846  0.24795663]]. Action = [[-0.04119673  0.0483098  -0.114519   -0.78173786]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 3831 is [True, False, False, False, False, True]
Current timestep = 3832. State = [[-0.26449007  0.24703276]]. Action = [[-0.18393442 -0.0325751   0.06740332  0.18057132]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 3832 is [True, False, False, False, False, True]
Scene graph at timestep 3832 is [True, False, False, False, False, True]
State prediction error at timestep 3832 is tensor(6.9996e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3833. State = [[-0.26451167  0.24660386]]. Action = [[0.0353134  0.01641393 0.08146802 0.45491767]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 3833 is [True, False, False, False, False, True]
Current timestep = 3834. State = [[-0.26457536  0.2464151 ]]. Action = [[-0.09835318 -0.21722026 -0.02885114  0.44128096]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 3834 is [True, False, False, False, False, True]
Current timestep = 3835. State = [[-0.2641938   0.24431713]]. Action = [[-0.2214065   0.08968699 -0.0375396   0.7077794 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 3835 is [True, False, False, False, False, True]
Current timestep = 3836. State = [[-0.26553282  0.24386714]]. Action = [[ 0.06725439 -0.07411768  0.0533098  -0.77992684]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 3836 is [True, False, False, False, False, True]
Current timestep = 3837. State = [[-0.26550785  0.24253973]]. Action = [[0.22426903 0.1458892  0.19348007 0.14021897]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 3837 is [True, False, False, False, False, True]
Current timestep = 3838. State = [[-0.2654534  0.2421933]]. Action = [[ 0.05492893 -0.08737773  0.04182768 -0.76749784]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 3838 is [True, False, False, False, False, True]
Current timestep = 3839. State = [[-0.26465377  0.24085322]]. Action = [[-0.005045   -0.15968724  0.13623136  0.33373046]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 3839 is [True, False, False, False, False, True]
Current timestep = 3840. State = [[-0.2635368   0.23852336]]. Action = [[-0.15811442 -0.18092722 -0.04372843 -0.7116941 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 3840 is [True, False, False, False, False, True]
Current timestep = 3841. State = [[-0.26314807  0.2362521 ]]. Action = [[-0.06715141  0.07125443  0.05999941  0.04700398]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 3841 is [True, False, False, False, False, True]
Current timestep = 3842. State = [[-0.26353195  0.23577467]]. Action = [[-0.1982274  -0.06665447  0.02226111  0.26879966]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 3842 is [True, False, False, False, False, True]
Current timestep = 3843. State = [[-0.2650808  0.2350266]]. Action = [[-0.21143475 -0.04903933 -0.16457325 -0.78133404]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 3843 is [True, False, False, False, False, True]
Current timestep = 3844. State = [[-0.26862088  0.23321913]]. Action = [[ 0.14778483  0.13362351 -0.21448523 -0.84038883]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 3844 is [True, False, False, False, False, True]
Scene graph at timestep 3844 is [True, False, False, False, False, True]
State prediction error at timestep 3844 is tensor(7.6003e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3845. State = [[-0.27046955  0.23275869]]. Action = [[-0.11324751  0.07935596  0.04111058  0.14101982]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 3845 is [True, False, False, False, False, True]
Scene graph at timestep 3845 is [True, False, False, False, False, True]
State prediction error at timestep 3845 is tensor(3.5472e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3846. State = [[-0.2722667   0.23385437]]. Action = [[-0.01366176  0.23532844 -0.07003164 -0.9269246 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 3846 is [True, False, False, False, False, True]
Scene graph at timestep 3846 is [True, False, False, False, False, True]
State prediction error at timestep 3846 is tensor(1.4089e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3847. State = [[-0.27480462  0.23674583]]. Action = [[0.00847024 0.21606222 0.17756522 0.3995124 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 3847 is [True, False, False, False, False, True]
Current timestep = 3848. State = [[-0.27802208  0.24055158]]. Action = [[ 0.16590366  0.09769082 -0.18754607 -0.891998  ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 3848 is [True, False, False, False, False, True]
Current timestep = 3849. State = [[-0.27954194  0.24264331]]. Action = [[-0.05290025  0.12954038  0.18350852  0.7494943 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 3849 is [True, False, False, False, False, True]
Current timestep = 3850. State = [[-0.28043064  0.24380343]]. Action = [[-0.06809099  0.00353482  0.22064194 -0.30183542]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 3850 is [True, False, False, False, False, True]
Scene graph at timestep 3850 is [True, False, False, False, False, True]
State prediction error at timestep 3850 is tensor(5.8732e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3851. State = [[-0.28084573  0.2446572 ]]. Action = [[-0.18122421 -0.12309745  0.03557226 -0.37950563]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 3851 is [True, False, False, False, False, True]
Current timestep = 3852. State = [[-0.28113246  0.2452238 ]]. Action = [[ 0.03429788 -0.17345938  0.15178761  0.96741056]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 3852 is [True, False, False, False, False, True]
Human Feedback received at timestep 3852 of -1
Current timestep = 3853. State = [[-0.2812172   0.24553375]]. Action = [[ 0.13935605  0.16094956  0.07232666 -0.07481885]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 3853 is [True, False, False, False, False, True]
Current timestep = 3854. State = [[-0.28069517  0.24699572]]. Action = [[-0.16066943 -0.04963844 -0.12250942  0.16855323]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 3854 is [True, False, False, False, False, True]
Current timestep = 3855. State = [[-0.28049508  0.24783677]]. Action = [[ 0.0390029   0.16528475 -0.06026866  0.12316799]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 3855 is [True, False, False, False, False, True]
Current timestep = 3856. State = [[-0.28049394  0.24843496]]. Action = [[-0.08335143  0.07285905  0.01745352 -0.9670614 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 3856 is [True, False, False, False, False, True]
Human Feedback received at timestep 3856 of -1
Current timestep = 3857. State = [[-0.28073078  0.24865113]]. Action = [[ 0.00168809 -0.11986142 -0.10103939  0.9291301 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 3857 is [True, False, False, False, False, True]
Scene graph at timestep 3857 is [True, False, False, False, False, True]
State prediction error at timestep 3857 is tensor(4.0152e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3858. State = [[-0.28068432  0.24872026]]. Action = [[ 6.53475523e-04 -1.01474360e-01  1.22134596e-01 -7.92927980e-01]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 3858 is [True, False, False, False, False, True]
Current timestep = 3859. State = [[-0.28063342  0.24875353]]. Action = [[-0.03580277 -0.02322535  0.01924726 -0.2432167 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 3859 is [True, False, False, False, False, True]
Current timestep = 3860. State = [[-0.28060928  0.24877943]]. Action = [[ 0.18661228  0.04458344 -0.02555999 -0.88488686]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 3860 is [True, False, False, False, False, True]
Current timestep = 3861. State = [[-0.27928224  0.24985191]]. Action = [[-0.10282588  0.03114924  0.14668095  0.4550823 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 3861 is [True, False, False, False, False, True]
Scene graph at timestep 3861 is [True, False, False, False, False, True]
State prediction error at timestep 3861 is tensor(1.9816e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3862. State = [[-0.27839932  0.25059006]]. Action = [[-0.15857615  0.14936638 -0.06549934 -0.13207352]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 3862 is [True, False, False, False, False, True]
Current timestep = 3863. State = [[-0.27795127  0.25088575]]. Action = [[-0.11558092 -0.02549841 -0.05845918 -0.21735543]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 3863 is [True, False, False, False, False, True]
Current timestep = 3864. State = [[-0.2776115   0.25106284]]. Action = [[ 0.20593947 -0.10455465 -0.1069763  -0.5698251 ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 3864 is [True, False, False, False, False, True]
Current timestep = 3865. State = [[-0.27590972  0.25077805]]. Action = [[-0.07926494 -0.1100795   0.1438928  -0.49509627]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 3865 is [True, False, False, False, False, True]
Scene graph at timestep 3865 is [True, False, False, False, False, True]
State prediction error at timestep 3865 is tensor(1.8411e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3866. State = [[-0.27475345  0.25091106]]. Action = [[ 0.04738405 -0.01892185 -0.20842493 -0.19728571]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 3866 is [True, False, False, False, False, True]
Scene graph at timestep 3866 is [True, False, False, False, False, True]
State prediction error at timestep 3866 is tensor(1.6004e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3867. State = [[-0.2731047   0.25095057]]. Action = [[-0.04131973 -0.03773674 -0.18413559 -0.26972634]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 3867 is [True, False, False, False, False, True]
Scene graph at timestep 3867 is [True, False, False, False, False, True]
State prediction error at timestep 3867 is tensor(3.7894e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3868. State = [[-0.2717898  0.2509267]]. Action = [[-0.08836189 -0.06896549  0.00300223  0.92219806]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 3868 is [True, False, False, False, False, True]
Current timestep = 3869. State = [[-0.27124414  0.25043046]]. Action = [[ 0.17694482 -0.22412637  0.12763327 -0.26286483]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 3869 is [True, False, False, False, False, True]
Scene graph at timestep 3869 is [True, False, False, False, False, True]
State prediction error at timestep 3869 is tensor(1.5242e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3870. State = [[-0.26903838  0.24756403]]. Action = [[ 0.07776856  0.00851747  0.00847924 -0.46784443]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 3870 is [True, False, False, False, False, True]
Current timestep = 3871. State = [[-0.26755092  0.2458074 ]]. Action = [[0.05380338 0.19899821 0.02034336 0.7607162 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 3871 is [True, False, False, False, False, True]
Current timestep = 3872. State = [[-0.2658219   0.24674731]]. Action = [[ 0.0247072   0.07900807 -0.18162352  0.678617  ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 3872 is [True, False, False, False, False, True]
Current timestep = 3873. State = [[-0.2640566   0.24858303]]. Action = [[-0.22258215  0.20133781  0.05059645  0.24931669]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 3873 is [True, False, False, False, False, True]
Current timestep = 3874. State = [[-0.26442698  0.25125474]]. Action = [[-0.06882979 -0.19662239  0.07012796 -0.16364115]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 3874 is [True, False, False, False, False, True]
Human Feedback received at timestep 3874 of -1
Current timestep = 3875. State = [[-0.26461414  0.25140363]]. Action = [[-0.05031055  0.00391126  0.14231762 -0.62498724]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 3875 is [True, False, False, False, False, True]
Current timestep = 3876. State = [[-0.26464134  0.2513702 ]]. Action = [[ 0.00474373  0.04481593  0.06071359 -0.5967769 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 3876 is [True, False, False, False, False, True]
Current timestep = 3877. State = [[-0.26470932  0.25137332]]. Action = [[ 0.0866231  -0.02154511 -0.02584198 -0.9197141 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 3877 is [True, False, False, False, False, True]
Current timestep = 3878. State = [[-0.2647783  0.2514813]]. Action = [[ 0.08959496  0.16330236 -0.20256728  0.9715543 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 3878 is [True, False, False, False, False, True]
Current timestep = 3879. State = [[-0.26449972  0.25221646]]. Action = [[-0.1826298   0.07562065 -0.05301517  0.908044  ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 3879 is [True, False, False, False, False, True]
Current timestep = 3880. State = [[-0.26584056  0.2540473 ]]. Action = [[-0.07197386  0.04491478 -0.00614783 -0.7441583 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 3880 is [True, False, False, False, False, True]
Scene graph at timestep 3880 is [True, False, False, False, False, True]
State prediction error at timestep 3880 is tensor(5.2422e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3881. State = [[-0.26724625  0.25573462]]. Action = [[-0.02874777 -0.18449862  0.19112802  0.5799866 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 3881 is [True, False, False, False, False, True]
Current timestep = 3882. State = [[-0.2673521  0.2559334]]. Action = [[-0.00861929  0.23435158  0.22125036  0.61671805]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 3882 is [True, False, False, False, False, True]
Current timestep = 3883. State = [[-0.26843217  0.25734076]]. Action = [[ 0.04731721  0.09803647 -0.09162059  0.1264193 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 3883 is [True, False, False, False, False, True]
Current timestep = 3884. State = [[-0.2696823   0.25935686]]. Action = [[-0.10390335  0.0857656   0.02736768 -0.28608775]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 3884 is [True, False, False, False, False, True]
Current timestep = 3885. State = [[-0.27128798  0.2615234 ]]. Action = [[ 0.06990489 -0.03665009  0.05074692  0.41450965]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 3885 is [True, False, False, False, False, True]
Human Feedback received at timestep 3885 of -1
Current timestep = 3886. State = [[-0.2716795   0.26246476]]. Action = [[ 0.05353901 -0.15230796 -0.00683092  0.91502905]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 3886 is [True, False, False, False, False, True]
Current timestep = 3887. State = [[-0.2715943   0.26227352]]. Action = [[0.14671195 0.11586261 0.16236147 0.802457  ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 3887 is [True, False, False, False, False, True]
Current timestep = 3888. State = [[-0.2707752   0.26279655]]. Action = [[-0.09971064 -0.14381234  0.04675916  0.22016644]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 3888 is [True, False, False, False, False, True]
Current timestep = 3889. State = [[-0.270596   0.2626723]]. Action = [[0.08792889 0.03904051 0.02410036 0.06067276]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 3889 is [True, False, False, False, False, True]
Current timestep = 3890. State = [[-0.2705512  0.2626055]]. Action = [[-0.01401491  0.10365593  0.20172685 -0.50316775]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 3890 is [True, False, False, False, False, True]
Current timestep = 3891. State = [[-0.26991016  0.26305622]]. Action = [[-0.03343329  0.12127548  0.09566358 -0.14999837]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 3891 is [True, False, False, False, False, True]
Current timestep = 3892. State = [[-0.26961145  0.2647322 ]]. Action = [[-0.14701615 -0.06826267 -0.21106559 -0.6082888 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 3892 is [True, False, False, False, False, True]
Scene graph at timestep 3892 is [True, False, False, False, False, True]
State prediction error at timestep 3892 is tensor(2.0863e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3893. State = [[-0.2701842   0.26533931]]. Action = [[0.03408802 0.20422506 0.15100718 0.27176964]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 3893 is [True, False, False, False, False, True]
Current timestep = 3894. State = [[-0.27085456  0.26724285]]. Action = [[ 0.05841622  0.09288979 -0.03776057  0.29754055]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 3894 is [True, False, False, False, False, True]
Current timestep = 3895. State = [[-0.27059367  0.26965973]]. Action = [[0.1536513  0.10798824 0.07580495 0.9620826 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 3895 is [True, False, False, False, False, True]
Scene graph at timestep 3895 is [True, False, False, False, False, True]
State prediction error at timestep 3895 is tensor(1.2449e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3896. State = [[-0.2686486   0.27238056]]. Action = [[-0.06552866  0.11242056  0.0106681  -0.3906231 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 3896 is [True, False, False, False, False, True]
Current timestep = 3897. State = [[-0.26821816  0.2758438 ]]. Action = [[-0.09232703  0.16354269 -0.21130691 -0.33573794]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 3897 is [True, False, False, False, False, True]
Current timestep = 3898. State = [[-0.2693032  0.2804316]]. Action = [[-0.2071169  -0.0068291  -0.07296333  0.76464677]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 3898 is [True, False, False, False, False, True]
Current timestep = 3899. State = [[-0.26951236  0.2838326 ]]. Action = [[-0.0534258  -0.15746911  0.03201446 -0.19536448]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 3899 is [True, False, False, False, False, True]
Current timestep = 3900. State = [[-0.26949894  0.28457755]]. Action = [[-0.19445655  0.03907418 -0.12742026  0.05730367]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 3900 is [True, False, False, False, False, True]
Current timestep = 3901. State = [[-0.2696209   0.28462705]]. Action = [[-0.2208618   0.04872337 -0.04854649 -0.9465987 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 3901 is [True, False, False, False, False, True]
Current timestep = 3902. State = [[-0.26976085  0.28472033]]. Action = [[-0.13551745  0.16593173 -0.17775433 -0.71466005]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 3902 is [True, False, False, False, False, True]
Scene graph at timestep 3902 is [True, False, False, False, False, True]
State prediction error at timestep 3902 is tensor(1.3650e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3903. State = [[-0.27171913  0.28656682]]. Action = [[-0.02296829 -0.2197544  -0.07129455  0.84716594]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 3903 is [True, False, False, False, False, True]
Current timestep = 3904. State = [[-0.2718938   0.28662595]]. Action = [[-0.20566708 -0.14575408 -0.1474909   0.21700966]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 3904 is [True, False, False, False, False, True]
Scene graph at timestep 3904 is [True, False, False, False, False, True]
State prediction error at timestep 3904 is tensor(7.0045e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3905. State = [[-0.27194935  0.2866301 ]]. Action = [[ 0.09474897  0.07597154 -0.22765784  0.49634945]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 3905 is [True, False, False, False, False, True]
Current timestep = 3906. State = [[-0.2718365   0.28653905]]. Action = [[-0.0943473  -0.2080998   0.04256505 -0.33465028]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 3906 is [True, False, False, False, False, True]
Scene graph at timestep 3906 is [True, False, False, False, False, True]
State prediction error at timestep 3906 is tensor(2.4289e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3907. State = [[-0.27155522  0.2856995 ]]. Action = [[ 0.08715081  0.12703177 -0.11053869  0.9623399 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 3907 is [True, False, False, False, False, True]
Scene graph at timestep 3907 is [True, False, False, False, False, True]
State prediction error at timestep 3907 is tensor(1.5464e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3908. State = [[-0.2715928   0.28567493]]. Action = [[-0.18846877 -0.14256278 -0.11009967 -0.7140063 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 3908 is [True, False, False, False, False, True]
Current timestep = 3909. State = [[-0.2716379   0.28572005]]. Action = [[0.00167686 0.15214598 0.11787423 0.22772253]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 3909 is [True, False, False, False, False, True]
Current timestep = 3910. State = [[-0.27205485  0.28613183]]. Action = [[-0.02178694 -0.18233192 -0.10166115  0.8157215 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 3910 is [True, False, False, False, False, True]
Current timestep = 3911. State = [[-0.2719912  0.286009 ]]. Action = [[-0.12301314  0.0884828  -0.07105967 -0.99145734]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 3911 is [True, False, False, False, False, True]
Current timestep = 3912. State = [[-0.27192757  0.2858862 ]]. Action = [[ 0.01580578  0.08906639 -0.03176416  0.27749467]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 3912 is [True, False, False, False, False, True]
Scene graph at timestep 3912 is [True, False, False, False, False, True]
State prediction error at timestep 3912 is tensor(8.8793e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3913. State = [[-0.2719638  0.2860076]]. Action = [[ 0.1392293   0.2309159  -0.03994744  0.5518491 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 3913 is [True, False, False, False, False, True]
Current timestep = 3914. State = [[-0.27130038  0.28756702]]. Action = [[-0.14995572  0.22874382  0.07751936 -0.28410816]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 3914 is [True, False, False, False, False, True]
Current timestep = 3915. State = [[-0.23652796 -0.00955644]]. Action = [[ 0.05664006  0.03933221 -0.02162796  0.08180773]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 3915 is [True, False, False, False, False, True]
Human Feedback received at timestep 3915 of -1
Current timestep = 3916. State = [[-0.23403782 -0.01074727]]. Action = [[-0.08643439 -0.03524016 -0.00148611  0.7390361 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 3916 is [True, False, False, False, True, False]
Current timestep = 3917. State = [[-0.2340665  -0.01098027]]. Action = [[0.14075738 0.12277645 0.18653947 0.26765966]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 3917 is [True, False, False, False, True, False]
Current timestep = 3918. State = [[-0.23396638 -0.01086888]]. Action = [[ 0.23694682 -0.09508395  0.06093454 -0.09875619]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 3918 is [True, False, False, False, True, False]
Current timestep = 3919. State = [[-0.2315702  -0.01140755]]. Action = [[-0.07468832 -0.21589135 -0.15496781 -0.8851473 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 3919 is [True, False, False, False, True, False]
Current timestep = 3920. State = [[-0.23007329 -0.0136719 ]]. Action = [[ 0.07914746 -0.17386608  0.00584266 -0.3692237 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 3920 is [True, False, False, False, True, False]
Current timestep = 3921. State = [[-0.22788617 -0.01760655]]. Action = [[ 0.21682888 -0.09958115 -0.05288653 -0.586069  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 3921 is [True, False, False, False, True, False]
Scene graph at timestep 3921 is [True, False, False, False, True, False]
State prediction error at timestep 3921 is tensor(4.5502e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3922. State = [[-0.22377364 -0.02101525]]. Action = [[-0.16262825  0.12112975  0.21966955 -0.2624703 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 3922 is [True, False, False, False, True, False]
Current timestep = 3923. State = [[-0.22248565 -0.02157775]]. Action = [[0.09571573 0.05587351 0.20354635 0.63782156]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 3923 is [True, False, False, False, True, False]
Current timestep = 3924. State = [[-0.22062074 -0.02162276]]. Action = [[-0.08513042 -0.0524641   0.10819241 -0.5097339 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 3924 is [True, False, False, False, True, False]
Current timestep = 3925. State = [[-0.21993786 -0.02223887]]. Action = [[-0.23031223 -0.18269147 -0.18463089  0.95727587]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 3925 is [True, False, False, False, True, False]
Current timestep = 3926. State = [[-0.22023723 -0.0253391 ]]. Action = [[ 0.00728786 -0.20138085 -0.03527623  0.46115327]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 3926 is [True, False, False, False, True, False]
Current timestep = 3927. State = [[-0.22085816 -0.02966134]]. Action = [[ 0.12449446 -0.0809285   0.24008703 -0.2917487 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 3927 is [True, False, False, False, True, False]
Current timestep = 3928. State = [[-0.22125338 -0.03396371]]. Action = [[ 0.08446154 -0.1264229   0.17872968 -0.3604952 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 3928 is [True, False, False, False, True, False]
Current timestep = 3929. State = [[-0.22110471 -0.03765502]]. Action = [[ 0.09702331  0.11640573 -0.19458692  0.8075321 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 3929 is [True, False, False, False, True, False]
Current timestep = 3930. State = [[-0.2200779  -0.03879894]]. Action = [[ 0.23253489  0.21080399 -0.21776025 -0.22335064]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 3930 is [True, False, False, False, True, False]
Current timestep = 3931. State = [[-0.21672156 -0.03824803]]. Action = [[ 0.03634664 -0.06640124 -0.05398594 -0.02321947]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 3931 is [True, False, False, False, True, False]
Current timestep = 3932. State = [[-0.21351679 -0.03827951]]. Action = [[-0.01497838 -0.07040565  0.19155717 -0.94780296]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 3932 is [True, False, False, False, True, False]
Current timestep = 3933. State = [[-0.2112315  -0.03863229]]. Action = [[ 0.15071207 -0.00803597 -0.11129427  0.3226527 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 3933 is [True, False, False, False, True, False]
Current timestep = 3934. State = [[-0.20938787 -0.03883253]]. Action = [[-0.2088207   0.19609985 -0.18695423 -0.1171838 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 3934 is [True, False, False, False, True, False]
Current timestep = 3935. State = [[-0.20960745 -0.03829489]]. Action = [[ 0.09503022 -0.20185602 -0.20377867  0.23137641]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 3935 is [True, False, False, False, True, False]
Current timestep = 3936. State = [[-0.2091723  -0.03870362]]. Action = [[ 0.16525483 -0.07473645 -0.11313814  0.16829574]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 3936 is [True, False, False, False, True, False]
Scene graph at timestep 3936 is [True, False, False, False, True, False]
State prediction error at timestep 3936 is tensor(5.8439e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3937. State = [[-0.20746373 -0.03954954]]. Action = [[-0.14072067  0.09362227  0.11170009  0.3205433 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 3937 is [True, False, False, False, True, False]
Scene graph at timestep 3937 is [True, False, False, False, True, False]
State prediction error at timestep 3937 is tensor(1.4700e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3938. State = [[-0.20743844 -0.03950237]]. Action = [[ 0.00886956  0.11692187  0.05992681 -0.15307623]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 3938 is [True, False, False, False, True, False]
Scene graph at timestep 3938 is [True, False, False, False, True, False]
State prediction error at timestep 3938 is tensor(2.0686e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3939. State = [[-0.20733836 -0.03903362]]. Action = [[ 0.22765958  0.01182303  0.00719249 -0.44748688]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 3939 is [True, False, False, False, True, False]
Current timestep = 3940. State = [[-0.20600884 -0.03849205]]. Action = [[-0.22996558  0.16546524 -0.17553082  0.26730835]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 3940 is [True, False, False, False, True, False]
Scene graph at timestep 3940 is [True, False, False, False, True, False]
State prediction error at timestep 3940 is tensor(1.5347e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3941. State = [[-0.20597537 -0.03640256]]. Action = [[-0.1009241   0.03748178  0.03711507 -0.2673291 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 3941 is [True, False, False, False, True, False]
Scene graph at timestep 3941 is [True, False, False, False, True, False]
State prediction error at timestep 3941 is tensor(5.1490e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3942. State = [[-0.20611636 -0.03511092]]. Action = [[-0.14298466 -0.01047787  0.0723086  -0.7780095 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 3942 is [True, False, False, False, True, False]
Current timestep = 3943. State = [[-0.20655479 -0.03464203]]. Action = [[ 0.13718873 -0.15686442 -0.17036837 -0.5271432 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 3943 is [True, False, False, False, True, False]
Current timestep = 3944. State = [[-0.20644188 -0.0349731 ]]. Action = [[ 0.1363774  -0.18791592 -0.17098473  0.8680687 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 3944 is [True, False, False, False, True, False]
Scene graph at timestep 3944 is [True, False, False, False, True, False]
State prediction error at timestep 3944 is tensor(4.8340e-09, grad_fn=<MseLossBackward0>)
Current timestep = 3945. State = [[-0.20618865 -0.03658853]]. Action = [[-0.23524204  0.19982216 -0.16932394 -0.2617305 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 3945 is [True, False, False, False, True, False]
Scene graph at timestep 3945 is [True, False, False, False, True, False]
State prediction error at timestep 3945 is tensor(9.0660e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3946. State = [[-0.20624939 -0.0363396 ]]. Action = [[-0.13962957 -0.13597572  0.07647207 -0.54945767]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 3946 is [True, False, False, False, True, False]
Human Feedback received at timestep 3946 of 1
Current timestep = 3947. State = [[-0.20650181 -0.03673282]]. Action = [[-0.03771231 -0.04033898 -0.16824219  0.18278551]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 3947 is [True, False, False, False, True, False]
Current timestep = 3948. State = [[-0.20672093 -0.03769131]]. Action = [[ 0.01943934 -0.06779209 -0.02446239 -0.3183366 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 3948 is [True, False, False, False, True, False]
Current timestep = 3949. State = [[-0.20705935 -0.03888503]]. Action = [[-0.16848223  0.10968885 -0.11176297  0.8505298 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 3949 is [True, False, False, False, True, False]
Current timestep = 3950. State = [[-0.20873784 -0.0387506 ]]. Action = [[ 0.18305889  0.22702217 -0.009222    0.06143522]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 3950 is [True, False, False, False, True, False]
Scene graph at timestep 3950 is [True, False, False, False, True, False]
State prediction error at timestep 3950 is tensor(8.6503e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3951. State = [[-0.20903902 -0.03716454]]. Action = [[-0.16736723 -0.03374794 -0.10401832  0.78428435]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 3951 is [True, False, False, False, True, False]
Scene graph at timestep 3951 is [True, False, False, False, True, False]
State prediction error at timestep 3951 is tensor(9.4957e-08, grad_fn=<MseLossBackward0>)
Current timestep = 3952. State = [[-0.20995581 -0.03672905]]. Action = [[-0.21393052 -0.21039243 -0.05532774 -0.8359823 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 3952 is [True, False, False, False, True, False]
Current timestep = 3953. State = [[-0.21173744 -0.03775125]]. Action = [[ 0.08119887  0.02315444 -0.21413697 -0.04764807]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 3953 is [True, False, False, False, True, False]
Scene graph at timestep 3953 is [True, False, False, False, True, False]
State prediction error at timestep 3953 is tensor(7.6926e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3954. State = [[-0.21243101 -0.0384626 ]]. Action = [[ 0.18993187 -0.19750476 -0.15784413 -0.9181147 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 3954 is [True, False, False, False, True, False]
Scene graph at timestep 3954 is [True, False, False, False, True, False]
State prediction error at timestep 3954 is tensor(1.4923e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3955. State = [[-0.21267085 -0.04096122]]. Action = [[ 0.1879043  -0.08116758 -0.22712648  0.8638978 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 3955 is [True, False, False, False, True, False]
Current timestep = 3956. State = [[-0.21259035 -0.04334101]]. Action = [[ 0.12915856 -0.21692134 -0.09764946 -0.5734501 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 3956 is [True, False, False, False, True, False]
Scene graph at timestep 3956 is [True, False, False, False, True, False]
State prediction error at timestep 3956 is tensor(3.1086e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3957. State = [[-0.21137416 -0.04763735]]. Action = [[ 0.11159474 -0.1426694   0.1080133   0.54597116]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 3957 is [True, False, False, False, True, False]
Current timestep = 3958. State = [[-0.20952775 -0.05209304]]. Action = [[ 0.1907602   0.17120135 -0.0442422   0.6496285 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 3958 is [True, False, False, False, True, False]
Scene graph at timestep 3958 is [True, False, False, False, True, False]
State prediction error at timestep 3958 is tensor(2.9067e-05, grad_fn=<MseLossBackward0>)
Current timestep = 3959. State = [[-0.20693842 -0.05278888]]. Action = [[-1.8948722e-01 -6.7861378e-04 -1.6332652e-01  9.5166111e-01]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 3959 is [True, False, False, False, True, False]
Current timestep = 3960. State = [[-0.20643225 -0.05306985]]. Action = [[-0.09865928  0.0972988   0.05689681 -0.3328526 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 3960 is [True, False, False, False, True, False]
Scene graph at timestep 3960 is [True, False, False, False, True, False]
State prediction error at timestep 3960 is tensor(1.1353e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3961. State = [[-0.20637484 -0.05281118]]. Action = [[-0.18521163  0.18885022 -0.15315863  0.6452018 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 3961 is [True, False, False, False, True, False]
Current timestep = 3962. State = [[-0.20671996 -0.05100802]]. Action = [[0.23049009 0.1889407  0.06183267 0.13339281]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 3962 is [True, False, False, False, True, False]
Scene graph at timestep 3962 is [True, False, False, False, True, False]
State prediction error at timestep 3962 is tensor(6.4871e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3963. State = [[-0.20693366 -0.04827802]]. Action = [[-0.16821787 -0.0959022   0.22655454 -0.69766897]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 3963 is [True, False, False, False, True, False]
Current timestep = 3964. State = [[-0.20697364 -0.04763131]]. Action = [[ 0.03987217  0.07111382 -0.16459082 -0.94154376]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 3964 is [True, False, False, False, True, False]
Current timestep = 3965. State = [[-0.20711839 -0.04670412]]. Action = [[-0.17969282  0.13192189 -0.2463856  -0.03692448]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 3965 is [True, False, False, False, True, False]
Current timestep = 3966. State = [[-0.20769417 -0.04425899]]. Action = [[-0.17043468 -0.0754133   0.20774812  0.97560644]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 3966 is [True, False, False, False, True, False]
Scene graph at timestep 3966 is [True, False, False, False, True, False]
State prediction error at timestep 3966 is tensor(5.1245e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3967. State = [[-0.20908038 -0.04335609]]. Action = [[ 0.22744107 -0.08354542 -0.1948095   0.9645767 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 3967 is [True, False, False, False, True, False]
Current timestep = 3968. State = [[-0.20906937 -0.0434077 ]]. Action = [[-0.12851118 -0.03217767 -0.23504318 -0.62906307]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 3968 is [True, False, False, False, True, False]
Current timestep = 3969. State = [[-0.20924108 -0.04331958]]. Action = [[0.228401   0.17775655 0.22969303 0.6342752 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 3969 is [True, False, False, False, True, False]
Current timestep = 3970. State = [[-0.20932144 -0.04261063]]. Action = [[-0.20011474 -0.13329428  0.12184933  0.80543184]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 3970 is [True, False, False, False, True, False]
Scene graph at timestep 3970 is [True, False, False, False, True, False]
State prediction error at timestep 3970 is tensor(5.2321e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3971. State = [[-0.2093568  -0.04257262]]. Action = [[0.20464295 0.0991756  0.14411443 0.83525777]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 3971 is [True, False, False, False, True, False]
Current timestep = 3972. State = [[-0.20928809 -0.0424272 ]]. Action = [[ 0.1024195  -0.18727514 -0.1069341   0.9493015 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 3972 is [True, False, False, False, True, False]
Current timestep = 3973. State = [[-0.20927982 -0.04270921]]. Action = [[-0.15780345  0.1417599  -0.08423032  0.2529583 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 3973 is [True, False, False, False, True, False]
Current timestep = 3974. State = [[-0.20925541 -0.04280716]]. Action = [[ 0.04232216 -0.2297068  -0.10452604  0.03386784]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 3974 is [True, False, False, False, True, False]
Scene graph at timestep 3974 is [True, False, False, False, True, False]
State prediction error at timestep 3974 is tensor(6.0213e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3975. State = [[-0.2093141  -0.04394912]]. Action = [[ 0.12654465 -0.00288633 -0.16004884  0.67888594]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 3975 is [True, False, False, False, True, False]
Current timestep = 3976. State = [[-0.20928106 -0.04418484]]. Action = [[-0.17023334  0.09158275  0.22617847 -0.73745614]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 3976 is [True, False, False, False, True, False]
Current timestep = 3977. State = [[-0.20929216 -0.04424473]]. Action = [[-0.08168033  0.00663432  0.21084589 -0.398879  ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 3977 is [True, False, False, False, True, False]
Current timestep = 3978. State = [[-0.20936829 -0.04418811]]. Action = [[-0.07330769  0.04090565  0.13186917 -0.08236015]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 3978 is [True, False, False, False, True, False]
Current timestep = 3979. State = [[-0.20939033 -0.04419642]]. Action = [[-0.02226089 -0.1727818   0.12392455  0.80604386]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 3979 is [True, False, False, False, True, False]
Scene graph at timestep 3979 is [True, False, False, False, True, False]
State prediction error at timestep 3979 is tensor(1.6503e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3980. State = [[-0.20966877 -0.04577046]]. Action = [[ 0.06256592 -0.18126366  0.08571839 -0.31324387]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 3980 is [True, False, False, False, True, False]
Current timestep = 3981. State = [[-0.20988931 -0.04805896]]. Action = [[-0.06194726  0.19640559  0.09666952 -0.8003897 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 3981 is [True, False, False, False, True, False]
Current timestep = 3982. State = [[-0.21003185 -0.04796733]]. Action = [[-0.23207287 -0.15449592  0.1330657  -0.01795614]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 3982 is [True, False, False, False, True, False]
Current timestep = 3983. State = [[-0.21190739 -0.04934794]]. Action = [[-0.1434367   0.07420021 -0.04825476 -0.6861362 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 3983 is [True, False, False, False, True, False]
Current timestep = 3984. State = [[-0.21403986 -0.04961992]]. Action = [[ 0.00705874  0.11343795 -0.23386815  0.02580106]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 3984 is [True, False, False, False, True, False]
Current timestep = 3985. State = [[-0.21563078 -0.04878033]]. Action = [[-0.00184765  0.19519031  0.16052476 -0.3565942 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 3985 is [True, False, False, False, True, False]
Current timestep = 3986. State = [[-0.21665803 -0.04665887]]. Action = [[ 0.21801472  0.01395601  0.14170176 -0.511218  ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 3986 is [True, False, False, False, True, False]
Scene graph at timestep 3986 is [True, False, False, False, True, False]
State prediction error at timestep 3986 is tensor(5.6056e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3987. State = [[-0.21681295 -0.04534416]]. Action = [[ 0.00318953  0.19726044 -0.129102    0.6019963 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 3987 is [True, False, False, False, True, False]
Current timestep = 3988. State = [[-0.21711402 -0.04213601]]. Action = [[-0.06916761  0.07246965 -0.20520335 -0.96423054]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 3988 is [True, False, False, False, True, False]
Scene graph at timestep 3988 is [True, False, False, False, True, False]
State prediction error at timestep 3988 is tensor(5.0073e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3989. State = [[-0.21759604 -0.03870633]]. Action = [[ 0.16573954 -0.09022729  0.14034608 -0.6363667 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 3989 is [True, False, False, False, True, False]
Scene graph at timestep 3989 is [True, False, False, False, True, False]
State prediction error at timestep 3989 is tensor(4.5451e-06, grad_fn=<MseLossBackward0>)
Current timestep = 3990. State = [[-0.2176069  -0.03772742]]. Action = [[-0.02196141  0.12671718  0.13192922  0.63730216]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 3990 is [True, False, False, False, True, False]
Current timestep = 3991. State = [[-0.21764626 -0.03613967]]. Action = [[ 0.0179345  -0.12698966 -0.05411859 -0.30034733]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 3991 is [True, False, False, False, True, False]
Current timestep = 3992. State = [[-0.21759498 -0.03587451]]. Action = [[0.15200949 0.09144676 0.03379917 0.915185  ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 3992 is [True, False, False, False, True, False]
Current timestep = 3993. State = [[-0.21709089 -0.03531516]]. Action = [[-0.1565085  -0.07762866 -0.16910066  0.8763397 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 3993 is [True, False, False, False, True, False]
Scene graph at timestep 3993 is [True, False, False, False, True, False]
State prediction error at timestep 3993 is tensor(1.9773e-10, grad_fn=<MseLossBackward0>)
Current timestep = 3994. State = [[-0.21712646 -0.03510384]]. Action = [[-0.16557628  0.12834421 -0.02618469  0.8053926 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 3994 is [True, False, False, False, True, False]
Scene graph at timestep 3994 is [True, False, False, False, True, False]
State prediction error at timestep 3994 is tensor(2.5447e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3995. State = [[-0.2173686  -0.03387864]]. Action = [[0.08788076 0.23540387 0.22186375 0.30680954]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 3995 is [True, False, False, False, True, False]
Current timestep = 3996. State = [[-0.21804678 -0.03030874]]. Action = [[-0.0741857  -0.20740798 -0.13575351 -0.679085  ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 3996 is [True, False, False, False, True, False]
Current timestep = 3997. State = [[-0.21814455 -0.02996523]]. Action = [[-0.07635766 -0.00255944 -0.10581148  0.5594473 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 3997 is [True, False, False, False, True, False]
Scene graph at timestep 3997 is [True, False, False, False, True, False]
State prediction error at timestep 3997 is tensor(8.3532e-07, grad_fn=<MseLossBackward0>)
Current timestep = 3998. State = [[-0.21819094 -0.02987382]]. Action = [[ 0.05264992  0.12035578  0.14492345 -0.6259447 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 3998 is [True, False, False, False, True, False]
Current timestep = 3999. State = [[-0.21847902 -0.02848427]]. Action = [[-0.01417293  0.22756994  0.02029085 -0.23524112]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 3999 is [True, False, False, False, True, False]
Current timestep = 4000. State = [[-0.21949235 -0.02438741]]. Action = [[ 0.18466198 -0.00788254 -0.00653766 -0.11082399]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 4000 is [True, False, False, False, True, False]
Scene graph at timestep 4000 is [True, False, False, False, True, False]
State prediction error at timestep 4000 is tensor(1.4683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4001. State = [[-0.21986312 -0.0222289 ]]. Action = [[-0.12920825 -0.15250318  0.11374179 -0.82849956]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 4001 is [True, False, False, False, True, False]
Current timestep = 4002. State = [[-0.21990618 -0.02224841]]. Action = [[-0.21356712  0.0357627  -0.22982033 -0.7947798 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 4002 is [True, False, False, False, True, False]
Scene graph at timestep 4002 is [True, False, False, False, True, False]
State prediction error at timestep 4002 is tensor(1.2116e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4003. State = [[-0.22035794 -0.0217595 ]]. Action = [[ 0.15608847  0.18605831  0.17943448 -0.800166  ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 4003 is [True, False, False, False, True, False]
Current timestep = 4004. State = [[-0.2207342  -0.01974192]]. Action = [[ 0.01693004 -0.02223657 -0.22525541  0.2555864 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 4004 is [True, False, False, False, True, False]
Current timestep = 4005. State = [[-0.22099425 -0.01858215]]. Action = [[-0.1699071  -0.02256583  0.14449897 -0.55762917]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 4005 is [True, False, False, False, True, False]
Current timestep = 4006. State = [[-0.22124135 -0.01781495]]. Action = [[-0.21635504  0.03735846  0.14819974 -0.7801658 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 4006 is [True, False, False, False, True, False]
Current timestep = 4007. State = [[-0.22259371 -0.01676007]]. Action = [[ 0.04857823 -0.06258675  0.24583179 -0.26488602]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 4007 is [True, False, False, False, True, False]
Current timestep = 4008. State = [[-0.22298399 -0.01673717]]. Action = [[ 0.08370864  0.10599983 -0.08925784  0.84253526]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 4008 is [True, False, False, False, True, False]
Current timestep = 4009. State = [[-0.2233722 -0.0159761]]. Action = [[-0.20911053 -0.14509371  0.01569098  0.89054656]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 4009 is [True, False, False, False, True, False]
Current timestep = 4010. State = [[-0.22440408 -0.01639584]]. Action = [[0.16317356 0.17464364 0.15094021 0.46101952]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 4010 is [True, False, False, False, True, False]
Scene graph at timestep 4010 is [True, False, False, False, True, False]
State prediction error at timestep 4010 is tensor(2.9261e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4011. State = [[-0.22472765 -0.01504579]]. Action = [[ 0.09400511  0.16883191  0.02718586 -0.22538811]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 4011 is [True, False, False, False, True, False]
Current timestep = 4012. State = [[-0.22527243 -0.01266877]]. Action = [[ 0.11037305 -0.07746209 -0.11987129  0.6370796 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 4012 is [True, False, False, False, True, False]
Scene graph at timestep 4012 is [True, False, False, False, True, False]
State prediction error at timestep 4012 is tensor(2.9285e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4013. State = [[-0.2254107  -0.01183156]]. Action = [[ 0.21175635  0.08741063 -0.11986119 -0.1662184 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 4013 is [True, False, False, False, True, False]
Current timestep = 4014. State = [[-0.22425085 -0.0105288 ]]. Action = [[-0.09547701  0.15783808 -0.10442472  0.9825494 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 4014 is [True, False, False, False, True, False]
Current timestep = 4015. State = [[-0.22462533 -0.00803199]]. Action = [[ 0.15772396 -0.03882152  0.16996562  0.7833102 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 4015 is [True, False, False, False, True, False]
Current timestep = 4016. State = [[-0.22348489 -0.00697711]]. Action = [[ 0.0389083  -0.15053776 -0.12754855 -0.02872318]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 4016 is [True, False, False, False, True, False]
Current timestep = 4017. State = [[-0.22294073 -0.00704825]]. Action = [[-0.01755404  0.01964763  0.21531945 -0.90208167]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 4017 is [True, False, False, False, True, False]
Current timestep = 4018. State = [[-0.22264817 -0.00705233]]. Action = [[-0.17451295  0.05032855 -0.2336628   0.8739301 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 4018 is [True, False, False, False, True, False]
Current timestep = 4019. State = [[-0.22267573 -0.00669952]]. Action = [[ 0.16433227  0.16747701 -0.20865434 -0.74838275]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 4019 is [True, False, False, False, True, False]
Current timestep = 4020. State = [[-0.22285858 -0.00520523]]. Action = [[ 0.10013825 -0.04059903 -0.19243345  0.28086305]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 4020 is [True, False, False, False, True, False]
Scene graph at timestep 4020 is [True, False, False, False, True, False]
State prediction error at timestep 4020 is tensor(2.7958e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4021. State = [[-0.22178829 -0.00463395]]. Action = [[ 0.22982979  0.04024783 -0.21770498 -0.03062004]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 4021 is [True, False, False, False, True, False]
Scene graph at timestep 4021 is [True, False, False, False, True, False]
State prediction error at timestep 4021 is tensor(2.1763e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4022. State = [[-0.21884453 -0.00402446]]. Action = [[ 0.04456583 -0.14745401  0.16131455 -0.42590195]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 4022 is [True, False, False, False, True, False]
Current timestep = 4023. State = [[-0.21617591 -0.00414831]]. Action = [[ 0.0476391   0.07206711  0.19028613 -0.74194586]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 4023 is [True, False, False, False, True, False]
Scene graph at timestep 4023 is [True, False, False, False, True, False]
State prediction error at timestep 4023 is tensor(3.2416e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4024. State = [[-0.21315771 -0.0039424 ]]. Action = [[ 0.2451616  -0.22543876  0.17405    -0.05374449]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 4024 is [True, False, False, False, True, False]
Current timestep = 4025. State = [[-0.20943776 -0.00491669]]. Action = [[ 0.16055906 -0.17128207 -0.2361637   0.5947348 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 4025 is [True, False, False, False, True, False]
Current timestep = 4026. State = [[-0.20519102 -0.00699619]]. Action = [[-0.07183108 -0.0420644  -0.05823204  0.1094861 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 4026 is [True, False, False, False, True, False]
Current timestep = 4027. State = [[-0.20213391 -0.0087387 ]]. Action = [[ 0.1733585   0.13562849  0.22980773 -0.6057997 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 4027 is [True, False, False, False, True, False]
Human Feedback received at timestep 4027 of 1
Current timestep = 4028. State = [[-0.19779581 -0.0086938 ]]. Action = [[ 0.24768162 -0.0205496  -0.00550844  0.6895175 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 4028 is [True, False, False, False, True, False]
Scene graph at timestep 4028 is [True, False, False, False, True, False]
State prediction error at timestep 4028 is tensor(1.8247e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4029. State = [[-0.19205429 -0.00846978]]. Action = [[ 1.9147044e-01  2.1487939e-01 -5.4322183e-04  7.0441926e-01]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 4029 is [True, False, False, False, True, False]
Current timestep = 4030. State = [[-0.18762341 -0.00738071]]. Action = [[ 0.110039   -0.17727329 -0.01270521  0.28335702]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 4030 is [True, False, False, False, True, False]
Current timestep = 4031. State = [[-0.18205869 -0.00731843]]. Action = [[ 0.17256403  0.16078505 -0.02647941 -0.32075673]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 4031 is [True, False, False, False, True, False]
Human Feedback received at timestep 4031 of 1
Current timestep = 4032. State = [[-0.17603104 -0.00667166]]. Action = [[-0.02539763  0.1036959   0.00126147 -0.2856096 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 4032 is [True, False, False, False, True, False]
Scene graph at timestep 4032 is [True, False, False, False, True, False]
State prediction error at timestep 4032 is tensor(8.4717e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4033. State = [[-0.17160271 -0.00523353]]. Action = [[-0.10872576 -0.10455558  0.20484605 -0.07424998]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 4033 is [True, False, False, False, True, False]
Scene graph at timestep 4033 is [True, False, False, False, True, False]
State prediction error at timestep 4033 is tensor(6.8293e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4034. State = [[-0.16989693 -0.00524876]]. Action = [[ 0.08156738 -0.12472805 -0.23345736 -0.6070193 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 4034 is [True, False, False, False, True, False]
Current timestep = 4035. State = [[-0.16960911 -0.00580276]]. Action = [[-0.12960422  0.18876326 -0.03919336  0.7268616 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 4035 is [True, False, False, False, True, False]
Human Feedback received at timestep 4035 of 1
Current timestep = 4036. State = [[-0.16967736 -0.00521739]]. Action = [[-0.22104658 -0.21635877 -0.18509656  0.4461069 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 4036 is [True, False, False, False, True, False]
Current timestep = 4037. State = [[-0.17001596 -0.00615066]]. Action = [[-0.17176509  0.06328243  0.21714148  0.75383043]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 4037 is [True, False, False, False, True, False]
Current timestep = 4038. State = [[-0.17028175 -0.00622175]]. Action = [[-0.20670274  0.09207666 -0.2085576  -0.9336513 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 4038 is [True, False, False, False, True, False]
Scene graph at timestep 4038 is [True, False, False, False, True, False]
State prediction error at timestep 4038 is tensor(1.1262e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4039. State = [[-0.17189597 -0.00558308]]. Action = [[ 0.23550865  0.11614406  0.20940512 -0.5718743 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 4039 is [True, False, False, False, True, False]
Current timestep = 4040. State = [[-0.17222548 -0.00417234]]. Action = [[-2.0317501e-01  1.8253922e-04 -6.4233750e-02  2.8746545e-01]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 4040 is [True, False, False, False, True, False]
Current timestep = 4041. State = [[-0.17337772 -0.00271174]]. Action = [[-0.06247626  0.10668594  0.11638629 -0.41265476]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 4041 is [True, False, False, False, True, False]
Human Feedback received at timestep 4041 of 1
Current timestep = 4042. State = [[-0.17463186 -0.00050032]]. Action = [[ 0.10130927 -0.08988431  0.17544821  0.09433198]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 4042 is [True, False, False, False, True, False]
Current timestep = 4043. State = [[-0.17480437 -0.000246  ]]. Action = [[ 0.00917679 -0.02266881 -0.05948544 -0.46659338]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 4043 is [True, False, False, False, True, False]
Current timestep = 4044. State = [[-0.1748924  -0.00035681]]. Action = [[-0.2238539  -0.14591399  0.06722641  0.32087183]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 4044 is [True, False, False, False, True, False]
Current timestep = 4045. State = [[-0.17633541 -0.00182121]]. Action = [[ 0.22774673 -0.09826764 -0.09811854 -0.296067  ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 4045 is [True, False, False, False, True, False]
Current timestep = 4046. State = [[-0.1762589  -0.00357502]]. Action = [[-0.22631246  0.01820832  0.11643881 -0.36842954]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 4046 is [True, False, False, False, True, False]
Current timestep = 4047. State = [[-0.17727737 -0.00420329]]. Action = [[0.20540053 0.16612262 0.19214368 0.3827343 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 4047 is [True, False, False, False, True, False]
Current timestep = 4048. State = [[-0.1774638  -0.00326799]]. Action = [[ 0.04667833 -0.14052656 -0.10861605 -0.92153233]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 4048 is [True, False, False, False, True, False]
Current timestep = 4049. State = [[-0.17731905 -0.0035118 ]]. Action = [[ 0.19680586  0.19890183  0.04604718 -0.9037345 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 4049 is [True, False, False, False, True, False]
Current timestep = 4050. State = [[-0.17721233 -0.00300644]]. Action = [[ 0.17884672 -0.10501496  0.1930196   0.26364148]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 4050 is [True, False, False, False, True, False]
Current timestep = 4051. State = [[-0.17574835 -0.00311678]]. Action = [[0.24370423 0.02524343 0.24184108 0.82555807]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 4051 is [True, False, False, False, True, False]
Current timestep = 4052. State = [[-0.1730766 -0.0028534]]. Action = [[ 0.18653458  0.17113322  0.18717772 -0.63476413]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 4052 is [True, False, False, False, True, False]
Current timestep = 4053. State = [[-0.169657   -0.00137969]]. Action = [[-0.19617057  0.23168427  0.146929    0.07371211]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 4053 is [True, False, False, False, True, False]
Current timestep = 4054. State = [[-0.16827519  0.00235584]]. Action = [[-1.8464029e-04 -6.2061936e-02 -1.7096141e-01  9.4949079e-01]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 4054 is [True, False, False, False, True, False]
Current timestep = 4055. State = [[-0.1674803   0.00337278]]. Action = [[ 0.02621767 -0.22316204  0.09944507 -0.7396052 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 4055 is [True, False, False, False, True, False]
Current timestep = 4056. State = [[-0.16715035  0.00223469]]. Action = [[-0.02301632 -0.07875261  0.23008275 -0.8972923 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 4056 is [True, False, False, False, True, False]
Current timestep = 4057. State = [[-0.16688865  0.00176364]]. Action = [[-0.01446478  0.12724608 -0.06778935  0.06511486]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 4057 is [True, False, False, False, True, False]
Current timestep = 4058. State = [[-0.16694435  0.00205809]]. Action = [[-0.2377229  -0.12602924 -0.05928735  0.52535224]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 4058 is [True, False, False, False, True, False]
Current timestep = 4059. State = [[-0.16705814  0.00149951]]. Action = [[-0.018215    0.15785903 -0.08895159  0.89219046]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 4059 is [True, False, False, False, True, False]
Current timestep = 4060. State = [[-0.1672831   0.00199774]]. Action = [[-0.09429276 -0.0315838   0.19531193 -0.5327182 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 4060 is [True, False, False, False, True, False]
Current timestep = 4061. State = [[-0.16775252  0.00247474]]. Action = [[-0.15177707  0.19876283 -0.15638256 -0.50442547]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 4061 is [True, False, False, False, True, False]
Current timestep = 4062. State = [[-0.16865686  0.00482018]]. Action = [[ 0.10762525  0.22384292 -0.08317234 -0.746954  ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 4062 is [True, False, False, False, True, False]
Scene graph at timestep 4062 is [True, False, False, False, True, False]
State prediction error at timestep 4062 is tensor(2.8093e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4063. State = [[-0.17002341  0.00950556]]. Action = [[ 0.12799025  0.12578952 -0.04129982 -0.5382945 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 4063 is [True, False, False, False, True, False]
Scene graph at timestep 4063 is [True, False, False, False, True, False]
State prediction error at timestep 4063 is tensor(1.7266e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4064. State = [[-0.1711442   0.01392644]]. Action = [[ 0.21648696  0.04841927 -0.21648867 -0.8424414 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 4064 is [True, False, False, False, True, False]
Current timestep = 4065. State = [[-0.17070942  0.01658119]]. Action = [[-0.07757752 -0.03019746  0.00477001  0.9781449 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 4065 is [True, False, False, False, True, False]
Current timestep = 4066. State = [[-0.17094103  0.01774061]]. Action = [[ 0.21051499  0.02698568  0.11923295 -0.5049603 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 4066 is [True, False, False, False, True, False]
Current timestep = 4067. State = [[-0.16966608  0.01865805]]. Action = [[ 0.0717876   0.1292482  -0.06679265 -0.76592505]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 4067 is [True, False, False, False, True, False]
Current timestep = 4068. State = [[-0.16811265  0.02073921]]. Action = [[-0.14606313  0.22059879  0.14720559 -0.9009891 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 4068 is [True, False, False, False, True, False]
Current timestep = 4069. State = [[-0.1683675  0.0256589]]. Action = [[-0.2121513   0.19567037 -0.11391535 -0.4014389 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 4069 is [True, False, False, False, True, False]
Scene graph at timestep 4069 is [True, False, False, False, True, False]
State prediction error at timestep 4069 is tensor(1.2724e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4070. State = [[-0.16974731  0.03239465]]. Action = [[ 0.04908612  0.20126334  0.03372282 -0.63296497]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 4070 is [True, False, False, False, True, False]
Current timestep = 4071. State = [[-0.17085978  0.0383416 ]]. Action = [[-0.22570965  0.06988239  0.2329101  -0.74250025]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 4071 is [True, False, False, False, True, False]
Scene graph at timestep 4071 is [True, False, False, False, True, False]
State prediction error at timestep 4071 is tensor(1.5427e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4072. State = [[-0.17243794  0.04373172]]. Action = [[ 0.03209975 -0.01001154 -0.13817517 -0.40864295]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 4072 is [True, False, False, False, True, False]
Scene graph at timestep 4072 is [True, False, False, False, True, False]
State prediction error at timestep 4072 is tensor(2.4852e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4073. State = [[-0.17345579  0.04685031]]. Action = [[-0.0332592   0.17554626 -0.10886998  0.8108586 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 4073 is [True, False, False, False, True, False]
Scene graph at timestep 4073 is [True, False, False, False, True, False]
State prediction error at timestep 4073 is tensor(2.8347e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4073 of -1
Current timestep = 4074. State = [[-0.1750385   0.05159497]]. Action = [[-0.07948634  0.12500408  0.18353301 -0.94986874]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 4074 is [True, False, False, False, True, False]
Current timestep = 4075. State = [[-0.17649385  0.05590339]]. Action = [[0.1565888  0.2115711  0.04372555 0.23095775]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 4075 is [True, False, False, False, True, False]
Current timestep = 4076. State = [[-0.17780846  0.06055459]]. Action = [[ 0.13562009  0.00314921  0.05687323 -0.97245336]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 4076 is [True, False, False, False, True, False]
Scene graph at timestep 4076 is [True, False, False, False, True, False]
State prediction error at timestep 4076 is tensor(6.9957e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4077. State = [[-0.17705962  0.06332993]]. Action = [[ 0.20401654 -0.04217935  0.08545473 -0.36460954]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 4077 is [True, False, False, False, True, False]
Scene graph at timestep 4077 is [True, False, False, False, True, False]
State prediction error at timestep 4077 is tensor(3.9466e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4078. State = [[-0.17416643  0.06442211]]. Action = [[ 0.00804618 -0.10917866 -0.22791497 -0.75613075]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 4078 is [True, False, False, False, True, False]
Scene graph at timestep 4078 is [True, False, False, False, True, False]
State prediction error at timestep 4078 is tensor(8.5922e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4079. State = [[-0.17067222  0.06485786]]. Action = [[ 0.23653188 -0.05244341 -0.22554381  0.5469222 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 4079 is [True, False, False, False, True, False]
Scene graph at timestep 4079 is [True, False, False, False, True, False]
State prediction error at timestep 4079 is tensor(2.1051e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4080. State = [[-0.16608806  0.06536822]]. Action = [[ 0.12719738 -0.00848316  0.21314931 -0.19396067]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 4080 is [True, False, False, False, True, False]
Current timestep = 4081. State = [[-0.16255233  0.0657476 ]]. Action = [[-0.21440728 -0.12551354 -0.09882924  0.8693702 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 4081 is [True, False, False, False, True, False]
Current timestep = 4082. State = [[-0.16056731  0.06508104]]. Action = [[-0.09628585  0.14939034 -0.08034247 -0.4409697 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 4082 is [True, False, False, False, True, False]
Current timestep = 4083. State = [[-0.15999097  0.06567777]]. Action = [[-0.14332753 -0.05038704 -0.18488166  0.4897164 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 4083 is [True, False, False, False, True, False]
Current timestep = 4084. State = [[-0.16059828  0.06572877]]. Action = [[0.1666941  0.19749144 0.20466954 0.8908613 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 4084 is [True, False, False, False, True, False]
Current timestep = 4085. State = [[-0.16030188  0.06654744]]. Action = [[ 0.14061794 -0.16949452 -0.22744939  0.9516835 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 4085 is [True, False, False, False, True, False]
Current timestep = 4086. State = [[-0.15907665  0.06642479]]. Action = [[-0.18467122  0.0802387  -0.00151981  0.04360354]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 4086 is [True, False, False, False, True, False]
Current timestep = 4087. State = [[-0.1591329  0.0666616]]. Action = [[0.23508853 0.07637334 0.07069981 0.6689267 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 4087 is [True, False, False, False, True, False]
Current timestep = 4088. State = [[-0.15781826  0.06720593]]. Action = [[ 0.19615242  0.12882924 -0.20721038 -0.9227286 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 4088 is [True, False, False, False, True, False]
Current timestep = 4089. State = [[-0.1550933   0.06854536]]. Action = [[ 0.1036872   0.06232777 -0.00230049  0.13592935]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 4089 is [True, False, False, False, True, False]
Current timestep = 4090. State = [[-0.15123469  0.07036874]]. Action = [[ 0.10400909  0.17377776 -0.09214115 -0.5321759 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 4090 is [True, False, False, False, True, False]
Scene graph at timestep 4090 is [True, False, False, False, True, False]
State prediction error at timestep 4090 is tensor(6.8425e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4091. State = [[-0.14755881  0.07361093]]. Action = [[-0.07901002 -0.18925186  0.01519319 -0.5510219 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 4091 is [True, False, False, False, True, False]
Current timestep = 4092. State = [[-0.14548658  0.07428117]]. Action = [[ 0.14328557  0.15092203 -0.16628392 -0.01110667]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 4092 is [True, False, False, False, True, False]
Current timestep = 4093. State = [[-0.14253543  0.07577988]]. Action = [[-0.09176847  0.07388192  0.20727235 -0.7923431 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 4093 is [True, False, False, False, True, False]
Human Feedback received at timestep 4093 of 1
Current timestep = 4094. State = [[-0.14099208  0.07816942]]. Action = [[ 0.21760118  0.14445227 -0.1800181   0.6147771 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 4094 is [True, False, False, False, True, False]
Scene graph at timestep 4094 is [True, False, False, False, True, False]
State prediction error at timestep 4094 is tensor(2.9619e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4095. State = [[-0.13893603  0.08098575]]. Action = [[-0.2253705  -0.13248992 -0.18167014  0.44401503]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 4095 is [True, False, False, False, True, False]
Current timestep = 4096. State = [[-0.13852604  0.08164716]]. Action = [[ 0.03365678 -0.0257903  -0.23339218  0.16437948]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 4096 is [True, False, False, False, True, False]
Current timestep = 4097. State = [[-0.13857347  0.08187995]]. Action = [[-0.20291781 -0.03767201 -0.06007797  0.8425411 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 4097 is [True, False, False, False, True, False]
Scene graph at timestep 4097 is [True, False, False, False, True, False]
State prediction error at timestep 4097 is tensor(5.6678e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4098. State = [[-0.13883606  0.08187005]]. Action = [[ 0.15123093 -0.03987457 -0.1710731  -0.7881181 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 4098 is [True, False, False, False, True, False]
Scene graph at timestep 4098 is [True, False, False, False, True, False]
State prediction error at timestep 4098 is tensor(8.5952e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4099. State = [[-0.1388123   0.08167262]]. Action = [[-0.04770017 -0.14874786  0.06643602 -0.86127806]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 4099 is [True, False, False, False, True, False]
Scene graph at timestep 4099 is [True, False, False, False, True, False]
State prediction error at timestep 4099 is tensor(7.0982e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4100. State = [[-0.13872264  0.08078376]]. Action = [[-0.07137555  0.21873397  0.01531324  0.7735385 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 4100 is [True, False, False, False, True, False]
Scene graph at timestep 4100 is [True, False, False, False, True, False]
State prediction error at timestep 4100 is tensor(1.0958e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4101. State = [[-0.13918471  0.08187257]]. Action = [[ 0.13059995  0.11607334 -0.20107242  0.38788235]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 4101 is [True, False, False, False, True, False]
Scene graph at timestep 4101 is [True, False, False, False, True, False]
State prediction error at timestep 4101 is tensor(4.1021e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4102. State = [[-0.13939387  0.08325332]]. Action = [[-0.11650789  0.14413622 -0.06537935  0.20614493]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 4102 is [True, False, False, False, True, False]
Scene graph at timestep 4102 is [True, False, False, False, True, False]
State prediction error at timestep 4102 is tensor(1.1058e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4103. State = [[-0.14024661  0.08596644]]. Action = [[-0.15607443  0.0254004   0.11074296 -0.47465205]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 4103 is [True, False, False, False, True, False]
Current timestep = 4104. State = [[-0.14126302  0.08826247]]. Action = [[-0.21650536 -0.20345883  0.04209024 -0.6052849 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 4104 is [True, False, False, False, True, False]
Current timestep = 4105. State = [[-0.14172661  0.08874256]]. Action = [[ 0.18508857  0.17772046 -0.14614956  0.7289785 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 4105 is [True, False, False, False, True, False]
Current timestep = 4106. State = [[-0.14232916  0.09004055]]. Action = [[0.03269491 0.19596702 0.13543949 0.6644094 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 4106 is [True, False, False, False, True, False]
Current timestep = 4107. State = [[-0.14355442  0.09333295]]. Action = [[-0.00936951  0.05548355 -0.18608564  0.16713369]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 4107 is [True, False, False, False, True, False]
Current timestep = 4108. State = [[-0.1448642   0.09634151]]. Action = [[-0.07449378  0.11428213 -0.02368665 -0.07266283]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 4108 is [True, False, False, False, True, False]
Current timestep = 4109. State = [[-0.1463014   0.09987538]]. Action = [[-0.15339205 -0.01781181  0.15613487 -0.54108065]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 4109 is [True, False, False, False, True, False]
Current timestep = 4110. State = [[-0.1475354  0.1023189]]. Action = [[ 0.03977716 -0.21025184  0.1897505  -0.8562231 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 4110 is [True, False, False, False, True, False]
Scene graph at timestep 4110 is [True, False, False, False, True, False]
State prediction error at timestep 4110 is tensor(4.4999e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4111. State = [[-0.14766203  0.10142238]]. Action = [[-0.19277242 -0.13124923 -0.00173245  0.14518368]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 4111 is [True, False, False, False, True, False]
Scene graph at timestep 4111 is [True, False, False, False, True, False]
State prediction error at timestep 4111 is tensor(4.0157e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4112. State = [[-0.1479227   0.10008437]]. Action = [[-0.22528677  0.2101255   0.03026277  0.86930096]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 4112 is [True, False, False, False, True, False]
Current timestep = 4113. State = [[-0.14943638  0.10240942]]. Action = [[-0.12138259  0.1272425  -0.07547522  0.24211204]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 4113 is [True, False, False, False, True, False]
Current timestep = 4114. State = [[-0.15149066  0.1056048 ]]. Action = [[-0.23094262 -0.10124552  0.02880877  0.8542416 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 4114 is [True, False, False, False, True, False]
Current timestep = 4115. State = [[-0.15435311  0.10666963]]. Action = [[0.0923807  0.03511447 0.1098226  0.40518045]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 4115 is [True, False, False, False, True, False]
Current timestep = 4116. State = [[-0.1564466   0.10767493]]. Action = [[ 0.09543115  0.11103824 -0.16603011 -0.47881746]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 4116 is [True, False, False, False, True, False]
Current timestep = 4117. State = [[-0.15753399  0.10969564]]. Action = [[ 0.10013866  0.22840905 -0.03869303  0.0944401 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 4117 is [True, False, False, False, True, False]
Current timestep = 4118. State = [[-0.15891314  0.11291128]]. Action = [[ 0.11464703 -0.13921554 -0.23248407 -0.6001093 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 4118 is [True, False, False, False, True, False]
Current timestep = 4119. State = [[-0.1588831   0.11283449]]. Action = [[ 0.23367256 -0.20251903 -0.18678139 -0.6632156 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 4119 is [True, False, False, False, True, False]
Scene graph at timestep 4119 is [True, False, False, False, True, False]
State prediction error at timestep 4119 is tensor(1.0405e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4120. State = [[-0.15794684  0.11074684]]. Action = [[-0.22391309 -0.06804925 -0.11860618  0.32957542]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 4120 is [True, False, False, False, True, False]
Current timestep = 4121. State = [[-0.1578262   0.10907154]]. Action = [[-0.11893553  0.11719471 -0.00106117  0.64520574]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 4121 is [True, False, False, False, True, False]
Current timestep = 4122. State = [[-0.15837847  0.10936971]]. Action = [[ 0.04385775 -0.07798892 -0.08257467 -0.91498566]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 4122 is [True, False, False, False, True, False]
Human Feedback received at timestep 4122 of -1
Current timestep = 4123. State = [[-0.15834472  0.10849179]]. Action = [[ 0.01279306  0.14697856 -0.11729765 -0.5224459 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 4123 is [True, False, False, False, True, False]
Current timestep = 4124. State = [[-0.15855697  0.10900467]]. Action = [[-0.14803812 -0.02388264  0.11578724 -0.85000175]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 4124 is [True, False, False, False, True, False]
Scene graph at timestep 4124 is [True, False, False, False, True, False]
State prediction error at timestep 4124 is tensor(5.7583e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4125. State = [[-0.15877712  0.10936681]]. Action = [[ 0.21089482  0.08678082  0.05358708 -0.52534443]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 4125 is [True, False, False, False, True, False]
Scene graph at timestep 4125 is [True, False, False, False, True, False]
State prediction error at timestep 4125 is tensor(1.7340e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4126. State = [[-0.15891066  0.10981626]]. Action = [[ 0.20129135 -0.10999855  0.19189227  0.19257188]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 4126 is [True, False, False, False, True, False]
Current timestep = 4127. State = [[-0.15855654  0.1096748 ]]. Action = [[-0.03440468 -0.07764368 -0.06354006 -0.76894754]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 4127 is [True, False, False, False, True, False]
Current timestep = 4128. State = [[-0.15834394  0.1088893 ]]. Action = [[-0.14273189  0.05004537 -0.10974155 -0.7981514 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 4128 is [True, False, False, False, True, False]
Current timestep = 4129. State = [[-0.15833008  0.10886601]]. Action = [[-0.1827628  -0.00692677 -0.10269926 -0.8310021 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 4129 is [True, False, False, False, True, False]
Current timestep = 4130. State = [[-0.1584627   0.10896525]]. Action = [[ 0.06298909 -0.1946875  -0.12728979  0.8753483 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 4130 is [True, False, False, False, True, False]
Current timestep = 4131. State = [[-0.15814814  0.10684564]]. Action = [[-0.0983668  -0.09878524  0.05047789 -0.8063575 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 4131 is [True, False, False, False, True, False]
Current timestep = 4132. State = [[-0.15808114  0.10435925]]. Action = [[-0.1143814   0.04946217 -0.18841419  0.64897275]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 4132 is [True, False, False, False, True, False]
Current timestep = 4133. State = [[-0.15889044  0.10328287]]. Action = [[-0.08129767 -0.17510523 -0.10418665 -0.6763596 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 4133 is [True, False, False, False, True, False]
Current timestep = 4134. State = [[-0.16013314  0.10086826]]. Action = [[-0.09315869  0.12262243 -0.07828695  0.06483662]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 4134 is [True, False, False, False, True, False]
Current timestep = 4135. State = [[-0.16246429  0.10043176]]. Action = [[-0.09102052  0.22128668 -0.12580836 -0.04285979]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 4135 is [True, False, False, False, True, False]
Current timestep = 4136. State = [[-0.16518582  0.10279659]]. Action = [[ 0.11106971  0.1617729  -0.07156028 -0.61469275]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 4136 is [True, False, False, False, True, False]
Current timestep = 4137. State = [[-0.16694838  0.10629327]]. Action = [[-0.18185247 -0.00221848 -0.00395025  0.7666384 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 4137 is [True, False, False, False, True, False]
Current timestep = 4138. State = [[-0.16915807  0.10879947]]. Action = [[ 0.10967016  0.20134592 -0.02363323  0.92243946]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 4138 is [True, False, False, False, True, False]
Current timestep = 4139. State = [[-0.17102507  0.1124374 ]]. Action = [[ 0.24330026 -0.15019995  0.20229125  0.9469454 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 4139 is [True, False, False, False, True, False]
Current timestep = 4140. State = [[-0.17065008  0.11233104]]. Action = [[ 0.04171196 -0.15239888 -0.19269806  0.6306493 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 4140 is [True, False, False, False, True, False]
Current timestep = 4141. State = [[-0.17023472  0.11076494]]. Action = [[-0.13144219  0.17222685  0.16010684  0.27150846]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 4141 is [True, False, False, False, True, False]
Current timestep = 4142. State = [[-0.1708046   0.11152007]]. Action = [[ 0.18249291  0.14332819 -0.17000113  0.7625177 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 4142 is [True, False, False, False, True, False]
Scene graph at timestep 4142 is [True, False, False, False, True, False]
State prediction error at timestep 4142 is tensor(1.4493e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4143. State = [[-0.17113899  0.11258705]]. Action = [[-0.155962    0.10266906  0.10665557 -0.781018  ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 4143 is [True, False, False, False, True, False]
Scene graph at timestep 4143 is [True, False, False, False, True, False]
State prediction error at timestep 4143 is tensor(1.8175e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4144. State = [[-0.17233802  0.11505754]]. Action = [[-0.15508789 -0.1087229  -0.08350952  0.21251488]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 4144 is [True, False, False, False, True, False]
Scene graph at timestep 4144 is [True, False, False, False, True, False]
State prediction error at timestep 4144 is tensor(1.2827e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4145. State = [[-0.17299381  0.11630102]]. Action = [[ 0.11138713  0.05712843 -0.01617569  0.7684947 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 4145 is [True, False, False, False, True, False]
Current timestep = 4146. State = [[-0.17335029  0.11701602]]. Action = [[-0.01568532  0.01136222 -0.10831669 -0.16207117]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 4146 is [True, False, False, False, True, False]
Current timestep = 4147. State = [[-0.17357485  0.11757724]]. Action = [[ 0.08991456  0.06653339 -0.06979577 -0.11226231]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 4147 is [True, False, False, False, True, False]
Current timestep = 4148. State = [[-0.17386091  0.11838609]]. Action = [[ 0.16247886  0.17378148 -0.0326357  -0.87542707]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 4148 is [True, False, False, False, True, False]
Scene graph at timestep 4148 is [True, False, False, False, True, False]
State prediction error at timestep 4148 is tensor(1.0033e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4149. State = [[-0.17397876  0.12023441]]. Action = [[ 0.00377607 -0.22933239  0.05789456 -0.5378908 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 4149 is [True, False, False, False, True, False]
Current timestep = 4150. State = [[-0.17362133  0.12009429]]. Action = [[-0.03160444  0.04823762 -0.03140968 -0.86061317]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 4150 is [True, False, False, False, True, False]
Current timestep = 4151. State = [[-0.17343444  0.12006223]]. Action = [[-0.15782487 -0.11444008 -0.07759111 -0.01960307]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 4151 is [True, False, False, False, True, False]
Current timestep = 4152. State = [[-0.17330137  0.11942019]]. Action = [[ 0.22011116 -0.22385542  0.07180321 -0.38374537]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 4152 is [True, False, False, False, True, False]
Scene graph at timestep 4152 is [True, False, False, False, True, False]
State prediction error at timestep 4152 is tensor(6.4699e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4153. State = [[-0.17231257  0.11647847]]. Action = [[0.2049799  0.13868976 0.01729667 0.6356907 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 4153 is [True, False, False, False, True, False]
Current timestep = 4154. State = [[-0.17122726  0.11650077]]. Action = [[-0.18315683  0.24339247 -0.07805492 -0.21638626]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 4154 is [True, False, False, False, True, False]
Scene graph at timestep 4154 is [True, False, False, False, True, False]
State prediction error at timestep 4154 is tensor(2.3672e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4155. State = [[-0.17152445  0.11897277]]. Action = [[ 0.15415964  0.22190908 -0.05449064  0.59101343]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 4155 is [True, False, False, False, True, False]
Scene graph at timestep 4155 is [True, False, False, False, True, False]
State prediction error at timestep 4155 is tensor(1.1210e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4156. State = [[-0.1713085  0.1221856]]. Action = [[ 0.03058949 -0.09814878 -0.1321322  -0.36206055]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 4156 is [True, False, False, False, True, False]
Current timestep = 4157. State = [[-0.17042913  0.12301997]]. Action = [[ 0.05019248 -0.07635686 -0.13742429 -0.9579701 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 4157 is [True, False, False, False, True, False]
Current timestep = 4158. State = [[-0.16961621  0.12333654]]. Action = [[ 0.01214042  0.15358645 -0.00166896 -0.9163488 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 4158 is [True, False, False, False, True, False]
Scene graph at timestep 4158 is [True, False, False, False, True, False]
State prediction error at timestep 4158 is tensor(1.2324e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4159. State = [[-0.16847481  0.12464365]]. Action = [[ 0.15488935 -0.00451486 -0.05091113  0.94598675]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 4159 is [True, False, False, False, True, False]
Current timestep = 4160. State = [[-0.16555825  0.125604  ]]. Action = [[0.02067    0.07524654 0.01717699 0.22008085]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 4160 is [True, False, False, False, True, False]
Current timestep = 4161. State = [[-0.16235608  0.12729383]]. Action = [[ 0.11200589  0.09282562 -0.07818669  0.02391231]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 4161 is [True, False, False, False, False, True]
Current timestep = 4162. State = [[-0.15910345  0.12909244]]. Action = [[-0.18706353 -0.01951164 -0.14309627 -0.59248227]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 4162 is [True, False, False, False, False, True]
Current timestep = 4163. State = [[-0.15924224  0.13001736]]. Action = [[ 0.12323201  0.14396477 -0.1073662   0.42111635]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 4163 is [True, False, False, False, False, True]
Current timestep = 4164. State = [[-0.15962598  0.13179946]]. Action = [[ 0.02949306 -0.0921905  -0.19285771 -0.7757286 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 4164 is [True, False, False, False, False, True]
Current timestep = 4165. State = [[-0.15955122  0.1319835 ]]. Action = [[-0.02283984  0.05835387  0.01952982 -0.5855451 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 4165 is [True, False, False, False, False, True]
Current timestep = 4166. State = [[-0.15965234  0.13286054]]. Action = [[-0.15242374  0.09819108  0.10736981 -0.17570674]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 4166 is [True, False, False, False, False, True]
Current timestep = 4167. State = [[-0.16039939  0.13477863]]. Action = [[ 0.20150432 -0.2172406  -0.03314039 -0.8237611 ]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 4167 is [True, False, False, False, False, True]
Current timestep = 4168. State = [[-0.1597639  0.1342372]]. Action = [[-0.107309   -0.13386935 -0.00508028 -0.5021133 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 4168 is [True, False, False, False, False, True]
Scene graph at timestep 4168 is [True, False, False, False, False, True]
State prediction error at timestep 4168 is tensor(2.8469e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4169. State = [[-0.15938121  0.13287997]]. Action = [[-0.13038822  0.03918964  0.06780064  0.9201257 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 4169 is [True, False, False, False, False, True]
Current timestep = 4170. State = [[-0.15938106  0.1328495 ]]. Action = [[ 0.21243891  0.20835584 -0.07266223  0.2521596 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 4170 is [True, False, False, False, False, True]
Current timestep = 4171. State = [[-0.15952066  0.13384114]]. Action = [[-0.08648509  0.04838556 -0.12899081  0.71340644]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 4171 is [True, False, False, False, False, True]
Scene graph at timestep 4171 is [True, False, False, False, False, True]
State prediction error at timestep 4171 is tensor(1.0379e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4172. State = [[-0.15987049  0.1348191 ]]. Action = [[-0.15431443  0.02018732 -0.07863466 -0.02353859]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 4172 is [True, False, False, False, False, True]
Scene graph at timestep 4172 is [True, False, False, False, False, True]
State prediction error at timestep 4172 is tensor(1.0123e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4173. State = [[-0.16049382  0.13622114]]. Action = [[0.00998163 0.0203149  0.12307981 0.88477397]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 4173 is [True, False, False, False, False, True]
Scene graph at timestep 4173 is [True, False, False, False, False, True]
State prediction error at timestep 4173 is tensor(2.9776e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4174. State = [[-0.16104305  0.13727489]]. Action = [[-0.06465724  0.14958909 -0.11801353  0.23693728]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 4174 is [True, False, False, False, False, True]
Current timestep = 4175. State = [[-0.16226071  0.1398321 ]]. Action = [[-0.05693217  0.0932456  -0.0352491   0.9760474 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 4175 is [True, False, False, False, False, True]
Scene graph at timestep 4175 is [True, False, False, False, False, True]
State prediction error at timestep 4175 is tensor(5.9491e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4175 of -1
Current timestep = 4176. State = [[-0.16401072  0.14322764]]. Action = [[ 0.14226937 -0.22300835  0.10670769 -0.21828449]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 4176 is [True, False, False, False, False, True]
Scene graph at timestep 4176 is [True, False, False, False, False, True]
State prediction error at timestep 4176 is tensor(1.9434e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4177. State = [[-0.16385092  0.14290828]]. Action = [[ 0.19623426 -0.13725275 -0.12179518  0.6922529 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 4177 is [True, False, False, False, False, True]
Current timestep = 4178. State = [[-0.16280141  0.14119582]]. Action = [[ 0.19387263  0.11898914  0.15532076 -0.90978104]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 4178 is [True, False, False, False, False, True]
Current timestep = 4179. State = [[-0.16097742  0.14156345]]. Action = [[ 0.0651882  -0.02329575  0.0236384   0.8742137 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 4179 is [True, False, False, False, False, True]
Current timestep = 4180. State = [[-0.15845412  0.14171237]]. Action = [[-0.20151673  0.05957431  0.15592712  0.6100981 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 4180 is [True, False, False, False, False, True]
Current timestep = 4181. State = [[-0.1571766   0.14264607]]. Action = [[ 0.18215522  0.12800813 -0.1021741  -0.35662907]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 4181 is [True, False, False, False, False, True]
Current timestep = 4182. State = [[-0.15431625  0.14427651]]. Action = [[-0.1424104   0.18781197  0.16147709  0.1893127 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 4182 is [True, False, False, False, False, True]
Current timestep = 4183. State = [[-0.15338163  0.14732704]]. Action = [[ 0.23984039  0.10333681 -0.09846614  0.75651455]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 4183 is [True, False, False, False, False, True]
Scene graph at timestep 4183 is [True, False, False, False, False, True]
State prediction error at timestep 4183 is tensor(1.2488e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4184. State = [[-0.15070549  0.15012942]]. Action = [[-0.09362608 -0.17331494 -0.12950477 -0.4279709 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 4184 is [True, False, False, False, False, True]
Current timestep = 4185. State = [[-0.14880244  0.1506172 ]]. Action = [[ 0.18880501  0.03303328 -0.02042355 -0.92307097]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 4185 is [True, False, False, False, False, True]
Current timestep = 4186. State = [[-0.1462329   0.15167256]]. Action = [[-0.13626225  0.23103026 -0.17714359  0.39483798]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 4186 is [True, False, False, False, False, True]
Current timestep = 4187. State = [[-0.14621314  0.15463233]]. Action = [[-0.1506263   0.05028781  0.05442801 -0.8260725 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 4187 is [True, False, False, False, False, True]
Current timestep = 4188. State = [[-0.14797688  0.15801802]]. Action = [[-0.04125345  0.19482079 -0.02896112  0.22290003]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 4188 is [True, False, False, False, False, True]
Current timestep = 4189. State = [[-0.15006602  0.16186316]]. Action = [[0.07008436 0.05074084 0.15465653 0.67881393]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 4189 is [True, False, False, False, False, True]
Current timestep = 4190. State = [[-0.15171213  0.16498081]]. Action = [[-0.15072386  0.09612519  0.03792346  0.98257494]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 4190 is [True, False, False, False, False, True]
Current timestep = 4191. State = [[-0.1536292   0.16855237]]. Action = [[-0.01391572  0.21637619 -0.15158005 -0.22214937]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 4191 is [True, False, False, False, False, True]
Current timestep = 4192. State = [[-0.15626606  0.17326084]]. Action = [[-0.14407891  0.04382661 -0.09485821 -0.74947625]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 4192 is [True, False, False, False, False, True]
Current timestep = 4193. State = [[-0.15866892  0.17748046]]. Action = [[0.17409542 0.10561085 0.09604526 0.31677532]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 4193 is [True, False, False, False, False, True]
Human Feedback received at timestep 4193 of -1
Current timestep = 4194. State = [[-0.15951411  0.18056576]]. Action = [[-0.21151733  0.0927085   0.10346997 -0.38536805]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 4194 is [True, False, False, False, False, True]
Scene graph at timestep 4194 is [True, False, False, False, False, True]
State prediction error at timestep 4194 is tensor(5.4224e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4195. State = [[-0.16273744  0.18704985]]. Action = [[-2.1693677e-01  2.1879897e-01  4.6432018e-05  2.4776936e-01]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 4195 is [True, False, False, False, False, True]
Current timestep = 4196. State = [[-0.16616361  0.19233906]]. Action = [[-0.17201619  0.1885314   0.03953534 -0.86858   ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 4196 is [True, False, False, False, False, True]
Scene graph at timestep 4196 is [True, False, False, False, False, True]
State prediction error at timestep 4196 is tensor(1.0656e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4196 of -1
Current timestep = 4197. State = [[-0.17054182  0.19862834]]. Action = [[ 0.02948081  0.1028609  -0.13122042  0.26622272]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 4197 is [True, False, False, False, False, True]
Human Feedback received at timestep 4197 of -1
Current timestep = 4198. State = [[-0.17341945  0.20362468]]. Action = [[ 0.10545081  0.20144805 -0.19332395  0.40223396]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 4198 is [True, False, False, False, False, True]
Current timestep = 4199. State = [[-0.17424014  0.20834544]]. Action = [[ 0.10529125 -0.11277592  0.1676842   0.7418512 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 4199 is [True, False, False, False, False, True]
Current timestep = 4200. State = [[-0.17230396  0.2104712 ]]. Action = [[-0.23530522  0.21715444 -0.19849013  0.20694768]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 4200 is [True, False, False, False, False, True]
Current timestep = 4201. State = [[-0.17383212  0.21447013]]. Action = [[0.15647286 0.14431861 0.0050174  0.26411557]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 4201 is [True, False, False, False, False, True]
Human Feedback received at timestep 4201 of -1
Current timestep = 4202. State = [[-0.17408918  0.21881413]]. Action = [[-0.00564303  0.08710179 -0.21089764  0.8547156 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 4202 is [True, False, False, False, False, True]
Current timestep = 4203. State = [[-0.1740356   0.22275949]]. Action = [[-0.09289216  0.23001897 -0.12433122 -0.3744105 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 4203 is [True, False, False, False, False, True]
Scene graph at timestep 4203 is [True, False, False, False, False, True]
State prediction error at timestep 4203 is tensor(6.5212e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4204. State = [[-0.17422418  0.22925586]]. Action = [[-0.12088415  0.03062382  0.05964211  0.02952468]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 4204 is [True, False, False, False, False, True]
Human Feedback received at timestep 4204 of -1
Current timestep = 4205. State = [[-0.17608483  0.23425128]]. Action = [[-0.21497376  0.17140615 -0.15434727  0.905584  ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 4205 is [True, False, False, False, False, True]
Current timestep = 4206. State = [[-0.17983119  0.23928861]]. Action = [[-0.1589163   0.17815113  0.08261633  0.72215986]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 4206 is [True, False, False, False, False, True]
Current timestep = 4207. State = [[-0.18458214  0.24553923]]. Action = [[-0.05165441  0.18424642 -0.15211059 -0.30817997]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 4207 is [True, False, False, False, False, True]
Current timestep = 4208. State = [[-0.18871307  0.25108105]]. Action = [[0.10440645 0.14199013 0.18925658 0.9139099 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 4208 is [True, False, False, False, False, True]
Scene graph at timestep 4208 is [True, False, False, False, False, True]
State prediction error at timestep 4208 is tensor(1.4760e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4208 of -1
Current timestep = 4209. State = [[-0.19138955  0.2558896 ]]. Action = [[-0.00087199  0.23673955  0.1466353   0.3447969 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 4209 is [True, False, False, False, False, True]
Scene graph at timestep 4209 is [True, False, False, False, False, True]
State prediction error at timestep 4209 is tensor(3.0754e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4210. State = [[-0.1934976   0.26149735]]. Action = [[-0.19777782 -0.16501012  0.0156284  -0.14666224]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 4210 is [True, False, False, False, False, True]
Current timestep = 4211. State = [[-0.19525763  0.26412117]]. Action = [[-0.1619841  -0.02308424 -0.12027994  0.8580899 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 4211 is [True, False, False, False, False, True]
Current timestep = 4212. State = [[-0.19710404  0.2661449 ]]. Action = [[-0.20782354  0.01247445 -0.21698597  0.9573748 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 4212 is [True, False, False, False, False, True]
Current timestep = 4213. State = [[-0.19944748  0.2683997 ]]. Action = [[-0.21321343 -0.21602398 -0.13989201 -0.46430898]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 4213 is [True, False, False, False, False, True]
Current timestep = 4214. State = [[-0.20113657  0.2688272 ]]. Action = [[-0.16502011  0.21352258  0.21032214  0.24205375]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 4214 is [True, False, False, False, False, True]
Current timestep = 4215. State = [[-0.20494004  0.27236837]]. Action = [[-0.00553194  0.19436044 -0.16899116 -0.33928156]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 4215 is [True, False, False, False, False, True]
Scene graph at timestep 4215 is [True, False, False, False, False, True]
State prediction error at timestep 4215 is tensor(1.3782e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4216. State = [[-0.20886621  0.27631766]]. Action = [[-0.078417   -0.0077412  -0.17920454  0.7390053 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 4216 is [True, False, False, False, False, True]
Human Feedback received at timestep 4216 of -1
Current timestep = 4217. State = [[-0.21183705  0.2791219 ]]. Action = [[-0.09731323 -0.09499481  0.1950413  -0.6008524 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 4217 is [True, False, False, False, False, True]
Current timestep = 4218. State = [[-0.21345696  0.2801513 ]]. Action = [[-0.23855883 -0.01872388 -0.06045207  0.1398058 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 4218 is [True, False, False, False, False, True]
Current timestep = 4219. State = [[-0.21656206  0.28141102]]. Action = [[-0.17262486  0.04585674 -0.08599027 -0.73479456]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 4219 is [True, False, False, False, False, True]
Current timestep = 4220. State = [[-0.2212668   0.28326675]]. Action = [[-0.21589017  0.12979352 -0.09876491  0.24111342]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 4220 is [True, False, False, False, False, True]
Scene graph at timestep 4220 is [True, False, False, False, False, True]
State prediction error at timestep 4220 is tensor(2.4812e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4221. State = [[-0.22721793  0.28618303]]. Action = [[ 0.15032119 -0.08249524 -0.20920384  0.9122567 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 4221 is [True, False, False, False, False, True]
Current timestep = 4222. State = [[-0.22915594  0.28615317]]. Action = [[-0.20075516 -0.16474664  0.13274187 -0.02045661]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 4222 is [True, False, False, False, False, True]
Current timestep = 4223. State = [[-0.23113538  0.28510767]]. Action = [[-0.05598202  0.04196879  0.14507043  0.34961772]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 4223 is [True, False, False, False, False, True]
Current timestep = 4224. State = [[-0.23302855  0.28491178]]. Action = [[-0.23420382 -0.01996776  0.01439947  0.05924988]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 4224 is [True, False, False, False, False, True]
Human Feedback received at timestep 4224 of -1
Current timestep = 4225. State = [[-0.23654012  0.28533116]]. Action = [[ 0.0930014  -0.1556145  -0.14575095 -0.4217347 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 4225 is [True, False, False, False, False, True]
Scene graph at timestep 4225 is [True, False, False, False, False, True]
State prediction error at timestep 4225 is tensor(2.6224e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4226. State = [[-0.23786585  0.28275833]]. Action = [[-0.00821318 -0.00849128  0.1557852   0.373178  ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 4226 is [True, False, False, False, False, True]
Scene graph at timestep 4226 is [True, False, False, False, False, True]
State prediction error at timestep 4226 is tensor(6.4011e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4227. State = [[-0.23973025  0.2808843 ]]. Action = [[-0.17300734  0.1299426   0.00125617  0.40645158]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 4227 is [True, False, False, False, False, True]
Current timestep = 4228. State = [[-0.24306633  0.28159258]]. Action = [[-0.04938085 -0.17914921 -0.08901566 -0.91324735]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 4228 is [True, False, False, False, False, True]
Scene graph at timestep 4228 is [True, False, False, False, False, True]
State prediction error at timestep 4228 is tensor(8.9579e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4229. State = [[-0.24591546  0.27981204]]. Action = [[-0.12667729  0.08477324 -0.16981803 -0.94678086]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 4229 is [True, False, False, False, False, True]
Scene graph at timestep 4229 is [True, False, False, False, False, True]
State prediction error at timestep 4229 is tensor(9.1652e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4230. State = [[-0.2495867   0.27938667]]. Action = [[ 0.06393495  0.01049653 -0.0768756   0.44124627]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 4230 is [True, False, False, False, False, True]
Current timestep = 4231. State = [[-0.25051707  0.27928382]]. Action = [[ 0.02404264  0.20560065  0.00495088 -0.71031773]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 4231 is [True, False, False, False, False, True]
Scene graph at timestep 4231 is [True, False, False, False, False, True]
State prediction error at timestep 4231 is tensor(9.2518e-08, grad_fn=<MseLossBackward0>)
Current timestep = 4232. State = [[-0.251954    0.28087643]]. Action = [[ 1.0517150e-02  1.0442734e-04 -2.3811473e-01 -9.5046997e-02]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 4232 is [True, False, False, False, False, True]
Current timestep = 4233. State = [[-0.2529066  0.2819491]]. Action = [[-0.0347147  -0.03032711 -0.13799086 -0.9394563 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 4233 is [True, False, False, False, False, True]
Current timestep = 4234. State = [[-0.2534677   0.28256992]]. Action = [[-0.12286937 -0.12700978  0.22242308 -0.7290538 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 4234 is [True, False, False, False, False, True]
Scene graph at timestep 4234 is [True, False, False, False, False, True]
State prediction error at timestep 4234 is tensor(2.8545e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4235. State = [[-0.2541057  0.282338 ]]. Action = [[ 0.05775222  0.05356181 -0.16580679 -0.9450745 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 4235 is [True, False, False, False, False, True]
Human Feedback received at timestep 4235 of -1
Current timestep = 4236. State = [[-0.25430936  0.28237492]]. Action = [[-0.00482252 -0.2288831  -0.00026499 -0.2511055 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 4236 is [True, False, False, False, False, True]
Scene graph at timestep 4236 is [True, False, False, False, False, True]
State prediction error at timestep 4236 is tensor(3.6545e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4237. State = [[-0.25446913  0.28001294]]. Action = [[ 0.04651713  0.1420939   0.11024767 -0.53709376]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 4237 is [True, False, False, False, False, True]
Current timestep = 4238. State = [[-0.25466678  0.2799021 ]]. Action = [[-0.23938596  0.21689963 -0.15649727 -0.9431397 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 4238 is [True, False, False, False, False, True]
Scene graph at timestep 4238 is [True, False, False, False, False, True]
State prediction error at timestep 4238 is tensor(1.7743e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4239. State = [[-0.25812602  0.28292647]]. Action = [[-0.2394699  -0.03731938 -0.00500041  0.6395782 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 4239 is [True, False, False, False, False, True]
Current timestep = 4240. State = [[-0.26297772  0.285458  ]]. Action = [[-0.11078987  0.07319111 -0.23123309  0.7347822 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 4240 is [True, False, False, False, False, True]
Current timestep = 4241. State = [[-0.26760247  0.28750616]]. Action = [[0.0904339  0.1373874  0.172943   0.55770123]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 4241 is [True, False, False, False, False, True]
Current timestep = 4242. State = [[-0.2707241   0.28983647]]. Action = [[-0.1704015   0.06782117 -0.23550224  0.02322793]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 4242 is [True, False, False, False, False, True]
Current timestep = 4243. State = [[-0.27468628  0.29258272]]. Action = [[-0.111571   -0.09535503 -0.00385183 -0.6689913 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 4243 is [True, False, False, False, False, True]
Current timestep = 4244. State = [[-0.2778964   0.29326677]]. Action = [[-0.15144347 -0.1311366   0.04024836 -0.18493617]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 4244 is [True, False, False, False, False, True]
Current timestep = 4245. State = [[-0.2802436  0.2936835]]. Action = [[ 0.21002331 -0.00682549 -0.13698593 -0.9203805 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 4245 is [True, False, False, False, False, True]
Current timestep = 4246. State = [[-0.2798335   0.29348922]]. Action = [[ 0.07876292 -0.02975148 -0.22356114  0.82659876]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 4246 is [True, False, False, False, False, True]
Current timestep = 4247. State = [[-0.27942345  0.29305238]]. Action = [[ 0.19680375  0.20433736  0.03217548 -0.08846033]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 4247 is [True, False, False, False, False, True]
Human Feedback received at timestep 4247 of -1
Current timestep = 4248. State = [[-0.27915946  0.2938385 ]]. Action = [[-0.17385532  0.09819597 -0.21654396  0.91035986]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 4248 is [True, False, False, False, False, True]
Current timestep = 4249. State = [[-0.2790149   0.29436302]]. Action = [[ 0.07559744 -0.02036142  0.00185803 -0.8371882 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 4249 is [True, False, False, False, False, True]
Current timestep = 4250. State = [[-0.27854088  0.294702  ]]. Action = [[ 0.1579726   0.12881732 -0.21182421  0.5631373 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 4250 is [True, False, False, False, False, True]
Scene graph at timestep 4250 is [True, False, False, False, False, True]
State prediction error at timestep 4250 is tensor(3.8737e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4251. State = [[-0.27788258  0.29587612]]. Action = [[-0.2384634   0.16659856  0.15195596  0.8735627 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 4251 is [True, False, False, False, False, True]
Current timestep = 4252. State = [[-0.2769475   0.29665503]]. Action = [[ 0.20276892 -0.11736991 -0.10729685  0.70648456]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 4252 is [True, False, False, False, False, True]
Current timestep = 4253. State = [[-0.27423105  0.29536587]]. Action = [[ 0.0539276   0.06540525 -0.06047271 -0.9921524 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 4253 is [True, False, False, False, False, True]
Current timestep = 4254. State = [[-0.27228913  0.29488286]]. Action = [[ 0.1725294   0.01128501 -0.12410045  0.97384214]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 4254 is [True, False, False, False, False, True]
Current timestep = 4255. State = [[-0.27012134  0.2944163 ]]. Action = [[-0.0069844   0.02246195  0.00783801 -0.09834641]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 4255 is [True, False, False, False, False, True]
Current timestep = 4256. State = [[-0.26901585  0.29394612]]. Action = [[-0.22243093  0.1520243   0.08468732  0.4077232 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 4256 is [True, False, False, False, False, True]
Current timestep = 4257. State = [[-0.2681738  0.29347  ]]. Action = [[ 0.11717969 -0.00126977  0.14842084  0.37719548]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 4257 is [True, False, False, False, False, True]
Current timestep = 4258. State = [[-0.26662788  0.29311368]]. Action = [[ 0.14012593  0.10585707  0.11763594 -0.43171674]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 4258 is [True, False, False, False, False, True]
Current timestep = 4259. State = [[-0.26399437  0.29377046]]. Action = [[ 0.11746061 -0.16966157 -0.13629094  0.7991445 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 4259 is [True, False, False, False, False, True]
Current timestep = 4260. State = [[-0.26027817  0.29299676]]. Action = [[-0.21837223  0.10383463  0.09792948  0.8436558 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 4260 is [True, False, False, False, False, True]
Scene graph at timestep 4260 is [True, False, False, False, False, True]
State prediction error at timestep 4260 is tensor(4.7151e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4261. State = [[-0.25880626  0.2940503 ]]. Action = [[ 0.0037975  -0.04653123 -0.06986256  0.70887876]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 4261 is [True, False, False, False, False, True]
Scene graph at timestep 4261 is [True, False, False, False, False, True]
State prediction error at timestep 4261 is tensor(7.1085e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4261 of 1
Current timestep = 4262. State = [[-0.25777328  0.29448116]]. Action = [[ 0.058267    0.11102343 -0.18961081 -0.36089844]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 4262 is [True, False, False, False, False, True]
Current timestep = 4263. State = [[-0.25703984  0.29516456]]. Action = [[-0.00609571  0.11873275 -0.04533273  0.04122281]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 4263 is [True, False, False, False, False, True]
Current timestep = 4264. State = [[-0.25632143  0.29601148]]. Action = [[-0.2283632   0.23573214 -0.03172363 -0.45309222]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 4264 is [True, False, False, False, False, True]
Scene graph at timestep 4264 is [True, False, False, False, False, True]
State prediction error at timestep 4264 is tensor(8.5171e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4265. State = [[-0.25576517  0.29679832]]. Action = [[-0.24474302 -0.21544169  0.13244018 -0.92125165]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 4265 is [True, False, False, False, False, True]
Current timestep = 4266. State = [[-0.25601196  0.2968931 ]]. Action = [[-0.10494456  0.08464542 -0.00979629  0.33673644]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 4266 is [True, False, False, False, False, True]
Human Feedback received at timestep 4266 of -1
Current timestep = 4267. State = [[-0.2570672   0.29776368]]. Action = [[-0.09599432  0.05198446  0.07814872 -0.7669097 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 4267 is [True, False, False, False, False, True]
Scene graph at timestep 4267 is [True, False, False, False, False, True]
State prediction error at timestep 4267 is tensor(4.4745e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4268. State = [[-0.25871488  0.29928535]]. Action = [[-0.19626544  0.14158666  0.04109812 -0.92264074]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 4268 is [True, False, False, False, False, True]
Current timestep = 4269. State = [[-0.26187977  0.30229035]]. Action = [[-0.087284    0.03990176  0.06042743 -0.9225663 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 4269 is [True, False, False, False, False, True]
Current timestep = 4270. State = [[-0.26468828  0.30510595]]. Action = [[ 0.03332579  0.03738397 -0.06836033 -0.17679036]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 4270 is [True, False, False, False, False, True]
Current timestep = 4271. State = [[-0.2665868   0.30683333]]. Action = [[-0.05421928  0.20928612 -0.20203485  0.40862024]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 4271 is [True, False, False, False, False, True]
Current timestep = 4272. State = [[-0.26751423  0.30770633]]. Action = [[0.04687503 0.20878592 0.04080731 0.7021811 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 4272 is [True, False, False, False, False, True]
Current timestep = 4273. State = [[-0.26796025  0.3079919 ]]. Action = [[ 0.08440307  0.14749467  0.19953388 -0.19627923]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 4273 is [True, False, False, False, False, True]
Current timestep = 4274. State = [[-0.26815408  0.30808488]]. Action = [[ 0.04866847 -0.20062505 -0.00490129  0.6010885 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 4274 is [True, False, False, False, False, True]
Current timestep = 4275. State = [[-0.26784772  0.30762464]]. Action = [[-0.00624625  0.24485415  0.08571133 -0.9925399 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 4275 is [True, False, False, False, False, True]
Current timestep = 4276. State = [[-0.26764488  0.30725464]]. Action = [[ 0.119169    0.01129073 -0.00095454 -0.24071336]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 4276 is [True, False, False, False, False, True]
Current timestep = 4277. State = [[-0.26758787  0.30710667]]. Action = [[-0.16083348  0.10707739 -0.1231249   0.4358678 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 4277 is [True, False, False, False, False, True]
Scene graph at timestep 4277 is [True, False, False, False, False, True]
State prediction error at timestep 4277 is tensor(1.5119e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4278. State = [[-0.2674238  0.3067589]]. Action = [[ 0.05005562 -0.24550436 -0.09653299 -0.76194376]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 4278 is [True, False, False, False, False, True]
Scene graph at timestep 4278 is [True, False, False, False, False, True]
State prediction error at timestep 4278 is tensor(1.0233e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4279. State = [[-0.26603547  0.3037777 ]]. Action = [[-0.06465265  0.14061081 -0.14659768  0.64040506]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 4279 is [True, False, False, False, False, True]
Current timestep = 4280. State = [[-0.26506686  0.3017282 ]]. Action = [[-0.09855604  0.24716294 -0.04852182  0.81777334]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 4280 is [True, False, False, False, False, True]
Current timestep = 4281. State = [[-0.26447004  0.3002635 ]]. Action = [[-0.15228492 -0.05588688  0.06550896 -0.82406896]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 4281 is [True, False, False, False, False, True]
Current timestep = 4282. State = [[-0.2652866  0.2988329]]. Action = [[-0.17333898 -0.01270393  0.02427241  0.6994548 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 4282 is [True, False, False, False, False, True]
Current timestep = 4283. State = [[-0.26701874  0.29799703]]. Action = [[-0.21660644  0.23424914 -0.14830479  0.51812375]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 4283 is [True, False, False, False, False, True]
Current timestep = 4284. State = [[-0.2678309   0.29744697]]. Action = [[ 0.12002283 -0.07926266 -0.21035711 -0.463215  ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 4284 is [True, False, False, False, False, True]
Scene graph at timestep 4284 is [True, False, False, False, False, True]
State prediction error at timestep 4284 is tensor(2.4140e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4285. State = [[-0.26733452  0.2962832 ]]. Action = [[-0.22893883  0.01218387 -0.14845368 -0.51539975]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 4285 is [True, False, False, False, False, True]
Scene graph at timestep 4285 is [True, False, False, False, False, True]
State prediction error at timestep 4285 is tensor(8.2195e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4286. State = [[-0.2670763   0.29579347]]. Action = [[-0.04699235  0.19916904 -0.17872621 -0.71466863]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 4286 is [True, False, False, False, False, True]
Scene graph at timestep 4286 is [True, False, False, False, False, True]
State prediction error at timestep 4286 is tensor(3.8182e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4287. State = [[-0.26691076  0.29536504]]. Action = [[ 0.13927057 -0.16764405  0.01009533  0.89776254]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 4287 is [True, False, False, False, False, True]
Current timestep = 4288. State = [[-0.2656601   0.29293594]]. Action = [[-0.01551358 -0.21934342 -0.07354575  0.11187756]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 4288 is [True, False, False, False, False, True]
Current timestep = 4289. State = [[-0.26433766  0.28870633]]. Action = [[ 0.1934542  -0.18387558 -0.20607826  0.16202879]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 4289 is [True, False, False, False, False, True]
Current timestep = 4290. State = [[-0.2618038   0.28372633]]. Action = [[-0.23453301  0.13116604 -0.2054682   0.50791   ]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 4290 is [True, False, False, False, False, True]
Current timestep = 4291. State = [[-0.26233247  0.28210646]]. Action = [[ 0.05475777  0.21540338 -0.17254129 -0.5089043 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 4291 is [True, False, False, False, False, True]
Current timestep = 4292. State = [[-0.2630864   0.28302336]]. Action = [[ 0.03371364  0.01483029 -0.12703283 -0.70211864]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 4292 is [True, False, False, False, False, True]
Current timestep = 4293. State = [[-0.2630792  0.2831235]]. Action = [[ 0.08237252 -0.11545548  0.10824952 -0.84204775]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 4293 is [True, False, False, False, False, True]
Current timestep = 4294. State = [[-0.2625617  0.2825005]]. Action = [[-0.15414998  0.10772353 -0.1115641  -0.37737006]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 4294 is [True, False, False, False, False, True]
Current timestep = 4295. State = [[-0.26312348  0.28295147]]. Action = [[ 0.09932473  0.16570342 -0.13776906 -0.15042973]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 4295 is [True, False, False, False, False, True]
Human Feedback received at timestep 4295 of 1
Current timestep = 4296. State = [[-0.26368776  0.2834801 ]]. Action = [[-0.22267845 -0.17065753  0.16149226 -0.13254642]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 4296 is [True, False, False, False, False, True]
Current timestep = 4297. State = [[-0.26443785  0.28388664]]. Action = [[-0.2113476   0.2475861   0.12606305  0.81005025]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 4297 is [True, False, False, False, False, True]
Current timestep = 4298. State = [[-0.26780152  0.28711143]]. Action = [[ 0.09083793 -0.08418286  0.03360936  0.6309788 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 4298 is [True, False, False, False, False, True]
Scene graph at timestep 4298 is [True, False, False, False, False, True]
State prediction error at timestep 4298 is tensor(3.6824e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4299. State = [[-0.26930636  0.28847265]]. Action = [[ 0.1441617   0.01138961 -0.12462604 -0.86435443]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 4299 is [True, False, False, False, False, True]
Scene graph at timestep 4299 is [True, False, False, False, False, True]
State prediction error at timestep 4299 is tensor(3.2609e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4300. State = [[-0.2694566   0.28859872]]. Action = [[-0.06601611 -0.14921513 -0.02310148  0.3278427 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 4300 is [True, False, False, False, False, True]
Current timestep = 4301. State = [[-0.2693436  0.288338 ]]. Action = [[-0.23602797 -0.16249985 -0.1379704  -0.74205184]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 4301 is [True, False, False, False, False, True]
Current timestep = 4302. State = [[-0.2692822  0.2881359]]. Action = [[-0.20565383  0.17442119 -0.13585249 -0.5654102 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 4302 is [True, False, False, False, False, True]
Current timestep = 4303. State = [[-0.26925084  0.2880751 ]]. Action = [[-0.20707828 -0.21092114 -0.19514857 -0.7354887 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 4303 is [True, False, False, False, False, True]
Current timestep = 4304. State = [[-0.2692294  0.2879872]]. Action = [[-0.22899982 -0.12824541 -0.09970425  0.9550735 ]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 4304 is [True, False, False, False, False, True]
Scene graph at timestep 4304 is [True, False, False, False, False, True]
State prediction error at timestep 4304 is tensor(1.6368e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4305. State = [[-0.2692294  0.2879872]]. Action = [[ 0.05252737  0.11662376  0.07887059 -0.26395112]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 4305 is [True, False, False, False, False, True]
Scene graph at timestep 4305 is [True, False, False, False, False, True]
State prediction error at timestep 4305 is tensor(1.0905e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4306. State = [[-0.26923928  0.28799188]]. Action = [[ 0.23156315  0.12053484 -0.08982973 -0.70019066]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 4306 is [True, False, False, False, False, True]
Scene graph at timestep 4306 is [True, False, False, False, False, True]
State prediction error at timestep 4306 is tensor(5.9484e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4307. State = [[-0.2690888   0.28823808]]. Action = [[ 0.04427493 -0.20682952 -0.11461008  0.00622833]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 4307 is [True, False, False, False, False, True]
Scene graph at timestep 4307 is [True, False, False, False, False, True]
State prediction error at timestep 4307 is tensor(4.3585e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4308. State = [[-0.26823658  0.28724545]]. Action = [[-0.10364059  0.22133905 -0.09288558 -0.17931479]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 4308 is [True, False, False, False, False, True]
Current timestep = 4309. State = [[-0.26897737  0.2881114 ]]. Action = [[-0.17529532  0.21953946 -0.07839066 -0.90860707]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 4309 is [True, False, False, False, False, True]
Current timestep = 4310. State = [[-0.2713945  0.2909809]]. Action = [[ 0.15592319 -0.12154797 -0.00858176 -0.3800795 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 4310 is [True, False, False, False, False, True]
Scene graph at timestep 4310 is [True, False, False, False, False, True]
State prediction error at timestep 4310 is tensor(6.6094e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4311. State = [[-0.27156338  0.2915571 ]]. Action = [[-0.13182361  0.21694994  0.01416823 -0.90637606]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 4311 is [True, False, False, False, False, True]
Current timestep = 4312. State = [[-0.2732644   0.29386044]]. Action = [[-0.09976295  0.21297315  0.04086813 -0.46302825]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 4312 is [True, False, False, False, False, True]
Current timestep = 4313. State = [[-0.27627537  0.29726186]]. Action = [[-0.0824782   0.11502641 -0.00723264 -0.6672671 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 4313 is [True, False, False, False, False, True]
Scene graph at timestep 4313 is [True, False, False, False, False, True]
State prediction error at timestep 4313 is tensor(2.0912e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4314. State = [[-0.2798554  0.3012567]]. Action = [[-0.13891937  0.03030458 -0.05688906 -0.68775576]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 4314 is [True, False, False, False, False, True]
Current timestep = 4315. State = [[-0.28161454  0.30345625]]. Action = [[-0.17178947 -0.12621352 -0.04688048 -0.30389094]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 4315 is [True, False, False, False, False, True]
Scene graph at timestep 4315 is [True, False, False, False, False, True]
State prediction error at timestep 4315 is tensor(9.1594e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4316. State = [[-0.2827834  0.3048579]]. Action = [[-0.23804516  0.16014373 -0.0235467   0.62518907]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 4316 is [True, False, False, False, False, True]
Current timestep = 4317. State = [[-0.28348672  0.305568  ]]. Action = [[ 0.14801896 -0.01336588 -0.1209071  -0.9493881 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 4317 is [True, False, False, False, False, True]
Human Feedback received at timestep 4317 of -1
Current timestep = 4318. State = [[-0.28338888  0.3055956 ]]. Action = [[-0.13440096  0.06186408  0.00157142 -0.9713552 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 4318 is [True, False, False, False, False, True]
Current timestep = 4319. State = [[-0.28319395  0.30575174]]. Action = [[ 0.08095589 -0.12327455  0.04723129  0.1432612 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 4319 is [True, False, False, False, False, True]
Scene graph at timestep 4319 is [True, False, False, False, False, True]
State prediction error at timestep 4319 is tensor(3.7155e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4320. State = [[-0.28300908  0.30591694]]. Action = [[-0.22448835  0.17688116  0.03856295  0.9928882 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 4320 is [True, False, False, False, False, True]
Current timestep = 4321. State = [[-0.28311116  0.30590236]]. Action = [[-0.1235747  -0.07713348  0.09329772 -0.1151287 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 4321 is [True, False, False, False, False, True]
Current timestep = 4322. State = [[-0.28309798  0.30591425]]. Action = [[-0.06702399 -0.16469568  0.01069629 -0.9260625 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 4322 is [True, False, False, False, False, True]
Scene graph at timestep 4322 is [True, False, False, False, False, True]
State prediction error at timestep 4322 is tensor(3.6361e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4323. State = [[-0.2830186   0.30589023]]. Action = [[ 0.03082445  0.09504288  0.09595746 -0.80194914]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 4323 is [True, False, False, False, False, True]
Current timestep = 4324. State = [[-0.2829346  0.3059562]]. Action = [[-0.222659    0.15604132 -0.04887836  0.12211025]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 4324 is [True, False, False, False, False, True]
Scene graph at timestep 4324 is [True, False, False, False, False, True]
State prediction error at timestep 4324 is tensor(1.1418e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4325. State = [[-0.2828864  0.3059945]]. Action = [[ 0.10035777  0.02842173 -0.08278196  0.31780446]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 4325 is [True, False, False, False, False, True]
Current timestep = 4326. State = [[-0.28292027  0.30605465]]. Action = [[ 0.09428823  0.03044337 -0.09714821 -0.36910498]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 4326 is [True, False, False, False, False, True]
Current timestep = 4327. State = [[-0.28292027  0.30605465]]. Action = [[-0.18673272  0.11979631 -0.04909694 -0.7490772 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 4327 is [True, False, False, False, False, True]
Current timestep = 4328. State = [[-0.28292027  0.30605465]]. Action = [[ 0.13298425  0.18855435 -0.04275969 -0.03631103]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 4328 is [True, False, False, False, False, True]
Current timestep = 4329. State = [[-0.28292027  0.30605465]]. Action = [[-0.1455095  -0.0152564   0.00681201 -0.9900247 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 4329 is [True, False, False, False, False, True]
Current timestep = 4330. State = [[-0.28292027  0.30605465]]. Action = [[-0.19016828  0.03579667  0.071704    0.96866536]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 4330 is [True, False, False, False, False, True]
Current timestep = 4331. State = [[-0.28292027  0.30605465]]. Action = [[-0.07998943  0.20038539 -0.04727581 -0.50993   ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 4331 is [True, False, False, False, False, True]
Current timestep = 4332. State = [[-0.28292027  0.30605465]]. Action = [[-0.0986065  -0.2161673  -0.09790298 -0.9530539 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 4332 is [True, False, False, False, False, True]
Current timestep = 4333. State = [[-0.28292027  0.30605465]]. Action = [[ 0.23234355  0.13266417  0.08742744 -0.8936034 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 4333 is [True, False, False, False, False, True]
Current timestep = 4334. State = [[-0.28292027  0.30605465]]. Action = [[ 0.15781489  0.23951152 -0.10441703 -0.9300504 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 4334 is [True, False, False, False, False, True]
Current timestep = 4335. State = [[-0.28292027  0.30605465]]. Action = [[-0.22213984  0.15155903  0.16929609  0.832705  ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 4335 is [True, False, False, False, False, True]
Scene graph at timestep 4335 is [True, False, False, False, False, True]
State prediction error at timestep 4335 is tensor(1.6211e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4336. State = [[-0.2828469   0.30591092]]. Action = [[ 0.14212355 -0.19909824  0.03998816 -0.8520582 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 4336 is [True, False, False, False, False, True]
Current timestep = 4337. State = [[-0.28159806  0.30520982]]. Action = [[-0.14838968 -0.00625671  0.18680239  0.3763467 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 4337 is [True, False, False, False, False, True]
Current timestep = 4338. State = [[-0.28109008  0.30499405]]. Action = [[ 0.01324379  0.20641935 -0.04548752 -0.6110687 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 4338 is [True, False, False, False, False, True]
Current timestep = 4339. State = [[-0.28062433  0.30490303]]. Action = [[-0.22699651  0.21317548 -0.04364729 -0.6534861 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 4339 is [True, False, False, False, False, True]
Current timestep = 4340. State = [[-0.2803307   0.30484706]]. Action = [[-0.23100378  0.03885344 -0.06213129  0.79537106]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 4340 is [True, False, False, False, False, True]
Current timestep = 4341. State = [[-0.28015774  0.3047853 ]]. Action = [[-0.09758636  0.05127734 -0.05329207  0.35544252]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 4341 is [True, False, False, False, False, True]
Current timestep = 4342. State = [[-0.27978656  0.30465722]]. Action = [[ 0.14790457 -0.01033774  0.10167301 -0.90549123]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 4342 is [True, False, False, False, False, True]
Current timestep = 4343. State = [[-0.2787375   0.30434287]]. Action = [[-0.15889464 -0.15648903  0.09034654  0.08594596]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 4343 is [True, False, False, False, False, True]
Current timestep = 4344. State = [[-0.2781002   0.30426025]]. Action = [[ 0.10548973  0.24758878 -0.16000524  0.14523101]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 4344 is [True, False, False, False, False, True]
Current timestep = 4345. State = [[-0.2777199   0.30417883]]. Action = [[0.10519615 0.09735286 0.03224406 0.98683345]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 4345 is [True, False, False, False, False, True]
Current timestep = 4346. State = [[-0.27739438  0.30408207]]. Action = [[-0.03637999  0.21443349  0.18581992  0.63550854]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 4346 is [True, False, False, False, False, True]
Current timestep = 4347. State = [[-0.27726543  0.3038827 ]]. Action = [[-0.19531332 -0.03507537  0.00770023  0.52802634]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 4347 is [True, False, False, False, False, True]
Current timestep = 4348. State = [[-0.27727732  0.30378658]]. Action = [[-0.22239222 -0.08325547 -0.07530454  0.45650506]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 4348 is [True, False, False, False, False, True]
Current timestep = 4349. State = [[-0.2772439  0.3037262]]. Action = [[-0.24112536  0.2072101   0.07621333  0.993894  ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 4349 is [True, False, False, False, False, True]
Current timestep = 4350. State = [[-0.2772105   0.30366582]]. Action = [[-0.06910607  0.18835989  0.04190069 -0.97996885]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 4350 is [True, False, False, False, False, True]
Scene graph at timestep 4350 is [True, False, False, False, False, True]
State prediction error at timestep 4350 is tensor(3.5221e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4351. State = [[-0.2772105   0.30366582]]. Action = [[-6.0942769e-04  1.8906188e-01 -2.0212382e-02  9.1337156e-01]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 4351 is [True, False, False, False, False, True]
Current timestep = 4352. State = [[-0.2771771   0.30360544]]. Action = [[-0.18451318  0.17932084 -0.09461367  0.9678104 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 4352 is [True, False, False, False, False, True]
Current timestep = 4353. State = [[-0.27701393  0.3034132 ]]. Action = [[ 0.20899594 -0.15099338 -0.02908811 -0.33942026]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 4353 is [True, False, False, False, False, True]
Current timestep = 4354. State = [[-0.27476266  0.3012634 ]]. Action = [[-0.2278304   0.1450063  -0.12170589 -0.20971394]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 4354 is [True, False, False, False, False, True]
Current timestep = 4355. State = [[-0.27362633  0.30003786]]. Action = [[-0.20728984  0.15700856 -0.1026957   0.39593792]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 4355 is [True, False, False, False, False, True]
Scene graph at timestep 4355 is [True, False, False, False, False, True]
State prediction error at timestep 4355 is tensor(8.6412e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4356. State = [[-0.27235663  0.29974768]]. Action = [[ 0.05567381 -0.01852429  0.20585448  0.9635303 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 4356 is [True, False, False, False, False, True]
Current timestep = 4357. State = [[-0.27082995  0.2994303 ]]. Action = [[-0.17916821  0.19555652 -0.16772272 -0.51234204]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 4357 is [True, False, False, False, False, True]
Current timestep = 4358. State = [[-0.27006733  0.2988929 ]]. Action = [[ 0.08717781  0.21329534 -0.0075638   0.13257611]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 4358 is [True, False, False, False, False, True]
Current timestep = 4359. State = [[-0.26982674  0.29856375]]. Action = [[ 0.1190019  -0.10917798  0.02268609 -0.27435672]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 4359 is [True, False, False, False, False, True]
Scene graph at timestep 4359 is [True, False, False, False, False, True]
State prediction error at timestep 4359 is tensor(6.4478e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4360. State = [[-0.26793265  0.2965392 ]]. Action = [[-0.07627127 -0.05724072  0.09531015  0.49551964]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 4360 is [True, False, False, False, False, True]
Current timestep = 4361. State = [[-0.26695263  0.29482886]]. Action = [[-0.03541297 -0.11261429 -0.03921349  0.34816158]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 4361 is [True, False, False, False, False, True]
Current timestep = 4362. State = [[-0.26595265  0.29279366]]. Action = [[-0.19259372  0.16636014  0.10264707 -0.3010311 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 4362 is [True, False, False, False, False, True]
Scene graph at timestep 4362 is [True, False, False, False, False, True]
State prediction error at timestep 4362 is tensor(1.3463e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4363. State = [[-0.26674476  0.29354122]]. Action = [[ 0.08936942  0.18185511 -0.10043046 -0.74914634]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 4363 is [True, False, False, False, False, True]
Scene graph at timestep 4363 is [True, False, False, False, False, True]
State prediction error at timestep 4363 is tensor(1.0961e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4364. State = [[-0.2673692  0.2941319]]. Action = [[-0.11117502 -0.10019287 -0.00419782  0.8412707 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 4364 is [True, False, False, False, False, True]
Current timestep = 4365. State = [[-0.26762998  0.29449996]]. Action = [[-0.04549406  0.11889756  0.06463847  0.36827576]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 4365 is [True, False, False, False, False, True]
Current timestep = 4366. State = [[-0.26826212  0.29524055]]. Action = [[ 0.1256265   0.16988164  0.01109156 -0.17375505]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 4366 is [True, False, False, False, False, True]
Current timestep = 4367. State = [[-0.2687318   0.29685357]]. Action = [[-0.13002087 -0.01531206 -0.13185188  0.5802293 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 4367 is [True, False, False, False, False, True]
Current timestep = 4368. State = [[-0.26940975  0.2980502 ]]. Action = [[-0.1910106  -0.09321402  0.05603653 -0.5140281 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 4368 is [True, False, False, False, False, True]
Scene graph at timestep 4368 is [True, False, False, False, False, True]
State prediction error at timestep 4368 is tensor(1.4981e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4369. State = [[-0.270666    0.29912266]]. Action = [[ 0.02447066  0.02273211  0.05293497 -0.91072184]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 4369 is [True, False, False, False, False, True]
Current timestep = 4370. State = [[-0.27143157  0.29983157]]. Action = [[ 0.16285953  0.14302367 -0.13802254  0.27124417]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 4370 is [True, False, False, False, False, True]
Current timestep = 4371. State = [[-0.27177045  0.30004704]]. Action = [[ 0.13734567 -0.19717959  0.19458574 -0.9653421 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 4371 is [True, False, False, False, False, True]
Current timestep = 4372. State = [[-0.27123022  0.29929367]]. Action = [[-0.20943233  0.04255396 -0.05986334 -0.846719  ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 4372 is [True, False, False, False, False, True]
Current timestep = 4373. State = [[-0.27100575  0.2989366 ]]. Action = [[-0.01083224  0.06463358 -0.12879375 -0.8083589 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 4373 is [True, False, False, False, False, True]
Current timestep = 4374. State = [[-0.27105147  0.2989849 ]]. Action = [[0.10806414 0.206433   0.0837366  0.7946278 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 4374 is [True, False, False, False, False, True]
Scene graph at timestep 4374 is [True, False, False, False, False, True]
State prediction error at timestep 4374 is tensor(1.5548e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4375. State = [[-0.2709878   0.29889607]]. Action = [[ 0.15411824 -0.07011068 -0.00653873 -0.6710125 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 4375 is [True, False, False, False, False, True]
Current timestep = 4376. State = [[-0.2703183  0.297983 ]]. Action = [[-0.21352145  0.17907354 -0.01067418  0.9154793 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 4376 is [True, False, False, False, False, True]
Current timestep = 4377. State = [[-0.26998457  0.29748437]]. Action = [[-0.01576036  0.23373872 -0.04283825  0.9104984 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 4377 is [True, False, False, False, False, True]
Current timestep = 4378. State = [[-0.26987308  0.29731765]]. Action = [[ 0.09794298  0.18898612 -0.11658922 -0.83427185]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 4378 is [True, False, False, False, False, True]
Current timestep = 4379. State = [[-0.2698797  0.2973117]]. Action = [[ 0.04799622  0.14330548 -0.07252054  0.07222867]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 4379 is [True, False, False, False, False, True]
Current timestep = 4380. State = [[-0.26983348  0.29742515]]. Action = [[0.16854548 0.21632588 0.08213359 0.6779382 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 4380 is [True, False, False, False, False, True]
Current timestep = 4381. State = [[-0.26961598  0.29752815]]. Action = [[-0.00227727  0.20643845  0.04663473  0.42882383]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 4381 is [True, False, False, False, False, True]
Current timestep = 4382. State = [[-0.26942962  0.29767153]]. Action = [[-0.24503218  0.07121757 -0.19735476 -0.8862816 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 4382 is [True, False, False, False, False, True]
Current timestep = 4383. State = [[-0.26930362  0.2978223 ]]. Action = [[ 0.10867119 -0.12932877 -0.23990352 -0.33841437]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 4383 is [True, False, False, False, False, True]
Current timestep = 4384. State = [[-0.26854303  0.29727295]]. Action = [[-0.0836809   0.208875    0.10674125 -0.97858983]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 4384 is [True, False, False, False, False, True]
Scene graph at timestep 4384 is [True, False, False, False, False, True]
State prediction error at timestep 4384 is tensor(7.4296e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4385. State = [[-0.26811212  0.2969578 ]]. Action = [[ 0.03545442 -0.12840866  0.0440293  -0.99156564]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 4385 is [True, False, False, False, False, True]
Current timestep = 4386. State = [[-0.26677114  0.29573455]]. Action = [[ 0.03985032 -0.24913238  0.090877   -0.7402961 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 4386 is [True, False, False, False, False, True]
Current timestep = 4387. State = [[-0.26428562  0.29257312]]. Action = [[-0.23509376  0.22837275 -0.05136897 -0.47461724]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 4387 is [True, False, False, False, False, True]
Current timestep = 4388. State = [[-0.2629979   0.29087773]]. Action = [[ 0.11989158  0.08245343 -0.11665441 -0.82326424]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 4388 is [True, False, False, False, False, True]
Current timestep = 4389. State = [[-0.2621603  0.2905034]]. Action = [[ 0.04837802  0.23813158 -0.0846689  -0.9678271 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 4389 is [True, False, False, False, False, True]
Current timestep = 4390. State = [[-0.26195234  0.29124606]]. Action = [[0.11664024 0.19015777 0.13492972 0.82223535]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 4390 is [True, False, False, False, False, True]
Scene graph at timestep 4390 is [True, False, False, False, False, True]
State prediction error at timestep 4390 is tensor(6.3071e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4391. State = [[-0.2599266   0.29340577]]. Action = [[-0.04777956 -0.01436931  0.13414413  0.44758606]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 4391 is [True, False, False, False, False, True]
Scene graph at timestep 4391 is [True, False, False, False, False, True]
State prediction error at timestep 4391 is tensor(6.7395e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4392. State = [[-0.25801685  0.29522833]]. Action = [[-0.24357082 -0.08967999 -0.09939352 -0.41406548]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 4392 is [True, False, False, False, False, True]
Scene graph at timestep 4392 is [True, False, False, False, False, True]
State prediction error at timestep 4392 is tensor(1.1611e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4393. State = [[-0.25809467  0.29564536]]. Action = [[-0.20746267  0.24110723  0.02109301  0.71681654]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 4393 is [True, False, False, False, False, True]
Current timestep = 4394. State = [[-0.25829354  0.29586375]]. Action = [[ 0.2052195  -0.14260794 -0.02897845  0.27184296]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 4394 is [True, False, False, False, False, True]
Scene graph at timestep 4394 is [True, False, False, False, False, True]
State prediction error at timestep 4394 is tensor(8.2129e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4395. State = [[-0.2571654  0.2952832]]. Action = [[ 0.11447346 -0.03054468  0.01298776  0.09038711]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 4395 is [True, False, False, False, False, True]
Scene graph at timestep 4395 is [True, False, False, False, False, True]
State prediction error at timestep 4395 is tensor(3.9619e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4396. State = [[-0.2559252   0.29435763]]. Action = [[-0.20821585 -0.06049544 -0.04062265  0.90149605]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 4396 is [True, False, False, False, False, True]
Current timestep = 4397. State = [[-0.2557665  0.2941366]]. Action = [[ 0.0795444   0.06265444 -0.03086829 -0.38782167]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 4397 is [True, False, False, False, False, True]
Current timestep = 4398. State = [[-0.25562888  0.2940709 ]]. Action = [[ 0.2192027  -0.02435414 -0.1526873  -0.17752755]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 4398 is [True, False, False, False, False, True]
Current timestep = 4399. State = [[-0.25420606  0.29322666]]. Action = [[-0.14214379  0.18459594  0.01613486  0.66753876]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 4399 is [True, False, False, False, False, True]
Scene graph at timestep 4399 is [True, False, False, False, False, True]
State prediction error at timestep 4399 is tensor(5.9021e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4400. State = [[-0.2538219   0.29424655]]. Action = [[-0.04616645  0.184223   -0.07846823 -0.8051238 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 4400 is [True, False, False, False, False, True]
Scene graph at timestep 4400 is [True, False, False, False, False, True]
State prediction error at timestep 4400 is tensor(3.2739e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4401. State = [[-0.25399995  0.29649845]]. Action = [[-0.04166789 -0.20925723 -0.18167809 -0.94170177]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 4401 is [True, False, False, False, False, True]
Current timestep = 4402. State = [[-0.2539472   0.29641828]]. Action = [[ 0.14751354 -0.14316018 -0.18528758 -0.0856126 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 4402 is [True, False, False, False, False, True]
Current timestep = 4403. State = [[-0.25248614  0.29476824]]. Action = [[ 0.05258188 -0.12804304 -0.01720142 -0.7632494 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 4403 is [True, False, False, False, False, True]
Current timestep = 4404. State = [[-0.25092036  0.29281923]]. Action = [[-0.0445815  -0.11963598  0.16347381  0.9688047 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 4404 is [True, False, False, False, False, True]
Current timestep = 4405. State = [[-0.2495813   0.29079205]]. Action = [[-0.1697403   0.0256615   0.00629106  0.7295166 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 4405 is [True, False, False, False, False, True]
Current timestep = 4406. State = [[-0.24937432  0.29017457]]. Action = [[-0.08669114 -0.09343655 -0.14007504 -0.9671174 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 4406 is [True, False, False, False, False, True]
Current timestep = 4407. State = [[-0.24914634  0.2893563 ]]. Action = [[-0.08285832  0.21605092 -0.02908666 -0.29552102]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 4407 is [True, False, False, False, False, True]
Current timestep = 4408. State = [[-0.25022554  0.29043737]]. Action = [[-0.03855675 -0.0134809  -0.09272881 -0.7584976 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 4408 is [True, False, False, False, False, True]
Current timestep = 4409. State = [[-0.25077772  0.2909296 ]]. Action = [[-0.17281155 -0.21844634  0.13097465  0.32122588]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 4409 is [True, False, False, False, False, True]
Current timestep = 4410. State = [[-0.2512829  0.2897139]]. Action = [[-0.19620556  0.05983075  0.06361371  0.05557573]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 4410 is [True, False, False, False, False, True]
Current timestep = 4411. State = [[-0.25327426  0.29001677]]. Action = [[ 0.05936548  0.14001453 -0.01618759  0.7958555 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 4411 is [True, False, False, False, False, True]
Scene graph at timestep 4411 is [True, False, False, False, False, True]
State prediction error at timestep 4411 is tensor(3.7007e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4412. State = [[-0.25460228  0.29120308]]. Action = [[ 0.0560731  -0.02580513 -0.00626394 -0.40190065]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 4412 is [True, False, False, False, False, True]
Current timestep = 4413. State = [[-0.25471485  0.29120907]]. Action = [[-0.1262004  -0.20774977 -0.10340439 -0.6933603 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 4413 is [True, False, False, False, False, True]
Scene graph at timestep 4413 is [True, False, False, False, False, True]
State prediction error at timestep 4413 is tensor(2.6879e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4414. State = [[-0.25499246  0.28978828]]. Action = [[ 0.0989747  -0.19599377 -0.13314863 -0.97352767]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 4414 is [True, False, False, False, False, True]
Current timestep = 4415. State = [[-0.25419518  0.28719652]]. Action = [[-0.22683732  0.1912868  -0.08878848  0.6799433 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 4415 is [True, False, False, False, False, True]
Scene graph at timestep 4415 is [True, False, False, False, False, True]
State prediction error at timestep 4415 is tensor(4.6966e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4416. State = [[-0.25643685  0.28770706]]. Action = [[-0.04567577 -0.18105373 -0.0529322  -0.82242143]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 4416 is [True, False, False, False, False, True]
Current timestep = 4417. State = [[-0.22343104  0.03576296]]. Action = [[ 0.06410661  0.06773001 -0.07594389  0.782207  ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 4417 is [True, False, False, False, False, True]
Human Feedback received at timestep 4417 of 1
Current timestep = 4418. State = [[-0.21667849  0.038916  ]]. Action = [[ 0.13825965  0.1838915  -0.05017628  0.9709792 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 4418 is [True, False, False, False, True, False]
Current timestep = 4419. State = [[-0.21591985  0.04085159]]. Action = [[0.02238134 0.1175946  0.21262735 0.458416  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 4419 is [True, False, False, False, True, False]
Scene graph at timestep 4419 is [True, False, False, False, True, False]
State prediction error at timestep 4419 is tensor(6.5750e-08, grad_fn=<MseLossBackward0>)
Current timestep = 4420. State = [[-0.21559854  0.04390825]]. Action = [[ 0.14675748  0.20628923  0.02352995 -0.894042  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 4420 is [True, False, False, False, True, False]
Current timestep = 4421. State = [[-0.21456379  0.0487482 ]]. Action = [[ 0.21091616 -0.22478142 -0.04588613  0.6569302 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 4421 is [True, False, False, False, True, False]
Scene graph at timestep 4421 is [True, False, False, False, True, False]
State prediction error at timestep 4421 is tensor(1.5083e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4422. State = [[-0.21087308  0.04939829]]. Action = [[-0.1800928  -0.16614726  0.15240431  0.7618818 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4422 is [True, False, False, False, True, False]
Current timestep = 4423. State = [[-0.20912041  0.04907122]]. Action = [[0.03617013 0.10522419 0.23792636 0.64819884]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 4423 is [True, False, False, False, True, False]
Current timestep = 4424. State = [[-0.2072803   0.04936459]]. Action = [[-0.10314986 -0.06016646  0.07690945 -0.1588664 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 4424 is [True, False, False, False, True, False]
Current timestep = 4425. State = [[-0.20667726  0.04946896]]. Action = [[-0.07676435 -0.10004583  0.1669274  -0.53829104]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 4425 is [True, False, False, False, True, False]
Scene graph at timestep 4425 is [True, False, False, False, True, False]
State prediction error at timestep 4425 is tensor(3.1779e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4426. State = [[-0.20675328  0.04870504]]. Action = [[-0.02435577  0.0497151  -0.01951975 -0.82777536]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 4426 is [True, False, False, False, True, False]
Current timestep = 4427. State = [[-0.20681396  0.0487976 ]]. Action = [[-0.12104046  0.21129167  0.0147247  -0.31350845]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 4427 is [True, False, False, False, True, False]
Current timestep = 4428. State = [[-0.20769876  0.0510049 ]]. Action = [[0.04658544 0.18017215 0.14630866 0.72767496]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 4428 is [True, False, False, False, True, False]
Current timestep = 4429. State = [[-0.20900227  0.05445313]]. Action = [[-0.07734068 -0.03571832  0.09010392  0.7469667 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 4429 is [True, False, False, False, True, False]
Current timestep = 4430. State = [[-0.20982577  0.05623106]]. Action = [[-0.15931502 -0.13906284 -0.11344343  0.06392574]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 4430 is [True, False, False, False, True, False]
Current timestep = 4431. State = [[-0.21005249  0.05595066]]. Action = [[ 0.10176504 -0.22810468 -0.13249138 -0.15581661]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 4431 is [True, False, False, False, True, False]
Current timestep = 4432. State = [[-0.21011908  0.05365934]]. Action = [[-0.1869527  -0.005301    0.03001675 -0.04737341]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 4432 is [True, False, False, False, True, False]
Current timestep = 4433. State = [[-0.21054792  0.05196658]]. Action = [[0.19530898 0.10904565 0.00849339 0.5634372 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 4433 is [True, False, False, False, True, False]
Current timestep = 4434. State = [[-0.21061596  0.05186789]]. Action = [[-3.2132864e-04 -1.3466606e-01  1.6818899e-01 -5.8070207e-01]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 4434 is [True, False, False, False, True, False]
Current timestep = 4435. State = [[-0.21058013  0.05076614]]. Action = [[ 0.01599923 -0.0571584  -0.105427    0.62315965]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 4435 is [True, False, False, False, True, False]
Scene graph at timestep 4435 is [True, False, False, False, True, False]
State prediction error at timestep 4435 is tensor(1.1102e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4436. State = [[-0.21056591  0.04959158]]. Action = [[-4.4784933e-02 -2.2013485e-04  5.8660209e-03  7.0231366e-01]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 4436 is [True, False, False, False, True, False]
Current timestep = 4437. State = [[-0.21063475  0.04895813]]. Action = [[-0.24245693  0.09096336  0.23572141 -0.97704697]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 4437 is [True, False, False, False, True, False]
Current timestep = 4438. State = [[-0.21153647  0.04917657]]. Action = [[0.16988677 0.09023097 0.18779331 0.1481111 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 4438 is [True, False, False, False, True, False]
Current timestep = 4439. State = [[-0.2115364   0.04964446]]. Action = [[ 0.24392605  0.20572433  0.01691321 -0.83619255]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 4439 is [True, False, False, False, True, False]
Current timestep = 4440. State = [[-0.21145642  0.05142017]]. Action = [[-0.14843975  0.03239399  0.21143425 -0.6071883 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 4440 is [True, False, False, False, True, False]
Scene graph at timestep 4440 is [True, False, False, False, True, False]
State prediction error at timestep 4440 is tensor(1.7694e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4441. State = [[-0.2122133   0.05347523]]. Action = [[-0.03637737 -0.08681774 -0.21279314 -0.31710994]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 4441 is [True, False, False, False, True, False]
Current timestep = 4442. State = [[-0.21235707  0.05414306]]. Action = [[ 0.19698346  0.17837524 -0.23440522 -0.9523267 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 4442 is [True, False, False, False, True, False]
Current timestep = 4443. State = [[-0.21219113  0.05563236]]. Action = [[ 0.19033498 -0.14778668 -0.13961744  0.8646722 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 4443 is [True, False, False, False, True, False]
Scene graph at timestep 4443 is [True, False, False, False, True, False]
State prediction error at timestep 4443 is tensor(1.0985e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4444. State = [[-0.2106304   0.05579551]]. Action = [[-0.20328048  0.03358102  0.05732504 -0.25452733]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 4444 is [True, False, False, False, True, False]
Scene graph at timestep 4444 is [True, False, False, False, True, False]
State prediction error at timestep 4444 is tensor(1.4272e-08, grad_fn=<MseLossBackward0>)
Current timestep = 4445. State = [[-0.21054448  0.05597223]]. Action = [[ 0.01066497 -0.03925766  0.14628014  0.70277023]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 4445 is [True, False, False, False, True, False]
Current timestep = 4446. State = [[-0.21047315  0.05610336]]. Action = [[ 0.10332838  0.05006856  0.06751975 -0.9338401 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 4446 is [True, False, False, False, True, False]
Current timestep = 4447. State = [[-0.21017073  0.05615614]]. Action = [[-0.10401228 -0.23083442 -0.13547878  0.8099685 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 4447 is [True, False, False, False, True, False]
Scene graph at timestep 4447 is [True, False, False, False, True, False]
State prediction error at timestep 4447 is tensor(5.1134e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4448. State = [[-0.21018687  0.05498585]]. Action = [[-0.0202724  -0.00122395  0.07457113  0.8916018 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 4448 is [True, False, False, False, True, False]
Current timestep = 4449. State = [[-0.21022472  0.05446921]]. Action = [[-0.1867587  -0.00580661 -0.14314961 -0.39408344]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 4449 is [True, False, False, False, True, False]
Scene graph at timestep 4449 is [True, False, False, False, True, False]
State prediction error at timestep 4449 is tensor(9.9645e-08, grad_fn=<MseLossBackward0>)
Current timestep = 4450. State = [[-0.21023229  0.05433559]]. Action = [[0.01885104 0.12722403 0.10904241 0.907691  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 4450 is [True, False, False, False, True, False]
Current timestep = 4451. State = [[-0.2104753   0.05480687]]. Action = [[-0.22638348  0.18498862 -0.20255871  0.9403144 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 4451 is [True, False, False, False, True, False]
Scene graph at timestep 4451 is [True, False, False, False, True, False]
State prediction error at timestep 4451 is tensor(1.0406e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4452. State = [[-0.21194658  0.05756016]]. Action = [[ 0.02762553 -0.10133165 -0.16862115  0.9376588 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 4452 is [True, False, False, False, True, False]
Scene graph at timestep 4452 is [True, False, False, False, True, False]
State prediction error at timestep 4452 is tensor(3.1670e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4453. State = [[-0.21252446  0.05799811]]. Action = [[-0.07249857 -0.19135699  0.0277465  -0.4915495 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 4453 is [True, False, False, False, True, False]
Current timestep = 4454. State = [[-0.21301001  0.05673822]]. Action = [[ 0.1553286   0.14781684 -0.08677626 -0.15482712]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 4454 is [True, False, False, False, True, False]
Current timestep = 4455. State = [[-0.21305172  0.05684696]]. Action = [[ 0.186952   -0.22416215 -0.08420232  0.56073344]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 4455 is [True, False, False, False, True, False]
Current timestep = 4456. State = [[-0.21280862  0.05518609]]. Action = [[-0.06457448  0.2290048   0.10185552  0.50817585]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 4456 is [True, False, False, False, True, False]
Current timestep = 4457. State = [[-0.21302253  0.05581872]]. Action = [[-0.10926318  0.03387299  0.2065076   0.46148646]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 4457 is [True, False, False, False, True, False]
Scene graph at timestep 4457 is [True, False, False, False, True, False]
State prediction error at timestep 4457 is tensor(4.9797e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4458. State = [[-0.21345179  0.0568698 ]]. Action = [[-0.22374694 -0.029043    0.17481813  0.11110699]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 4458 is [True, False, False, False, True, False]
Scene graph at timestep 4458 is [True, False, False, False, True, False]
State prediction error at timestep 4458 is tensor(6.7770e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4459. State = [[-0.21410362  0.05759028]]. Action = [[ 0.07480505  0.13028666 -0.21453454  0.8955178 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 4459 is [True, False, False, False, True, False]
Scene graph at timestep 4459 is [True, False, False, False, True, False]
State prediction error at timestep 4459 is tensor(6.5008e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4460. State = [[-0.21460378  0.05883749]]. Action = [[ 0.10174069 -0.06584805  0.07974786 -0.09859806]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 4460 is [True, False, False, False, True, False]
Current timestep = 4461. State = [[-0.21463327  0.05882561]]. Action = [[ 0.09056473 -0.19544514  0.10181642 -0.7957089 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 4461 is [True, False, False, False, True, False]
Current timestep = 4462. State = [[-0.2145198  0.0576331]]. Action = [[-0.21360517  0.03339887 -0.03993112  0.3165903 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 4462 is [True, False, False, False, True, False]
Current timestep = 4463. State = [[-0.2145517  0.0572944]]. Action = [[ 0.12695837  0.08951187 -0.00724404  0.19945383]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 4463 is [True, False, False, False, True, False]
Current timestep = 4464. State = [[-0.21459602  0.05745748]]. Action = [[-9.4000846e-02 -7.8735024e-02  7.2434545e-04 -8.8496464e-01]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 4464 is [True, False, False, False, True, False]
Scene graph at timestep 4464 is [True, False, False, False, True, False]
State prediction error at timestep 4464 is tensor(1.9887e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4465. State = [[-0.21465467  0.05712802]]. Action = [[-0.18120494 -0.18903244  0.0635736  -0.7567408 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 4465 is [True, False, False, False, True, False]
Current timestep = 4466. State = [[-0.21619059  0.05459573]]. Action = [[-0.1427729  -0.06156453 -0.08467011 -0.947955  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 4466 is [True, False, False, False, True, False]
Current timestep = 4467. State = [[-0.21794823  0.05231972]]. Action = [[-0.12232858  0.19380617  0.01502129 -0.08425242]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 4467 is [True, False, False, False, True, False]
Current timestep = 4468. State = [[-0.22082339  0.05321321]]. Action = [[-0.14137049  0.16407183 -0.23398466 -0.5261866 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 4468 is [True, False, False, False, True, False]
Scene graph at timestep 4468 is [True, False, False, False, True, False]
State prediction error at timestep 4468 is tensor(7.7165e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4469. State = [[-0.223996    0.05545691]]. Action = [[-0.1887569  -0.23406173  0.1484302  -0.14345837]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 4469 is [True, False, False, False, True, False]
Scene graph at timestep 4469 is [True, False, False, False, True, False]
State prediction error at timestep 4469 is tensor(7.1843e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4470. State = [[-0.22754088  0.05414798]]. Action = [[ 0.09423074  0.04599613  0.07832918 -0.9101607 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 4470 is [True, False, False, False, True, False]
Current timestep = 4471. State = [[-0.22919194  0.05391578]]. Action = [[-0.18916975  0.0126445   0.14059812 -0.844749  ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 4471 is [True, False, False, False, True, False]
Human Feedback received at timestep 4471 of -1
Current timestep = 4472. State = [[-0.23275489  0.05408695]]. Action = [[-0.02840698  0.21994126 -0.08624969  0.71097434]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 4472 is [True, False, False, False, True, False]
Current timestep = 4473. State = [[-0.23586711  0.05666402]]. Action = [[ 0.02200389 -0.14304039  0.1287677   0.24132061]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 4473 is [True, False, False, False, True, False]
Current timestep = 4474. State = [[-0.23731369  0.05647554]]. Action = [[ 0.08159983 -0.16891083  0.18814352  0.71562576]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 4474 is [True, False, False, False, True, False]
Current timestep = 4475. State = [[-0.23756087  0.0547462 ]]. Action = [[ 0.07460266 -0.1100086   0.02102774 -0.27476346]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 4475 is [True, False, False, False, True, False]
Scene graph at timestep 4475 is [True, False, False, False, True, False]
State prediction error at timestep 4475 is tensor(1.8724e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4476. State = [[-0.23745164  0.05160487]]. Action = [[ 0.23499623 -0.14668514  0.24289474 -0.28120077]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 4476 is [True, False, False, False, True, False]
Current timestep = 4477. State = [[-0.23720102  0.04763729]]. Action = [[ 0.18184614  0.01500812 -0.08769253  0.87322044]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 4477 is [True, False, False, False, True, False]
Current timestep = 4478. State = [[-0.23651853  0.04552672]]. Action = [[-0.20317641  0.0721474  -0.07628663 -0.54878855]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 4478 is [True, False, False, False, True, False]
Scene graph at timestep 4478 is [True, False, False, False, True, False]
State prediction error at timestep 4478 is tensor(7.3637e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4479. State = [[-0.23651698  0.0448742 ]]. Action = [[ 0.07130235 -0.08456059 -0.20147118 -0.50269556]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 4479 is [True, False, False, False, True, False]
Current timestep = 4480. State = [[-0.2364487   0.04388737]]. Action = [[ 0.19119656 -0.0399534   0.07392657 -0.34361064]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 4480 is [True, False, False, False, True, False]
Current timestep = 4481. State = [[-0.23559736  0.04305018]]. Action = [[-0.20464021  0.04313985  0.08394787 -0.76255774]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 4481 is [True, False, False, False, True, False]
Current timestep = 4482. State = [[-0.23555976  0.04281993]]. Action = [[-0.20758393 -0.07581407 -0.19219352  0.68583465]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 4482 is [True, False, False, False, True, False]
Current timestep = 4483. State = [[-0.23574229  0.04168162]]. Action = [[-0.05941473 -0.16449586 -0.1549656   0.5351944 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 4483 is [True, False, False, False, True, False]
Current timestep = 4484. State = [[-0.23621106  0.03907387]]. Action = [[-0.02028966  0.11100066  0.21311766  0.42464042]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 4484 is [True, False, False, False, True, False]
Scene graph at timestep 4484 is [True, False, False, False, True, False]
State prediction error at timestep 4484 is tensor(1.0686e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4485. State = [[-0.23702548  0.03807088]]. Action = [[ 0.01009482  0.16551518 -0.14852206 -0.9880884 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 4485 is [True, False, False, False, True, False]
Scene graph at timestep 4485 is [True, False, False, False, True, False]
State prediction error at timestep 4485 is tensor(6.9682e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4486. State = [[-0.23759952  0.03924171]]. Action = [[-0.0168668   0.15431234  0.08650756 -0.70889753]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 4486 is [True, False, False, False, True, False]
Current timestep = 4487. State = [[-0.23855081  0.04149996]]. Action = [[-0.17231435 -0.12136185  0.0671843   0.95566964]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 4487 is [True, False, False, False, True, False]
Current timestep = 4488. State = [[-0.23990439  0.04226331]]. Action = [[-0.15318924  0.23671454  0.19568497  0.44388318]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 4488 is [True, False, False, False, True, False]
Current timestep = 4489. State = [[-0.24235943  0.04553561]]. Action = [[ 0.19027779 -0.05384143  0.09447268 -0.14144033]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 4489 is [True, False, False, False, True, False]
Current timestep = 4490. State = [[-0.24300653  0.04699112]]. Action = [[ 0.17271304  0.16072547 -0.09471902 -0.27644265]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 4490 is [True, False, False, False, True, False]
Current timestep = 4491. State = [[-0.24366318  0.04903065]]. Action = [[-0.08625802  0.15846711 -0.20640445 -0.03717566]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 4491 is [True, False, False, False, True, False]
Current timestep = 4492. State = [[-0.24517177  0.05269601]]. Action = [[ 0.05210131  0.21369374  0.05160585 -0.66727847]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 4492 is [True, False, False, False, True, False]
Scene graph at timestep 4492 is [True, False, False, False, True, False]
State prediction error at timestep 4492 is tensor(1.7689e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4493. State = [[-0.24653776  0.05768646]]. Action = [[ 0.14662114 -0.00573339 -0.16794921 -0.6821783 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 4493 is [True, False, False, False, True, False]
Scene graph at timestep 4493 is [True, False, False, False, True, False]
State prediction error at timestep 4493 is tensor(1.7455e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4494. State = [[-0.24616423  0.06023524]]. Action = [[-0.09012079 -0.14057763  0.17767513  0.9120122 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 4494 is [True, False, False, False, True, False]
Current timestep = 4495. State = [[-0.2462776   0.06035607]]. Action = [[0.03745806 0.09247202 0.19047022 0.9233551 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 4495 is [True, False, False, False, True, False]
Human Feedback received at timestep 4495 of -1
Current timestep = 4496. State = [[-0.24606246  0.06087374]]. Action = [[ 0.10571232 -0.02817693 -0.01241733  0.46841216]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 4496 is [True, False, False, False, True, False]
Current timestep = 4497. State = [[-0.24555387  0.0610559 ]]. Action = [[-0.00993189 -0.19802713  0.15612572 -0.7601478 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 4497 is [True, False, False, False, True, False]
Scene graph at timestep 4497 is [True, False, False, False, True, False]
State prediction error at timestep 4497 is tensor(1.5277e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4498. State = [[-0.24521503  0.06030283]]. Action = [[-0.00138995 -0.05360049 -0.02191824  0.51354957]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 4498 is [True, False, False, False, True, False]
Current timestep = 4499. State = [[-0.24506363  0.05951066]]. Action = [[-0.01276459  0.04585189  0.21082458 -0.34056985]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 4499 is [True, False, False, False, True, False]
Current timestep = 4500. State = [[-0.24511994  0.05919908]]. Action = [[-0.22502317 -0.15294129 -0.16927604  0.58840716]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 4500 is [True, False, False, False, True, False]
Scene graph at timestep 4500 is [True, False, False, False, True, False]
State prediction error at timestep 4500 is tensor(3.1173e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4501. State = [[-0.24522658  0.05746344]]. Action = [[-0.20351222  0.00901946  0.01172468  0.9505515 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 4501 is [True, False, False, False, True, False]
Scene graph at timestep 4501 is [True, False, False, False, True, False]
State prediction error at timestep 4501 is tensor(1.7011e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4502. State = [[-0.24629237  0.05622598]]. Action = [[-0.1375845  -0.03065142  0.045746    0.6608393 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 4502 is [True, False, False, False, True, False]
Current timestep = 4503. State = [[-0.24742617  0.05526567]]. Action = [[ 0.19136947 -0.03829011  0.01012373 -0.9227253 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 4503 is [True, False, False, False, True, False]
Current timestep = 4504. State = [[-0.24748947  0.05430819]]. Action = [[-0.11376062  0.11669201 -0.13903874  0.61063075]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 4504 is [True, False, False, False, True, False]
Scene graph at timestep 4504 is [True, False, False, False, True, False]
State prediction error at timestep 4504 is tensor(1.1046e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4505. State = [[-0.24771997  0.05463937]]. Action = [[-0.1186883  -0.05788955  0.05536321 -0.5177908 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 4505 is [True, False, False, False, True, False]
Current timestep = 4506. State = [[-0.24871476  0.05460005]]. Action = [[ 0.00913188 -0.04875469 -0.12758969  0.1741668 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 4506 is [True, False, False, False, True, False]
Current timestep = 4507. State = [[-0.24901141  0.05391059]]. Action = [[ 0.14933825 -0.0671356   0.19363359 -0.75646454]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 4507 is [True, False, False, False, True, False]
Current timestep = 4508. State = [[-0.24905403  0.05305599]]. Action = [[-0.12625085  0.16140065  0.0346995  -0.754276  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 4508 is [True, False, False, False, True, False]
Current timestep = 4509. State = [[-0.24949253  0.05366708]]. Action = [[-0.24305215 -0.00415389 -0.1017206   0.45519376]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 4509 is [True, False, False, False, True, False]
Current timestep = 4510. State = [[-0.2509248   0.05434395]]. Action = [[0.04453871 0.12648594 0.17692646 0.52341354]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 4510 is [True, False, False, False, True, False]
Current timestep = 4511. State = [[-0.25198966  0.05602266]]. Action = [[ 0.20594093 -0.22764993 -0.01746333  0.69015574]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 4511 is [True, False, False, False, True, False]
Current timestep = 4512. State = [[-0.25214407  0.05491664]]. Action = [[-0.10683599 -0.03427503  0.11603236 -0.03195018]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 4512 is [True, False, False, False, True, False]
Current timestep = 4513. State = [[-0.25213188  0.05364837]]. Action = [[-0.13906968 -0.10381475  0.1873253  -0.73474413]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 4513 is [True, False, False, False, True, False]
Current timestep = 4514. State = [[-0.25311217  0.05141106]]. Action = [[ 0.05840436 -0.03531571  0.11510026  0.68957424]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 4514 is [True, False, False, False, True, False]
Current timestep = 4515. State = [[-0.2532927   0.04915411]]. Action = [[-0.09412029 -0.11306791 -0.03442959  0.9737936 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 4515 is [True, False, False, False, True, False]
Scene graph at timestep 4515 is [True, False, False, False, True, False]
State prediction error at timestep 4515 is tensor(1.8062e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4516. State = [[-0.25418016  0.04659493]]. Action = [[ 0.174465    0.20638329  0.02528754 -0.27081895]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 4516 is [True, False, False, False, True, False]
Current timestep = 4517. State = [[-0.25420496  0.04690196]]. Action = [[0.15927574 0.02449396 0.06393921 0.50774634]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 4517 is [True, False, False, False, True, False]
Scene graph at timestep 4517 is [True, False, False, False, True, False]
State prediction error at timestep 4517 is tensor(6.0712e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4518. State = [[-0.25420383  0.04696703]]. Action = [[ 0.00588772 -0.17834525  0.2241382   0.9029697 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 4518 is [True, False, False, False, True, False]
Current timestep = 4519. State = [[-0.25415984  0.04575247]]. Action = [[-0.18860738 -0.11047944 -0.1470635  -0.9284795 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 4519 is [True, False, False, False, True, False]
Scene graph at timestep 4519 is [True, False, False, False, True, False]
State prediction error at timestep 4519 is tensor(2.1178e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4520. State = [[-0.25419995  0.04391521]]. Action = [[-0.19294998 -0.10079697 -0.18939398 -0.3454157 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 4520 is [True, False, False, False, True, False]
Current timestep = 4521. State = [[-0.2552679   0.04172727]]. Action = [[ 0.1402412   0.21259886 -0.222293    0.44143736]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 4521 is [True, False, False, False, True, False]
Scene graph at timestep 4521 is [True, False, False, False, True, False]
State prediction error at timestep 4521 is tensor(6.5857e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4522. State = [[-0.2555744   0.04216668]]. Action = [[ 0.09770465  0.06597918 -0.10708255 -0.15017134]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 4522 is [True, False, False, False, True, False]
Current timestep = 4523. State = [[-0.25567195  0.04277969]]. Action = [[0.22517157 0.09182572 0.09760338 0.9206674 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 4523 is [True, False, False, False, True, False]
Scene graph at timestep 4523 is [True, False, False, False, True, False]
State prediction error at timestep 4523 is tensor(1.2666e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4524. State = [[-0.25569865  0.04324102]]. Action = [[ 0.02034396 -0.00302538 -0.00475807 -0.01779926]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 4524 is [True, False, False, False, True, False]
Current timestep = 4525. State = [[-0.2556637   0.04362541]]. Action = [[-0.17762987  0.12217036 -0.18215318 -0.60865074]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 4525 is [True, False, False, False, True, False]
Current timestep = 4526. State = [[-0.2562101  0.0453664]]. Action = [[-0.13090082 -0.0883671   0.12361518 -0.76520663]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 4526 is [True, False, False, False, True, False]
Current timestep = 4527. State = [[-0.25644398  0.04603136]]. Action = [[0.1332413  0.0795525  0.18875378 0.7522383 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 4527 is [True, False, False, False, True, False]
Current timestep = 4528. State = [[-0.25667316  0.04685334]]. Action = [[ 0.0490801  -0.02842681  0.21374595  0.22847986]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 4528 is [True, False, False, False, True, False]
Current timestep = 4529. State = [[-0.256735    0.04722926]]. Action = [[ 0.19159532  0.00222793 -0.0289802   0.09204972]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 4529 is [True, False, False, False, True, False]
Current timestep = 4530. State = [[-0.25649646  0.04751856]]. Action = [[-0.21583426 -0.19506808 -0.1777128   0.89278054]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 4530 is [True, False, False, False, True, False]
Current timestep = 4531. State = [[-0.25646368  0.04694646]]. Action = [[-0.2160619   0.12402064  0.10945329  0.9680667 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 4531 is [True, False, False, False, True, False]
Current timestep = 4532. State = [[-0.25682148  0.04736282]]. Action = [[-0.24066217  0.1949057  -0.20989396  0.6623435 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 4532 is [True, False, False, False, True, False]
Scene graph at timestep 4532 is [True, False, False, False, True, False]
State prediction error at timestep 4532 is tensor(1.8991e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4533. State = [[-0.25963184  0.05086342]]. Action = [[ 0.02051407  0.02896529 -0.21250524 -0.723632  ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 4533 is [True, False, False, False, True, False]
Current timestep = 4534. State = [[-0.2615195   0.05343885]]. Action = [[-0.19337092 -0.09674677 -0.1280649  -0.55053663]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 4534 is [True, False, False, False, True, False]
Scene graph at timestep 4534 is [True, False, False, False, True, False]
State prediction error at timestep 4534 is tensor(1.2738e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4535. State = [[-0.26466057  0.0538575 ]]. Action = [[ 0.13925949 -0.0676844   0.01070395  0.48109448]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 4535 is [True, False, False, False, True, False]
Current timestep = 4536. State = [[-0.26555446  0.05351132]]. Action = [[ 0.16302598  0.22083044  0.1765644  -0.01568121]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 4536 is [True, False, False, False, True, False]
Current timestep = 4537. State = [[-0.26615384  0.05489344]]. Action = [[-0.12315059  0.19421434  0.14128655  0.9887155 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 4537 is [True, False, False, False, True, False]
Scene graph at timestep 4537 is [True, False, False, False, True, False]
State prediction error at timestep 4537 is tensor(2.5483e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4538. State = [[-0.26774144  0.05868676]]. Action = [[0.11725646 0.14719322 0.05788219 0.20113456]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 4538 is [True, False, False, False, True, False]
Current timestep = 4539. State = [[-0.26910108  0.06244731]]. Action = [[-0.08671877 -0.21719465  0.13131595 -0.38202852]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 4539 is [True, False, False, False, True, False]
Scene graph at timestep 4539 is [True, False, False, False, True, False]
State prediction error at timestep 4539 is tensor(6.4798e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4539 of -1
Current timestep = 4540. State = [[-0.2692132   0.06293795]]. Action = [[ 0.17127097 -0.05103818  0.18078762 -0.61559767]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 4540 is [True, False, False, False, True, False]
Current timestep = 4541. State = [[-0.26902997  0.06291306]]. Action = [[-0.10961509  0.18818119  0.20516676 -0.12450063]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 4541 is [True, False, False, False, True, False]
Current timestep = 4542. State = [[-0.26939204  0.0639374 ]]. Action = [[ 0.17011034  0.17587626 -0.05003327 -0.8445907 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 4542 is [True, False, False, False, True, False]
Current timestep = 4543. State = [[-0.26964006  0.06611314]]. Action = [[-0.04403505  0.13597792 -0.16050214  0.00516427]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 4543 is [True, False, False, False, True, False]
Current timestep = 4544. State = [[-0.27015063  0.06924032]]. Action = [[ 0.19312516  0.2224859  -0.16159314  0.07951009]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 4544 is [True, False, False, False, True, False]
Current timestep = 4545. State = [[-0.26925188  0.07418332]]. Action = [[-0.09653285 -0.00981164 -0.06523925 -0.5547808 ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 4545 is [True, False, False, False, True, False]
Current timestep = 4546. State = [[-0.2694166   0.07705344]]. Action = [[-0.04576188  0.22200054  0.11906284 -0.8713683 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 4546 is [True, False, False, False, True, False]
Current timestep = 4547. State = [[-0.27055275  0.08214232]]. Action = [[-0.07225993  0.02374756 -0.04969983  0.82456434]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 4547 is [True, False, False, False, True, False]
Scene graph at timestep 4547 is [True, False, False, False, True, False]
State prediction error at timestep 4547 is tensor(2.0285e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4548. State = [[-0.27222687  0.08650862]]. Action = [[-0.01916401  0.18859017  0.027008    0.02989614]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 4548 is [True, False, False, False, True, False]
Scene graph at timestep 4548 is [True, False, False, False, True, False]
State prediction error at timestep 4548 is tensor(1.6464e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4548 of -1
Current timestep = 4549. State = [[-0.27381617  0.0916283 ]]. Action = [[-0.02136651 -0.1158115  -0.02869295 -0.49321324]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 4549 is [True, False, False, False, True, False]
Current timestep = 4550. State = [[-0.27383664  0.09315628]]. Action = [[-0.18945901 -0.00354187 -0.01843403  0.4623356 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 4550 is [True, False, False, False, True, False]
Current timestep = 4551. State = [[-0.27403978  0.09363622]]. Action = [[ 0.07530701 -0.09924473 -0.03370458  0.5932362 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 4551 is [True, False, False, False, True, False]
Current timestep = 4552. State = [[-0.27394927  0.09376432]]. Action = [[0.16308212 0.13443533 0.11720151 0.963326  ]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 4552 is [True, False, False, False, True, False]
Scene graph at timestep 4552 is [True, False, False, False, True, False]
State prediction error at timestep 4552 is tensor(2.1602e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4553. State = [[-0.27198315  0.09455255]]. Action = [[-0.20688993  0.07654482 -0.04719722 -0.3299824 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 4553 is [True, False, False, False, True, False]
Current timestep = 4554. State = [[-0.27057266  0.09507754]]. Action = [[-0.19733083  0.02106422 -0.1498266   0.53030276]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 4554 is [True, False, False, False, True, False]
Current timestep = 4555. State = [[-0.26963183  0.09544237]]. Action = [[-0.0695115   0.05273709  0.10826015  0.66401196]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 4555 is [True, False, False, False, True, False]
Scene graph at timestep 4555 is [True, False, False, False, True, False]
State prediction error at timestep 4555 is tensor(1.1518e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4556. State = [[-0.2694802   0.09597288]]. Action = [[ 0.01212811 -0.08914532  0.18548936 -0.38976407]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 4556 is [True, False, False, False, True, False]
Current timestep = 4557. State = [[-0.26944315  0.09601002]]. Action = [[ 0.18743706  0.15706486 -0.02365953  0.8481673 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 4557 is [True, False, False, False, True, False]
Scene graph at timestep 4557 is [True, False, False, False, True, False]
State prediction error at timestep 4557 is tensor(3.7879e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4558. State = [[-0.26791683  0.09677287]]. Action = [[-0.0823496   0.12524676  0.08651656 -0.7548299 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 4558 is [True, False, False, False, True, False]
Current timestep = 4559. State = [[-0.26788914  0.0988397 ]]. Action = [[ 0.00897795 -0.12267391 -0.22944474 -0.0673331 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 4559 is [True, False, False, False, True, False]
Scene graph at timestep 4559 is [True, False, False, False, True, False]
State prediction error at timestep 4559 is tensor(5.6502e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4560. State = [[-0.26760885  0.0991419 ]]. Action = [[-0.19704312 -0.21756673  0.17858112  0.7102002 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 4560 is [True, False, False, False, True, False]
Current timestep = 4561. State = [[-0.2674491   0.09844992]]. Action = [[ 0.05881059  0.17927206 -0.01920444  0.35911775]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 4561 is [True, False, False, False, True, False]
Current timestep = 4562. State = [[-0.2677456   0.09904931]]. Action = [[-0.16928566  0.18740547 -0.20051762 -0.40000468]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 4562 is [True, False, False, False, True, False]
Scene graph at timestep 4562 is [True, False, False, False, True, False]
State prediction error at timestep 4562 is tensor(3.8452e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4563. State = [[-0.26926428  0.10200366]]. Action = [[ 0.12450856 -0.04525243  0.0961864  -0.9441812 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 4563 is [True, False, False, False, True, False]
Current timestep = 4564. State = [[-0.2697536   0.10300311]]. Action = [[-0.20024791 -0.03651701 -0.22420579  0.5079967 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 4564 is [True, False, False, False, True, False]
Current timestep = 4565. State = [[-0.26975402  0.10328294]]. Action = [[ 0.22114736 -0.06278595 -0.07761043  0.06153607]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 4565 is [True, False, False, False, True, False]
Current timestep = 4566. State = [[-0.26902288  0.10379054]]. Action = [[-0.09192252  0.1803979  -0.0227817   0.2533449 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 4566 is [True, False, False, False, True, False]
Current timestep = 4567. State = [[-0.26918337  0.10480382]]. Action = [[-0.2295035   0.21105903 -0.01094301 -0.048195  ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 4567 is [True, False, False, False, True, False]
Current timestep = 4568. State = [[-0.26900104  0.1052514 ]]. Action = [[0.1099959  0.0853847  0.11262423 0.55690765]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 4568 is [True, False, False, False, True, False]
Scene graph at timestep 4568 is [True, False, False, False, True, False]
State prediction error at timestep 4568 is tensor(6.6064e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4569. State = [[-0.26820648  0.10616242]]. Action = [[-0.20584439 -0.18709736  0.15146318  0.02920902]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 4569 is [True, False, False, False, True, False]
Scene graph at timestep 4569 is [True, False, False, False, True, False]
State prediction error at timestep 4569 is tensor(7.3588e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4570. State = [[-0.2676536   0.10703911]]. Action = [[ 0.22788435 -0.14724495 -0.11152768  0.34897113]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 4570 is [True, False, False, False, True, False]
Current timestep = 4571. State = [[-0.26557493  0.10768332]]. Action = [[-0.10248432  0.16984281  0.11000502 -0.80762553]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 4571 is [True, False, False, False, True, False]
Current timestep = 4572. State = [[-0.26442698  0.10868273]]. Action = [[ 0.10124525  0.21192074  0.1732885  -0.00682342]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 4572 is [True, False, False, False, True, False]
Scene graph at timestep 4572 is [True, False, False, False, True, False]
State prediction error at timestep 4572 is tensor(1.2775e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4573. State = [[-0.2625508   0.11204947]]. Action = [[-0.0095837  -0.04408517 -0.02280512  0.8267603 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 4573 is [True, False, False, False, True, False]
Current timestep = 4574. State = [[-0.26061141  0.11391692]]. Action = [[ 0.23465306  0.13866633  0.18828645 -0.4239539 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 4574 is [True, False, False, False, True, False]
Current timestep = 4575. State = [[-0.25687298  0.11627443]]. Action = [[ 0.14295667 -0.15262684 -0.03639521 -0.6933617 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 4575 is [True, False, False, False, True, False]
Human Feedback received at timestep 4575 of -1
Current timestep = 4576. State = [[-0.25297147  0.11741754]]. Action = [[-0.06322008  0.12032866 -0.11306837  0.5973033 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 4576 is [True, False, False, False, True, False]
Current timestep = 4577. State = [[-0.24992982  0.11896048]]. Action = [[-0.24107748 -0.09723282  0.10004053 -0.5112874 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 4577 is [True, False, False, False, True, False]
Current timestep = 4578. State = [[-0.24982871  0.11931532]]. Action = [[0.19832146 0.07203692 0.15477988 0.2846887 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 4578 is [True, False, False, False, True, False]
Current timestep = 4579. State = [[-0.24914174  0.11965602]]. Action = [[ 0.11529648  0.02988073 -0.05203825 -0.05072439]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 4579 is [True, False, False, False, True, False]
Human Feedback received at timestep 4579 of -1
Current timestep = 4580. State = [[-0.24798037  0.12018719]]. Action = [[-0.0369872  -0.1365755  -0.13078329 -0.23214006]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 4580 is [True, False, False, False, True, False]
Current timestep = 4581. State = [[-0.24759997  0.12009794]]. Action = [[-0.1726488  -0.07372323 -0.2322887  -0.9535739 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 4581 is [True, False, False, False, True, False]
Current timestep = 4582. State = [[-0.24771552  0.1197619 ]]. Action = [[-0.01292935  0.23036814 -0.00482975  0.43051994]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 4582 is [True, False, False, False, True, False]
Current timestep = 4583. State = [[-0.24845205  0.12116923]]. Action = [[-0.21003014  0.12575561  0.20717776 -0.7843184 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 4583 is [True, False, False, False, True, False]
Current timestep = 4584. State = [[-0.2502035   0.12424619]]. Action = [[ 0.0003109   0.22112131 -0.16901419  0.02369606]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 4584 is [True, False, False, False, True, False]
Current timestep = 4585. State = [[-0.2528398   0.12909523]]. Action = [[ 0.23056322  0.16546261 -0.0797517  -0.71152854]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 4585 is [True, False, False, False, True, False]
Scene graph at timestep 4585 is [True, False, False, False, False, True]
State prediction error at timestep 4585 is tensor(1.5406e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4586. State = [[-0.2535348   0.13384815]]. Action = [[ 0.10581493  0.02195063 -0.09711128  0.8700161 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 4586 is [True, False, False, False, False, True]
Current timestep = 4587. State = [[-0.25288042  0.13656686]]. Action = [[ 0.07887635  0.20913154 -0.07716468  0.05620611]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 4587 is [True, False, False, False, False, True]
Current timestep = 4588. State = [[-0.25241414  0.14109002]]. Action = [[-0.22633     0.20933682 -0.17416205  0.9087348 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 4588 is [True, False, False, False, False, True]
Current timestep = 4589. State = [[-0.25407547  0.14683993]]. Action = [[-0.16105606 -0.07145403  0.06209862 -0.9568444 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 4589 is [True, False, False, False, False, True]
Human Feedback received at timestep 4589 of -1
Current timestep = 4590. State = [[-0.25645137  0.1504478 ]]. Action = [[-0.20972157  0.03464866  0.06574672 -0.88237345]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 4590 is [True, False, False, False, False, True]
Current timestep = 4591. State = [[-0.25876227  0.1540246 ]]. Action = [[ 0.16275156  0.07967252 -0.12979013  0.9381497 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 4591 is [True, False, False, False, False, True]
Current timestep = 4592. State = [[-0.26005194  0.15625362]]. Action = [[-0.12043294  0.08756155 -0.20127499 -0.3552109 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 4592 is [True, False, False, False, False, True]
Current timestep = 4593. State = [[-0.26180264  0.15890804]]. Action = [[-0.1692509  -0.04404682 -0.19234583 -0.44723022]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 4593 is [True, False, False, False, False, True]
Human Feedback received at timestep 4593 of -1
Current timestep = 4594. State = [[-0.26353347  0.16139013]]. Action = [[-0.00545059 -0.06855825 -0.03450105  0.21621287]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 4594 is [True, False, False, False, False, True]
Current timestep = 4595. State = [[-0.26396668  0.16196764]]. Action = [[ 0.01487267 -0.07391319 -0.03502761 -0.32915765]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 4595 is [True, False, False, False, False, True]
Current timestep = 4596. State = [[-0.2639192   0.16186033]]. Action = [[ 0.08375278 -0.04028636  0.14764667  0.6280409 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 4596 is [True, False, False, False, False, True]
Human Feedback received at timestep 4596 of -1
Current timestep = 4597. State = [[-0.2638727   0.16167489]]. Action = [[-0.03387131 -0.14214826 -0.04724762 -0.47569954]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 4597 is [True, False, False, False, False, True]
Current timestep = 4598. State = [[-0.26337805  0.16019994]]. Action = [[-0.03377292  0.03492376  0.20654213  0.38828707]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 4598 is [True, False, False, False, False, True]
Current timestep = 4599. State = [[-0.26324102  0.1593757 ]]. Action = [[0.06326061 0.0640687  0.16212979 0.50752616]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 4599 is [True, False, False, False, False, True]
Scene graph at timestep 4599 is [True, False, False, False, False, True]
State prediction error at timestep 4599 is tensor(1.3040e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4600. State = [[-0.26322582  0.15931383]]. Action = [[-0.16287135 -0.07498719 -0.10971424 -0.50415653]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 4600 is [True, False, False, False, False, True]
Current timestep = 4601. State = [[-0.26322263  0.15857063]]. Action = [[-0.1565827  -0.19147332  0.18112099 -0.57958317]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 4601 is [True, False, False, False, False, True]
Scene graph at timestep 4601 is [True, False, False, False, False, True]
State prediction error at timestep 4601 is tensor(2.5054e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4602. State = [[-0.26404724  0.15587072]]. Action = [[ 0.03410938 -0.16254885  0.00798607  0.13419557]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 4602 is [True, False, False, False, False, True]
Current timestep = 4603. State = [[-0.2646379  0.1519009]]. Action = [[-0.0956315  -0.17588829  0.12038478  0.5593704 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 4603 is [True, False, False, False, False, True]
Current timestep = 4604. State = [[-0.26581565  0.14748065]]. Action = [[ 0.10725635 -0.08936051  0.14929622 -0.89567405]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 4604 is [True, False, False, False, False, True]
Current timestep = 4605. State = [[-0.2660035   0.14413193]]. Action = [[-0.1193296   0.10084909  0.01356378  0.281106  ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 4605 is [True, False, False, False, False, True]
Current timestep = 4606. State = [[-0.26704198  0.1429544 ]]. Action = [[-0.17996924  0.18754876  0.13182613  0.3198701 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 4606 is [True, False, False, False, False, True]
Current timestep = 4607. State = [[-0.26943442  0.1441846 ]]. Action = [[-0.20366411  0.07692963  0.2020011  -0.29831612]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 4607 is [True, False, False, False, False, True]
Current timestep = 4608. State = [[-0.27301314  0.14674012]]. Action = [[-0.11826515 -0.05171922  0.16670176  0.776073  ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 4608 is [True, False, False, False, False, True]
Scene graph at timestep 4608 is [True, False, False, False, False, True]
State prediction error at timestep 4608 is tensor(4.7399e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4609. State = [[-0.27624634  0.14811887]]. Action = [[ 0.00337544  0.05730438 -0.14656675  0.7528715 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 4609 is [True, False, False, False, False, True]
Scene graph at timestep 4609 is [True, False, False, False, False, True]
State prediction error at timestep 4609 is tensor(5.1491e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4610. State = [[-0.27891815  0.14940208]]. Action = [[-0.03079276 -0.15295964 -0.07064286  0.91981447]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 4610 is [True, False, False, False, False, True]
Current timestep = 4611. State = [[-0.28020486  0.15014099]]. Action = [[-0.1301048   0.04612479  0.0259122   0.87990713]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 4611 is [True, False, False, False, False, True]
Scene graph at timestep 4611 is [True, False, False, False, False, True]
State prediction error at timestep 4611 is tensor(3.4943e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4612. State = [[-0.28118542  0.15039836]]. Action = [[-0.12651087  0.11103937 -0.03241277  0.15102208]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 4612 is [True, False, False, False, False, True]
Human Feedback received at timestep 4612 of -1
Current timestep = 4613. State = [[-0.2814991   0.15056802]]. Action = [[-0.12143934 -0.22853222 -0.11848778 -0.67817694]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 4613 is [True, False, False, False, False, True]
Current timestep = 4614. State = [[-0.28153783  0.15061279]]. Action = [[ 0.20590544 -0.12037134 -0.10273713  0.3523594 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 4614 is [True, False, False, False, False, True]
Scene graph at timestep 4614 is [True, False, False, False, False, True]
State prediction error at timestep 4614 is tensor(6.5900e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4615. State = [[-0.2811935   0.14914915]]. Action = [[ 0.03676492  0.21446127 -0.13372415  0.21574521]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 4615 is [True, False, False, False, False, True]
Current timestep = 4616. State = [[-0.28090012  0.14835821]]. Action = [[0.23135778 0.09034243 0.22457117 0.91396534]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 4616 is [True, False, False, False, False, True]
Scene graph at timestep 4616 is [True, False, False, False, False, True]
State prediction error at timestep 4616 is tensor(2.2275e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4617. State = [[-0.28038576  0.14855708]]. Action = [[-0.20974676 -0.22339827 -0.0373562  -0.7514215 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 4617 is [True, False, False, False, False, True]
Current timestep = 4618. State = [[-0.28006548  0.14855133]]. Action = [[-0.18516882 -0.06053591 -0.21675822 -0.87321526]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 4618 is [True, False, False, False, False, True]
Current timestep = 4619. State = [[-0.27993107  0.14855336]]. Action = [[ 0.07290018  0.07743773 -0.11851299 -0.67250985]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 4619 is [True, False, False, False, False, True]
Current timestep = 4620. State = [[-0.27957192  0.14897795]]. Action = [[-0.09441686  0.04975891  0.03233433 -0.7960305 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 4620 is [True, False, False, False, False, True]
Current timestep = 4621. State = [[-0.2792294   0.14937939]]. Action = [[-0.21838215 -0.11221579  0.01847827  0.41526377]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 4621 is [True, False, False, False, False, True]
Current timestep = 4622. State = [[-0.27897647  0.14967367]]. Action = [[ 0.08501378  0.03473455 -0.1125243  -0.6294376 ]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 4622 is [True, False, False, False, False, True]
Scene graph at timestep 4622 is [True, False, False, False, False, True]
State prediction error at timestep 4622 is tensor(5.6673e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4623. State = [[-0.2781137   0.14992712]]. Action = [[ 0.11883816 -0.24120678 -0.02591066 -0.28297222]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 4623 is [True, False, False, False, False, True]
Current timestep = 4624. State = [[-0.27612156  0.1476007 ]]. Action = [[ 0.1686832   0.04754996 -0.12840463  0.66071117]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 4624 is [True, False, False, False, False, True]
Current timestep = 4625. State = [[-0.27357003  0.14636196]]. Action = [[-0.19234796  0.14961559 -0.0962749   0.11740923]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 4625 is [True, False, False, False, False, True]
Current timestep = 4626. State = [[-0.27202103  0.14566483]]. Action = [[ 0.02463385  0.15079576  0.16568083 -0.3149408 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 4626 is [True, False, False, False, False, True]
Current timestep = 4627. State = [[-0.2711196   0.14615789]]. Action = [[-0.19158727 -0.14240009 -0.09115347  0.36149597]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 4627 is [True, False, False, False, False, True]
Current timestep = 4628. State = [[-0.2706614  0.1464105]]. Action = [[-0.08348966  0.11354089  0.15492573 -0.64639604]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 4628 is [True, False, False, False, False, True]
Current timestep = 4629. State = [[-0.27087185  0.1470238 ]]. Action = [[ 0.0784288   0.15462834  0.11947376 -0.9085194 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 4629 is [True, False, False, False, False, True]
Current timestep = 4630. State = [[-0.27039742  0.14873019]]. Action = [[-0.14880621 -0.2058376  -0.03232983 -0.66102314]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 4630 is [True, False, False, False, False, True]
Current timestep = 4631. State = [[-0.27009988  0.14999244]]. Action = [[-0.22905211  0.21353859  0.09782764 -0.9876374 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 4631 is [True, False, False, False, False, True]
Scene graph at timestep 4631 is [True, False, False, False, False, True]
State prediction error at timestep 4631 is tensor(2.3462e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4632. State = [[-0.26961157  0.15076797]]. Action = [[ 1.5291572e-04  4.1380495e-02 -6.6089094e-02  7.4790454e-01]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 4632 is [True, False, False, False, False, True]
Scene graph at timestep 4632 is [True, False, False, False, False, True]
State prediction error at timestep 4632 is tensor(1.3816e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4633. State = [[-0.2692504   0.15158103]]. Action = [[ 0.18359327  0.10176975 -0.03699376 -0.5470668 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 4633 is [True, False, False, False, False, True]
Current timestep = 4634. State = [[-0.26731622  0.15297283]]. Action = [[ 0.13967639  0.16793317 -0.04962513  0.5363722 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 4634 is [True, False, False, False, False, True]
Current timestep = 4635. State = [[-0.2643008   0.15625109]]. Action = [[-0.21505938 -0.22572595  0.0409157  -0.14884865]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 4635 is [True, False, False, False, False, True]
Current timestep = 4636. State = [[-0.2636273   0.15672196]]. Action = [[ 0.19186771  0.06761497  0.03193387 -0.5354575 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 4636 is [True, False, False, False, False, True]
Current timestep = 4637. State = [[-0.26230234  0.1574291 ]]. Action = [[ 0.02527243 -0.18783568  0.13367444  0.09601927]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 4637 is [True, False, False, False, False, True]
Current timestep = 4638. State = [[-0.26136518  0.15743558]]. Action = [[0.24763364 0.05309844 0.1343478  0.09570074]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 4638 is [True, False, False, False, False, True]
Current timestep = 4639. State = [[-0.257972    0.15795672]]. Action = [[-0.02850854 -0.14696212  0.06146559  0.9054525 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 4639 is [True, False, False, False, False, True]
Scene graph at timestep 4639 is [True, False, False, False, False, True]
State prediction error at timestep 4639 is tensor(2.0893e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4639 of -1
Current timestep = 4640. State = [[-0.25493073  0.15656698]]. Action = [[-0.21506193 -0.16495292 -0.07729676 -0.374143  ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 4640 is [True, False, False, False, False, True]
Current timestep = 4641. State = [[-0.25418875  0.15376164]]. Action = [[ 0.20145375 -0.02158946  0.02346426  0.3664075 ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 4641 is [True, False, False, False, False, True]
Current timestep = 4642. State = [[-0.2533803   0.15165956]]. Action = [[ 0.0981046   0.2407763   0.07577726 -0.8757931 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 4642 is [True, False, False, False, False, True]
Scene graph at timestep 4642 is [True, False, False, False, False, True]
State prediction error at timestep 4642 is tensor(6.5820e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4643. State = [[-0.251854    0.15243359]]. Action = [[-0.13286686  0.13062072  0.04500312  0.21849215]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 4643 is [True, False, False, False, False, True]
Current timestep = 4644. State = [[-0.25149348  0.15374292]]. Action = [[-0.18730584 -0.21329148 -0.16472888 -0.89634836]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 4644 is [True, False, False, False, False, True]
Current timestep = 4645. State = [[-0.2517485  0.1537925]]. Action = [[-0.19014569  0.21554512 -0.21215986 -0.6522509 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 4645 is [True, False, False, False, False, True]
Current timestep = 4646. State = [[-0.2535586   0.15595768]]. Action = [[-0.13604093  0.13864785 -0.02428477  0.10359025]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 4646 is [True, False, False, False, False, True]
Scene graph at timestep 4646 is [True, False, False, False, False, True]
State prediction error at timestep 4646 is tensor(1.1506e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4647. State = [[-0.25631562  0.1596878 ]]. Action = [[ 0.0878019   0.14991823 -0.02578342  0.8923578 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 4647 is [True, False, False, False, False, True]
Current timestep = 4648. State = [[-0.2584363   0.16285741]]. Action = [[-0.07920708 -0.1913687  -0.09825492 -0.7887696 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 4648 is [True, False, False, False, False, True]
Scene graph at timestep 4648 is [True, False, False, False, False, True]
State prediction error at timestep 4648 is tensor(7.6345e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4649. State = [[-0.25872436  0.16310668]]. Action = [[-0.03809769  0.00846851  0.0636282   0.6584892 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 4649 is [True, False, False, False, False, True]
Current timestep = 4650. State = [[-0.2589845  0.1633931]]. Action = [[-0.04986896 -0.02844232 -0.1771805  -0.83370256]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 4650 is [True, False, False, False, False, True]
Scene graph at timestep 4650 is [True, False, False, False, False, True]
State prediction error at timestep 4650 is tensor(3.3396e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4651. State = [[-0.25914943  0.16356836]]. Action = [[-0.13429497  0.10152024 -0.23251106  0.26170468]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 4651 is [True, False, False, False, False, True]
Scene graph at timestep 4651 is [True, False, False, False, False, True]
State prediction error at timestep 4651 is tensor(1.5989e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4652. State = [[-0.26042926  0.1653643 ]]. Action = [[-0.12087342 -0.08166318  0.17629993  0.96421874]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 4652 is [True, False, False, False, False, True]
Current timestep = 4653. State = [[-0.2614315   0.16611098]]. Action = [[-0.20473452 -0.18628563 -0.21701422  0.5527899 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 4653 is [True, False, False, False, False, True]
Current timestep = 4654. State = [[-0.2632302   0.16476473]]. Action = [[-0.02945668 -0.18241149  0.21272108  0.64995813]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 4654 is [True, False, False, False, False, True]
Current timestep = 4655. State = [[-0.26543376  0.1618641 ]]. Action = [[ 0.11124808  0.23673916 -0.18527879  0.64432895]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 4655 is [True, False, False, False, False, True]
Current timestep = 4656. State = [[-0.2665909   0.16205509]]. Action = [[ 0.11408415 -0.05095074 -0.06766334  0.436085  ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 4656 is [True, False, False, False, False, True]
Current timestep = 4657. State = [[-0.26654834  0.16198927]]. Action = [[ 0.22062156  0.08495283  0.10993215 -0.7212474 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 4657 is [True, False, False, False, False, True]
Scene graph at timestep 4657 is [True, False, False, False, False, True]
State prediction error at timestep 4657 is tensor(2.3898e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4658. State = [[-0.2665534   0.16204046]]. Action = [[-0.01135889  0.22464195  0.20749015  0.9766128 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 4658 is [True, False, False, False, False, True]
Current timestep = 4659. State = [[-0.26704827  0.16324589]]. Action = [[-0.08342431 -0.19481333  0.05155906  0.01358628]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 4659 is [True, False, False, False, False, True]
Current timestep = 4660. State = [[-0.2669699   0.16320938]]. Action = [[-0.0374798  -0.17541341 -0.12162578  0.1969347 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 4660 is [True, False, False, False, False, True]
Scene graph at timestep 4660 is [True, False, False, False, False, True]
State prediction error at timestep 4660 is tensor(1.4288e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4661. State = [[-0.2665969   0.16190533]]. Action = [[-0.15285045  0.06417328 -0.05281544  0.3835268 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 4661 is [True, False, False, False, False, True]
Scene graph at timestep 4661 is [True, False, False, False, False, True]
State prediction error at timestep 4661 is tensor(2.9966e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4662. State = [[-0.26689097  0.16205488]]. Action = [[-0.19336805  0.15158683 -0.14240424  0.49827003]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 4662 is [True, False, False, False, False, True]
Current timestep = 4663. State = [[-0.26900724  0.1638213 ]]. Action = [[-0.02006105  0.1384033  -0.02197306 -0.9880901 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 4663 is [True, False, False, False, False, True]
Scene graph at timestep 4663 is [True, False, False, False, False, True]
State prediction error at timestep 4663 is tensor(8.6744e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4664. State = [[-0.27136403  0.16682135]]. Action = [[ 0.1404987  -0.18909171  0.12350702 -0.55812824]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 4664 is [True, False, False, False, False, True]
Current timestep = 4665. State = [[-0.27131736  0.16612695]]. Action = [[-0.1864795  -0.08974686 -0.21425189  0.6213846 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 4665 is [True, False, False, False, False, True]
Scene graph at timestep 4665 is [True, False, False, False, False, True]
State prediction error at timestep 4665 is tensor(9.6320e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4666. State = [[-0.27135348  0.16575316]]. Action = [[ 0.07421157 -0.15442489 -0.23084553  0.44013023]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 4666 is [True, False, False, False, False, True]
Current timestep = 4667. State = [[-0.2708165  0.1639832]]. Action = [[-0.21705161 -0.06451347  0.18611148 -0.7986511 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 4667 is [True, False, False, False, False, True]
Current timestep = 4668. State = [[-0.2704662   0.16211714]]. Action = [[ 0.10009816 -0.13356186 -0.17402019 -0.58549786]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 4668 is [True, False, False, False, False, True]
Scene graph at timestep 4668 is [True, False, False, False, False, True]
State prediction error at timestep 4668 is tensor(8.7049e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4669. State = [[-0.2697725   0.15941763]]. Action = [[ 0.02691707  0.16050005 -0.03175318 -0.92088825]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 4669 is [True, False, False, False, False, True]
Scene graph at timestep 4669 is [True, False, False, False, False, True]
State prediction error at timestep 4669 is tensor(6.8712e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4670. State = [[-0.26975298  0.1591458 ]]. Action = [[ 0.04260868  0.11520976 -0.07905021 -0.6800966 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 4670 is [True, False, False, False, False, True]
Current timestep = 4671. State = [[-0.2697188   0.15910432]]. Action = [[ 0.00421795 -0.14384007 -0.22815752 -0.95477444]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 4671 is [True, False, False, False, False, True]
Current timestep = 4672. State = [[-0.26959768  0.15873608]]. Action = [[-0.11440751  0.05922127 -0.19942074  0.3766135 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 4672 is [True, False, False, False, False, True]
Current timestep = 4673. State = [[-0.26959768  0.15873608]]. Action = [[-0.19586149 -0.20559019  0.13550228 -0.69992965]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 4673 is [True, False, False, False, False, True]
Scene graph at timestep 4673 is [True, False, False, False, False, True]
State prediction error at timestep 4673 is tensor(3.3234e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4674. State = [[-0.26962492  0.15873067]]. Action = [[ 0.08622703 -0.05874416 -0.01966971 -0.18488997]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 4674 is [True, False, False, False, False, True]
Current timestep = 4675. State = [[-0.26956254  0.15848169]]. Action = [[-0.2116135  -0.10037631 -0.17118776  0.5274172 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 4675 is [True, False, False, False, False, True]
Current timestep = 4676. State = [[-0.26949924  0.15822847]]. Action = [[ 0.05649245 -0.05365866  0.04504097  0.6640502 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 4676 is [True, False, False, False, False, True]
Scene graph at timestep 4676 is [True, False, False, False, False, True]
State prediction error at timestep 4676 is tensor(5.1349e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4677. State = [[-0.26933238  0.15767337]]. Action = [[-0.00778729  0.16855332 -0.19560203 -0.7693304 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 4677 is [True, False, False, False, False, True]
Scene graph at timestep 4677 is [True, False, False, False, False, True]
State prediction error at timestep 4677 is tensor(9.9590e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4678. State = [[-0.26942208  0.15783727]]. Action = [[-0.14623396 -0.14530171 -0.08099362  0.01715684]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 4678 is [True, False, False, False, False, True]
Current timestep = 4679. State = [[-0.26940656  0.15777491]]. Action = [[-0.22595733  0.04599878 -0.18142746  0.94251764]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 4679 is [True, False, False, False, False, True]
Current timestep = 4680. State = [[-0.26937932  0.15778027]]. Action = [[0.09454328 0.08735079 0.00911054 0.6638651 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 4680 is [True, False, False, False, False, True]
Current timestep = 4681. State = [[-0.2693226   0.15770955]]. Action = [[ 0.16761038 -0.16414537 -0.03028636  0.7764534 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 4681 is [True, False, False, False, False, True]
Scene graph at timestep 4681 is [True, False, False, False, False, True]
State prediction error at timestep 4681 is tensor(2.0288e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4682. State = [[-0.2685508   0.15633245]]. Action = [[ 0.04375041  0.01474988  0.04908356 -0.06249362]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 4682 is [True, False, False, False, False, True]
Current timestep = 4683. State = [[-0.26820704  0.1559544 ]]. Action = [[ 0.12471533  0.16859454 -0.1358312   0.1897589 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 4683 is [True, False, False, False, False, True]
Current timestep = 4684. State = [[-0.26775858  0.15604776]]. Action = [[ 0.10435712 -0.23633613 -0.17866027 -0.7997216 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 4684 is [True, False, False, False, False, True]
Current timestep = 4685. State = [[-0.26630077  0.15469803]]. Action = [[-0.05875061  0.11241472  0.01448888  0.5929358 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 4685 is [True, False, False, False, False, True]
Current timestep = 4686. State = [[-0.26583263  0.15486667]]. Action = [[-0.06435338 -0.24717636  0.11626649 -0.80840236]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 4686 is [True, False, False, False, False, True]
Current timestep = 4687. State = [[-0.264923    0.15274152]]. Action = [[-0.19511282  0.11677277  0.07528138  0.21476829]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 4687 is [True, False, False, False, False, True]
Scene graph at timestep 4687 is [True, False, False, False, False, True]
State prediction error at timestep 4687 is tensor(1.2747e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4688. State = [[-0.26519647  0.15265231]]. Action = [[-0.08153963  0.14337945  0.07600465 -0.4177118 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 4688 is [True, False, False, False, False, True]
Current timestep = 4689. State = [[-0.26604     0.15392491]]. Action = [[-0.22550958  0.01707816 -0.13367295  0.39795554]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 4689 is [True, False, False, False, False, True]
Current timestep = 4690. State = [[-0.26765665  0.1556317 ]]. Action = [[-0.02193224  0.19867343  0.09404388  0.3985219 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 4690 is [True, False, False, False, False, True]
Current timestep = 4691. State = [[-0.26943976  0.15836057]]. Action = [[ 0.06132889 -0.24430466 -0.15561771 -0.35315782]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 4691 is [True, False, False, False, False, True]
Current timestep = 4692. State = [[-0.26967698  0.1577429 ]]. Action = [[ 0.18315786 -0.14312881  0.08086789  0.57109344]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 4692 is [True, False, False, False, False, True]
Current timestep = 4693. State = [[-0.2688187   0.15565392]]. Action = [[-0.10313004 -0.12639661  0.09539843  0.19756663]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 4693 is [True, False, False, False, False, True]
Scene graph at timestep 4693 is [True, False, False, False, False, True]
State prediction error at timestep 4693 is tensor(3.5541e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4694. State = [[-0.26847506  0.15287797]]. Action = [[ 0.0682416   0.14684677 -0.19471517  0.9698942 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 4694 is [True, False, False, False, False, True]
Scene graph at timestep 4694 is [True, False, False, False, False, True]
State prediction error at timestep 4694 is tensor(1.2063e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4695. State = [[-0.26846847  0.15246207]]. Action = [[-0.15970688 -0.02147958 -0.20672345 -0.3659767 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 4695 is [True, False, False, False, False, True]
Current timestep = 4696. State = [[-0.26856917  0.15219812]]. Action = [[ 0.04274511 -0.09054342  0.05969509 -0.09081876]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 4696 is [True, False, False, False, False, True]
Scene graph at timestep 4696 is [True, False, False, False, False, True]
State prediction error at timestep 4696 is tensor(6.4315e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4697. State = [[-0.26864275  0.15116665]]. Action = [[-0.17800434  0.18407017  0.13064575  0.3135574 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 4697 is [True, False, False, False, False, True]
Current timestep = 4698. State = [[-0.2703353   0.15229239]]. Action = [[0.08411843 0.13902265 0.05864084 0.259871  ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 4698 is [True, False, False, False, False, True]
Current timestep = 4699. State = [[-0.27142614  0.1538316 ]]. Action = [[-0.20059472 -0.14884917 -0.23784514  0.9058279 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 4699 is [True, False, False, False, False, True]
Current timestep = 4700. State = [[-0.27220467  0.15490295]]. Action = [[ 0.1392721  -0.04298797 -0.08128086  0.26729298]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 4700 is [True, False, False, False, False, True]
Scene graph at timestep 4700 is [True, False, False, False, False, True]
State prediction error at timestep 4700 is tensor(9.4315e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4701. State = [[-0.2722276   0.15495199]]. Action = [[-0.08015728 -0.12350059  0.11847576 -0.6206989 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 4701 is [True, False, False, False, False, True]
Scene graph at timestep 4701 is [True, False, False, False, False, True]
State prediction error at timestep 4701 is tensor(1.8619e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4702. State = [[-0.27212092  0.15451625]]. Action = [[-0.02578942 -0.08360741 -0.2043255  -0.7647144 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 4702 is [True, False, False, False, False, True]
Current timestep = 4703. State = [[-0.271881    0.15337817]]. Action = [[-0.14397702  0.15649104 -0.20968032 -0.22514737]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 4703 is [True, False, False, False, False, True]
Current timestep = 4704. State = [[-0.27176076  0.15287991]]. Action = [[-0.1952121  -0.15537666  0.17650455 -0.87681216]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 4704 is [True, False, False, False, False, True]
Current timestep = 4705. State = [[-0.27165598  0.15244392]]. Action = [[-0.19398569 -0.12308618 -0.0526479   0.37182188]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 4705 is [True, False, False, False, False, True]
Current timestep = 4706. State = [[-0.2716667   0.15209748]]. Action = [[-0.04115134 -0.10725184 -0.08588839 -0.45798802]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 4706 is [True, False, False, False, False, True]
Scene graph at timestep 4706 is [True, False, False, False, False, True]
State prediction error at timestep 4706 is tensor(2.6108e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4707. State = [[-0.2719216  0.1509265]]. Action = [[-0.07825029 -0.00662671  0.0795249   0.90157247]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 4707 is [True, False, False, False, False, True]
Current timestep = 4708. State = [[-0.2724354   0.15012954]]. Action = [[ 0.00455937 -0.0618131   0.05984333  0.87625265]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 4708 is [True, False, False, False, False, True]
Scene graph at timestep 4708 is [True, False, False, False, False, True]
State prediction error at timestep 4708 is tensor(1.6475e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4709. State = [[-0.27289793  0.1487802 ]]. Action = [[-0.0076175  -0.24301809 -0.19392924 -0.6830987 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 4709 is [True, False, False, False, False, True]
Current timestep = 4710. State = [[-0.27318135  0.1453517 ]]. Action = [[-0.01109512  0.09368461  0.12866259 -0.1867739 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 4710 is [True, False, False, False, False, True]
Scene graph at timestep 4710 is [True, False, False, False, False, True]
State prediction error at timestep 4710 is tensor(1.2470e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4711. State = [[-0.27333382  0.14332405]]. Action = [[-0.15054531 -0.16955599  0.08983737  0.8487346 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 4711 is [True, False, False, False, False, True]
Scene graph at timestep 4711 is [True, False, False, False, False, True]
State prediction error at timestep 4711 is tensor(3.1252e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4712. State = [[-0.27349728  0.14188807]]. Action = [[ 0.20041299 -0.08665121  0.06575286 -0.90034586]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 4712 is [True, False, False, False, False, True]
Current timestep = 4713. State = [[-0.27299526  0.14021021]]. Action = [[ 0.04048005  0.18361282 -0.20271862  0.97126555]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 4713 is [True, False, False, False, False, True]
Scene graph at timestep 4713 is [True, False, False, False, False, True]
State prediction error at timestep 4713 is tensor(4.3437e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4714. State = [[-0.27313778  0.14020751]]. Action = [[-0.14863813  0.05293229  0.20034868  0.55332494]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 4714 is [True, False, False, False, False, True]
Current timestep = 4715. State = [[-0.27315125  0.14026971]]. Action = [[-0.17786334 -0.03267297  0.12323684  0.6611581 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 4715 is [True, False, False, False, False, True]
Current timestep = 4716. State = [[-0.27296004  0.14008936]]. Action = [[ 0.16458175 -0.15206863  0.01768604  0.09373474]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 4716 is [True, False, False, False, False, True]
Current timestep = 4717. State = [[-0.2721675   0.13856706]]. Action = [[ 0.19901648 -0.23629287  0.13966644 -0.28136098]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 4717 is [True, False, False, False, False, True]
Current timestep = 4718. State = [[-0.26962602  0.13500452]]. Action = [[ 0.14020836  0.08916169 -0.05152194 -0.36089003]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 4718 is [True, False, False, False, False, True]
Scene graph at timestep 4718 is [True, False, False, False, False, True]
State prediction error at timestep 4718 is tensor(4.3910e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4719. State = [[-0.2677232  0.1339045]]. Action = [[ 0.09028143 -0.09158525 -0.08947802  0.8365607 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 4719 is [True, False, False, False, False, True]
Current timestep = 4720. State = [[-0.26515782  0.13227159]]. Action = [[ 0.1628713   0.15926248 -0.11927864 -0.6767105 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 4720 is [True, False, False, False, False, True]
Scene graph at timestep 4720 is [True, False, False, False, False, True]
State prediction error at timestep 4720 is tensor(3.0328e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4721. State = [[-0.2624218  0.1323241]]. Action = [[ 0.19269884 -0.24537525 -0.18688881 -0.7530805 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 4721 is [True, False, False, False, False, True]
Current timestep = 4722. State = [[-0.2582173   0.12929834]]. Action = [[-0.12057348  0.20395559 -0.04793547  0.34089816]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 4722 is [True, False, False, False, False, True]
Current timestep = 4723. State = [[-0.25682908  0.12960207]]. Action = [[-0.03183106 -0.10524228  0.06265211  0.46592605]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 4723 is [True, False, False, False, False, True]
Scene graph at timestep 4723 is [True, False, False, False, False, True]
State prediction error at timestep 4723 is tensor(1.2521e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4724. State = [[-0.25615487  0.12927164]]. Action = [[ 0.01756421  0.11980423 -0.04219453 -0.36977494]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 4724 is [True, False, False, False, False, True]
Scene graph at timestep 4724 is [True, False, False, False, False, True]
State prediction error at timestep 4724 is tensor(7.1045e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4725. State = [[-0.25591996  0.12932861]]. Action = [[-0.20461528  0.19275504  0.09130439 -0.79415214]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 4725 is [True, False, False, False, False, True]
Current timestep = 4726. State = [[-0.25682107  0.13125981]]. Action = [[-0.20929746 -0.21520892 -0.20433488  0.35064733]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 4726 is [True, False, False, False, False, True]
Current timestep = 4727. State = [[-0.25685132  0.13146666]]. Action = [[ 0.10481605  0.130505    0.24471056 -0.5398999 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 4727 is [True, False, False, False, False, True]
Current timestep = 4728. State = [[-0.2570583   0.13178986]]. Action = [[ 0.10701352  0.18255574 -0.15205261 -0.8836948 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 4728 is [True, False, False, False, False, True]
Scene graph at timestep 4728 is [True, False, False, False, False, True]
State prediction error at timestep 4728 is tensor(7.9584e-08, grad_fn=<MseLossBackward0>)
Current timestep = 4729. State = [[-0.25763226  0.13366061]]. Action = [[ 0.14319074  0.19234094 -0.19311447  0.867991  ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 4729 is [True, False, False, False, False, True]
Current timestep = 4730. State = [[-0.25735828  0.13725688]]. Action = [[-0.16103214  0.11736017  0.17255807  0.92361164]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 4730 is [True, False, False, False, False, True]
Scene graph at timestep 4730 is [True, False, False, False, False, True]
State prediction error at timestep 4730 is tensor(9.6784e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4731. State = [[-0.25800225  0.14091998]]. Action = [[-0.0343495   0.14099944  0.18185073  0.5621662 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 4731 is [True, False, False, False, False, True]
Current timestep = 4732. State = [[-0.25907555  0.14477208]]. Action = [[ 0.21100104  0.15777493  0.03983369 -0.8152008 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 4732 is [True, False, False, False, False, True]
Current timestep = 4733. State = [[-0.25753352  0.14864594]]. Action = [[-0.1231969  -0.23034339  0.01876786  0.07019937]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 4733 is [True, False, False, False, False, True]
Current timestep = 4734. State = [[-0.25663733  0.14951107]]. Action = [[-0.1736374   0.14613801  0.12454185 -0.9800665 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 4734 is [True, False, False, False, False, True]
Current timestep = 4735. State = [[-0.25778806  0.15163109]]. Action = [[-0.0211968   0.05528453  0.12634122 -0.06115413]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 4735 is [True, False, False, False, False, True]
Current timestep = 4736. State = [[-0.25909978  0.1537493 ]]. Action = [[-0.11091653 -0.13876776 -0.02394481  0.39678574]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 4736 is [True, False, False, False, False, True]
Scene graph at timestep 4736 is [True, False, False, False, False, True]
State prediction error at timestep 4736 is tensor(1.1516e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4736 of -1
Current timestep = 4737. State = [[-0.25942641  0.15419035]]. Action = [[ 0.06290936 -0.15263653 -0.09996966 -0.13656473]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 4737 is [True, False, False, False, False, True]
Current timestep = 4738. State = [[-0.2592724  0.1534444]]. Action = [[-0.06963274 -0.20238066 -0.17975491  0.31868124]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 4738 is [True, False, False, False, False, True]
Current timestep = 4739. State = [[-0.25876725  0.15136681]]. Action = [[ 0.01857033  0.17856121 -0.08149067 -0.8461636 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 4739 is [True, False, False, False, False, True]
Scene graph at timestep 4739 is [True, False, False, False, False, True]
State prediction error at timestep 4739 is tensor(1.2560e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4740. State = [[-0.2588767   0.15148883]]. Action = [[-0.04721849  0.03400096 -0.15883262 -0.716712  ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 4740 is [True, False, False, False, False, True]
Current timestep = 4741. State = [[-0.25917396  0.15201639]]. Action = [[-0.11019292  0.2174561  -0.19886093  0.33060384]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 4741 is [True, False, False, False, False, True]
Current timestep = 4742. State = [[-0.26081082  0.15443872]]. Action = [[-0.22318529 -0.06141943 -0.13921814 -0.84319174]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 4742 is [True, False, False, False, False, True]
Scene graph at timestep 4742 is [True, False, False, False, False, True]
State prediction error at timestep 4742 is tensor(1.1455e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4743. State = [[-0.262877    0.15684892]]. Action = [[-0.09698075  0.15091532  0.17325923 -0.41064292]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 4743 is [True, False, False, False, False, True]
Current timestep = 4744. State = [[-0.2653491   0.16030939]]. Action = [[-0.08509514  0.01972944 -0.09763554 -0.18722445]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 4744 is [True, False, False, False, False, True]
Current timestep = 4745. State = [[-0.26776177  0.16308995]]. Action = [[ 0.12960646  0.051759    0.0825839  -0.941134  ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 4745 is [True, False, False, False, False, True]
Current timestep = 4746. State = [[-0.26882327  0.16458398]]. Action = [[-0.01250112 -0.14033589 -0.16123873 -0.7017995 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 4746 is [True, False, False, False, False, True]
Scene graph at timestep 4746 is [True, False, False, False, False, True]
State prediction error at timestep 4746 is tensor(1.9301e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4747. State = [[-0.26899287  0.16448802]]. Action = [[-0.16587006  0.03544176  0.11941412  0.5455563 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 4747 is [True, False, False, False, False, True]
Current timestep = 4748. State = [[-0.2700508   0.16513754]]. Action = [[-0.15371598  0.16070962  0.10552716  0.97402   ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 4748 is [True, False, False, False, False, True]
Current timestep = 4749. State = [[-0.27241808  0.16774572]]. Action = [[-0.19908601  0.13192132  0.05714893 -0.08261234]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 4749 is [True, False, False, False, False, True]
Current timestep = 4750. State = [[-0.27434734  0.16935286]]. Action = [[ 0.11500362 -0.11002383 -0.15548448 -0.25282848]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 4750 is [True, False, False, False, False, True]
Scene graph at timestep 4750 is [True, False, False, False, False, True]
State prediction error at timestep 4750 is tensor(8.0778e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4751. State = [[-0.2746074  0.1691723]]. Action = [[-0.15015881  0.11119133  0.15483755  0.8571739 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 4751 is [True, False, False, False, False, True]
Scene graph at timestep 4751 is [True, False, False, False, False, True]
State prediction error at timestep 4751 is tensor(1.4483e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4752. State = [[-0.2747298   0.16887903]]. Action = [[-0.09191707  0.013282   -0.0871091   0.18541491]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 4752 is [True, False, False, False, False, True]
Current timestep = 4753. State = [[-0.27470496  0.16868708]]. Action = [[ 0.0581044  -0.0596078  -0.14738593 -0.9765118 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 4753 is [True, False, False, False, False, True]
Current timestep = 4754. State = [[-0.2744528   0.16802648]]. Action = [[-0.1279822  -0.24926895 -0.13348871 -0.88505566]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 4754 is [True, False, False, False, False, True]
Human Feedback received at timestep 4754 of -1
Current timestep = 4755. State = [[-0.27438417  0.16763006]]. Action = [[ 0.10133317  0.1777148  -0.04017204  0.8780408 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 4755 is [True, False, False, False, False, True]
Scene graph at timestep 4755 is [True, False, False, False, False, True]
State prediction error at timestep 4755 is tensor(1.4531e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4756. State = [[-0.2744565  0.1678323]]. Action = [[0.1257711  0.2332043  0.1713708  0.85479176]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 4756 is [True, False, False, False, False, True]
Current timestep = 4757. State = [[-0.27527577  0.16935764]]. Action = [[-0.04084569  0.10830849 -0.05667949 -0.2665525 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 4757 is [True, False, False, False, False, True]
Current timestep = 4758. State = [[-0.27610448  0.17143954]]. Action = [[ 0.16905487  0.14930153 -0.13672857 -0.39443576]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 4758 is [True, False, False, False, False, True]
Current timestep = 4759. State = [[-0.27526948  0.1742268 ]]. Action = [[-0.22772461  0.2473489  -0.138545   -0.66830224]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 4759 is [True, False, False, False, False, True]
Scene graph at timestep 4759 is [True, False, False, False, False, True]
State prediction error at timestep 4759 is tensor(2.4635e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4760. State = [[-0.2749263   0.17645659]]. Action = [[-0.21604905  0.21319968  0.09626758  0.98605514]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 4760 is [True, False, False, False, False, True]
Current timestep = 4761. State = [[-0.27501404  0.17754316]]. Action = [[ 0.1273669   0.23687115 -0.16378509 -0.1968956 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 4761 is [True, False, False, False, False, True]
Current timestep = 4762. State = [[-0.27427498  0.1810215 ]]. Action = [[0.05208528 0.12498778 0.03205931 0.80753946]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 4762 is [True, False, False, False, False, True]
Current timestep = 4763. State = [[-0.27365148  0.18505932]]. Action = [[ 0.07912928 -0.01319514 -0.14791846  0.39829314]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 4763 is [True, False, False, False, False, True]
Current timestep = 4764. State = [[-0.2727965  0.187441 ]]. Action = [[-0.14470077 -0.09223267 -0.04932542 -0.3373797 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 4764 is [True, False, False, False, False, True]
Current timestep = 4765. State = [[-0.27208707  0.18894519]]. Action = [[ 0.09897587  0.2339237   0.03451875 -0.9272469 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 4765 is [True, False, False, False, False, True]
Human Feedback received at timestep 4765 of -1
Current timestep = 4766. State = [[-0.27054897  0.19238377]]. Action = [[ 0.07580787  0.06733698 -0.08960895 -0.10863715]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 4766 is [True, False, False, False, False, True]
Current timestep = 4767. State = [[-0.26741454  0.19595923]]. Action = [[-0.10161206  0.2417163   0.00359383  0.24603319]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 4767 is [True, False, False, False, False, True]
Scene graph at timestep 4767 is [True, False, False, False, False, True]
State prediction error at timestep 4767 is tensor(8.6741e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4768. State = [[-0.2661813   0.20207681]]. Action = [[-0.01491544  0.01926941 -0.19573583 -0.95717454]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 4768 is [True, False, False, False, False, True]
Current timestep = 4769. State = [[-0.26560947  0.2058066 ]]. Action = [[-0.09547189 -0.09951591  0.13045752 -0.35407186]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 4769 is [True, False, False, False, False, True]
Current timestep = 4770. State = [[-0.26560777  0.20752221]]. Action = [[ 0.1541791  -0.16372482 -0.17003985 -0.00302345]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 4770 is [True, False, False, False, False, True]
Scene graph at timestep 4770 is [True, False, False, False, False, True]
State prediction error at timestep 4770 is tensor(1.8899e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4771. State = [[-0.26495862  0.20747796]]. Action = [[-0.06654441  0.23554862 -0.05327103 -0.10189366]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 4771 is [True, False, False, False, False, True]
Human Feedback received at timestep 4771 of -1
Current timestep = 4772. State = [[-0.26496017  0.20900273]]. Action = [[-0.02993111 -0.17269537 -0.11852756 -0.4484231 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 4772 is [True, False, False, False, False, True]
Current timestep = 4773. State = [[-0.2649976   0.20877609]]. Action = [[-0.11822158 -0.22461538 -0.09898722  0.82311714]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 4773 is [True, False, False, False, False, True]
Scene graph at timestep 4773 is [True, False, False, False, False, True]
State prediction error at timestep 4773 is tensor(1.3867e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4774. State = [[-0.2642529   0.20686062]]. Action = [[ 0.19025469 -0.21086517  0.13177884  0.31815362]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 4774 is [True, False, False, False, False, True]
Current timestep = 4775. State = [[-0.26275307  0.20347929]]. Action = [[ 0.0432739   0.20733139 -0.01824    -0.97904575]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 4775 is [True, False, False, False, False, True]
Current timestep = 4776. State = [[-0.2626655   0.20345077]]. Action = [[ 0.15958053  0.21751702 -0.20896997 -0.3514216 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 4776 is [True, False, False, False, False, True]
Current timestep = 4777. State = [[-0.2614232  0.2046192]]. Action = [[ 0.07716131 -0.09075642 -0.08748758 -0.84445554]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 4777 is [True, False, False, False, False, True]
Current timestep = 4778. State = [[-0.25988445  0.20532967]]. Action = [[-0.02576134  0.11568183 -0.19366671 -0.00220811]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 4778 is [True, False, False, False, False, True]
Scene graph at timestep 4778 is [True, False, False, False, False, True]
State prediction error at timestep 4778 is tensor(8.3696e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4779. State = [[-0.25865337  0.20614663]]. Action = [[-0.12151292 -0.05377403 -0.20075786  0.4642179 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 4779 is [True, False, False, False, False, True]
Scene graph at timestep 4779 is [True, False, False, False, False, True]
State prediction error at timestep 4779 is tensor(2.3142e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4780. State = [[-0.25885376  0.20611024]]. Action = [[-0.04922479 -0.07295719 -0.012229    0.9694809 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 4780 is [True, False, False, False, False, True]
Scene graph at timestep 4780 is [True, False, False, False, False, True]
State prediction error at timestep 4780 is tensor(1.2878e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4780 of -1
Current timestep = 4781. State = [[-0.25885883  0.20604078]]. Action = [[-0.114765   -0.16759992 -0.11737423 -0.41528797]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 4781 is [True, False, False, False, False, True]
Current timestep = 4782. State = [[-0.25890276  0.20539297]]. Action = [[-0.11547235 -0.03723243 -0.10240877  0.32148004]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 4782 is [True, False, False, False, False, True]
Current timestep = 4783. State = [[-0.25928155  0.20505507]]. Action = [[-0.10460913  0.17240453 -0.05535528 -0.8787402 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 4783 is [True, False, False, False, False, True]
Current timestep = 4784. State = [[-0.260436   0.2060364]]. Action = [[-0.23746847 -0.1513041  -0.13009018 -0.9226183 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 4784 is [True, False, False, False, False, True]
Current timestep = 4785. State = [[-0.26223522  0.20583054]]. Action = [[-0.05740508 -0.02675587 -0.00672124  0.9241178 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 4785 is [True, False, False, False, False, True]
Scene graph at timestep 4785 is [True, False, False, False, False, True]
State prediction error at timestep 4785 is tensor(4.3604e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4786. State = [[-0.26420125  0.2056357 ]]. Action = [[-0.13409035  0.22577268 -0.1571182   0.7384101 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 4786 is [True, False, False, False, False, True]
Scene graph at timestep 4786 is [True, False, False, False, False, True]
State prediction error at timestep 4786 is tensor(4.3461e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4787. State = [[-0.26725736  0.20860119]]. Action = [[ 0.07059276  0.24505419 -0.0700442   0.73369   ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 4787 is [True, False, False, False, False, True]
Current timestep = 4788. State = [[-0.2704653  0.2127386]]. Action = [[ 0.14409381  0.20280415 -0.00713436  0.6667937 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 4788 is [True, False, False, False, False, True]
Current timestep = 4789. State = [[-0.27302045  0.21629356]]. Action = [[ 0.2043888   0.20055327  0.16263595 -0.7224732 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 4789 is [True, False, False, False, False, True]
Current timestep = 4790. State = [[-0.27309996  0.21992952]]. Action = [[-0.1450602   0.07098261  0.01202545 -0.35958624]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 4790 is [True, False, False, False, False, True]
Current timestep = 4791. State = [[-0.27307874  0.22195062]]. Action = [[-0.14110199  0.18517342  0.00433043 -0.9189192 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 4791 is [True, False, False, False, False, True]
Current timestep = 4792. State = [[-0.27321362  0.22310796]]. Action = [[-0.02396329  0.07568198  0.07879061 -0.17830455]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 4792 is [True, False, False, False, False, True]
Current timestep = 4793. State = [[-0.27369767  0.22446361]]. Action = [[-0.05089201 -0.06698662 -0.11527845 -0.49741668]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 4793 is [True, False, False, False, False, True]
Scene graph at timestep 4793 is [True, False, False, False, False, True]
State prediction error at timestep 4793 is tensor(7.1897e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4793 of -1
Current timestep = 4794. State = [[-0.27404457  0.22519335]]. Action = [[-0.05491002  0.22312164 -0.21420036  0.68713784]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 4794 is [True, False, False, False, False, True]
Current timestep = 4795. State = [[-0.27591756  0.22790736]]. Action = [[-0.22824226 -0.03475039  0.10649255 -0.6116941 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 4795 is [True, False, False, False, False, True]
Current timestep = 4796. State = [[-0.27693093  0.22970985]]. Action = [[-0.02965844  0.01571065 -0.05940357 -0.8795595 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 4796 is [True, False, False, False, False, True]
Scene graph at timestep 4796 is [True, False, False, False, False, True]
State prediction error at timestep 4796 is tensor(1.7028e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4796 of -1
Current timestep = 4797. State = [[-0.27792913  0.2311404 ]]. Action = [[-0.14027148  0.21380126 -0.12937936  0.48928487]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 4797 is [True, False, False, False, False, True]
Current timestep = 4798. State = [[-0.27850989  0.23190854]]. Action = [[-0.17434472 -0.07090977  0.02584255 -0.14836115]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 4798 is [True, False, False, False, False, True]
Current timestep = 4799. State = [[-0.2788199  0.2323641]]. Action = [[-0.19950655 -0.20232683 -0.08735254  0.9936173 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 4799 is [True, False, False, False, False, True]
Human Feedback received at timestep 4799 of -1
Current timestep = 4800. State = [[-0.2789888   0.23274319]]. Action = [[ 0.13883144  0.22033149 -0.16095033 -0.28979623]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 4800 is [True, False, False, False, False, True]
Current timestep = 4801. State = [[-0.2782851   0.23471352]]. Action = [[-0.1028648   0.11410797  0.01572901  0.5507133 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 4801 is [True, False, False, False, False, True]
Current timestep = 4802. State = [[-0.27736005  0.23649241]]. Action = [[-0.02655873  0.0775449  -0.05546851 -0.5032725 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 4802 is [True, False, False, False, False, True]
Scene graph at timestep 4802 is [True, False, False, False, False, True]
State prediction error at timestep 4802 is tensor(1.2556e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4802 of -1
Current timestep = 4803. State = [[-0.27633289  0.23802505]]. Action = [[-0.1820328   0.19388556  0.10003096 -0.4679979 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 4803 is [True, False, False, False, False, True]
Current timestep = 4804. State = [[-0.2756077   0.23885684]]. Action = [[ 0.13868055  0.12994593 -0.12991576 -0.71214926]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 4804 is [True, False, False, False, False, True]
Current timestep = 4805. State = [[-0.2733669   0.24089508]]. Action = [[-0.19643608  0.17772746 -0.10636808 -0.00973368]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 4805 is [True, False, False, False, False, True]
Current timestep = 4806. State = [[-0.271264    0.24294893]]. Action = [[-0.2021854  -0.10654598  0.03674549 -0.85348994]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 4806 is [True, False, False, False, False, True]
Current timestep = 4807. State = [[-0.26950708  0.2443907 ]]. Action = [[-0.10014227 -0.17416255  0.09773764  0.6576822 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 4807 is [True, False, False, False, False, True]
Current timestep = 4808. State = [[-0.2694134  0.244221 ]]. Action = [[ 0.02052116 -0.13577828  0.16584784  0.696146  ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 4808 is [True, False, False, False, False, True]
Human Feedback received at timestep 4808 of -1
Current timestep = 4809. State = [[-0.26917142  0.24309367]]. Action = [[-0.17689368 -0.07910898  0.07530367  0.2920866 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 4809 is [True, False, False, False, False, True]
Current timestep = 4810. State = [[-0.26922858  0.24217092]]. Action = [[-0.07421561  0.09963182 -0.20043597  0.9141605 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 4810 is [True, False, False, False, False, True]
Current timestep = 4811. State = [[-0.26968664  0.24248874]]. Action = [[-0.00534178  0.16471976  0.03297645  0.8013494 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 4811 is [True, False, False, False, False, True]
Current timestep = 4812. State = [[-0.27131775  0.2442361 ]]. Action = [[-0.13109314  0.24064836 -0.11066164  0.97998583]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 4812 is [True, False, False, False, False, True]
Current timestep = 4813. State = [[-0.2749482   0.24829178]]. Action = [[-0.15240479 -0.23850454 -0.14959131  0.75638556]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 4813 is [True, False, False, False, False, True]
Current timestep = 4814. State = [[-0.27755654  0.25103986]]. Action = [[-0.1201185   0.10573986 -0.0133152   0.71169305]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 4814 is [True, False, False, False, False, True]
Current timestep = 4815. State = [[-0.27923974  0.2527741 ]]. Action = [[-0.00444379  0.04677975 -0.1373414   0.97076786]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 4815 is [True, False, False, False, False, True]
Human Feedback received at timestep 4815 of -1
Current timestep = 4816. State = [[-0.28011075  0.2536989 ]]. Action = [[-0.16271694 -0.08365163 -0.04122424  0.9995729 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 4816 is [True, False, False, False, False, True]
Scene graph at timestep 4816 is [True, False, False, False, False, True]
State prediction error at timestep 4816 is tensor(5.6546e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4817. State = [[-0.28048447  0.25409448]]. Action = [[-0.03267945  0.24500534 -0.16662881  0.7765144 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 4817 is [True, False, False, False, False, True]
Scene graph at timestep 4817 is [True, False, False, False, False, True]
State prediction error at timestep 4817 is tensor(9.3287e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4818. State = [[-0.28080392  0.25436115]]. Action = [[-0.03970042  0.18911076 -0.14090095 -0.9568776 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 4818 is [True, False, False, False, False, True]
Human Feedback received at timestep 4818 of -1
Current timestep = 4819. State = [[-0.28087205  0.25447196]]. Action = [[-0.184125   -0.06279239 -0.15605389 -0.7256881 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 4819 is [True, False, False, False, False, True]
Current timestep = 4820. State = [[-0.28092864  0.25449285]]. Action = [[-0.22746263 -0.04375966 -0.23248126  0.22905266]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 4820 is [True, False, False, False, False, True]
Current timestep = 4821. State = [[-0.28103867  0.25464776]]. Action = [[-0.20297049  0.23780239  0.0454897  -0.8854476 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 4821 is [True, False, False, False, False, True]
Current timestep = 4822. State = [[-0.28105384  0.2546249 ]]. Action = [[-0.05230963  0.19768488 -0.23832966  0.68914926]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 4822 is [True, False, False, False, False, True]
Scene graph at timestep 4822 is [True, False, False, False, False, True]
State prediction error at timestep 4822 is tensor(6.1655e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4823. State = [[-0.2810953   0.25466865]]. Action = [[-0.16200936 -0.15774806 -0.00390351 -0.8470607 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 4823 is [True, False, False, False, False, True]
Scene graph at timestep 4823 is [True, False, False, False, False, True]
State prediction error at timestep 4823 is tensor(7.8496e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4824. State = [[-0.2810953   0.25466865]]. Action = [[-0.10461521  0.19429201 -0.20703456  0.9098947 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 4824 is [True, False, False, False, False, True]
Current timestep = 4825. State = [[-0.28113678  0.25471237]]. Action = [[-0.1748415   0.05160269 -0.06325899 -0.9249256 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 4825 is [True, False, False, False, False, True]
Current timestep = 4826. State = [[-0.28113678  0.25471237]]. Action = [[-0.06054127 -0.18437801 -0.13771491  0.96237874]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 4826 is [True, False, False, False, False, True]
Current timestep = 4827. State = [[-0.28113678  0.25471237]]. Action = [[-0.24363104  0.16254678 -0.18673484 -0.5126995 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 4827 is [True, False, False, False, False, True]
Current timestep = 4828. State = [[-0.28113678  0.25471237]]. Action = [[-0.12736896  0.19647473  0.12093616 -0.63820887]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 4828 is [True, False, False, False, False, True]
Current timestep = 4829. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.20636955 -0.00508668  0.01615128  0.9633695 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 4829 is [True, False, False, False, False, True]
Current timestep = 4830. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.18222837 -0.02969524  0.17278084  0.45347047]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 4830 is [True, False, False, False, False, True]
Current timestep = 4831. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.04647781  0.2315562   0.00320542  0.67707515]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 4831 is [True, False, False, False, False, True]
Current timestep = 4832. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.01324998 -0.05180794  0.13272691 -0.46540654]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 4832 is [True, False, False, False, False, True]
Current timestep = 4833. State = [[-0.28117865  0.2547565 ]]. Action = [[ 0.01392195 -0.12090775 -0.10955662 -0.8611091 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 4833 is [True, False, False, False, False, True]
Current timestep = 4834. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.08970514  0.09196198 -0.17938435  0.15097451]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 4834 is [True, False, False, False, False, True]
Scene graph at timestep 4834 is [True, False, False, False, False, True]
State prediction error at timestep 4834 is tensor(5.1769e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4835. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.10263652  0.20098478  0.06290895  0.9496659 ]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 4835 is [True, False, False, False, False, True]
Scene graph at timestep 4835 is [True, False, False, False, False, True]
State prediction error at timestep 4835 is tensor(2.1172e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4836. State = [[-0.28117865  0.2547565 ]]. Action = [[ 0.01274687  0.16354847  0.19763339 -0.9976416 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 4836 is [True, False, False, False, False, True]
Scene graph at timestep 4836 is [True, False, False, False, False, True]
State prediction error at timestep 4836 is tensor(4.4495e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4837. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.04735483  0.02935493 -0.21456179  0.95152044]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 4837 is [True, False, False, False, False, True]
Current timestep = 4838. State = [[-0.28117865  0.2547565 ]]. Action = [[-0.04268083 -0.05637392  0.15156347  0.6538718 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 4838 is [True, False, False, False, False, True]
Scene graph at timestep 4838 is [True, False, False, False, False, True]
State prediction error at timestep 4838 is tensor(1.1993e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4839. State = [[-0.28117865  0.2547565 ]]. Action = [[ 0.01812282  0.20942175 -0.20614776  0.79908776]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 4839 is [True, False, False, False, False, True]
Current timestep = 4840. State = [[-0.28117865  0.2547565 ]]. Action = [[ 0.14988685  0.04137543 -0.15216982  0.20650518]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 4840 is [True, False, False, False, False, True]
Current timestep = 4841. State = [[-0.2811635   0.25477937]]. Action = [[-0.04811719  0.23402855 -0.20523484 -0.18314779]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 4841 is [True, False, False, False, False, True]
Current timestep = 4842. State = [[-0.2807043  0.255065 ]]. Action = [[-0.21329787  0.16771641  0.1611445   0.28276348]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 4842 is [True, False, False, False, False, True]
Human Feedback received at timestep 4842 of -1
Current timestep = 4843. State = [[-0.2803308   0.25529915]]. Action = [[ 0.00251502  0.02541447 -0.19335909 -0.06735271]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 4843 is [True, False, False, False, False, True]
Current timestep = 4844. State = [[-0.28010976  0.25544977]]. Action = [[-0.0491257  -0.21395668 -0.10427313 -0.9150641 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 4844 is [True, False, False, False, False, True]
Scene graph at timestep 4844 is [True, False, False, False, False, True]
State prediction error at timestep 4844 is tensor(2.2809e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4845. State = [[-0.28012383  0.25542843]]. Action = [[-0.04974121  0.18018737 -0.09959988  0.53753877]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 4845 is [True, False, False, False, False, True]
Current timestep = 4846. State = [[-0.27999476  0.2555172 ]]. Action = [[-0.09237984  0.1947349  -0.12416336 -0.9235825 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 4846 is [True, False, False, False, False, True]
Current timestep = 4847. State = [[-0.2800098  0.2554944]]. Action = [[-0.04925969  0.10347545 -0.05603263  0.4980601 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 4847 is [True, False, False, False, False, True]
Current timestep = 4848. State = [[-0.27990732  0.25560516]]. Action = [[ 0.12172788  0.24361688 -0.19888206  0.47470045]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 4848 is [True, False, False, False, False, True]
Current timestep = 4849. State = [[-0.27886128  0.2574303 ]]. Action = [[-0.13557945  0.16251302  0.04168269  0.1135695 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 4849 is [True, False, False, False, False, True]
Current timestep = 4850. State = [[-0.2775482   0.25899464]]. Action = [[0.06657696 0.22960567 0.1305117  0.78411937]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 4850 is [True, False, False, False, False, True]
Current timestep = 4851. State = [[-0.2769062  0.2620635]]. Action = [[0.0932408  0.20915562 0.09061605 0.9256604 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 4851 is [True, False, False, False, False, True]
Scene graph at timestep 4851 is [True, False, False, False, False, True]
State prediction error at timestep 4851 is tensor(3.6588e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4852. State = [[-0.27571312  0.26650915]]. Action = [[-0.11484426  0.21979308 -0.05061378  0.5636283 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 4852 is [True, False, False, False, False, True]
Current timestep = 4853. State = [[-0.27365473  0.27036077]]. Action = [[ 0.06644237 -0.14735341  0.08655468  0.915081  ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 4853 is [True, False, False, False, False, True]
Scene graph at timestep 4853 is [True, False, False, False, False, True]
State prediction error at timestep 4853 is tensor(2.0632e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4854. State = [[-0.27122477  0.27231708]]. Action = [[ 0.20645654  0.21811002 -0.15799984 -0.4894879 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 4854 is [True, False, False, False, False, True]
Human Feedback received at timestep 4854 of -1
Current timestep = 4855. State = [[-0.2674759   0.27504456]]. Action = [[-0.18777774  0.18305922 -0.06693326 -0.6408003 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 4855 is [True, False, False, False, False, True]
Current timestep = 4856. State = [[-0.2641902   0.27805907]]. Action = [[-0.18423057  0.22453773 -0.05785352  0.74865294]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 4856 is [True, False, False, False, False, True]
Human Feedback received at timestep 4856 of -1
Current timestep = 4857. State = [[-0.26481646  0.2822983 ]]. Action = [[-0.1523062   0.22775656 -0.00270973 -0.90937686]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 4857 is [True, False, False, False, False, True]
Current timestep = 4858. State = [[-0.26763797  0.2885789 ]]. Action = [[-0.07138629  0.1979185  -0.21890567 -0.7850329 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 4858 is [True, False, False, False, False, True]
Current timestep = 4859. State = [[-0.27084753  0.29495454]]. Action = [[-0.21014808 -0.00932464 -0.10603252 -0.21268898]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 4859 is [True, False, False, False, False, True]
Human Feedback received at timestep 4859 of -1
Current timestep = 4860. State = [[-0.27554953  0.300051  ]]. Action = [[0.12285835 0.2455864  0.04917577 0.99931705]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 4860 is [True, False, False, False, False, True]
Current timestep = 4861. State = [[-0.27813834  0.30277953]]. Action = [[-0.23944505  0.06868002 -0.17432547  0.8307352 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 4861 is [True, False, False, False, False, True]
Current timestep = 4862. State = [[-0.2796251  0.3043117]]. Action = [[-0.1742448   0.17377657 -0.07051563  0.8977488 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 4862 is [True, False, False, False, False, True]
Current timestep = 4863. State = [[-0.2804665   0.30500087]]. Action = [[-0.21290949  0.1412782   0.0409199  -0.032242  ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 4863 is [True, False, False, False, False, True]
Current timestep = 4864. State = [[-0.2808403  0.3052489]]. Action = [[-0.11262669  0.03945616 -0.17842911  0.9747778 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 4864 is [True, False, False, False, False, True]
Human Feedback received at timestep 4864 of -1
Current timestep = 4865. State = [[-0.28098315  0.30558679]]. Action = [[-0.15042819  0.17321393 -0.02427918 -0.88628405]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 4865 is [True, False, False, False, False, True]
Current timestep = 4866. State = [[-0.28104064  0.30586666]]. Action = [[-0.12054618  0.19002885 -0.07916208  0.06201971]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 4866 is [True, False, False, False, False, True]
Scene graph at timestep 4866 is [True, False, False, False, False, True]
State prediction error at timestep 4866 is tensor(2.8140e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4867. State = [[-0.28122845  0.3060424 ]]. Action = [[-0.10125813 -0.19657521 -0.11874834  0.69463944]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 4867 is [True, False, False, False, False, True]
Current timestep = 4868. State = [[-0.28135648  0.30619106]]. Action = [[-0.10623206 -0.21601273 -0.16893485 -0.007438  ]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 4868 is [True, False, False, False, False, True]
Human Feedback received at timestep 4868 of -1
Current timestep = 4869. State = [[-0.28135648  0.30619106]]. Action = [[-0.03878085 -0.12730235 -0.11565946 -0.48535055]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 4869 is [True, False, False, False, False, True]
Current timestep = 4870. State = [[-0.28135648  0.30619106]]. Action = [[-0.19205433  0.06437463 -0.07022056  0.6482053 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 4870 is [True, False, False, False, False, True]
Current timestep = 4871. State = [[-0.28135648  0.30619106]]. Action = [[-0.17800052  0.09640914  0.01309943 -0.98140323]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 4871 is [True, False, False, False, False, True]
Current timestep = 4872. State = [[-0.28135648  0.30619106]]. Action = [[-0.14003816  0.24216354 -0.05038461 -0.9981375 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 4872 is [True, False, False, False, False, True]
Current timestep = 4873. State = [[-0.28135648  0.30619106]]. Action = [[-0.05405644  0.20752314 -0.08476986 -0.97896326]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 4873 is [True, False, False, False, False, True]
Current timestep = 4874. State = [[-0.28130928  0.30614695]]. Action = [[ 0.1605567  -0.20240274 -0.04866731  0.6749171 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 4874 is [True, False, False, False, False, True]
Scene graph at timestep 4874 is [True, False, False, False, False, True]
State prediction error at timestep 4874 is tensor(1.0590e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4875. State = [[-0.28058818  0.30505657]]. Action = [[ 0.07608888 -0.164682   -0.10245207  0.69198716]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 4875 is [True, False, False, False, False, True]
Human Feedback received at timestep 4875 of -1
Current timestep = 4876. State = [[-0.28015122  0.3044485 ]]. Action = [[-0.16316098  0.14816454  0.10956264 -0.9631026 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 4876 is [True, False, False, False, False, True]
Current timestep = 4877. State = [[-0.28000796  0.30413392]]. Action = [[-0.02180976  0.0650146  -0.20396703 -0.9636911 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 4877 is [True, False, False, False, False, True]
Current timestep = 4878. State = [[-0.27969936  0.30367437]]. Action = [[-0.17425184 -0.13426936 -0.16304587 -0.93291014]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 4878 is [True, False, False, False, False, True]
Current timestep = 4879. State = [[-0.27962384  0.30348116]]. Action = [[-0.11265378 -0.17634583 -0.17810844 -0.8634805 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 4879 is [True, False, False, False, False, True]
Current timestep = 4880. State = [[-0.27922633  0.30298695]]. Action = [[ 0.06141496 -0.22852047 -0.22982602  0.12848341]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 4880 is [True, False, False, False, False, True]
Human Feedback received at timestep 4880 of -1
Current timestep = 4881. State = [[-0.2776424   0.30056524]]. Action = [[-0.14252888  0.05676317  0.05538291 -0.9126885 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 4881 is [True, False, False, False, False, True]
Current timestep = 4882. State = [[-0.27661955  0.2986722 ]]. Action = [[-0.21601254 -0.01272683 -0.09415044 -0.65344   ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 4882 is [True, False, False, False, False, True]
Scene graph at timestep 4882 is [True, False, False, False, False, True]
State prediction error at timestep 4882 is tensor(1.0158e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4883. State = [[-0.27610373  0.29767668]]. Action = [[-0.20132202 -0.12674256  0.02747586  0.997252  ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 4883 is [True, False, False, False, False, True]
Current timestep = 4884. State = [[-0.2758074  0.2971263]]. Action = [[-0.1629399   0.2298578  -0.05540973  0.925869  ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 4884 is [True, False, False, False, False, True]
Scene graph at timestep 4884 is [True, False, False, False, False, True]
State prediction error at timestep 4884 is tensor(5.9863e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4885. State = [[-0.27564302  0.2968204 ]]. Action = [[-0.1329001   0.08354983  0.04958501 -0.574335  ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 4885 is [True, False, False, False, False, True]
Current timestep = 4886. State = [[-0.27557722  0.29669785]]. Action = [[ 0.05309144  0.2232067  -0.11846375 -0.567693  ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 4886 is [True, False, False, False, False, True]
Current timestep = 4887. State = [[-0.27554432  0.29663655]]. Action = [[ 0.02308989  0.13239554 -0.1550917   0.4429555 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 4887 is [True, False, False, False, False, True]
Current timestep = 4888. State = [[-0.275623    0.29674214]]. Action = [[-0.12005943 -0.13055408 -0.15070592 -0.5149904 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 4888 is [True, False, False, False, False, True]
Scene graph at timestep 4888 is [True, False, False, False, False, True]
State prediction error at timestep 4888 is tensor(4.7933e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4889. State = [[-0.275623    0.29674214]]. Action = [[-0.19499785  0.10030371 -0.03578532 -0.6180801 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 4889 is [True, False, False, False, False, True]
Current timestep = 4890. State = [[-0.275623    0.29674214]]. Action = [[-0.21123265  0.20551574 -0.02517067  0.60445035]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 4890 is [True, False, False, False, False, True]
Current timestep = 4891. State = [[-0.275623    0.29674214]]. Action = [[ 0.09780383  0.2291925   0.07622108 -0.13423312]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 4891 is [True, False, False, False, False, True]
Current timestep = 4892. State = [[-0.275623    0.29674214]]. Action = [[-0.16706795  0.21849072  0.04346734 -0.88519514]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 4892 is [True, False, False, False, False, True]
Scene graph at timestep 4892 is [True, False, False, False, False, True]
State prediction error at timestep 4892 is tensor(1.4333e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4893. State = [[-0.275623    0.29674214]]. Action = [[-0.18319365 -0.06725803  0.17657077  0.9979086 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 4893 is [True, False, False, False, False, True]
Current timestep = 4894. State = [[-0.275623    0.29674214]]. Action = [[ 0.04587334  0.21250454 -0.15520777  0.56060815]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 4894 is [True, False, False, False, False, True]
Current timestep = 4895. State = [[-0.275623    0.29674214]]. Action = [[-0.13286728  0.20735598 -0.06917086  0.56033874]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 4895 is [True, False, False, False, False, True]
Scene graph at timestep 4895 is [True, False, False, False, False, True]
State prediction error at timestep 4895 is tensor(2.1481e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4896. State = [[-0.275623    0.29674214]]. Action = [[-0.1947416   0.00972638 -0.05122113  0.9593861 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 4896 is [True, False, False, False, False, True]
Current timestep = 4897. State = [[-0.275623    0.29674214]]. Action = [[-0.21882147 -0.1313995  -0.04425871  0.93897545]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 4897 is [True, False, False, False, False, True]
Current timestep = 4898. State = [[-0.275623    0.29674214]]. Action = [[ 0.03076571  0.10338673 -0.18740101  0.12352419]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 4898 is [True, False, False, False, False, True]
Scene graph at timestep 4898 is [True, False, False, False, False, True]
State prediction error at timestep 4898 is tensor(9.2038e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4899. State = [[-0.27591598  0.29724267]]. Action = [[-0.02204123  0.15639079 -0.10464728 -0.8826451 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 4899 is [True, False, False, False, False, True]
Current timestep = 4900. State = [[-0.27588806  0.2995919 ]]. Action = [[-0.10461161  0.11317173 -0.21997991  0.3937899 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 4900 is [True, False, False, False, False, True]
Current timestep = 4901. State = [[-0.27487576  0.30122912]]. Action = [[-0.09427339  0.22919977 -0.19717745 -0.6373675 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 4901 is [True, False, False, False, False, True]
Current timestep = 4902. State = [[-0.27364913  0.30289954]]. Action = [[-0.22909513  0.09161451 -0.07356459 -0.37464327]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 4902 is [True, False, False, False, False, True]
Current timestep = 4903. State = [[-0.27228123  0.30409193]]. Action = [[-0.00767553  0.22954166 -0.04717952 -0.37014735]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 4903 is [True, False, False, False, False, True]
Current timestep = 4904. State = [[-0.2716865  0.3046156]]. Action = [[-0.21473084  0.02604488 -0.07634558  0.92631626]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 4904 is [True, False, False, False, False, True]
Current timestep = 4905. State = [[-0.27166975  0.30479845]]. Action = [[-0.13399619  0.16215393 -0.02891061 -0.8676069 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 4905 is [True, False, False, False, False, True]
Scene graph at timestep 4905 is [True, False, False, False, False, True]
State prediction error at timestep 4905 is tensor(2.8977e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4905 of -1
Current timestep = 4906. State = [[-0.27178276  0.30473343]]. Action = [[ 0.02669507  0.24888882  0.02927816 -0.3524133 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 4906 is [True, False, False, False, False, True]
Scene graph at timestep 4906 is [True, False, False, False, False, True]
State prediction error at timestep 4906 is tensor(1.7980e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4907. State = [[-0.27178276  0.30473343]]. Action = [[-0.10787687  0.2060914   0.04931375  0.58492947]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 4907 is [True, False, False, False, False, True]
Current timestep = 4908. State = [[-0.27178276  0.30473343]]. Action = [[-0.18584576  0.17986429 -0.14852531 -0.94976276]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 4908 is [True, False, False, False, False, True]
Human Feedback received at timestep 4908 of -1
Current timestep = 4909. State = [[-0.27180845  0.3047232 ]]. Action = [[-0.22882906 -0.2202287  -0.14777431  0.90615857]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 4909 is [True, False, False, False, False, True]
Current timestep = 4910. State = [[-0.27180845  0.3047232 ]]. Action = [[-0.09849396  0.16260529 -0.09741217  0.7163924 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 4910 is [True, False, False, False, False, True]
Current timestep = 4911. State = [[-0.27180845  0.3047232 ]]. Action = [[ 0.1638149   0.17707077 -0.18684721 -0.84282744]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 4911 is [True, False, False, False, False, True]
Current timestep = 4912. State = [[-0.27180845  0.3047232 ]]. Action = [[-0.13561852  0.17757326 -0.12604989  0.45474982]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 4912 is [True, False, False, False, False, True]
Current timestep = 4913. State = [[-0.27180845  0.3047232 ]]. Action = [[-0.10916135  0.11364576 -0.0070374  -0.37730265]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 4913 is [True, False, False, False, False, True]
Current timestep = 4914. State = [[-0.27180845  0.3047232 ]]. Action = [[-0.12266761  0.22148463 -0.13861668 -0.975415  ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 4914 is [True, False, False, False, False, True]
Current timestep = 4915. State = [[-0.27180845  0.3047232 ]]. Action = [[ 0.02689487  0.05012545 -0.02256367 -0.99501234]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 4915 is [True, False, False, False, False, True]
Current timestep = 4916. State = [[-0.27180845  0.3047232 ]]. Action = [[-0.01435389  0.24351442 -0.1684505  -0.55078155]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 4916 is [True, False, False, False, False, True]
Human Feedback received at timestep 4916 of -1
Current timestep = 4917. State = [[-0.27174723  0.30472663]]. Action = [[-0.05156752 -0.21161182 -0.10519052  0.9477308 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 4917 is [True, False, False, False, False, True]
Current timestep = 4918. State = [[-0.271613    0.30448225]]. Action = [[ 0.05612391  0.21191555 -0.13299313  0.691821  ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 4918 is [True, False, False, False, False, True]
Current timestep = 4919. State = [[-0.2243006   0.05950757]]. Action = [[ 0.02023801 -0.14380719 -0.02691647 -0.11652225]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 4919 is [True, False, False, False, False, True]
Current timestep = 4920. State = [[-0.22349855  0.06299333]]. Action = [[ 0.21286902 -0.16575885 -0.01131338  0.60488343]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 4920 is [True, False, False, False, True, False]
Scene graph at timestep 4920 is [True, False, False, False, True, False]
State prediction error at timestep 4920 is tensor(5.7960e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4921. State = [[-0.2218445   0.06277649]]. Action = [[-0.01426707  0.1169332   0.07563591  0.5567486 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 4921 is [True, False, False, False, True, False]
Current timestep = 4922. State = [[-0.22089759  0.06289454]]. Action = [[-0.03553411 -0.10528992  0.09500435 -0.9813527 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 4922 is [True, False, False, False, True, False]
Scene graph at timestep 4922 is [True, False, False, False, True, False]
State prediction error at timestep 4922 is tensor(1.7589e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4923. State = [[-0.22050454  0.06274933]]. Action = [[ 0.16538179 -0.11766344  0.09222046  0.7856941 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 4923 is [True, False, False, False, True, False]
Current timestep = 4924. State = [[-0.21901734  0.06120661]]. Action = [[-0.1669913  -0.13740987 -0.17281033  0.82410765]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 4924 is [True, False, False, False, True, False]
Current timestep = 4925. State = [[-0.2188817   0.05855739]]. Action = [[-0.22140794 -0.16048321 -0.15901653  0.86416554]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 4925 is [True, False, False, False, True, False]
Current timestep = 4926. State = [[-0.21890329  0.05585679]]. Action = [[ 0.20540619  0.1392253  -0.19851449 -0.88717175]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 4926 is [True, False, False, False, True, False]
Current timestep = 4927. State = [[-0.2188382   0.05564013]]. Action = [[-0.21515559 -0.0808301   0.08119079  0.84159064]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 4927 is [True, False, False, False, True, False]
Current timestep = 4928. State = [[-0.21914418  0.0544442 ]]. Action = [[-0.05265382 -0.1071853   0.20334989 -0.9289346 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 4928 is [True, False, False, False, True, False]
Current timestep = 4929. State = [[-0.21937817  0.0525041 ]]. Action = [[-0.21321677 -0.01534459 -0.16662592  0.5717503 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 4929 is [True, False, False, False, True, False]
Scene graph at timestep 4929 is [True, False, False, False, True, False]
State prediction error at timestep 4929 is tensor(3.9126e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4930. State = [[-0.21983722  0.05126473]]. Action = [[0.11640793 0.20474568 0.16513327 0.9089401 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 4930 is [True, False, False, False, True, False]
Scene graph at timestep 4930 is [True, False, False, False, True, False]
State prediction error at timestep 4930 is tensor(6.0639e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4931. State = [[-0.22032337  0.05200775]]. Action = [[ 0.2251957  -0.19015545  0.19711801 -0.20206773]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 4931 is [True, False, False, False, True, False]
Scene graph at timestep 4931 is [True, False, False, False, True, False]
State prediction error at timestep 4931 is tensor(1.5619e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4932. State = [[-0.22021881  0.05054921]]. Action = [[-0.05954032 -0.09912103  0.1395958   0.02613986]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 4932 is [True, False, False, False, True, False]
Current timestep = 4933. State = [[-0.22012213  0.04893841]]. Action = [[-0.10146284  0.10768318 -0.0599879  -0.14730543]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 4933 is [True, False, False, False, True, False]
Current timestep = 4934. State = [[-0.22018853  0.04878892]]. Action = [[-0.02851105  0.099904   -0.12822019 -0.8537741 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 4934 is [True, False, False, False, True, False]
Current timestep = 4935. State = [[-0.22031313  0.0492789 ]]. Action = [[ 0.16947645 -0.18340673  0.15126014  0.858852  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 4935 is [True, False, False, False, True, False]
Current timestep = 4936. State = [[-0.22021107  0.04796226]]. Action = [[ 0.11091059 -0.10144302  0.22230357 -0.25061548]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 4936 is [True, False, False, False, True, False]
Current timestep = 4937. State = [[-0.21954162  0.0458401 ]]. Action = [[ 0.22830781 -0.0402793  -0.0700821  -0.9035137 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 4937 is [True, False, False, False, True, False]
Scene graph at timestep 4937 is [True, False, False, False, True, False]
State prediction error at timestep 4937 is tensor(4.8164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4938. State = [[-0.21618907  0.04317185]]. Action = [[-0.07062323 -0.15512915  0.11930025  0.31325054]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 4938 is [True, False, False, False, True, False]
Current timestep = 4939. State = [[-0.21499789  0.04074062]]. Action = [[-0.05088106  0.00156534 -0.12572654  0.10392046]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 4939 is [True, False, False, False, True, False]
Scene graph at timestep 4939 is [True, False, False, False, True, False]
State prediction error at timestep 4939 is tensor(3.1175e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4940. State = [[-0.21430936  0.03864953]]. Action = [[ 0.2316081   0.03172597 -0.01579389  0.693084  ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 4940 is [True, False, False, False, True, False]
Current timestep = 4941. State = [[-0.21190403  0.03761101]]. Action = [[ 0.21939763  0.11511955 -0.03523782 -0.00180602]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 4941 is [True, False, False, False, True, False]
Current timestep = 4942. State = [[-0.20787618  0.03811117]]. Action = [[-0.18709251  0.23206854 -0.14010797  0.69788194]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 4942 is [True, False, False, False, True, False]
Current timestep = 4943. State = [[-0.20583588  0.04019697]]. Action = [[-0.0577613  -0.15466085 -0.18967116 -0.72784424]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 4943 is [True, False, False, False, True, False]
Scene graph at timestep 4943 is [True, False, False, False, True, False]
State prediction error at timestep 4943 is tensor(2.7816e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4944. State = [[-0.20470741  0.04032539]]. Action = [[ 0.1992138   0.20176736 -0.20390289 -0.08884436]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 4944 is [True, False, False, False, True, False]
Current timestep = 4945. State = [[-0.20292419  0.04179753]]. Action = [[-0.23827258 -0.01905367 -0.2445345   0.5785512 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 4945 is [True, False, False, False, True, False]
Scene graph at timestep 4945 is [True, False, False, False, True, False]
State prediction error at timestep 4945 is tensor(2.8820e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4946. State = [[-0.20334429  0.04255364]]. Action = [[ 0.06858799  0.05077791 -0.00343782  0.44989765]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 4946 is [True, False, False, False, True, False]
Human Feedback received at timestep 4946 of 1
Current timestep = 4947. State = [[-0.20339605  0.04336495]]. Action = [[ 0.1401554  -0.06563529 -0.02531877 -0.873394  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 4947 is [True, False, False, False, True, False]
Current timestep = 4948. State = [[-0.20283848  0.04342996]]. Action = [[0.03494149 0.11926839 0.1954714  0.40135252]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 4948 is [True, False, False, False, True, False]
Current timestep = 4949. State = [[-0.20207971  0.04436024]]. Action = [[ 0.21736145  0.09177113 -0.18036601  0.93473804]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 4949 is [True, False, False, False, True, False]
Current timestep = 4950. State = [[-0.20003974  0.04585481]]. Action = [[-0.16918571  0.15764439  0.23676288 -0.68218344]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 4950 is [True, False, False, False, True, False]
Scene graph at timestep 4950 is [True, False, False, False, True, False]
State prediction error at timestep 4950 is tensor(1.0142e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4951. State = [[-0.19939356  0.04955336]]. Action = [[ 0.10699624  0.15630054 -0.15405388  0.08512807]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 4951 is [True, False, False, False, True, False]
Scene graph at timestep 4951 is [True, False, False, False, True, False]
State prediction error at timestep 4951 is tensor(8.9377e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4951 of 1
Current timestep = 4952. State = [[-0.1980778   0.05408938]]. Action = [[ 0.05614775  0.18206224  0.08143625 -0.04108429]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 4952 is [True, False, False, False, True, False]
Current timestep = 4953. State = [[-0.19646592  0.05924807]]. Action = [[-0.16657677  0.03996402 -0.17521529  0.6179168 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 4953 is [True, False, False, False, True, False]
Scene graph at timestep 4953 is [True, False, False, False, True, False]
State prediction error at timestep 4953 is tensor(1.0609e-05, grad_fn=<MseLossBackward0>)
Current timestep = 4954. State = [[-0.19676612  0.06277993]]. Action = [[ 0.05360919 -0.08928385 -0.06831007  0.21174514]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 4954 is [True, False, False, False, True, False]
Current timestep = 4955. State = [[-0.19650537  0.06375672]]. Action = [[ 0.1036824  -0.07889792 -0.03478044  0.918627  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 4955 is [True, False, False, False, True, False]
Current timestep = 4956. State = [[-0.19585261  0.06385089]]. Action = [[ 0.21477616  0.16266078 -0.20037392  0.9564793 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 4956 is [True, False, False, False, True, False]
Current timestep = 4957. State = [[-0.19365786  0.06554142]]. Action = [[-0.09473845 -0.0947492  -0.20070203 -0.45559424]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 4957 is [True, False, False, False, True, False]
Current timestep = 4958. State = [[-0.19167747  0.06582854]]. Action = [[ 0.1813035  -0.10342827  0.07008338  0.9876735 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 4958 is [True, False, False, False, True, False]
Current timestep = 4959. State = [[-0.18880431  0.06590103]]. Action = [[ 0.1493311   0.04088184 -0.14992814 -0.04450309]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 4959 is [True, False, False, False, True, False]
Scene graph at timestep 4959 is [True, False, False, False, True, False]
State prediction error at timestep 4959 is tensor(5.2983e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4960. State = [[-0.18436179  0.06635809]]. Action = [[-0.15594359 -0.15748827 -0.19339897 -0.5050715 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 4960 is [True, False, False, False, True, False]
Current timestep = 4961. State = [[-0.18274583  0.06587998]]. Action = [[ 0.04321125  0.10344863  0.01788622 -0.30090582]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 4961 is [True, False, False, False, True, False]
Scene graph at timestep 4961 is [True, False, False, False, True, False]
State prediction error at timestep 4961 is tensor(3.3623e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4962. State = [[-0.18235958  0.06597961]]. Action = [[-0.1785964   0.07018888 -0.02490611 -0.75734913]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 4962 is [True, False, False, False, True, False]
Current timestep = 4963. State = [[-0.18267429  0.0663049 ]]. Action = [[ 0.20458356 -0.01244934  0.13956225 -0.3604939 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 4963 is [True, False, False, False, True, False]
Current timestep = 4964. State = [[-0.18242788  0.06642864]]. Action = [[-0.1526133   0.13132322 -0.21976852  0.65815663]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 4964 is [True, False, False, False, True, False]
Human Feedback received at timestep 4964 of 1
Current timestep = 4965. State = [[-0.18269807  0.06714313]]. Action = [[0.13618198 0.11257085 0.1800321  0.956779  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 4965 is [True, False, False, False, True, False]
Current timestep = 4966. State = [[-0.18295139  0.06898665]]. Action = [[-0.20010911  0.06669992  0.18735123  0.17500877]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 4966 is [True, False, False, False, True, False]
Current timestep = 4967. State = [[-0.18350653  0.07073235]]. Action = [[ 0.02975062 -0.2338509  -0.11229831 -0.9602688 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 4967 is [True, False, False, False, True, False]
Scene graph at timestep 4967 is [True, False, False, False, True, False]
State prediction error at timestep 4967 is tensor(4.0902e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4968. State = [[-0.18341741  0.07062096]]. Action = [[ 0.04379866  0.05797666 -0.04131421 -0.9574832 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 4968 is [True, False, False, False, True, False]
Current timestep = 4969. State = [[-0.1833913  0.0705016]]. Action = [[ 0.01000091 -0.15556243 -0.1514207   0.01922321]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 4969 is [True, False, False, False, True, False]
Current timestep = 4970. State = [[-0.18327798  0.06983877]]. Action = [[ 0.12617835 -0.06166817 -0.21762756 -0.6259189 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 4970 is [True, False, False, False, True, False]
Current timestep = 4971. State = [[-0.18297327  0.0689024 ]]. Action = [[ 0.14455396  0.10866457 -0.12447838 -0.98082644]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 4971 is [True, False, False, False, True, False]
Scene graph at timestep 4971 is [True, False, False, False, True, False]
State prediction error at timestep 4971 is tensor(4.1747e-07, grad_fn=<MseLossBackward0>)
Current timestep = 4972. State = [[-0.18128936  0.06911369]]. Action = [[ 0.1882993  -0.04258862  0.06271806  0.6855452 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 4972 is [True, False, False, False, True, False]
Current timestep = 4973. State = [[-0.17738308  0.06925397]]. Action = [[ 0.10847342  0.00890273 -0.21315932 -0.7659755 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 4973 is [True, False, False, False, True, False]
Current timestep = 4974. State = [[-0.17285566  0.06953031]]. Action = [[-0.17978671  0.06257421 -0.11509661 -0.2667991 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 4974 is [True, False, False, False, True, False]
Current timestep = 4975. State = [[-0.1703956   0.07013377]]. Action = [[-0.2115563   0.03955692  0.12127426 -0.27183038]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 4975 is [True, False, False, False, True, False]
Scene graph at timestep 4975 is [True, False, False, False, True, False]
State prediction error at timestep 4975 is tensor(1.7220e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4976. State = [[-0.17086111  0.07034871]]. Action = [[ 0.24776906  0.04817721  0.01359412 -0.9010093 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 4976 is [True, False, False, False, True, False]
Current timestep = 4977. State = [[-0.16955659  0.0705521 ]]. Action = [[0.15816319 0.09538788 0.04793292 0.5750785 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 4977 is [True, False, False, False, True, False]
Human Feedback received at timestep 4977 of 1
Current timestep = 4978. State = [[-0.16762805  0.07157049]]. Action = [[ 0.07204011  0.11751539 -0.19835003  0.88265395]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 4978 is [True, False, False, False, True, False]
Current timestep = 4979. State = [[-0.1657474   0.07333878]]. Action = [[-0.11802909 -0.24466188  0.04390779  0.01768875]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 4979 is [True, False, False, False, True, False]
Scene graph at timestep 4979 is [True, False, False, False, True, False]
State prediction error at timestep 4979 is tensor(2.9745e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4980. State = [[-0.16507718  0.07297013]]. Action = [[ 0.02046442 -0.09345931  0.20705086  0.7042742 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 4980 is [True, False, False, False, True, False]
Current timestep = 4981. State = [[-0.16499354  0.07196574]]. Action = [[-0.09508525  0.0392586  -0.02344728 -0.83874327]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 4981 is [True, False, False, False, True, False]
Current timestep = 4982. State = [[-0.16490726  0.07180025]]. Action = [[ 0.21391422 -0.05884667  0.14360428 -0.59536386]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 4982 is [True, False, False, False, True, False]
Current timestep = 4983. State = [[-0.16435401  0.07098769]]. Action = [[ 0.24561918 -0.20478934  0.1003167   0.09340334]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 4983 is [True, False, False, False, True, False]
Current timestep = 4984. State = [[-0.1612528   0.06774839]]. Action = [[-0.04471906 -0.00863373 -0.1545971  -0.38032055]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 4984 is [True, False, False, False, True, False]
Current timestep = 4985. State = [[-0.15840977  0.06600787]]. Action = [[0.01066887 0.00563952 0.09765014 0.45243526]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 4985 is [True, False, False, False, True, False]
Current timestep = 4986. State = [[-0.15528663  0.06503046]]. Action = [[ 0.00799972  0.15326715  0.19024092 -0.7943323 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 4986 is [True, False, False, False, True, False]
Scene graph at timestep 4986 is [True, False, False, False, True, False]
State prediction error at timestep 4986 is tensor(6.1433e-06, grad_fn=<MseLossBackward0>)
Current timestep = 4987. State = [[-0.15315218  0.06569565]]. Action = [[ 0.01958248  0.17114621  0.1786212  -0.4415412 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 4987 is [True, False, False, False, True, False]
Current timestep = 4988. State = [[-0.15212743  0.06751102]]. Action = [[-0.1954788   0.21611372  0.12891316  0.88335276]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 4988 is [True, False, False, False, True, False]
Current timestep = 4989. State = [[-0.15253162  0.07148533]]. Action = [[ 0.18401158 -0.17489734 -0.1805772  -0.3113165 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 4989 is [True, False, False, False, True, False]
Current timestep = 4990. State = [[-0.15062363  0.07204372]]. Action = [[ 0.21489972  0.15725422 -0.19302131 -0.2705933 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 4990 is [True, False, False, False, True, False]
Human Feedback received at timestep 4990 of 1
Current timestep = 4991. State = [[-0.147156    0.07330088]]. Action = [[-0.23144463 -0.04896207 -0.01925233  0.34102893]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 4991 is [True, False, False, False, True, False]
Current timestep = 4992. State = [[-0.14676246  0.07377795]]. Action = [[-0.00522624  0.06974757  0.0790675   0.56253576]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 4992 is [True, False, False, False, True, False]
Current timestep = 4993. State = [[-0.14705604  0.07486133]]. Action = [[ 0.14395589 -0.03754669 -0.11217779  0.9097822 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 4993 is [True, False, False, False, True, False]
Current timestep = 4994. State = [[-0.14699385  0.07500558]]. Action = [[-0.17732154 -0.01228495 -0.00379819  0.6737195 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 4994 is [True, False, False, False, True, False]
Current timestep = 4995. State = [[-0.14714059  0.0751101 ]]. Action = [[-0.00097091 -0.1312093   0.1370506  -0.61061794]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 4995 is [True, False, False, False, True, False]
Current timestep = 4996. State = [[-0.1472005   0.07459232]]. Action = [[-0.09636322 -0.21240447  0.17062885  0.5256345 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 4996 is [True, False, False, False, True, False]
Current timestep = 4997. State = [[-0.14701591  0.07223302]]. Action = [[ 0.22516885  0.15226394  0.21469218 -0.09484351]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 4997 is [True, False, False, False, True, False]
Current timestep = 4998. State = [[-0.14696756  0.07205967]]. Action = [[ 0.11651748  0.04911396 -0.15801114 -0.266402  ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 4998 is [True, False, False, False, True, False]
Current timestep = 4999. State = [[-0.14692602  0.07232264]]. Action = [[-0.01498424 -0.05347297  0.19329327  0.5935142 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 4999 is [True, False, False, False, True, False]
Current timestep = 5000. State = [[-0.14641427  0.07211055]]. Action = [[-0.1858701  -0.21262488  0.03440264 -0.18839484]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 5000 is [True, False, False, False, True, False]
Scene graph at timestep 5000 is [True, False, False, False, True, False]
State prediction error at timestep 5000 is tensor(9.4176e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5001. State = [[-0.14624214  0.06991158]]. Action = [[-0.11634341  0.22631845 -0.18767004 -0.67756575]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 5001 is [True, False, False, False, True, False]
Current timestep = 5002. State = [[-0.14663325  0.07065982]]. Action = [[-0.00686415  0.00539428 -0.17518091 -0.7629853 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 5002 is [True, False, False, False, True, False]
Current timestep = 5003. State = [[-0.14686431  0.07108314]]. Action = [[ 0.01811358  0.04339778 -0.02025534  0.7708237 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 5003 is [True, False, False, False, True, False]
Current timestep = 5004. State = [[-0.1471585   0.07145452]]. Action = [[-0.18529789 -0.03741924 -0.1151309   0.00570285]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 5004 is [True, False, False, False, True, False]
Current timestep = 5005. State = [[-0.14740345  0.07159343]]. Action = [[ 0.11083424 -0.04137121  0.04858336  0.35000134]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 5005 is [True, False, False, False, True, False]
Scene graph at timestep 5005 is [True, False, False, False, True, False]
State prediction error at timestep 5005 is tensor(5.2464e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5006. State = [[-0.14746976  0.07157034]]. Action = [[-0.08240783 -0.1163591  -0.17521745  0.7758732 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 5006 is [True, False, False, False, True, False]
Current timestep = 5007. State = [[-0.14743744  0.0706709 ]]. Action = [[-0.23946689  0.14773405  0.01145869  0.08232689]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 5007 is [True, False, False, False, True, False]
Current timestep = 5008. State = [[-0.14881001  0.07153692]]. Action = [[-0.17060784 -0.10123202 -0.0601283  -0.22621751]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 5008 is [True, False, False, False, True, False]
Current timestep = 5009. State = [[-0.15047492  0.07122691]]. Action = [[ 0.17089808 -0.09103577  0.06674632 -0.8702347 ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 5009 is [True, False, False, False, True, False]
Scene graph at timestep 5009 is [True, False, False, False, True, False]
State prediction error at timestep 5009 is tensor(6.5721e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5010. State = [[-0.15063895  0.0701037 ]]. Action = [[ 0.18349165  0.13558185  0.06558216 -0.57185286]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 5010 is [True, False, False, False, True, False]
Scene graph at timestep 5010 is [True, False, False, False, True, False]
State prediction error at timestep 5010 is tensor(2.5725e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5011. State = [[-0.15078327  0.0701887 ]]. Action = [[-0.18236111  0.06866446 -0.08743274 -0.09810519]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 5011 is [True, False, False, False, True, False]
Current timestep = 5012. State = [[-0.15110178  0.07112785]]. Action = [[-0.13258025  0.16733009 -0.12303342  0.05908716]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 5012 is [True, False, False, False, True, False]
Scene graph at timestep 5012 is [True, False, False, False, True, False]
State prediction error at timestep 5012 is tensor(4.9275e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5012 of 1
Current timestep = 5013. State = [[-0.15280469  0.0748272 ]]. Action = [[-0.14987841 -0.02676101 -0.20746902  0.11008489]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 5013 is [True, False, False, False, True, False]
Current timestep = 5014. State = [[-0.15460126  0.07673306]]. Action = [[-0.1283047  -0.13012037 -0.15515284 -0.26194644]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 5014 is [True, False, False, False, True, False]
Current timestep = 5015. State = [[-0.1563282   0.07631935]]. Action = [[-0.09438109 -0.14115874 -0.20188382 -0.9655249 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 5015 is [True, False, False, False, True, False]
Current timestep = 5016. State = [[-0.15787502  0.0746114 ]]. Action = [[ 0.15620214  0.21389312 -0.08079281  0.72205925]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 5016 is [True, False, False, False, True, False]
Current timestep = 5017. State = [[-0.15859014  0.07571955]]. Action = [[ 0.12348595  0.06829774 -0.07038578  0.24396026]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 5017 is [True, False, False, False, True, False]
Current timestep = 5018. State = [[-0.15898967  0.07674681]]. Action = [[-0.09695405 -0.07830323  0.09316614  0.43831408]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 5018 is [True, False, False, False, True, False]
Current timestep = 5019. State = [[-0.15914822  0.07686365]]. Action = [[-0.18764536  0.10515586 -0.12487601 -0.8294675 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 5019 is [True, False, False, False, True, False]
Current timestep = 5020. State = [[-0.16023335  0.07850683]]. Action = [[-0.04266879 -0.13939677  0.05219823  0.7541528 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 5020 is [True, False, False, False, True, False]
Scene graph at timestep 5020 is [True, False, False, False, True, False]
State prediction error at timestep 5020 is tensor(4.2701e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5021. State = [[-0.16126359  0.07772886]]. Action = [[-0.05241393 -0.09915674 -0.04525879  0.8287842 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 5021 is [True, False, False, False, True, False]
Scene graph at timestep 5021 is [True, False, False, False, True, False]
State prediction error at timestep 5021 is tensor(5.3287e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5022. State = [[-0.16226968  0.075583  ]]. Action = [[-0.07779478 -0.1884484   0.14392528  0.08609879]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 5022 is [True, False, False, False, True, False]
Current timestep = 5023. State = [[-0.16346622  0.07238198]]. Action = [[-0.1416163  -0.10332319  0.02606273  0.37984192]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 5023 is [True, False, False, False, True, False]
Current timestep = 5024. State = [[-0.16593231  0.06875172]]. Action = [[ 0.01159483 -0.19274883  0.02463478 -0.45887238]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 5024 is [True, False, False, False, True, False]
Current timestep = 5025. State = [[-0.16876586  0.06440909]]. Action = [[-0.24671891  0.08485657  0.07086745  0.8011713 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 5025 is [True, False, False, False, True, False]
Current timestep = 5026. State = [[-0.17343299  0.06201707]]. Action = [[ 0.08055007 -0.24193823  0.15275541 -0.5145124 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 5026 is [True, False, False, False, True, False]
Current timestep = 5027. State = [[-0.17678615  0.05746057]]. Action = [[-0.23797573 -0.170108   -0.2399111   0.18958259]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 5027 is [True, False, False, False, True, False]
Scene graph at timestep 5027 is [True, False, False, False, True, False]
State prediction error at timestep 5027 is tensor(2.2638e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5027 of -1
Current timestep = 5028. State = [[-0.18174033  0.05232558]]. Action = [[-0.14207247  0.02033538 -0.04166961 -0.18769431]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 5028 is [True, False, False, False, True, False]
Current timestep = 5029. State = [[-0.18501084  0.04922556]]. Action = [[-0.22507864 -0.10339852 -0.09060496 -0.92233783]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 5029 is [True, False, False, False, True, False]
Current timestep = 5030. State = [[-0.1890353   0.04580515]]. Action = [[-0.00250264 -0.08227366 -0.19731902  0.4020269 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 5030 is [True, False, False, False, True, False]
Current timestep = 5031. State = [[-0.1935831   0.04315172]]. Action = [[ 0.1029042   0.12826303  0.06835797 -0.9825498 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 5031 is [True, False, False, False, True, False]
Current timestep = 5032. State = [[-0.1954834   0.04274245]]. Action = [[ 0.03781193 -0.2340825  -0.06889571  0.60800934]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 5032 is [True, False, False, False, True, False]
Scene graph at timestep 5032 is [True, False, False, False, True, False]
State prediction error at timestep 5032 is tensor(3.8403e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5032 of -1
Current timestep = 5033. State = [[-0.1961472   0.03979121]]. Action = [[-0.07779062 -0.13208385 -0.2353448  -0.71562946]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 5033 is [True, False, False, False, True, False]
Scene graph at timestep 5033 is [True, False, False, False, True, False]
State prediction error at timestep 5033 is tensor(2.3242e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5034. State = [[-0.19738907  0.03559815]]. Action = [[-0.11386776 -0.00766909  0.2303589  -0.49868637]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 5034 is [True, False, False, False, True, False]
Current timestep = 5035. State = [[-0.19862793  0.03329655]]. Action = [[ 0.08162257  0.20071763  0.06735486 -0.43261838]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 5035 is [True, False, False, False, True, False]
Current timestep = 5036. State = [[-0.1991371   0.03402632]]. Action = [[-0.20915021  0.03903633 -0.1787301   0.01080287]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 5036 is [True, False, False, False, True, False]
Current timestep = 5037. State = [[-0.20070608  0.03480849]]. Action = [[0.17547286 0.10228086 0.20869055 0.529829  ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 5037 is [True, False, False, False, True, False]
Current timestep = 5038. State = [[-0.20130028  0.0359704 ]]. Action = [[ 0.23962444  0.09718454 -0.21729782  0.8073238 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 5038 is [True, False, False, False, True, False]
Current timestep = 5039. State = [[-0.2017014   0.03717438]]. Action = [[-0.0405713   0.19089976 -0.05614004 -0.6679692 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 5039 is [True, False, False, False, True, False]
Current timestep = 5040. State = [[-0.20258921  0.04067856]]. Action = [[ 0.19899827  0.20936668 -0.04516855  0.22366345]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 5040 is [True, False, False, False, True, False]
Current timestep = 5041. State = [[-0.20333144  0.04531308]]. Action = [[-0.1981199  -0.09428811  0.08917341 -0.57385474]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 5041 is [True, False, False, False, True, False]
Current timestep = 5042. State = [[-0.20408407  0.04745648]]. Action = [[ 0.17347199 -0.01381591 -0.0530065  -0.7529261 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 5042 is [True, False, False, False, True, False]
Current timestep = 5043. State = [[-0.20402245  0.04808928]]. Action = [[-0.07347646 -0.08476982 -0.12371105 -0.11205029]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 5043 is [True, False, False, False, True, False]
Current timestep = 5044. State = [[-0.2037267   0.04801459]]. Action = [[ 0.17019182 -0.17888415  0.19503099 -0.47284275]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 5044 is [True, False, False, False, True, False]
Current timestep = 5045. State = [[-0.20262975  0.04717956]]. Action = [[ 0.21891308 -0.1921358   0.24344707 -0.03852898]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 5045 is [True, False, False, False, True, False]
Current timestep = 5046. State = [[-0.20033582  0.04442571]]. Action = [[-0.11523616  0.00881901  0.05746403 -0.27997035]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 5046 is [True, False, False, False, True, False]
Current timestep = 5047. State = [[-0.19974819  0.04313314]]. Action = [[-0.14848623  0.03640735  0.0853582  -0.34564012]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 5047 is [True, False, False, False, True, False]
Human Feedback received at timestep 5047 of -1
Current timestep = 5048. State = [[-0.1998688   0.04240509]]. Action = [[ 0.08699077 -0.0503379   0.17238289 -0.49695873]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 5048 is [True, False, False, False, True, False]
Current timestep = 5049. State = [[-0.19959739  0.04151445]]. Action = [[-0.08785233 -0.1832846   0.05996269  0.1839943 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 5049 is [True, False, False, False, True, False]
Current timestep = 5050. State = [[-0.19929202  0.03855087]]. Action = [[ 0.03809401 -0.18510465 -0.21246627  0.6721343 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 5050 is [True, False, False, False, True, False]
Current timestep = 5051. State = [[-0.19879839  0.03435618]]. Action = [[-0.18140961  0.06126526  0.13485199 -0.71178335]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 5051 is [True, False, False, False, True, False]
Current timestep = 5052. State = [[-0.19911025  0.03193073]]. Action = [[-0.08458659  0.09272811  0.1763072   0.59669757]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 5052 is [True, False, False, False, True, False]
Current timestep = 5053. State = [[-0.1995593   0.03153364]]. Action = [[ 0.19467628  0.12569946 -0.22079454  0.5242126 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 5053 is [True, False, False, False, True, False]
Scene graph at timestep 5053 is [True, False, False, False, True, False]
State prediction error at timestep 5053 is tensor(4.6420e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5054. State = [[-0.199652    0.03212862]]. Action = [[ 0.2353099  -0.05861972 -0.04012384  0.79724383]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 5054 is [True, False, False, False, True, False]
Current timestep = 5055. State = [[-0.19930115  0.03205747]]. Action = [[-0.21843904 -0.11635101 -0.11261825 -0.17308247]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 5055 is [True, False, False, False, True, False]
Current timestep = 5056. State = [[-0.19917487  0.03094895]]. Action = [[-0.23780018 -0.23000325 -0.08361682 -0.3138973 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 5056 is [True, False, False, False, True, False]
Current timestep = 5057. State = [[-0.19960877  0.02779685]]. Action = [[-0.12263042 -0.00488225 -0.05264463  0.38572502]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 5057 is [True, False, False, False, True, False]
Current timestep = 5058. State = [[-0.20102909  0.02582954]]. Action = [[-0.23309623  0.10533378 -0.14920807  0.37541604]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 5058 is [True, False, False, False, True, False]
Current timestep = 5059. State = [[-0.20395154  0.02564446]]. Action = [[ 0.20639098  0.09431446 -0.21226121 -0.8458628 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 5059 is [True, False, False, False, True, False]
Current timestep = 5060. State = [[-0.20447025  0.0260105 ]]. Action = [[-0.22187054  0.08161452  0.18981248 -0.8652704 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 5060 is [True, False, False, False, True, False]
Scene graph at timestep 5060 is [True, False, False, False, True, False]
State prediction error at timestep 5060 is tensor(1.0820e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5061. State = [[-0.20641764  0.02700117]]. Action = [[-0.22273152 -0.21634993 -0.08631584  0.59831476]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 5061 is [True, False, False, False, True, False]
Human Feedback received at timestep 5061 of -1
Current timestep = 5062. State = [[-0.20953931  0.02568624]]. Action = [[-0.17620316  0.05076444  0.04012787 -0.13750672]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 5062 is [True, False, False, False, True, False]
Current timestep = 5063. State = [[-0.21287     0.02505478]]. Action = [[ 0.08746928 -0.2229207  -0.08605884  0.7467015 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 5063 is [True, False, False, False, True, False]
Current timestep = 5064. State = [[-0.21525562  0.02257303]]. Action = [[-0.21576004  0.13197711 -0.14064172 -0.94193226]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 5064 is [True, False, False, False, True, False]
Current timestep = 5065. State = [[-0.22004957  0.02184799]]. Action = [[-0.01767902 -0.02342767  0.1044718   0.2908492 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 5065 is [True, False, False, False, True, False]
Current timestep = 5066. State = [[-0.22329949  0.02074056]]. Action = [[ 0.01718968 -0.20086506 -0.14030503  0.63842154]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 5066 is [True, False, False, False, True, False]
Current timestep = 5067. State = [[-0.22518864  0.01785787]]. Action = [[ 0.17245096 -0.18401465 -0.07508957 -0.06511271]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 5067 is [True, False, False, False, True, False]
Current timestep = 5068. State = [[-0.22562581  0.014078  ]]. Action = [[-0.15154266 -0.01069415  0.01678842  0.80568266]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 5068 is [True, False, False, False, True, False]
Human Feedback received at timestep 5068 of -1
Current timestep = 5069. State = [[-0.22691983  0.01118063]]. Action = [[-0.17162476 -0.110906   -0.18363035  0.43597507]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 5069 is [True, False, False, False, True, False]
Current timestep = 5070. State = [[-0.22908276  0.00793006]]. Action = [[-0.14746018 -0.02863717  0.11225682  0.546167  ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 5070 is [True, False, False, False, True, False]
Current timestep = 5071. State = [[-0.2314231   0.00512117]]. Action = [[ 0.16909784 -0.12084273  0.19277462 -0.6723174 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 5071 is [True, False, False, False, True, False]
Scene graph at timestep 5071 is [True, False, False, False, True, False]
State prediction error at timestep 5071 is tensor(9.9552e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5071 of -1
Current timestep = 5072. State = [[-0.23180416  0.00199893]]. Action = [[ 0.00818256  0.10398063 -0.13008799 -0.26547462]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 5072 is [True, False, False, False, True, False]
Current timestep = 5073. State = [[-0.23191194  0.00173871]]. Action = [[ 0.18824106  0.15743059 -0.13604142 -0.94743305]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 5073 is [True, False, False, False, True, False]
Current timestep = 5074. State = [[-0.23184901  0.0019966 ]]. Action = [[ 0.17077786 -0.09054145  0.11889839 -0.6251821 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 5074 is [True, False, False, False, True, False]
Scene graph at timestep 5074 is [True, False, False, False, True, False]
State prediction error at timestep 5074 is tensor(4.0615e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5075. State = [[-0.23161489  0.00187821]]. Action = [[-0.20877028 -0.12985657 -0.04671946 -0.20844722]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 5075 is [True, False, False, False, True, False]
Current timestep = 5076. State = [[-0.23157834  0.00110717]]. Action = [[ 0.06500539 -0.00282851  0.02772373  0.27484632]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 5076 is [True, False, False, False, True, False]
Current timestep = 5077. State = [[-0.23149042  0.00062329]]. Action = [[ 0.23779488  0.1186294  -0.1804654  -0.92754644]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 5077 is [True, False, False, False, True, False]
Scene graph at timestep 5077 is [True, False, False, False, True, False]
State prediction error at timestep 5077 is tensor(8.1544e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5078. State = [[-0.23103434  0.0008797 ]]. Action = [[-0.02486832 -0.11833718 -0.24273658 -0.5641802 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 5078 is [True, False, False, False, True, False]
Current timestep = 5079. State = [[-2.3050223e-01  1.9172106e-04]]. Action = [[ 0.07782668 -0.18590406  0.00788203  0.32268822]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 5079 is [True, False, False, False, True, False]
Scene graph at timestep 5079 is [True, False, False, False, True, False]
State prediction error at timestep 5079 is tensor(7.5743e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5080. State = [[-0.22911054 -0.00233139]]. Action = [[-0.24012607 -0.19770816 -0.01086453  0.49218607]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 5080 is [True, False, False, False, True, False]
Current timestep = 5081. State = [[-0.22896288 -0.00645756]]. Action = [[ 0.16664624  0.02616993  0.05604005 -0.773948  ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 5081 is [True, False, False, False, True, False]
Current timestep = 5082. State = [[-0.22914767 -0.00892565]]. Action = [[-0.21084605 -0.03460087 -0.12553121  0.35897565]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 5082 is [True, False, False, False, True, False]
Current timestep = 5083. State = [[-0.2293964  -0.01113048]]. Action = [[ 0.00321674 -0.09717584 -0.00306763  0.7231288 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 5083 is [True, False, False, False, True, False]
Current timestep = 5084. State = [[-0.22970141 -0.01369942]]. Action = [[0.17120832 0.10435191 0.16517085 0.39379406]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 5084 is [True, False, False, False, True, False]
Scene graph at timestep 5084 is [True, False, False, False, True, False]
State prediction error at timestep 5084 is tensor(1.6955e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5085. State = [[-0.22968301 -0.01391038]]. Action = [[-0.210631    0.00149053 -0.013723   -0.3977912 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 5085 is [True, False, False, False, True, False]
Human Feedback received at timestep 5085 of -1
Current timestep = 5086. State = [[-0.22981407 -0.01395175]]. Action = [[-0.13718359  0.20401049  0.10125035  0.3598938 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 5086 is [True, False, False, False, True, False]
Current timestep = 5087. State = [[-0.23082446 -0.01238353]]. Action = [[ 0.04028326  0.22965655 -0.0302732  -0.8161609 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 5087 is [True, False, False, False, True, False]
Scene graph at timestep 5087 is [True, False, False, False, True, False]
State prediction error at timestep 5087 is tensor(4.4386e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5088. State = [[-0.23173754 -0.00904643]]. Action = [[ 0.04975066  0.14201564 -0.05726486 -0.25703025]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 5088 is [True, False, False, False, True, False]
Human Feedback received at timestep 5088 of -1
Current timestep = 5089. State = [[-0.23257402 -0.00511102]]. Action = [[ 0.16700333  0.19497743 -0.2141656  -0.31064856]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 5089 is [True, False, False, False, True, False]
Current timestep = 5090. State = [[-0.23313111 -0.00074928]]. Action = [[ 0.01025721  0.1971016  -0.18792252  0.65386724]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 5090 is [True, False, False, False, True, False]
Current timestep = 5091. State = [[-0.23365647  0.00472167]]. Action = [[ 0.16528478  0.05403665 -0.14388855 -0.9291124 ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 5091 is [True, False, False, False, True, False]
Current timestep = 5092. State = [[-0.23284571  0.00812909]]. Action = [[ 0.05964687 -0.10948327 -0.20884097  0.7435169 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 5092 is [True, False, False, False, True, False]
Scene graph at timestep 5092 is [True, False, False, False, True, False]
State prediction error at timestep 5092 is tensor(1.3042e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5093. State = [[-0.2320097   0.00914936]]. Action = [[ 0.03193727 -0.17325972  0.08934331 -0.8549684 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 5093 is [True, False, False, False, True, False]
Current timestep = 5094. State = [[-0.23110205  0.00942742]]. Action = [[ 0.08825216  0.16171783  0.17556691 -0.8902588 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 5094 is [True, False, False, False, True, False]
Current timestep = 5095. State = [[-0.22988923  0.00992601]]. Action = [[ 0.13269255 -0.2104426  -0.21214567  0.31341898]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 5095 is [True, False, False, False, True, False]
Current timestep = 5096. State = [[-0.22805905  0.00956972]]. Action = [[-0.20453332 -0.20080258 -0.07234693  0.36937332]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 5096 is [True, False, False, False, True, False]
Current timestep = 5097. State = [[-0.22714671  0.00756089]]. Action = [[-0.06733483  0.21458966  0.04480705 -0.91361964]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 5097 is [True, False, False, False, True, False]
Scene graph at timestep 5097 is [True, False, False, False, True, False]
State prediction error at timestep 5097 is tensor(5.7632e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5098. State = [[-0.22712865  0.00777682]]. Action = [[ 0.15367213 -0.09834084  0.18434563  0.28271008]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 5098 is [True, False, False, False, True, False]
Current timestep = 5099. State = [[-0.22678418  0.00783236]]. Action = [[-0.03654629  0.15448341 -0.12923971 -0.8996917 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 5099 is [True, False, False, False, True, False]
Current timestep = 5100. State = [[-0.22678843  0.00795247]]. Action = [[ 0.03455725 -0.24060702  0.04796666  0.7299061 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 5100 is [True, False, False, False, True, False]
Current timestep = 5101. State = [[-0.22632599  0.00648775]]. Action = [[-0.15203337 -0.19003293 -0.12754098  0.506474  ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 5101 is [True, False, False, False, True, False]
Current timestep = 5102. State = [[-0.22630326  0.00355457]]. Action = [[ 0.21997201 -0.03251959 -0.06952667 -0.10556   ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 5102 is [True, False, False, False, True, False]
Scene graph at timestep 5102 is [True, False, False, False, True, False]
State prediction error at timestep 5102 is tensor(7.8948e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5103. State = [[-0.22599722  0.00150067]]. Action = [[-0.14155887  0.14426696 -0.05360255  0.52578473]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 5103 is [True, False, False, False, True, False]
Current timestep = 5104. State = [[-0.225894    0.00138546]]. Action = [[ 0.094612   -0.15448381 -0.08202058  0.6784208 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 5104 is [True, False, False, False, True, False]
Current timestep = 5105. State = [[-2.2556095e-01  1.1725877e-04]]. Action = [[0.24654233 0.12687582 0.00856602 0.8129275 ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 5105 is [True, False, False, False, True, False]
Current timestep = 5106. State = [[-0.22404934  0.00034707]]. Action = [[ 0.1038864   0.09044519  0.23460096 -0.1973995 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 5106 is [True, False, False, False, True, False]
Scene graph at timestep 5106 is [True, False, False, False, True, False]
State prediction error at timestep 5106 is tensor(5.3844e-08, grad_fn=<MseLossBackward0>)
Current timestep = 5107. State = [[-0.22180761  0.00073555]]. Action = [[-0.18529972 -0.13903798  0.09510672  0.370093  ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 5107 is [True, False, False, False, True, False]
Current timestep = 5108. State = [[-0.22138493  0.00069965]]. Action = [[-0.10577616  0.08603039  0.2203998  -0.77016217]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 5108 is [True, False, False, False, True, False]
Scene graph at timestep 5108 is [True, False, False, False, True, False]
State prediction error at timestep 5108 is tensor(1.9110e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5109. State = [[-0.22138837  0.00075999]]. Action = [[-0.0494639  -0.0050398   0.00742322  0.17871642]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 5109 is [True, False, False, False, True, False]
Current timestep = 5110. State = [[-0.22146165  0.00070614]]. Action = [[-0.2203438  -0.06944397  0.10290241  0.84669137]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 5110 is [True, False, False, False, True, False]
Current timestep = 5111. State = [[-0.22246869  0.00047045]]. Action = [[-0.06064786  0.04944268 -0.06234406  0.6591518 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 5111 is [True, False, False, False, True, False]
Current timestep = 5112. State = [[-0.22362949  0.00046503]]. Action = [[ 0.01510826 -0.07620299  0.21515232 -0.65773726]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 5112 is [True, False, False, False, True, False]
Current timestep = 5113. State = [[-2.2375241e-01 -2.0593956e-04]]. Action = [[ 0.20750663 -0.18551794 -0.11222249  0.9120743 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 5113 is [True, False, False, False, True, False]
Current timestep = 5114. State = [[-0.22330044 -0.00263516]]. Action = [[-0.19415003 -0.21155287  0.0280318  -0.3446257 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 5114 is [True, False, False, False, True, False]
Current timestep = 5115. State = [[-0.2235263  -0.00735842]]. Action = [[ 0.13314107  0.00172573 -0.20255208 -0.7548547 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 5115 is [True, False, False, False, True, False]
Current timestep = 5116. State = [[-0.22368978 -0.01017178]]. Action = [[-0.06867959  0.15442929  0.1689347  -0.1720807 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 5116 is [True, False, False, False, True, False]
Scene graph at timestep 5116 is [True, False, False, False, True, False]
State prediction error at timestep 5116 is tensor(1.5067e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5117. State = [[-0.22390792 -0.01059022]]. Action = [[-0.09781933  0.15657431  0.13541377 -0.36588687]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 5117 is [True, False, False, False, True, False]
Current timestep = 5118. State = [[-0.22419278 -0.00991398]]. Action = [[-0.14258018 -0.11905843 -0.0840541   0.82636476]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 5118 is [True, False, False, False, True, False]
Current timestep = 5119. State = [[-0.2255456  -0.01027055]]. Action = [[ 0.05746239 -0.07136273 -0.10730815 -0.9215778 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 5119 is [True, False, False, False, True, False]
Current timestep = 5120. State = [[-0.22587366 -0.01110499]]. Action = [[-0.1371442  -0.01493968  0.24356762 -0.43381047]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 5120 is [True, False, False, False, True, False]
Scene graph at timestep 5120 is [True, False, False, False, True, False]
State prediction error at timestep 5120 is tensor(9.6582e-08, grad_fn=<MseLossBackward0>)
Current timestep = 5121. State = [[-0.2270298 -0.0118667]]. Action = [[ 0.07257459 -0.10368863 -0.20177042  0.30753756]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 5121 is [True, False, False, False, True, False]
Scene graph at timestep 5121 is [True, False, False, False, True, False]
State prediction error at timestep 5121 is tensor(5.6450e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5122. State = [[-0.22732691 -0.01326845]]. Action = [[ 0.21311468  0.08763707 -0.13740389  0.66645575]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 5122 is [True, False, False, False, True, False]
Current timestep = 5123. State = [[-0.22731103 -0.01325533]]. Action = [[ 0.05062902  0.00812903 -0.00200552  0.78082705]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 5123 is [True, False, False, False, True, False]
Scene graph at timestep 5123 is [True, False, False, False, True, False]
State prediction error at timestep 5123 is tensor(1.3114e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5124. State = [[-0.22731999 -0.01325712]]. Action = [[-0.21086876 -0.04701024  0.14573434 -0.02032167]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 5124 is [True, False, False, False, True, False]
Current timestep = 5125. State = [[-0.22736824 -0.01350408]]. Action = [[-0.23357098  0.01168978 -0.06909528  0.87514997]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 5125 is [True, False, False, False, True, False]
Current timestep = 5126. State = [[-0.22856472 -0.01400271]]. Action = [[ 0.11021101 -0.13558003  0.1759547   0.09071612]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 5126 is [True, False, False, False, True, False]
Scene graph at timestep 5126 is [True, False, False, False, True, False]
State prediction error at timestep 5126 is tensor(9.9530e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5127. State = [[-0.22886774 -0.01549297]]. Action = [[-0.020888   -0.09486717  0.00566003  0.7163849 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 5127 is [True, False, False, False, True, False]
Current timestep = 5128. State = [[-0.22905372 -0.01724029]]. Action = [[-0.22146104  0.07028091 -0.20524412  0.9605739 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 5128 is [True, False, False, False, True, False]
Current timestep = 5129. State = [[-0.2304661  -0.01870365]]. Action = [[-0.08052817 -0.19687223  0.13647896 -0.2633801 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 5129 is [True, False, False, False, True, False]
Scene graph at timestep 5129 is [True, False, False, False, True, False]
State prediction error at timestep 5129 is tensor(4.2068e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5130. State = [[-0.23234051 -0.02260732]]. Action = [[-0.02383497 -0.176833    0.16586933 -0.54022694]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 5130 is [True, False, False, False, True, False]
Current timestep = 5131. State = [[-0.2338638  -0.02738486]]. Action = [[ 0.22586104 -0.18114153 -0.12412843 -0.64192235]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 5131 is [True, False, False, False, True, False]
Current timestep = 5132. State = [[-0.23398593 -0.03220507]]. Action = [[0.09105244 0.01284769 0.14275372 0.27124202]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 5132 is [True, False, False, False, True, False]
Scene graph at timestep 5132 is [True, False, False, False, True, False]
State prediction error at timestep 5132 is tensor(1.1878e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5132 of -1
Current timestep = 5133. State = [[-0.23421952 -0.03518433]]. Action = [[-0.08779816  0.03873506 -0.16054012 -0.23790634]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 5133 is [True, False, False, False, True, False]
Scene graph at timestep 5133 is [True, False, False, False, True, False]
State prediction error at timestep 5133 is tensor(2.2220e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5134. State = [[-0.23448849 -0.03711738]]. Action = [[ 0.03132948  0.09990424  0.2017765  -0.5827539 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 5134 is [True, False, False, False, True, False]
Human Feedback received at timestep 5134 of -1
Current timestep = 5135. State = [[-0.23442931 -0.03713909]]. Action = [[ 0.14931735 -0.21947137  0.18133724 -0.6282733 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 5135 is [True, False, False, False, True, False]
Current timestep = 5136. State = [[-0.23396812 -0.03902968]]. Action = [[-0.2443166   0.12149581 -0.16386358 -0.05118281]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 5136 is [True, False, False, False, True, False]
Current timestep = 5137. State = [[-0.23425221 -0.03941827]]. Action = [[-0.09928405 -0.05520187  0.24624902 -0.44011968]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 5137 is [True, False, False, False, True, False]
Scene graph at timestep 5137 is [True, False, False, False, True, False]
State prediction error at timestep 5137 is tensor(4.3715e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5137 of -1
Current timestep = 5138. State = [[-0.23490949 -0.04055472]]. Action = [[ 0.2376647  -0.18281637  0.01328003 -0.01989341]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 5138 is [True, False, False, False, True, False]
Current timestep = 5139. State = [[-0.23441531 -0.04260921]]. Action = [[-0.0330766   0.16642708  0.2258951   0.2287091 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 5139 is [True, False, False, False, True, False]
Current timestep = 5140. State = [[-0.23436101 -0.04243508]]. Action = [[ 0.12957555 -0.04978576 -0.16332538 -0.2713732 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 5140 is [True, False, False, False, True, False]
Scene graph at timestep 5140 is [True, False, False, False, True, False]
State prediction error at timestep 5140 is tensor(4.0984e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5141. State = [[-0.23376372 -0.04249088]]. Action = [[-0.14329611 -0.0495308   0.1933977   0.45784545]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 5141 is [True, False, False, False, True, False]
Scene graph at timestep 5141 is [True, False, False, False, True, False]
State prediction error at timestep 5141 is tensor(1.4611e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5142. State = [[-0.23375592 -0.0429226 ]]. Action = [[ 0.21151504  0.14873695 -0.15127634 -0.18228298]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 5142 is [True, False, False, False, True, False]
Current timestep = 5143. State = [[-0.23359878 -0.04276775]]. Action = [[ 0.181445   -0.10086957 -0.18818793 -0.14303368]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 5143 is [True, False, False, False, True, False]
Scene graph at timestep 5143 is [True, False, False, False, True, False]
State prediction error at timestep 5143 is tensor(1.3454e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5144. State = [[-0.23253022 -0.04275721]]. Action = [[-0.1118066   0.0958367   0.13551104  0.7839875 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 5144 is [True, False, False, False, True, False]
Current timestep = 5145. State = [[-0.23220856 -0.04265119]]. Action = [[-0.09640145 -0.00299841  0.12555957  0.9269273 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 5145 is [True, False, False, False, True, False]
Current timestep = 5146. State = [[-0.23225406 -0.0426169 ]]. Action = [[-0.05130082  0.0690465  -0.06601295 -0.8589334 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 5146 is [True, False, False, False, True, False]
Human Feedback received at timestep 5146 of -1
Current timestep = 5147. State = [[-0.2322324  -0.04238736]]. Action = [[-0.04719007 -0.11910877 -0.20710722 -0.15305138]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 5147 is [True, False, False, False, True, False]
Current timestep = 5148. State = [[-0.2322542  -0.04250745]]. Action = [[-0.02841833 -0.05217852  0.0133495   0.30710387]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 5148 is [True, False, False, False, True, False]
Current timestep = 5149. State = [[-0.23232305 -0.04300018]]. Action = [[-0.22617252 -0.16683055  0.08381212  0.6956639 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 5149 is [True, False, False, False, True, False]
Current timestep = 5150. State = [[-0.233501   -0.04517187]]. Action = [[ 0.23236841 -0.08994162  0.0328767  -0.6776944 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 5150 is [True, False, False, False, True, False]
Current timestep = 5151. State = [[-0.23356265 -0.04738911]]. Action = [[-0.13798778  0.17291188  0.12313759  0.09038913]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 5151 is [True, False, False, False, True, False]
Current timestep = 5152. State = [[-0.23372443 -0.04758036]]. Action = [[ 0.16059208 -0.01798984 -0.08585799  0.03829086]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 5152 is [True, False, False, False, True, False]
Current timestep = 5153. State = [[-0.23365204 -0.04772557]]. Action = [[ 0.13689345 -0.13547112 -0.13338546 -0.2148205 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 5153 is [True, False, False, False, True, False]
Current timestep = 5154. State = [[-0.23299143 -0.04919102]]. Action = [[-0.05692476 -0.10961799 -0.15965827  0.16146886]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 5154 is [True, False, False, False, True, False]
Current timestep = 5155. State = [[-0.23271845 -0.05192672]]. Action = [[-0.21012837 -0.1126643   0.07956207  0.15850532]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 5155 is [True, False, False, False, True, False]
Current timestep = 5156. State = [[-0.2332009  -0.05481287]]. Action = [[0.2095393  0.20366663 0.08518365 0.8857229 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 5156 is [True, False, False, False, True, False]
Current timestep = 5157. State = [[-0.23333769 -0.05486834]]. Action = [[-0.06388693  0.10873052 -0.20015214  0.8612025 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 5157 is [True, False, False, False, True, False]
Scene graph at timestep 5157 is [True, False, False, False, True, False]
State prediction error at timestep 5157 is tensor(3.2602e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5158. State = [[-0.23335074 -0.05465834]]. Action = [[ 0.21485874  0.06606206 -0.22883655 -0.57265025]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 5158 is [True, False, False, False, True, False]
Current timestep = 5159. State = [[-0.23322847 -0.05378838]]. Action = [[-0.2459623   0.16860855 -0.22042626 -0.3779987 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 5159 is [True, False, False, False, True, False]
Human Feedback received at timestep 5159 of -1
Current timestep = 5160. State = [[-0.23345973 -0.05148447]]. Action = [[ 0.11522675  0.17417023 -0.03049892 -0.6513701 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 5160 is [True, False, False, False, True, False]
Current timestep = 5161. State = [[-0.23354715 -0.04809424]]. Action = [[ 0.16830039  0.0823915  -0.05934393  0.7652744 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 5161 is [True, False, False, False, True, False]
Current timestep = 5162. State = [[-0.23308237 -0.04470624]]. Action = [[-0.03383639 -0.11071892  0.20423907 -0.8694051 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 5162 is [True, False, False, False, True, False]
Current timestep = 5163. State = [[-0.23266688 -0.04324312]]. Action = [[0.11413538 0.04234421 0.22960347 0.8644376 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 5163 is [True, False, False, False, True, False]
Current timestep = 5164. State = [[-0.23190239 -0.04178769]]. Action = [[-0.02407095  0.01589909 -0.14791763 -0.4065938 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 5164 is [True, False, False, False, True, False]
Current timestep = 5165. State = [[-0.23132509 -0.04103713]]. Action = [[ 0.23695502 -0.10227183  0.02449417 -0.5934577 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 5165 is [True, False, False, False, True, False]
Scene graph at timestep 5165 is [True, False, False, False, True, False]
State prediction error at timestep 5165 is tensor(3.9695e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5166. State = [[-0.22857623 -0.04085728]]. Action = [[ 0.2005691  -0.0620032   0.10313633 -0.21565425]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 5166 is [True, False, False, False, True, False]
Scene graph at timestep 5166 is [True, False, False, False, True, False]
State prediction error at timestep 5166 is tensor(1.9890e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5167. State = [[-0.22483496 -0.04132294]]. Action = [[-0.02413702 -0.08737466 -0.21017794 -0.39392233]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 5167 is [True, False, False, False, True, False]
Current timestep = 5168. State = [[-0.22269401 -0.04195974]]. Action = [[-0.2358033   0.06161368  0.01865765 -0.07251364]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 5168 is [True, False, False, False, True, False]
Current timestep = 5169. State = [[-0.22243431 -0.0419753 ]]. Action = [[ 0.19140705  0.02255535 -0.11996602  0.33409357]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 5169 is [True, False, False, False, True, False]
Current timestep = 5170. State = [[-0.22188327 -0.04173865]]. Action = [[-0.17325445 -0.05610907  0.18752643  0.2191639 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 5170 is [True, False, False, False, True, False]
Current timestep = 5171. State = [[-0.22201334 -0.0419266 ]]. Action = [[0.11051905 0.1078313  0.03147897 0.65250087]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 5171 is [True, False, False, False, True, False]
Current timestep = 5172. State = [[-0.22200744 -0.04175538]]. Action = [[ 0.24258447  0.10346922 -0.19761711  0.8634125 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 5172 is [True, False, False, False, True, False]
Current timestep = 5173. State = [[-0.22010294 -0.04081294]]. Action = [[ 0.1515103  -0.10094479 -0.09066251  0.57749367]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 5173 is [True, False, False, False, True, False]
Current timestep = 5174. State = [[-0.21749952 -0.04057365]]. Action = [[ 0.04704103 -0.11000988  0.14890033  0.1849277 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 5174 is [True, False, False, False, True, False]
Current timestep = 5175. State = [[-0.21499038 -0.04120965]]. Action = [[ 0.19086766 -0.068749    0.0872767  -0.7965073 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 5175 is [True, False, False, False, True, False]
Current timestep = 5176. State = [[-0.21220842 -0.04203992]]. Action = [[-0.22128503  0.16621816 -0.01364391  0.40098107]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 5176 is [True, False, False, False, True, False]
Scene graph at timestep 5176 is [True, False, False, False, True, False]
State prediction error at timestep 5176 is tensor(3.8958e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5177. State = [[-0.21186267 -0.04190902]]. Action = [[ 0.00505051 -0.02945632 -0.04715654  0.06442785]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 5177 is [True, False, False, False, True, False]
Current timestep = 5178. State = [[-0.21181278 -0.04183424]]. Action = [[-0.23481071 -0.04106668 -0.05665699  0.5041057 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 5178 is [True, False, False, False, True, False]
Current timestep = 5179. State = [[-0.21181278 -0.04183424]]. Action = [[ 0.01280147  0.07887557 -0.19887339  0.10903978]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 5179 is [True, False, False, False, True, False]
Current timestep = 5180. State = [[-0.21181571 -0.04165756]]. Action = [[-0.10330141  0.17379814  0.21070462  0.16032481]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 5180 is [True, False, False, False, True, False]
Scene graph at timestep 5180 is [True, False, False, False, True, False]
State prediction error at timestep 5180 is tensor(6.3620e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5181. State = [[-0.21205223 -0.04019516]]. Action = [[ 0.1749621  -0.218621    0.02656811  0.28882778]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 5181 is [True, False, False, False, True, False]
Current timestep = 5182. State = [[-0.21194103 -0.04057362]]. Action = [[ 2.4255472e-01 -4.8312545e-04  1.5037137e-01  7.9040718e-01]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 5182 is [True, False, False, False, True, False]
Current timestep = 5183. State = [[-0.21130143 -0.04079606]]. Action = [[-0.08870625  0.05953881  0.0913077   0.08362389]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 5183 is [True, False, False, False, True, False]
Scene graph at timestep 5183 is [True, False, False, False, True, False]
State prediction error at timestep 5183 is tensor(2.4018e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5184. State = [[-0.21118103 -0.04060024]]. Action = [[-0.09242137  0.06687003  0.13046128 -0.54035777]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 5184 is [True, False, False, False, True, False]
Scene graph at timestep 5184 is [True, False, False, False, True, False]
State prediction error at timestep 5184 is tensor(7.8788e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5185. State = [[-0.21113934 -0.04035209]]. Action = [[ 0.05178335 -0.1263722  -0.06133245  0.03980958]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 5185 is [True, False, False, False, True, False]
Current timestep = 5186. State = [[-0.21110453 -0.04071285]]. Action = [[-0.13075699 -0.16353193 -0.01482403  0.4232906 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 5186 is [True, False, False, False, True, False]
Current timestep = 5187. State = [[-0.21112412 -0.04205093]]. Action = [[ 0.20211568  0.16813654 -0.06219576 -0.5241316 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 5187 is [True, False, False, False, True, False]
Current timestep = 5188. State = [[-0.21100205 -0.04209011]]. Action = [[ 0.17340958 -0.16006435  0.10857207 -0.83805054]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 5188 is [True, False, False, False, True, False]
Current timestep = 5189. State = [[-0.2098303  -0.04283709]]. Action = [[-0.07224888  0.04119006  0.12867716  0.17229402]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 5189 is [True, False, False, False, True, False]
Current timestep = 5190. State = [[-0.20897184 -0.04292075]]. Action = [[-0.09500466  0.04992872 -0.10198921 -0.62055725]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 5190 is [True, False, False, False, True, False]
Current timestep = 5191. State = [[-0.20870908 -0.04269105]]. Action = [[-0.09034936  0.23417318  0.02543098 -0.26868463]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 5191 is [True, False, False, False, True, False]
Current timestep = 5192. State = [[-0.20882009 -0.04086805]]. Action = [[-0.12315813  0.1787898   0.00887045 -0.50303954]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 5192 is [True, False, False, False, True, False]
Current timestep = 5193. State = [[-0.20912069 -0.03850469]]. Action = [[ 0.10211837 -0.0977824  -0.00936779 -0.04242915]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 5193 is [True, False, False, False, True, False]
Current timestep = 5194. State = [[-0.20909747 -0.0380913 ]]. Action = [[-0.09752759 -0.1802525   0.211676    0.18409014]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 5194 is [True, False, False, False, True, False]
Current timestep = 5195. State = [[-0.20902604 -0.03862619]]. Action = [[ 0.23760879  0.13539565 -0.1966754  -0.79524803]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 5195 is [True, False, False, False, True, False]
Current timestep = 5196. State = [[-0.20890163 -0.03825385]]. Action = [[ 0.15861851  0.06036824 -0.06330866 -0.2722953 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 5196 is [True, False, False, False, True, False]
Current timestep = 5197. State = [[-0.2077399  -0.03740007]]. Action = [[-0.2395501  -0.06335799 -0.06815711 -0.38900673]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 5197 is [True, False, False, False, True, False]
Current timestep = 5198. State = [[-0.20781034 -0.0374123 ]]. Action = [[ 0.02757007 -0.07214561  0.13277704  0.17804778]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 5198 is [True, False, False, False, True, False]
Current timestep = 5199. State = [[-0.20773451 -0.03763545]]. Action = [[-0.08114035 -0.23928656 -0.05965292 -0.49106675]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 5199 is [True, False, False, False, True, False]
Current timestep = 5200. State = [[-0.20758112 -0.04037616]]. Action = [[ 0.16453278  0.03812969 -0.19392873  0.5795815 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 5200 is [True, False, False, False, True, False]
Current timestep = 5201. State = [[-0.20738477 -0.04095374]]. Action = [[ 0.04797876 -0.10884893  0.16209146  0.95583344]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 5201 is [True, False, False, False, True, False]
Current timestep = 5202. State = [[-0.20685051 -0.04243655]]. Action = [[ 0.13528174 -0.04715505  0.13415337  0.50184536]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 5202 is [True, False, False, False, True, False]
Current timestep = 5203. State = [[-0.20566912 -0.04411732]]. Action = [[0.219594   0.02990413 0.21568066 0.17853308]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 5203 is [True, False, False, False, True, False]
Current timestep = 5204. State = [[-0.20245317 -0.04491761]]. Action = [[-0.06409033  0.18329862 -0.01280807 -0.89703065]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 5204 is [True, False, False, False, True, False]
Scene graph at timestep 5204 is [True, False, False, False, True, False]
State prediction error at timestep 5204 is tensor(3.3359e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5205. State = [[-0.19910432 -0.0448718 ]]. Action = [[ 0.07618386 -0.01472098  0.24397686  0.39799726]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 5205 is [True, False, False, False, True, False]
Current timestep = 5206. State = [[-0.19618233 -0.04487151]]. Action = [[ 0.09463722 -0.05631831  0.15630293 -0.07450813]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 5206 is [True, False, False, False, True, False]
Scene graph at timestep 5206 is [True, False, False, False, True, False]
State prediction error at timestep 5206 is tensor(7.9425e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5207. State = [[-0.19267054 -0.04486667]]. Action = [[ 0.08931935  0.23307729 -0.22641422  0.54184484]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 5207 is [True, False, False, False, True, False]
Current timestep = 5208. State = [[-0.18938136 -0.04333111]]. Action = [[-0.01849027 -0.21864976 -0.22876902 -0.65422696]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 5208 is [True, False, False, False, True, False]
Human Feedback received at timestep 5208 of 1
Current timestep = 5209. State = [[-0.18772407 -0.04368715]]. Action = [[ 0.17301506 -0.05751386  0.1620633  -0.6020332 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 5209 is [True, False, False, False, True, False]
Current timestep = 5210. State = [[-0.18621553 -0.04461122]]. Action = [[-0.13659656 -0.20769346  0.07317996 -0.51953816]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 5210 is [True, False, False, False, True, False]
Current timestep = 5211. State = [[-0.18584637 -0.04690542]]. Action = [[0.20863718 0.00160083 0.24593595 0.52730095]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 5211 is [True, False, False, False, True, False]
Current timestep = 5212. State = [[-0.1845046  -0.04903232]]. Action = [[-0.06920633 -0.04058997 -0.08121356  0.05266547]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 5212 is [True, False, False, False, True, False]
Current timestep = 5213. State = [[-0.18383723 -0.05101463]]. Action = [[ 0.13260621 -0.16929018 -0.2109529  -0.35689783]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 5213 is [True, False, False, False, True, False]
Human Feedback received at timestep 5213 of 1
Current timestep = 5214. State = [[-0.18225637 -0.05429863]]. Action = [[0.04286826 0.05326977 0.20608407 0.44007647]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 5214 is [True, False, False, False, True, False]
Scene graph at timestep 5214 is [True, False, False, False, True, False]
State prediction error at timestep 5214 is tensor(1.2335e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5215. State = [[-0.18054575 -0.0559087 ]]. Action = [[ 0.10590512 -0.10355464 -0.01746774  0.23462462]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 5215 is [True, False, False, False, True, False]
Current timestep = 5216. State = [[-0.17778204 -0.05761357]]. Action = [[-0.20889112  0.05996019  0.03701922  0.9678402 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 5216 is [True, False, False, False, True, False]
Scene graph at timestep 5216 is [True, False, False, False, True, False]
State prediction error at timestep 5216 is tensor(4.1052e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5217. State = [[-0.17702402 -0.05819805]]. Action = [[ 0.04271132 -0.24500994 -0.04795361  0.6540226 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 5217 is [True, False, False, False, True, False]
Scene graph at timestep 5217 is [True, False, False, False, True, False]
State prediction error at timestep 5217 is tensor(2.5510e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5218. State = [[-0.17647974 -0.06142928]]. Action = [[-0.17841522  0.18958437 -0.03536744  0.8733958 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 5218 is [True, False, False, False, True, False]
Current timestep = 5219. State = [[-0.17647061 -0.06148426]]. Action = [[-0.06253909  0.08428663 -0.21394184 -0.00041199]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 5219 is [True, False, False, False, True, False]
Current timestep = 5220. State = [[-0.17653172 -0.0612746 ]]. Action = [[ 0.02188906 -0.03493416 -0.18189842 -0.8144492 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 5220 is [True, False, False, False, True, False]
Current timestep = 5221. State = [[-0.1765729  -0.06138181]]. Action = [[-0.21671455 -0.12266991 -0.06852093 -0.00654256]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 5221 is [True, False, False, False, True, False]
Current timestep = 5222. State = [[-0.17691092 -0.06243608]]. Action = [[ 0.19310755  0.04268661 -0.16985205  0.44827056]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 5222 is [True, False, False, False, True, False]
Current timestep = 5223. State = [[-0.17694975 -0.06250986]]. Action = [[ 0.12369108  0.10209012 -0.2180211   0.2759893 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 5223 is [True, False, False, False, True, False]
Scene graph at timestep 5223 is [True, False, False, False, True, False]
State prediction error at timestep 5223 is tensor(4.7908e-08, grad_fn=<MseLossBackward0>)
Current timestep = 5224. State = [[-0.17689414 -0.06222053]]. Action = [[ 0.16549751  0.1731379  -0.05858935  0.18994725]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 5224 is [True, False, False, False, True, False]
Current timestep = 5225. State = [[-0.17616928 -0.06015212]]. Action = [[ 0.03744003  0.12491199  0.21537602 -0.70184016]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 5225 is [True, False, False, False, True, False]
Current timestep = 5226. State = [[-0.17508873 -0.05774269]]. Action = [[0.15790164 0.06112513 0.11790034 0.48670435]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 5226 is [True, False, False, False, True, False]
Current timestep = 5227. State = [[-0.17293382 -0.05560106]]. Action = [[ 0.13083816  0.11956048 -0.02224746  0.22626877]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 5227 is [True, False, False, False, True, False]
Current timestep = 5228. State = [[-0.16985556 -0.05323374]]. Action = [[-0.22551684 -0.18109    -0.06113926 -0.3845924 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 5228 is [True, False, False, False, True, False]
Scene graph at timestep 5228 is [True, False, False, False, True, False]
State prediction error at timestep 5228 is tensor(7.7990e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5229. State = [[-0.16889088 -0.05318712]]. Action = [[ 0.15800944 -0.11776574 -0.23048663  0.58452535]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 5229 is [True, False, False, False, True, False]
Scene graph at timestep 5229 is [True, False, False, False, True, False]
State prediction error at timestep 5229 is tensor(2.9799e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5230. State = [[-0.16751204 -0.05393705]]. Action = [[ 0.23435551 -0.15395992  0.09615153 -0.39005202]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 5230 is [True, False, False, False, True, False]
Scene graph at timestep 5230 is [True, False, False, False, True, False]
State prediction error at timestep 5230 is tensor(2.5454e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5231. State = [[-0.16411805 -0.05599177]]. Action = [[ 0.1522364 -0.2052034 -0.1008426  0.4293741]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 5231 is [True, False, False, False, True, False]
Human Feedback received at timestep 5231 of 1
Current timestep = 5232. State = [[-0.15985045 -0.0592254 ]]. Action = [[0.19054809 0.12016988 0.17798954 0.7041404 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 5232 is [True, False, False, False, True, False]
Current timestep = 5233. State = [[-0.1552857  -0.05994863]]. Action = [[ 0.05048254  0.12275693 -0.18528362 -0.51707083]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 5233 is [True, False, False, False, True, False]
Current timestep = 5234. State = [[-0.15094867 -0.05961027]]. Action = [[-0.12046358  0.19996923 -0.04044193  0.34413636]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 5234 is [True, False, False, False, True, False]
Scene graph at timestep 5234 is [True, False, False, False, True, False]
State prediction error at timestep 5234 is tensor(4.3101e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5235. State = [[-0.14897545 -0.05766328]]. Action = [[ 0.22044736 -0.09881002 -0.07580405  0.70172477]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 5235 is [True, False, False, False, True, False]
Current timestep = 5236. State = [[-0.1464388  -0.05709531]]. Action = [[ 0.08812886  0.10322854 -0.14135642  0.1327126 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 5236 is [True, False, False, False, True, False]
Human Feedback received at timestep 5236 of 1
Current timestep = 5237. State = [[-0.14286013 -0.05572644]]. Action = [[0.07520252 0.10925701 0.18653834 0.46951318]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 5237 is [True, False, False, False, True, False]
Current timestep = 5238. State = [[-0.13902175 -0.05357069]]. Action = [[ 0.05606323 -0.20011838  0.18196118  0.33297276]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 5238 is [True, False, False, False, True, False]
Current timestep = 5239. State = [[-0.13604616 -0.05394376]]. Action = [[ 0.13675714 -0.10774297  0.08475566  0.02154422]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 5239 is [True, False, False, False, True, False]
Current timestep = 5240. State = [[-0.13383505 -0.05471703]]. Action = [[ 0.21316269 -0.09183061  0.21636456  0.6938777 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 5240 is [True, False, False, False, True, False]
Current timestep = 5241. State = [[-0.12988354 -0.05629416]]. Action = [[-0.2235901  -0.06729929  0.23733655  0.8532133 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 5241 is [True, False, False, False, True, False]
Current timestep = 5242. State = [[-0.12818298 -0.05805441]]. Action = [[-0.1765645  -0.15755883  0.21866387  0.55964303]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 5242 is [True, False, False, False, True, False]
Human Feedback received at timestep 5242 of 1
Current timestep = 5243. State = [[-0.12771691 -0.06164212]]. Action = [[ 0.21335709 -0.20321727 -0.08463447  0.7237966 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 5243 is [True, False, False, False, True, False]
Current timestep = 5244. State = [[-0.12678339 -0.06703243]]. Action = [[-0.03598192  0.24820739  0.14895248 -0.5436942 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 5244 is [True, False, False, False, True, False]
Scene graph at timestep 5244 is [True, False, False, False, True, False]
State prediction error at timestep 5244 is tensor(1.3728e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5245. State = [[-0.12663236 -0.06709988]]. Action = [[ 0.2360478  -0.0327239   0.13814062 -0.27595788]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 5245 is [True, False, False, False, True, False]
Scene graph at timestep 5245 is [True, False, False, False, True, False]
State prediction error at timestep 5245 is tensor(1.5292e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5246. State = [[-0.12463717 -0.06687488]]. Action = [[-0.00396301  0.23937047 -0.04636677  0.04691279]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 5246 is [True, False, False, False, True, False]
Current timestep = 5247. State = [[-0.12261308 -0.06509557]]. Action = [[ 0.19650441 -0.08331299  0.13776505 -0.6484942 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 5247 is [True, False, False, False, True, False]
Human Feedback received at timestep 5247 of 1
Current timestep = 5248. State = [[-0.11974958 -0.06461613]]. Action = [[ 0.19935045  0.04390541 -0.01364058  0.06510365]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 5248 is [True, False, False, False, True, False]
Scene graph at timestep 5248 is [True, False, False, False, True, False]
State prediction error at timestep 5248 is tensor(3.1408e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5249. State = [[-0.11469056 -0.06396476]]. Action = [[ 0.13248885 -0.14064793  0.15536046 -0.3766389 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 5249 is [True, False, False, False, True, False]
Current timestep = 5250. State = [[-0.10953789 -0.06455275]]. Action = [[ 0.07564354 -0.2478587  -0.22173302 -0.52288496]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 5250 is [True, False, False, False, True, False]
Current timestep = 5251. State = [[-0.10531141 -0.06745044]]. Action = [[0.17589012 0.04943818 0.13065755 0.14498842]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 5251 is [True, False, False, False, True, False]
Current timestep = 5252. State = [[-0.10021965 -0.06956372]]. Action = [[ 0.10083219 -0.1616745   0.2026099   0.21337819]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 5252 is [True, False, False, False, True, False]
Scene graph at timestep 5252 is [True, False, False, False, True, False]
State prediction error at timestep 5252 is tensor(1.0569e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5253. State = [[-0.09494945 -0.07252254]]. Action = [[0.2209863  0.20644113 0.06974781 0.4379356 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 5253 is [True, False, False, False, True, False]
Current timestep = 5254. State = [[-0.08888483 -0.07205062]]. Action = [[0.14690042 0.19441965 0.1607947  0.31667292]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 5254 is [True, False, False, False, True, False]
Human Feedback received at timestep 5254 of 1
Current timestep = 5255. State = [[-0.08325945 -0.07005827]]. Action = [[-0.15560816 -0.01489291  0.14012975 -0.94203854]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 5255 is [True, False, False, False, True, False]
Current timestep = 5256. State = [[-0.08011881 -0.0690634 ]]. Action = [[-0.08384457 -0.07148007 -0.22875525  0.90600204]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 5256 is [True, False, False, False, True, False]
Current timestep = 5257. State = [[-0.07949974 -0.06909999]]. Action = [[ 0.21243507  0.16215613 -0.20596063 -0.1409074 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 5257 is [True, False, False, False, True, False]
Current timestep = 5258. State = [[-0.07788106 -0.06757362]]. Action = [[ 0.04322299 -0.07135639 -0.04363242 -0.39059722]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 5258 is [True, False, False, False, True, False]
Scene graph at timestep 5258 is [True, False, False, False, True, False]
State prediction error at timestep 5258 is tensor(3.5734e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5259. State = [[-0.0759962 -0.0668992]]. Action = [[ 0.18695176  0.19956374  0.11080119 -0.3138939 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 5259 is [True, False, False, False, True, False]
Human Feedback received at timestep 5259 of 1
Current timestep = 5260. State = [[-0.07208081 -0.06424438]]. Action = [[0.13521639 0.1851395  0.00487223 0.780805  ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 5260 is [True, False, False, False, True, False]
Current timestep = 5261. State = [[-0.06852672 -0.06108918]]. Action = [[-0.16812687 -0.21685888 -0.05249886 -0.4690761 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 5261 is [True, False, False, False, True, False]
Current timestep = 5262. State = [[-0.06639465 -0.06110995]]. Action = [[ 0.00640029  0.06929603  0.13956398 -0.195485  ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 5262 is [True, False, False, False, True, False]
Current timestep = 5263. State = [[-0.06478181 -0.06085615]]. Action = [[-0.13307765 -0.13627519 -0.16651897 -0.5366604 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 5263 is [True, False, False, False, True, False]
Current timestep = 5264. State = [[-0.06494255 -0.06089099]]. Action = [[ 0.18889207  0.00686809 -0.06885424  0.5289495 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 5264 is [True, False, False, False, True, False]
Current timestep = 5265. State = [[-0.06420837 -0.06131922]]. Action = [[ 0.144912   -0.06413995 -0.10432217  0.8344053 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 5265 is [True, False, False, False, True, False]
Current timestep = 5266. State = [[-0.06282723 -0.0620897 ]]. Action = [[-0.1742814  -0.13637805  0.02596712 -0.15358365]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 5266 is [True, False, False, False, True, False]
Human Feedback received at timestep 5266 of 1
Current timestep = 5267. State = [[-0.06271558 -0.06368391]]. Action = [[-0.01727316  0.21965551  0.13749611 -0.00832832]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 5267 is [True, False, False, False, True, False]
Scene graph at timestep 5267 is [True, False, False, False, True, False]
State prediction error at timestep 5267 is tensor(2.0284e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5268. State = [[-0.0628326  -0.06301984]]. Action = [[0.00140902 0.05752772 0.03815955 0.35346687]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 5268 is [True, False, False, False, True, False]
Current timestep = 5269. State = [[-0.06292547 -0.06226655]]. Action = [[ 0.18055367 -0.0624022   0.14759395  0.7713785 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 5269 is [True, False, False, False, True, False]
Current timestep = 5270. State = [[-0.06229339 -0.06251766]]. Action = [[ 0.03529489 -0.05768353  0.23830658  0.34852648]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 5270 is [True, False, False, False, True, False]
Current timestep = 5271. State = [[-0.06195734 -0.06263803]]. Action = [[-0.00494619  0.06366572 -0.20779876  0.480901  ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 5271 is [True, False, False, False, True, False]
Scene graph at timestep 5271 is [True, False, False, False, True, False]
State prediction error at timestep 5271 is tensor(6.8637e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5272. State = [[-0.06143196 -0.06238225]]. Action = [[ 0.12310922  0.1424169  -0.09618777  0.00971556]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 5272 is [True, False, False, False, True, False]
Current timestep = 5273. State = [[-0.05978938 -0.06065965]]. Action = [[-0.0064299   0.01833868  0.11916408 -0.6485124 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 5273 is [True, False, False, False, True, False]
Current timestep = 5274. State = [[-0.05794656 -0.05955225]]. Action = [[-0.12890261 -0.08853346 -0.24209546 -0.6950695 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 5274 is [True, False, False, False, True, False]
Scene graph at timestep 5274 is [True, False, False, False, True, False]
State prediction error at timestep 5274 is tensor(2.6094e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5274 of 1
Current timestep = 5275. State = [[-0.0567719 -0.0596162]]. Action = [[ 0.2102105   0.06021604  0.1376707  -0.71075624]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 5275 is [True, False, False, False, True, False]
Scene graph at timestep 5275 is [True, False, False, False, True, False]
State prediction error at timestep 5275 is tensor(7.4816e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5276. State = [[-0.0545179  -0.05947864]]. Action = [[-0.19926113 -0.09610668 -0.23052873  0.8507893 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 5276 is [True, False, False, False, True, False]
Scene graph at timestep 5276 is [True, False, False, False, True, False]
State prediction error at timestep 5276 is tensor(4.9181e-08, grad_fn=<MseLossBackward0>)
Current timestep = 5277. State = [[-0.0540812  -0.05978521]]. Action = [[ 0.20567942 -0.12776439 -0.22354698  0.33494794]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 5277 is [True, False, False, False, True, False]
Current timestep = 5278. State = [[-0.0531874  -0.06103866]]. Action = [[ 0.14453185 -0.10490844 -0.2429254   0.62738395]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 5278 is [True, False, False, False, True, False]
Current timestep = 5279. State = [[-0.05180022 -0.0629869 ]]. Action = [[ 0.10252202 -0.08182034 -0.05751942  0.17538881]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 5279 is [True, False, False, False, True, False]
Current timestep = 5280. State = [[-0.04976763 -0.06512747]]. Action = [[-0.0568504   0.19675463 -0.23866676  0.5711405 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 5280 is [True, False, False, False, True, False]
Current timestep = 5281. State = [[-0.04810658 -0.06455717]]. Action = [[0.15461591 0.08229077 0.15043783 0.20402384]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 5281 is [False, True, False, False, True, False]
Scene graph at timestep 5281 is [False, True, False, False, True, False]
State prediction error at timestep 5281 is tensor(4.5905e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5281 of 1
Current timestep = 5282. State = [[-0.04502787 -0.06350026]]. Action = [[ 0.06265533  0.12306768 -0.18636945  0.42782617]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 5282 is [False, True, False, False, True, False]
Current timestep = 5283. State = [[-0.04196899 -0.06162447]]. Action = [[0.2182306  0.08179265 0.00706273 0.9092239 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 5283 is [False, True, False, False, True, False]
Current timestep = 5284. State = [[-0.2634371   0.03146531]]. Action = [[ 0.18391073 -0.13701567 -0.0724037   0.62984943]]. Reward = [100.]
Curr episode timestep = 364
Scene graph at timestep 5284 is [False, True, False, False, True, False]
Human Feedback received at timestep 5284 of 1
Current timestep = 5285. State = [[-0.26247358  0.03407862]]. Action = [[-0.08573486 -0.02920632  0.10702914 -0.63142663]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 5285 is [True, False, False, False, True, False]
Current timestep = 5286. State = [[-0.26247358  0.03407862]]. Action = [[-0.04046258 -0.03085382  0.14625767 -0.45209384]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 5286 is [True, False, False, False, True, False]
Current timestep = 5287. State = [[-0.26242876  0.03406624]]. Action = [[ 0.23592743  0.06509954 -0.12335178 -0.0845015 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 5287 is [True, False, False, False, True, False]
Current timestep = 5288. State = [[-0.2616265   0.03406669]]. Action = [[ 0.13719997 -0.17593262  0.10742268  0.34374   ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 5288 is [True, False, False, False, True, False]
Scene graph at timestep 5288 is [True, False, False, False, True, False]
State prediction error at timestep 5288 is tensor(3.8407e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5289. State = [[-0.25984833  0.03384641]]. Action = [[-0.02097444 -0.041307   -0.17060268 -0.89977604]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 5289 is [True, False, False, False, True, False]
Current timestep = 5290. State = [[-0.2589967   0.03339435]]. Action = [[-0.22951357 -0.176814    0.15941852 -0.19814909]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5290 is [True, False, False, False, True, False]
Current timestep = 5291. State = [[-0.25885713  0.03115758]]. Action = [[-0.05680266 -0.003663   -0.0675557  -0.17211813]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 5291 is [True, False, False, False, True, False]
Current timestep = 5292. State = [[-0.25900486  0.02940229]]. Action = [[-0.05803528 -0.01908579  0.03200403 -0.5431178 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 5292 is [True, False, False, False, True, False]
Current timestep = 5293. State = [[-0.25904953  0.02822457]]. Action = [[ 0.13523877  0.15263253 -0.03710593  0.8410814 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 5293 is [True, False, False, False, True, False]
Scene graph at timestep 5293 is [True, False, False, False, True, False]
State prediction error at timestep 5293 is tensor(6.9007e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5294. State = [[-0.25913903  0.02851153]]. Action = [[-0.03212523  0.03051144 -0.1336578   0.5092387 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 5294 is [True, False, False, False, True, False]
Current timestep = 5295. State = [[-0.2592229   0.02887163]]. Action = [[ 0.18578935  0.20282686 -0.14404953 -0.620624  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 5295 is [True, False, False, False, True, False]
Current timestep = 5296. State = [[-0.25924563  0.03030996]]. Action = [[ 0.1400528  -0.11556834 -0.21828136  0.7968714 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 5296 is [True, False, False, False, True, False]
Current timestep = 5297. State = [[-0.2577708   0.03035338]]. Action = [[ 0.18197477  0.05946213 -0.23119237 -0.5323747 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 5297 is [True, False, False, False, True, False]
Scene graph at timestep 5297 is [True, False, False, False, True, False]
State prediction error at timestep 5297 is tensor(1.3717e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5298. State = [[-0.25425804  0.03064615]]. Action = [[ 0.03296539 -0.02531436  0.04867497 -0.5418239 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 5298 is [True, False, False, False, True, False]
Current timestep = 5299. State = [[-0.2515      0.03092501]]. Action = [[-0.04322623  0.12798238 -0.17198549  0.23978055]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 5299 is [True, False, False, False, True, False]
Current timestep = 5300. State = [[-0.24950883  0.03228185]]. Action = [[0.01573008 0.02194482 0.05908036 0.9780843 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 5300 is [True, False, False, False, True, False]
Current timestep = 5301. State = [[-0.2487459   0.03306616]]. Action = [[ 0.02938625  0.03234056 -0.1235178  -0.8726418 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 5301 is [True, False, False, False, True, False]
Current timestep = 5302. State = [[-0.24813654  0.03365137]]. Action = [[ 0.06467703 -0.24101476  0.03706038 -0.66697115]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 5302 is [True, False, False, False, True, False]
Current timestep = 5303. State = [[-0.24709898  0.03316583]]. Action = [[ 0.02979356 -0.1293276   0.16891748 -0.889115  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 5303 is [True, False, False, False, True, False]
Current timestep = 5304. State = [[-0.24644841  0.03212032]]. Action = [[ 0.0240387   0.03915885  0.0496265  -0.02988714]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 5304 is [True, False, False, False, True, False]
Current timestep = 5305. State = [[-0.24569902  0.03162959]]. Action = [[ 0.11595541 -0.14054613  0.09833956  0.8586123 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 5305 is [True, False, False, False, True, False]
Scene graph at timestep 5305 is [True, False, False, False, True, False]
State prediction error at timestep 5305 is tensor(2.0308e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5305 of 1
Current timestep = 5306. State = [[-0.2439166   0.02980134]]. Action = [[ 0.08369818 -0.13772146 -0.03824291  0.26709425]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 5306 is [True, False, False, False, True, False]
Current timestep = 5307. State = [[-0.2416476   0.02707082]]. Action = [[ 0.03299442 -0.06226964 -0.24270873 -0.68574655]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 5307 is [True, False, False, False, True, False]
Current timestep = 5308. State = [[-0.23966715  0.02492606]]. Action = [[-0.14733271  0.22250938  0.13349986  0.70500994]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 5308 is [True, False, False, False, True, False]
Scene graph at timestep 5308 is [True, False, False, False, True, False]
State prediction error at timestep 5308 is tensor(5.1204e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5309. State = [[-0.23971303  0.02512897]]. Action = [[-0.0104837  -0.05614834  0.04562941  0.6829388 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 5309 is [True, False, False, False, True, False]
Current timestep = 5310. State = [[-0.2398306   0.02510418]]. Action = [[-0.0853646  -0.09305033 -0.02843666  0.926703  ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 5310 is [True, False, False, False, True, False]
Current timestep = 5311. State = [[-0.23988242  0.02399011]]. Action = [[-0.05049753  0.01610848  0.0982933  -0.2152729 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 5311 is [True, False, False, False, True, False]
Current timestep = 5312. State = [[-0.23982275  0.02305338]]. Action = [[ 0.22634965 -0.11238435  0.0781413  -0.670008  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 5312 is [True, False, False, False, True, False]
Current timestep = 5313. State = [[-0.23978557  0.02172303]]. Action = [[-0.20968376  0.15784228  0.21043646  0.81318617]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 5313 is [True, False, False, False, True, False]
Current timestep = 5314. State = [[-0.23982193  0.02176631]]. Action = [[-0.11285654 -0.16428335 -0.2214648   0.9707322 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 5314 is [True, False, False, False, True, False]
Scene graph at timestep 5314 is [True, False, False, False, True, False]
State prediction error at timestep 5314 is tensor(7.7162e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5315. State = [[-0.2399755   0.02013896]]. Action = [[ 0.01944178 -0.1478473  -0.1885202   0.7274802 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 5315 is [True, False, False, False, True, False]
Current timestep = 5316. State = [[-0.2400899   0.01785762]]. Action = [[ 0.06633294  0.02974406  0.22761708 -0.95474964]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 5316 is [True, False, False, False, True, False]
Scene graph at timestep 5316 is [True, False, False, False, True, False]
State prediction error at timestep 5316 is tensor(6.4091e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5317. State = [[-0.24009998  0.01633564]]. Action = [[ 0.10397512 -0.17929345  0.18326795 -0.26883924]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 5317 is [True, False, False, False, True, False]
Current timestep = 5318. State = [[-0.24005026  0.01387619]]. Action = [[ 0.00700611 -0.11220273 -0.02252316 -0.8700428 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 5318 is [True, False, False, False, True, False]
Current timestep = 5319. State = [[-0.24002281  0.01079184]]. Action = [[ 0.17671746  0.0896858   0.12323186 -0.7037951 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 5319 is [True, False, False, False, True, False]
Current timestep = 5320. State = [[-0.23974553  0.00981842]]. Action = [[-0.18275212  0.0994364   0.05406487 -0.77351624]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 5320 is [True, False, False, False, True, False]
Scene graph at timestep 5320 is [True, False, False, False, True, False]
State prediction error at timestep 5320 is tensor(1.7959e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5321. State = [[-0.23977566  0.00976534]]. Action = [[ 0.03460231 -0.0903236  -0.00254408 -0.45681828]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 5321 is [True, False, False, False, True, False]
Current timestep = 5322. State = [[-0.23987044  0.00910001]]. Action = [[ 0.01120687  0.14496171 -0.21372074 -0.7618738 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 5322 is [True, False, False, False, True, False]
Current timestep = 5323. State = [[-0.23980998  0.00918936]]. Action = [[ 0.18379802 -0.117112    0.14211428  0.07940471]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 5323 is [True, False, False, False, True, False]
Current timestep = 5324. State = [[-0.23922636  0.00889228]]. Action = [[-0.18816482 -0.09373501 -0.05754495  0.01482582]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 5324 is [True, False, False, False, True, False]
Scene graph at timestep 5324 is [True, False, False, False, True, False]
State prediction error at timestep 5324 is tensor(1.9481e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5325. State = [[-0.2391714   0.00749156]]. Action = [[-0.09150082 -0.15685058 -0.20130466  0.4508245 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 5325 is [True, False, False, False, True, False]
Current timestep = 5326. State = [[-0.2391432   0.00431346]]. Action = [[ 0.17541713 -0.22256489  0.20281285  0.30181372]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 5326 is [True, False, False, False, True, False]
Current timestep = 5327. State = [[-0.23898673  0.00024962]]. Action = [[-0.19882086  0.07311615 -0.07444724 -0.76676494]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 5327 is [True, False, False, False, True, False]
Current timestep = 5328. State = [[-0.23897848 -0.00140585]]. Action = [[-0.01808964  0.04495615  0.18901509  0.4526502 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 5328 is [True, False, False, False, True, False]
Current timestep = 5329. State = [[-0.23908272 -0.00204831]]. Action = [[0.12512875 0.09347394 0.19892651 0.13687587]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 5329 is [True, False, False, False, True, False]
Current timestep = 5330. State = [[-0.23912138 -0.00201507]]. Action = [[ 0.14116174  0.09966582 -0.00458941 -0.890029  ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 5330 is [True, False, False, False, True, False]
Current timestep = 5331. State = [[-0.23911889 -0.00140532]]. Action = [[-0.10617422  0.09931737  0.16465065  0.6649313 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 5331 is [True, False, False, False, True, False]
Current timestep = 5332. State = [[-0.2392052  -0.00066695]]. Action = [[-0.04230456 -0.03770463  0.0253405  -0.20866776]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 5332 is [True, False, False, False, True, False]
Current timestep = 5333. State = [[-0.23924893 -0.00030016]]. Action = [[-0.10027272  0.03724688  0.09143364 -0.8141555 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 5333 is [True, False, False, False, True, False]
Current timestep = 5334. State = [[-2.3934576e-01  1.9713948e-04]]. Action = [[ 0.09736812  0.10802114 -0.23166434  0.15265548]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 5334 is [True, False, False, False, True, False]
Current timestep = 5335. State = [[-0.2394835   0.00155004]]. Action = [[ 0.12144133 -0.00167266 -0.2077473   0.8950434 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 5335 is [True, False, False, False, True, False]
Scene graph at timestep 5335 is [True, False, False, False, True, False]
State prediction error at timestep 5335 is tensor(2.9153e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5336. State = [[-0.2394045   0.00232956]]. Action = [[ 0.0691933   0.10378566 -0.14589842  0.7453146 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 5336 is [True, False, False, False, True, False]
Current timestep = 5337. State = [[-0.2389922   0.00370058]]. Action = [[ 0.23730195 -0.14866303 -0.0035997  -0.843959  ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 5337 is [True, False, False, False, True, False]
Current timestep = 5338. State = [[-0.2365818   0.00372177]]. Action = [[-0.12404563  0.21987471 -0.14899611  0.2327143 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 5338 is [True, False, False, False, True, False]
Current timestep = 5339. State = [[-0.23536325  0.00563063]]. Action = [[-0.2245201   0.03376055  0.10783312 -0.79534054]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 5339 is [True, False, False, False, True, False]
Current timestep = 5340. State = [[-0.23582698  0.00702078]]. Action = [[-0.04910894 -0.00383864  0.2101422  -0.3200673 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 5340 is [True, False, False, False, True, False]
Scene graph at timestep 5340 is [True, False, False, False, True, False]
State prediction error at timestep 5340 is tensor(2.7177e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5341. State = [[-0.23612675  0.00812988]]. Action = [[ 0.11588594 -0.05455717 -0.02365868 -0.8417717 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 5341 is [True, False, False, False, True, False]
Current timestep = 5342. State = [[-0.23615986  0.00822224]]. Action = [[ 0.12365282  0.11007082  0.05935669 -0.9328925 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 5342 is [True, False, False, False, True, False]
Scene graph at timestep 5342 is [True, False, False, False, True, False]
State prediction error at timestep 5342 is tensor(8.0282e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5343. State = [[-0.235453    0.00956141]]. Action = [[ 0.20458335 -0.02856961  0.13417488  0.76223516]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 5343 is [True, False, False, False, True, False]
Scene graph at timestep 5343 is [True, False, False, False, True, False]
State prediction error at timestep 5343 is tensor(1.2944e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5344. State = [[-0.23260503  0.01006163]]. Action = [[0.21631515 0.1222266  0.14775634 0.23976183]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 5344 is [True, False, False, False, True, False]
Current timestep = 5345. State = [[-0.22821955  0.01183923]]. Action = [[-0.12237267 -0.08931202 -0.07619148 -0.5674532 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 5345 is [True, False, False, False, True, False]
Current timestep = 5346. State = [[-0.224809    0.01218301]]. Action = [[-0.11614035 -0.03803585 -0.128296   -0.8809008 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 5346 is [True, False, False, False, True, False]
Current timestep = 5347. State = [[-0.22325134  0.01228009]]. Action = [[ 0.22652107  0.08189029 -0.24707837 -0.19478774]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 5347 is [True, False, False, False, True, False]
Human Feedback received at timestep 5347 of 1
Current timestep = 5348. State = [[-0.22114857  0.01258768]]. Action = [[-0.03895018  0.20085365  0.21754974  0.01130652]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 5348 is [True, False, False, False, True, False]
Scene graph at timestep 5348 is [True, False, False, False, True, False]
State prediction error at timestep 5348 is tensor(1.7621e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5349. State = [[-0.22016637  0.01595181]]. Action = [[ 0.16840866  0.04673743 -0.22328135 -0.7168095 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 5349 is [True, False, False, False, True, False]
Current timestep = 5350. State = [[-0.21865664  0.0183604 ]]. Action = [[-0.08698943 -0.05327384  0.2153264  -0.32904756]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 5350 is [True, False, False, False, True, False]
Current timestep = 5351. State = [[-0.21807328  0.01994338]]. Action = [[ 0.10099021 -0.00448589  0.02347088  0.06610441]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 5351 is [True, False, False, False, True, False]
Current timestep = 5352. State = [[-0.21758936  0.02067674]]. Action = [[ 0.146931   -0.18338534 -0.11593193  0.6778953 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 5352 is [True, False, False, False, True, False]
Scene graph at timestep 5352 is [True, False, False, False, True, False]
State prediction error at timestep 5352 is tensor(3.2240e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5353. State = [[-0.21554804  0.01982047]]. Action = [[ 0.07796666 -0.04103556  0.1889931   0.02315652]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 5353 is [True, False, False, False, True, False]
Current timestep = 5354. State = [[-0.21268633  0.01920775]]. Action = [[-0.02985933 -0.02762006 -0.11470203  0.5997038 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 5354 is [True, False, False, False, True, False]
Current timestep = 5355. State = [[-0.21043387  0.01863556]]. Action = [[-0.16902168 -0.00878365 -0.02816826  0.14864135]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 5355 is [True, False, False, False, True, False]
Scene graph at timestep 5355 is [True, False, False, False, True, False]
State prediction error at timestep 5355 is tensor(2.1977e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5356. State = [[-0.20961696  0.01867742]]. Action = [[-0.18313578  0.12931737 -0.09523664  0.2463677 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 5356 is [True, False, False, False, True, False]
Scene graph at timestep 5356 is [True, False, False, False, True, False]
State prediction error at timestep 5356 is tensor(3.6206e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5357. State = [[-0.2099217   0.01931475]]. Action = [[0.24269518 0.03206235 0.1409511  0.10526633]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 5357 is [True, False, False, False, True, False]
Scene graph at timestep 5357 is [True, False, False, False, True, False]
State prediction error at timestep 5357 is tensor(1.9318e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5357 of 1
Current timestep = 5358. State = [[-0.20940487  0.01967147]]. Action = [[0.24337894 0.01587507 0.2036264  0.9581189 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 5358 is [True, False, False, False, True, False]
Current timestep = 5359. State = [[-0.20690353  0.01986939]]. Action = [[-7.01993704e-05 -1.12372994e-01 -2.09418476e-01  4.32370901e-01]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 5359 is [True, False, False, False, True, False]
Scene graph at timestep 5359 is [True, False, False, False, True, False]
State prediction error at timestep 5359 is tensor(4.2520e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5360. State = [[-0.20458226  0.01980851]]. Action = [[ 0.17914957 -0.04966962 -0.15913218  0.87344646]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 5360 is [True, False, False, False, True, False]
Current timestep = 5361. State = [[-0.20179297  0.01904877]]. Action = [[-0.24651247  0.00669196 -0.03634983 -0.8396306 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 5361 is [True, False, False, False, True, False]
Current timestep = 5362. State = [[-0.20074084  0.01896732]]. Action = [[-0.01148723 -0.05235615 -0.10528228 -0.80124545]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 5362 is [True, False, False, False, True, False]
Human Feedback received at timestep 5362 of 1
Current timestep = 5363. State = [[-0.20030183  0.01823884]]. Action = [[-0.20219494 -0.15039775 -0.23897588 -0.10189915]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 5363 is [True, False, False, False, True, False]
Current timestep = 5364. State = [[-0.20043886  0.01620064]]. Action = [[ 0.22812188  0.02816412  0.18456775 -0.09201223]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 5364 is [True, False, False, False, True, False]
Current timestep = 5365. State = [[-0.2004126   0.01524371]]. Action = [[-0.10665521 -0.15954353  0.07430795  0.00693798]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 5365 is [True, False, False, False, True, False]
Current timestep = 5366. State = [[-0.20032065  0.01276683]]. Action = [[-0.08095279  0.05777955 -0.00378338 -0.01731497]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 5366 is [True, False, False, False, True, False]
Current timestep = 5367. State = [[-0.20047112  0.01119584]]. Action = [[-0.13816017 -0.11550421  0.18316627  0.09295809]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 5367 is [True, False, False, False, True, False]
Current timestep = 5368. State = [[-0.20099029  0.00876029]]. Action = [[-0.18695188 -0.15894403  0.1422652  -0.65610075]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 5368 is [True, False, False, False, True, False]
Scene graph at timestep 5368 is [True, False, False, False, True, False]
State prediction error at timestep 5368 is tensor(2.0116e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5369. State = [[-0.20179345  0.00565971]]. Action = [[ 0.24225184  0.20140705  0.07932392 -0.44811076]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 5369 is [True, False, False, False, True, False]
Scene graph at timestep 5369 is [True, False, False, False, True, False]
State prediction error at timestep 5369 is tensor(6.3720e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5370. State = [[-0.20196892  0.0055389 ]]. Action = [[-0.07950151 -0.21336399 -0.1758665   0.38356483]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 5370 is [True, False, False, False, True, False]
Current timestep = 5371. State = [[-0.20214112  0.00393908]]. Action = [[0.15679061 0.09807053 0.03246999 0.14324331]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 5371 is [True, False, False, False, True, False]
Scene graph at timestep 5371 is [True, False, False, False, True, False]
State prediction error at timestep 5371 is tensor(2.9599e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5372. State = [[-0.20208651  0.0036917 ]]. Action = [[ 0.08714014 -0.11960763  0.14188617  0.53589463]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 5372 is [True, False, False, False, True, False]
Current timestep = 5373. State = [[-0.20163563  0.00232443]]. Action = [[ 0.17711878  0.065606   -0.17251948 -0.5944119 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 5373 is [True, False, False, False, True, False]
Current timestep = 5374. State = [[-0.20045345  0.00187091]]. Action = [[-0.15716057 -0.13367178  0.19529468  0.93475676]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 5374 is [True, False, False, False, True, False]
Current timestep = 5375. State = [[-0.20031781  0.0004429 ]]. Action = [[-0.1972501   0.08444464  0.08543354  0.81509113]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 5375 is [True, False, False, False, True, False]
Scene graph at timestep 5375 is [True, False, False, False, True, False]
State prediction error at timestep 5375 is tensor(1.5429e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5376. State = [[-2.0044981e-01  1.2288013e-04]]. Action = [[-0.12312683 -0.09586358 -0.10271925  0.08733308]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 5376 is [True, False, False, False, True, False]
Current timestep = 5377. State = [[-0.20094395 -0.0015503 ]]. Action = [[-0.13901739 -0.06844062 -0.05249695  0.7019696 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 5377 is [True, False, False, False, True, False]
Current timestep = 5378. State = [[-0.20225221 -0.00388293]]. Action = [[-0.1373745  -0.09062871 -0.21814069 -0.7258246 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 5378 is [True, False, False, False, True, False]
Scene graph at timestep 5378 is [True, False, False, False, True, False]
State prediction error at timestep 5378 is tensor(1.0200e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5379. State = [[-0.2039392  -0.00669648]]. Action = [[-0.10878754  0.14876285  0.2398571   0.20529008]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 5379 is [True, False, False, False, True, False]
Current timestep = 5380. State = [[-0.20571464 -0.00685028]]. Action = [[ 0.24338192 -0.06794658  0.1813168   0.87293243]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 5380 is [True, False, False, False, True, False]
Current timestep = 5381. State = [[-0.20581189 -0.00744952]]. Action = [[-0.13326406  0.0236032  -0.20519109  0.7007096 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 5381 is [True, False, False, False, True, False]
Current timestep = 5382. State = [[-0.20587897 -0.00774772]]. Action = [[ 0.04996663  0.00894758 -0.11603433 -0.17907155]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 5382 is [True, False, False, False, True, False]
Current timestep = 5383. State = [[-0.2058887  -0.00774833]]. Action = [[ 0.09696427 -0.02351248  0.22152776 -0.7081177 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 5383 is [True, False, False, False, True, False]
Scene graph at timestep 5383 is [True, False, False, False, True, False]
State prediction error at timestep 5383 is tensor(1.3552e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5384. State = [[-0.20590885 -0.00793713]]. Action = [[-0.12245566  0.00301179  0.049128    0.09831846]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 5384 is [True, False, False, False, True, False]
Scene graph at timestep 5384 is [True, False, False, False, True, False]
State prediction error at timestep 5384 is tensor(5.3470e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5385. State = [[-0.2059447  -0.00789558]]. Action = [[0.12424845 0.15500727 0.13931257 0.51925504]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 5385 is [True, False, False, False, True, False]
Current timestep = 5386. State = [[-0.20607129 -0.00730241]]. Action = [[-0.12960196 -0.19588496 -0.19978088 -0.730072  ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 5386 is [True, False, False, False, True, False]
Current timestep = 5387. State = [[-0.20620473 -0.00798668]]. Action = [[0.08369905 0.0773645  0.17927939 0.17801023]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 5387 is [True, False, False, False, True, False]
Current timestep = 5388. State = [[-0.20617697 -0.00802305]]. Action = [[ 0.16904974 -0.1103828   0.0105876  -0.53490406]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 5388 is [True, False, False, False, True, False]
Current timestep = 5389. State = [[-0.20607117 -0.0084205 ]]. Action = [[0.03698292 0.09011433 0.1928615  0.29200006]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 5389 is [True, False, False, False, True, False]
Current timestep = 5390. State = [[-0.2060582  -0.00833339]]. Action = [[ 0.17730907  0.23058996  0.2055628  -0.53558594]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 5390 is [True, False, False, False, True, False]
Current timestep = 5391. State = [[-0.2053557  -0.00680166]]. Action = [[-0.06615925 -0.1907103  -0.01753336  0.8240595 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 5391 is [True, False, False, False, True, False]
Current timestep = 5392. State = [[-0.20528413 -0.00692126]]. Action = [[-0.04778972 -0.01553863 -0.02020687 -0.04118413]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 5392 is [True, False, False, False, True, False]
Current timestep = 5393. State = [[-0.20515072 -0.00742167]]. Action = [[ 0.19359672 -0.06426162  0.00592405  0.4615991 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 5393 is [True, False, False, False, True, False]
Scene graph at timestep 5393 is [True, False, False, False, True, False]
State prediction error at timestep 5393 is tensor(6.1491e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5394. State = [[-0.20390877 -0.00789281]]. Action = [[ 0.1690987  -0.04773453  0.17796671  0.36063242]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 5394 is [True, False, False, False, True, False]
Current timestep = 5395. State = [[-0.20207855 -0.00868714]]. Action = [[ 0.1868186  -0.0938876  -0.10975006 -0.39327347]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 5395 is [True, False, False, False, True, False]
Scene graph at timestep 5395 is [True, False, False, False, True, False]
State prediction error at timestep 5395 is tensor(4.7207e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5396. State = [[-0.19841653 -0.00997342]]. Action = [[ 0.13592157 -0.02037467  0.00328881 -0.17902255]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 5396 is [True, False, False, False, True, False]
Current timestep = 5397. State = [[-0.19504371 -0.01078225]]. Action = [[-0.02984408  0.01494467 -0.05516255 -0.64821655]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 5397 is [True, False, False, False, True, False]
Current timestep = 5398. State = [[-0.19339894 -0.01124412]]. Action = [[ 0.17311832 -0.17320901 -0.11293882  0.08416045]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 5398 is [True, False, False, False, True, False]
Human Feedback received at timestep 5398 of 1
Current timestep = 5399. State = [[-0.19057941 -0.01343291]]. Action = [[ 0.15985858  0.11774853  0.12766528 -0.23489356]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 5399 is [True, False, False, False, True, False]
Current timestep = 5400. State = [[-0.18654358 -0.01384008]]. Action = [[0.22180784 0.00161588 0.03612161 0.06432652]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 5400 is [True, False, False, False, True, False]
Current timestep = 5401. State = [[-0.18153718 -0.01391203]]. Action = [[ 0.11159933 -0.09319842  0.05770293 -0.9410103 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 5401 is [True, False, False, False, True, False]
Scene graph at timestep 5401 is [True, False, False, False, True, False]
State prediction error at timestep 5401 is tensor(7.1820e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5402. State = [[-0.17489037 -0.01489933]]. Action = [[ 0.15240842  0.20559627 -0.16797937 -0.61933297]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 5402 is [True, False, False, False, True, False]
Current timestep = 5403. State = [[-0.16908461 -0.01450814]]. Action = [[ 0.16477981  0.06984475 -0.14250275  0.05743432]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 5403 is [True, False, False, False, True, False]
Current timestep = 5404. State = [[-0.16355178 -0.01371883]]. Action = [[-0.10478276 -0.08261523 -0.04300913  0.19500256]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 5404 is [True, False, False, False, True, False]
Current timestep = 5405. State = [[-0.16094501 -0.01373884]]. Action = [[ 0.18510187 -0.05765215  0.18318546  0.51314473]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 5405 is [True, False, False, False, True, False]
Human Feedback received at timestep 5405 of 1
Current timestep = 5406. State = [[-0.15737292 -0.01384723]]. Action = [[ 0.10425398  0.00994354 -0.1003148   0.46780884]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 5406 is [True, False, False, False, True, False]
Current timestep = 5407. State = [[-0.15327092 -0.01379409]]. Action = [[-0.03375939  0.02930799  0.19802544  0.947001  ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 5407 is [True, False, False, False, True, False]
Current timestep = 5408. State = [[-0.1503861  -0.01380998]]. Action = [[ 0.2298634   0.04145843  0.05489972 -0.6020152 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 5408 is [True, False, False, False, True, False]
Current timestep = 5409. State = [[-0.14728737 -0.01363729]]. Action = [[ 0.16643637  0.09717688 -0.11411682 -0.09706688]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 5409 is [True, False, False, False, True, False]
Current timestep = 5410. State = [[-0.14302886 -0.01278039]]. Action = [[ 0.22928247 -0.06782068  0.07198352  0.7381797 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 5410 is [True, False, False, False, True, False]
Scene graph at timestep 5410 is [True, False, False, False, True, False]
State prediction error at timestep 5410 is tensor(7.8009e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5411. State = [[-0.13680807 -0.01274312]]. Action = [[ 0.06683362 -0.07642871  0.15396532 -0.09884918]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 5411 is [True, False, False, False, True, False]
Human Feedback received at timestep 5411 of 1
Current timestep = 5412. State = [[-0.13107726 -0.01297378]]. Action = [[ 0.01460779 -0.14839984  0.23555851  0.22997117]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 5412 is [True, False, False, False, True, False]
Current timestep = 5413. State = [[-0.12718931 -0.01437069]]. Action = [[ 0.05548093  0.02507055  0.19629642 -0.6585122 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 5413 is [True, False, False, False, True, False]
Current timestep = 5414. State = [[-0.12413708 -0.01464428]]. Action = [[ 0.24450278  0.21473837 -0.15980983  0.7251575 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 5414 is [True, False, False, False, True, False]
Current timestep = 5415. State = [[-0.11994582 -0.01371707]]. Action = [[-0.17103212 -0.21570183  0.16531712 -0.06044519]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 5415 is [True, False, False, False, True, False]
Current timestep = 5416. State = [[-0.11734603 -0.01512512]]. Action = [[-0.24624139 -0.1766993  -0.04767087  0.62476826]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 5416 is [True, False, False, False, True, False]
Current timestep = 5417. State = [[-0.11739659 -0.01761923]]. Action = [[ 0.07791778 -0.12535103  0.20849663  0.6899328 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 5417 is [True, False, False, False, True, False]
Scene graph at timestep 5417 is [True, False, False, False, True, False]
State prediction error at timestep 5417 is tensor(6.3226e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5417 of 1
Current timestep = 5418. State = [[-0.11713848 -0.02097441]]. Action = [[ 0.21777171  0.15389779 -0.0619421  -0.02303267]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 5418 is [True, False, False, False, True, False]
Current timestep = 5419. State = [[-0.11569441 -0.02100658]]. Action = [[-0.18741526  0.05933073 -0.21849649  0.20627177]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 5419 is [True, False, False, False, True, False]
Current timestep = 5420. State = [[-0.11575451 -0.02109994]]. Action = [[-0.1097382  -0.14400177 -0.05736618 -0.1249671 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 5420 is [True, False, False, False, True, False]
Scene graph at timestep 5420 is [True, False, False, False, True, False]
State prediction error at timestep 5420 is tensor(1.9881e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5421. State = [[-0.11574825 -0.02198721]]. Action = [[ 0.24214551 -0.1405677   0.1288647   0.89690065]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 5421 is [True, False, False, False, True, False]
Current timestep = 5422. State = [[-0.11533052 -0.02456924]]. Action = [[-0.22959743  0.01991212 -0.03015311  0.02948725]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 5422 is [True, False, False, False, True, False]
Current timestep = 5423. State = [[-0.11531479 -0.02580875]]. Action = [[-0.01247098 -0.10285127  0.11908209 -0.40000677]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 5423 is [True, False, False, False, True, False]
Current timestep = 5424. State = [[-0.11546276 -0.02831134]]. Action = [[ 0.16513646 -0.13122761 -0.21654348  0.22444391]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 5424 is [True, False, False, False, True, False]
Scene graph at timestep 5424 is [True, False, False, False, True, False]
State prediction error at timestep 5424 is tensor(3.9049e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5425. State = [[-0.11540247 -0.03157885]]. Action = [[-0.09334233  0.06552836  0.02343848  0.13377571]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 5425 is [True, False, False, False, True, False]
Scene graph at timestep 5425 is [True, False, False, False, True, False]
State prediction error at timestep 5425 is tensor(1.5784e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5426. State = [[-0.11544113 -0.03300156]]. Action = [[ 0.20645714  0.08453161  0.22009146 -0.7373685 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 5426 is [True, False, False, False, True, False]
Scene graph at timestep 5426 is [True, False, False, False, True, False]
State prediction error at timestep 5426 is tensor(4.9927e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5427. State = [[-0.11436657 -0.03313953]]. Action = [[ 0.19885051 -0.12544991  0.00802499 -0.01645368]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 5427 is [True, False, False, False, True, False]
Current timestep = 5428. State = [[-0.11166522 -0.03450546]]. Action = [[ 0.21243402 -0.08663884 -0.2173025   0.7358036 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 5428 is [True, False, False, False, True, False]
Scene graph at timestep 5428 is [True, False, False, False, True, False]
State prediction error at timestep 5428 is tensor(1.4198e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5428 of 1
Current timestep = 5429. State = [[-0.10789622 -0.03646125]]. Action = [[ 0.22944891  0.06461146  0.10234269 -0.17156738]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 5429 is [True, False, False, False, True, False]
Current timestep = 5430. State = [[-0.10297471 -0.03693249]]. Action = [[-0.00615558 -0.1625059   0.14730245 -0.15627307]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 5430 is [True, False, False, False, True, False]
Scene graph at timestep 5430 is [True, False, False, False, True, False]
State prediction error at timestep 5430 is tensor(7.5271e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5431. State = [[-0.09837632 -0.03901907]]. Action = [[ 0.09005785  0.08933061 -0.16014956  0.25935268]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 5431 is [True, False, False, False, True, False]
Scene graph at timestep 5431 is [True, False, False, False, True, False]
State prediction error at timestep 5431 is tensor(5.0432e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5432. State = [[-0.09430339 -0.03917504]]. Action = [[ 0.03001726  0.18657377 -0.24748482 -0.52869797]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 5432 is [True, False, False, False, True, False]
Scene graph at timestep 5432 is [True, False, False, False, True, False]
State prediction error at timestep 5432 is tensor(1.4906e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5433. State = [[-0.09076002 -0.03831107]]. Action = [[ 0.170887    0.16547728 -0.19278793 -0.456707  ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 5433 is [True, False, False, False, True, False]
Current timestep = 5434. State = [[-0.08731281 -0.03657584]]. Action = [[ 0.04848638 -0.09629938 -0.18020946  0.3842125 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 5434 is [True, False, False, False, True, False]
Current timestep = 5435. State = [[-0.08476044 -0.03623178]]. Action = [[ 0.24598649 -0.12520133  0.16492647  0.40493727]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 5435 is [True, False, False, False, True, False]
Current timestep = 5436. State = [[-0.07989501 -0.03686429]]. Action = [[-0.19216941  0.02441126  0.01651451 -0.3408996 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 5436 is [True, False, False, False, True, False]
Human Feedback received at timestep 5436 of 1
Current timestep = 5437. State = [[-0.07810342 -0.0369473 ]]. Action = [[ 0.22545975  0.18390614 -0.2243213   0.768157  ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 5437 is [True, False, False, False, True, False]
Current timestep = 5438. State = [[-0.07528153 -0.03538657]]. Action = [[ 0.09030783 -0.18202955  0.07499656  0.24154103]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 5438 is [True, False, False, False, True, False]
Current timestep = 5439. State = [[-0.07134843 -0.03616688]]. Action = [[ 0.15646297 -0.16510342 -0.22751243 -0.7510119 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 5439 is [True, False, False, False, True, False]
Scene graph at timestep 5439 is [True, False, False, False, True, False]
State prediction error at timestep 5439 is tensor(1.5178e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5440. State = [[-0.06727555 -0.03771646]]. Action = [[-0.09113435  0.11354107  0.22436598 -0.8495587 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 5440 is [True, False, False, False, True, False]
Current timestep = 5441. State = [[-0.06575396 -0.03804423]]. Action = [[-0.02883472 -0.23041594  0.10130128  0.63776064]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 5441 is [True, False, False, False, True, False]
Scene graph at timestep 5441 is [True, False, False, False, True, False]
State prediction error at timestep 5441 is tensor(6.8697e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5441 of 1
Current timestep = 5442. State = [[-0.06509549 -0.04057762]]. Action = [[-0.03330135 -0.14774401 -0.1827608  -0.13091576]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 5442 is [True, False, False, False, True, False]
Current timestep = 5443. State = [[-0.06463091 -0.04444658]]. Action = [[-0.09264737 -0.17183325  0.04672566  0.0069505 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 5443 is [True, False, False, False, True, False]
Current timestep = 5444. State = [[-0.06420351 -0.04877092]]. Action = [[-0.18881239  0.03805852 -0.05802594  0.29856503]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 5444 is [True, False, False, False, True, False]
Current timestep = 5445. State = [[-0.06457122 -0.05093076]]. Action = [[-0.01166603  0.03605264  0.24323702 -0.43314654]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 5445 is [True, False, False, False, True, False]
Current timestep = 5446. State = [[-0.06492677 -0.0518251 ]]. Action = [[0.21615213 0.1687572  0.03901151 0.61405206]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 5446 is [True, False, False, False, True, False]
Current timestep = 5447. State = [[-0.06489924 -0.05137495]]. Action = [[ 0.15557757 -0.14989345 -0.05736873 -0.30946672]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 5447 is [True, False, False, False, True, False]
Human Feedback received at timestep 5447 of 1
Current timestep = 5448. State = [[-0.06444821 -0.05238199]]. Action = [[ 0.15807396 -0.06615706  0.23519748 -0.57298845]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 5448 is [True, False, False, False, True, False]
Current timestep = 5449. State = [[-0.06216707 -0.05412104]]. Action = [[-0.09589937  0.11650336 -0.19188653  0.49169326]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 5449 is [True, False, False, False, True, False]
Current timestep = 5450. State = [[-0.06037517 -0.05414871]]. Action = [[ 0.0533646  -0.04786709 -0.00796968  0.21462333]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 5450 is [True, False, False, False, True, False]
Current timestep = 5451. State = [[-0.05850042 -0.05425838]]. Action = [[ 0.20182842 -0.12439781 -0.11193341 -0.78865415]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 5451 is [True, False, False, False, True, False]
Current timestep = 5452. State = [[-0.0560912  -0.05604278]]. Action = [[-0.12727498 -0.2244531  -0.10963517  0.8889594 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 5452 is [True, False, False, False, True, False]
Current timestep = 5453. State = [[-0.05513404 -0.05980529]]. Action = [[ 0.23556569 -0.01654989 -0.16193736 -0.33609104]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 5453 is [True, False, False, False, True, False]
Current timestep = 5454. State = [[-0.05297141 -0.06241045]]. Action = [[0.20795384 0.17299628 0.17526677 0.14563084]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 5454 is [True, False, False, False, True, False]
Current timestep = 5455. State = [[-0.04912242 -0.0622337 ]]. Action = [[ 0.23510915 -0.08976799 -0.22952847  0.3370409 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 5455 is [True, False, False, False, True, False]
Current timestep = 5456. State = [[-0.04477891 -0.0626711 ]]. Action = [[-0.14445113  0.11909091 -0.12043774 -0.34197736]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 5456 is [False, True, False, False, True, False]
Current timestep = 5457. State = [[-0.04100982 -0.06229343]]. Action = [[ 0.17174494 -0.03915726 -0.04715152  0.18247604]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 5457 is [False, True, False, False, True, False]
Current timestep = 5458. State = [[-0.03687279 -0.06237932]]. Action = [[0.07669312 0.01938009 0.1836111  0.64210176]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 5458 is [False, True, False, False, True, False]
Scene graph at timestep 5458 is [False, True, False, False, True, False]
State prediction error at timestep 5458 is tensor(6.1721e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5459. State = [[-0.24839754 -0.02039198]]. Action = [[ 0.0295276  -0.17706284 -0.1801574   0.4396875 ]]. Reward = [100.]
Curr episode timestep = 174
Scene graph at timestep 5459 is [False, True, False, False, True, False]
Scene graph at timestep 5459 is [True, False, False, False, True, False]
State prediction error at timestep 5459 is tensor(0.0237, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5459 of 1
Current timestep = 5460. State = [[-0.24585742 -0.02254172]]. Action = [[ 0.18273258 -0.17315981 -0.21737063  0.581342  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 5460 is [True, False, False, False, True, False]
Current timestep = 5461. State = [[-0.24407299 -0.02456999]]. Action = [[ 0.06372601 -0.02301097 -0.22371037 -0.03800285]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 5461 is [True, False, False, False, True, False]
Current timestep = 5462. State = [[-0.24187937 -0.02592497]]. Action = [[ 0.23609543  0.01256534  0.02651638 -0.35279143]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 5462 is [True, False, False, False, True, False]
Current timestep = 5463. State = [[-0.23753369 -0.02700321]]. Action = [[ 0.13649419 -0.12917043 -0.16320546  0.56340003]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 5463 is [True, False, False, False, True, False]
Scene graph at timestep 5463 is [True, False, False, False, True, False]
State prediction error at timestep 5463 is tensor(1.1200e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5464. State = [[-0.23298283 -0.02926109]]. Action = [[ 0.11098623 -0.13849425  0.21084535  0.7772424 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 5464 is [True, False, False, False, True, False]
Current timestep = 5465. State = [[-0.22887085 -0.03254026]]. Action = [[ 0.20667148 -0.15267353  0.03747508  0.7002888 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5465 is [True, False, False, False, True, False]
Current timestep = 5466. State = [[-0.22420642 -0.03636269]]. Action = [[ 0.10110724 -0.00688469  0.11996299  0.7345998 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 5466 is [True, False, False, False, True, False]
Current timestep = 5467. State = [[-0.21906777 -0.03966329]]. Action = [[-0.00285047 -0.15907717 -0.00163843  0.5371163 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 5467 is [True, False, False, False, True, False]
Current timestep = 5468. State = [[-0.21520734 -0.04334889]]. Action = [[0.0510177  0.09442726 0.22917074 0.56310797]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 5468 is [True, False, False, False, True, False]
Current timestep = 5469. State = [[-0.21215077 -0.0443952 ]]. Action = [[0.21088195 0.15565264 0.0579271  0.00163054]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 5469 is [True, False, False, False, True, False]
Current timestep = 5470. State = [[-0.20837751 -0.04444876]]. Action = [[ 0.22489572  0.00728381 -0.14967604 -0.2894926 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 5470 is [True, False, False, False, True, False]
Scene graph at timestep 5470 is [True, False, False, False, True, False]
State prediction error at timestep 5470 is tensor(6.0718e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5471. State = [[-0.20376153 -0.04466042]]. Action = [[ 0.23061365 -0.08123806 -0.21657985 -0.7127334 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 5471 is [True, False, False, False, True, False]
Current timestep = 5472. State = [[-0.19786747 -0.0453608 ]]. Action = [[-0.03080977 -0.20376235 -0.23237397 -0.7589585 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 5472 is [True, False, False, False, True, False]
Scene graph at timestep 5472 is [True, False, False, False, True, False]
State prediction error at timestep 5472 is tensor(1.8409e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5473. State = [[-0.19287811 -0.04731842]]. Action = [[-0.04850864  0.19524127 -0.21944842  0.72880006]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 5473 is [True, False, False, False, True, False]
Current timestep = 5474. State = [[-0.1899077  -0.04728124]]. Action = [[ 0.21697012  0.04405716 -0.1264635   0.10569358]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 5474 is [True, False, False, False, True, False]
Scene graph at timestep 5474 is [True, False, False, False, True, False]
State prediction error at timestep 5474 is tensor(2.8575e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5475. State = [[-0.18525769 -0.04738365]]. Action = [[ 0.23855448  0.04599869 -0.2196733   0.50679827]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 5475 is [True, False, False, False, True, False]
Current timestep = 5476. State = [[-0.1811556  -0.04700533]]. Action = [[ 0.15787435 -0.15463573  0.17437944  0.48267674]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 5476 is [True, False, False, False, True, False]
Current timestep = 5477. State = [[-0.17676523 -0.04765124]]. Action = [[-0.06430167  0.15042377  0.0622035   0.573941  ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 5477 is [True, False, False, False, True, False]
Current timestep = 5478. State = [[-0.1730868  -0.04737632]]. Action = [[ 0.14594358  0.00968078 -0.22120444  0.33941305]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 5478 is [True, False, False, False, True, False]
Scene graph at timestep 5478 is [True, False, False, False, True, False]
State prediction error at timestep 5478 is tensor(6.1801e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5479. State = [[-0.16884275 -0.04683833]]. Action = [[-0.21569657 -0.1460232  -0.12202713  0.24400735]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 5479 is [True, False, False, False, True, False]
Scene graph at timestep 5479 is [True, False, False, False, True, False]
State prediction error at timestep 5479 is tensor(5.2062e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5480. State = [[-0.16725664 -0.04731303]]. Action = [[-0.10484584 -0.13516401  0.02544004  0.80343866]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 5480 is [True, False, False, False, True, False]
Current timestep = 5481. State = [[-0.16753215 -0.04897495]]. Action = [[-0.18051292  0.02068174 -0.0692337  -0.58023906]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 5481 is [True, False, False, False, True, False]
Current timestep = 5482. State = [[-0.16764376 -0.05021183]]. Action = [[0.21454531 0.00577277 0.00477377 0.56266   ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 5482 is [True, False, False, False, True, False]
Current timestep = 5483. State = [[-0.16759713 -0.05026904]]. Action = [[ 0.21587968  0.03616703 -0.09936246  0.8071003 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 5483 is [True, False, False, False, True, False]
Current timestep = 5484. State = [[-0.16650176 -0.05025591]]. Action = [[0.18909204 0.01106009 0.22045958 0.82632005]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 5484 is [True, False, False, False, True, False]
Scene graph at timestep 5484 is [True, False, False, False, True, False]
State prediction error at timestep 5484 is tensor(5.6798e-07, grad_fn=<MseLossBackward0>)
Current timestep = 5485. State = [[-0.16396883 -0.0504753 ]]. Action = [[ 0.19423997 -0.09575611 -0.03250298 -0.06993663]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 5485 is [True, False, False, False, True, False]
Current timestep = 5486. State = [[-0.15985252 -0.05103203]]. Action = [[ 0.11180905 -0.02152497  0.04559356  0.5774437 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 5486 is [True, False, False, False, True, False]
Current timestep = 5487. State = [[-0.1556027  -0.05141176]]. Action = [[ 0.23687863 -0.00953327  0.0525969   0.39743435]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 5487 is [True, False, False, False, True, False]
Current timestep = 5488. State = [[-0.15030286 -0.05181182]]. Action = [[-0.1570165  -0.11890952 -0.22945186  0.820971  ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 5488 is [True, False, False, False, True, False]
Current timestep = 5489. State = [[-0.14731637 -0.05384658]]. Action = [[-0.110434   -0.16314308 -0.1796388   0.3975048 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 5489 is [True, False, False, False, True, False]
Current timestep = 5490. State = [[-0.14574124 -0.05753269]]. Action = [[ 0.19533241 -0.19931197 -0.23916426  0.4829985 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 5490 is [True, False, False, False, True, False]
Current timestep = 5491. State = [[-0.14322956 -0.06242805]]. Action = [[ 0.08173427 -0.107998   -0.11198536 -0.31114745]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 5491 is [True, False, False, False, True, False]
Current timestep = 5492. State = [[-0.14037937 -0.06669309]]. Action = [[ 0.13338363 -0.02561875  0.1256761   0.7271441 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 5492 is [True, False, False, False, True, False]
Current timestep = 5493. State = [[-0.13810049 -0.06980816]]. Action = [[ 0.09716246 -0.2128172   0.05108404  0.28502297]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 5493 is [True, False, False, False, True, False]
Human Feedback received at timestep 5493 of 1
Current timestep = 5494. State = [[-0.13482095 -0.07562536]]. Action = [[ 0.11634234 -0.12301478  0.02847177  0.8143885 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 5494 is [True, False, False, False, True, False]
Current timestep = 5495. State = [[-0.13068779 -0.08081011]]. Action = [[ 0.22067362 -0.07182512  0.0657576  -0.06395257]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 5495 is [True, False, False, False, True, False]
Current timestep = 5496. State = [[-0.12581949 -0.08471067]]. Action = [[-0.12444635 -0.0817378  -0.13928692 -0.00973445]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 5496 is [True, False, False, False, True, False]
Current timestep = 5497. State = [[-0.12238155 -0.08808746]]. Action = [[ 0.23365226 -0.07650839 -0.22354473  0.7545819 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 5497 is [True, False, False, False, True, False]
Current timestep = 5498. State = [[-0.11846577 -0.09134817]]. Action = [[ 0.2332837  -0.15619019 -0.14152637  0.0796665 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 5498 is [True, False, False, False, True, False]
Human Feedback received at timestep 5498 of 1
Current timestep = 5499. State = [[-0.11366186 -0.09588683]]. Action = [[ 0.17293403 -0.13602526 -0.11972642  0.5460136 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 5499 is [True, False, False, False, True, False]
Current timestep = 5500. State = [[-0.10728819 -0.10083881]]. Action = [[ 0.10279113 -0.19052607  0.01503557  0.08006096]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 5500 is [True, False, False, False, True, False]
Scene graph at timestep 5500 is [True, False, False, False, True, False]
State prediction error at timestep 5500 is tensor(2.1649e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5501. State = [[-0.10167047 -0.10622011]]. Action = [[ 0.17866492 -0.19969428 -0.19987796 -0.27184784]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 5501 is [True, False, False, False, True, False]
Human Feedback received at timestep 5501 of 1
Current timestep = 5502. State = [[-0.09506352 -0.11261303]]. Action = [[ 0.24407312 -0.10829102  0.17602915 -0.259086  ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 5502 is [True, False, False, False, True, False]
Current timestep = 5503. State = [[-0.08529023 -0.12075775]]. Action = [[ 0.22278571 -0.05836047  0.2326833   0.1281302 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 5503 is [True, False, False, False, True, False]
Current timestep = 5504. State = [[-0.0794394  -0.12391248]]. Action = [[-0.09862722 -0.20762244 -0.13918728  0.69547534]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 5504 is [True, False, False, False, True, False]
Current timestep = 5505. State = [[-0.07602596 -0.12868908]]. Action = [[-0.02613325 -0.22160295  0.10473022  0.07457137]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 5505 is [True, False, False, False, True, False]
Human Feedback received at timestep 5505 of 1
Current timestep = 5506. State = [[-0.0744451  -0.13430963]]. Action = [[ 0.23502696 -0.05284458 -0.19938344  0.6319611 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 5506 is [True, False, False, True, False, False]
Current timestep = 5507. State = [[-0.07080006 -0.13950127]]. Action = [[ 0.09008271 -0.1001559  -0.10554953  0.05274844]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 5507 is [True, False, False, True, False, False]
Current timestep = 5508. State = [[-0.06742248 -0.14354023]]. Action = [[ 0.23815227 -0.14529525 -0.212499    0.6215501 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 5508 is [True, False, False, True, False, False]
Current timestep = 5509. State = [[-0.0633653  -0.14785358]]. Action = [[ 0.1219427  -0.07378301  0.061609    0.46974146]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 5509 is [True, False, False, True, False, False]
Current timestep = 5510. State = [[-0.05790177 -0.15155326]]. Action = [[ 0.2094233   0.00737411 -0.20869073  0.28443694]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 5510 is [True, False, False, True, False, False]
Current timestep = 5511. State = [[-0.05258693 -0.15343727]]. Action = [[ 0.24452358 -0.02761137  0.12088448  0.72567034]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 5511 is [True, False, False, True, False, False]
Scene graph at timestep 5511 is [True, False, False, True, False, False]
State prediction error at timestep 5511 is tensor(7.5197e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5512. State = [[-0.04580606 -0.15504271]]. Action = [[ 0.00272778 -0.14009589 -0.20428194  0.21354544]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 5512 is [True, False, False, True, False, False]
Current timestep = 5513. State = [[-0.03994056 -0.15782504]]. Action = [[-0.03533711 -0.04142892 -0.14140889  0.19003522]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 5513 is [False, True, False, True, False, False]
Current timestep = 5514. State = [[-0.03688029 -0.16002437]]. Action = [[ 0.14466038  0.00160304 -0.00250039 -0.17250931]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 5514 is [False, True, False, True, False, False]
Scene graph at timestep 5514 is [False, True, False, True, False, False]
State prediction error at timestep 5514 is tensor(9.0084e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5515. State = [[-0.03284148 -0.16188419]]. Action = [[ 0.20673746 -0.19906053 -0.1697549   0.5077338 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 5515 is [False, True, False, True, False, False]
Human Feedback received at timestep 5515 of 1
Current timestep = 5516. State = [[-0.02780494 -0.16599676]]. Action = [[ 0.19107991 -0.1297917   0.15678197  0.0117507 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 5516 is [False, True, False, True, False, False]
Current timestep = 5517. State = [[-0.02266486 -0.17023423]]. Action = [[ 0.24220186 -0.03554332 -0.15075454  0.16393638]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 5517 is [False, True, False, True, False, False]
Current timestep = 5518. State = [[-0.01720037 -0.17321809]]. Action = [[ 0.15270716  0.05444628 -0.22770135  0.67553973]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 5518 is [False, True, False, True, False, False]
Scene graph at timestep 5518 is [False, True, False, True, False, False]
State prediction error at timestep 5518 is tensor(2.1317e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5519. State = [[-0.01024078 -0.17484348]]. Action = [[ 0.20654374 -0.23164424 -0.22824016  0.2212038 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 5519 is [False, True, False, True, False, False]
Scene graph at timestep 5519 is [False, True, False, True, False, False]
State prediction error at timestep 5519 is tensor(3.4733e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5519 of -1
Current timestep = 5520. State = [[-0.002451   -0.17926039]]. Action = [[ 0.24390337 -0.19385922 -0.1292447   0.8061532 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 5520 is [False, True, False, True, False, False]
Current timestep = 5521. State = [[ 0.0046724  -0.18421856]]. Action = [[ 0.17450011 -0.17342487 -0.17643271  0.35889077]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 5521 is [False, True, False, True, False, False]
Scene graph at timestep 5521 is [False, True, False, True, False, False]
State prediction error at timestep 5521 is tensor(6.5089e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5521 of -1
Current timestep = 5522. State = [[ 0.01221205 -0.19005045]]. Action = [[ 0.22243094 -0.1674876  -0.21011172  0.1302712 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 5522 is [False, True, False, True, False, False]
Current timestep = 5523. State = [[ 0.01964304 -0.19646998]]. Action = [[ 0.21655175 -0.21513645 -0.23515464 -0.09971237]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 5523 is [False, True, False, True, False, False]
Current timestep = 5524. State = [[ 0.02676944 -0.20263708]]. Action = [[ 0.21550703 -0.21401006 -0.16509779  0.250165  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 5524 is [False, True, False, True, False, False]
Human Feedback received at timestep 5524 of -1
Current timestep = 5525. State = [[ 0.03486975 -0.21018846]]. Action = [[ 0.0297578  -0.20969784 -0.16243969  0.5536642 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 5525 is [False, True, False, True, False, False]
Current timestep = 5526. State = [[ 0.04101523 -0.2174726 ]]. Action = [[ 0.2278373  -0.19032165 -0.2270004   0.5637884 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 5526 is [False, True, False, True, False, False]
Scene graph at timestep 5526 is [False, True, False, True, False, False]
State prediction error at timestep 5526 is tensor(4.2870e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5526 of -1
Current timestep = 5527. State = [[ 0.04827085 -0.22520636]]. Action = [[ 0.2364878  -0.22514418 -0.14042364  0.812346  ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 5527 is [False, True, False, True, False, False]
Current timestep = 5528. State = [[ 0.05513168 -0.23334353]]. Action = [[ 0.11379594 -0.21704663 -0.12673575  0.5976728 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 5528 is [False, True, False, True, False, False]
Human Feedback received at timestep 5528 of -1
Current timestep = 5529. State = [[ 0.06166476 -0.24103436]]. Action = [[ 0.2414496  -0.23142858 -0.18230438  0.31640983]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 5529 is [False, False, True, True, False, False]
Scene graph at timestep 5529 is [False, False, True, True, False, False]
State prediction error at timestep 5529 is tensor(5.2153e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5530. State = [[ 0.06926072 -0.24890047]]. Action = [[ 0.22753984 -0.20629731 -0.1978612   0.68527055]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 5530 is [False, False, True, True, False, False]
Scene graph at timestep 5530 is [False, False, True, True, False, False]
State prediction error at timestep 5530 is tensor(1.8080e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5530 of -1
Current timestep = 5531. State = [[ 0.07578509 -0.25403848]]. Action = [[ 0.23620632 -0.16451214 -0.19759314  0.73221636]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 5531 is [False, False, True, True, False, False]
Current timestep = 5532. State = [[ 0.08018482 -0.25674137]]. Action = [[ 0.20262736 -0.21737736 -0.15041481  0.7219479 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 5532 is [False, False, True, True, False, False]
Human Feedback received at timestep 5532 of -1
Current timestep = 5533. State = [[ 0.08383939 -0.2584579 ]]. Action = [[ 0.24511945 -0.23091888 -0.21647027  0.7298645 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 5533 is [False, False, True, True, False, False]
Current timestep = 5534. State = [[ 0.08609358 -0.25954565]]. Action = [[ 0.24651274 -0.22603884 -0.19970863  0.6682401 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 5534 is [False, False, True, True, False, False]
Scene graph at timestep 5534 is [False, False, True, True, False, False]
State prediction error at timestep 5534 is tensor(3.3200e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5534 of -1
Current timestep = 5535. State = [[ 0.08739835 -0.26016623]]. Action = [[ 0.22877371 -0.22759998 -0.13303362  0.857862  ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 5535 is [False, False, True, True, False, False]
Current timestep = 5536. State = [[ 0.08822995 -0.260736  ]]. Action = [[ 0.23435158 -0.21713105 -0.20166789  0.89232457]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 5536 is [False, False, True, True, False, False]
Human Feedback received at timestep 5536 of -1
Current timestep = 5537. State = [[ 0.08840103 -0.26093853]]. Action = [[ 0.2282427  -0.21870743 -0.1932474   0.67813206]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 5537 is [False, False, True, True, False, False]
Current timestep = 5538. State = [[ 0.08859456 -0.26122212]]. Action = [[ 0.24617046 -0.2346009  -0.19560564  0.7702774 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 5538 is [False, False, True, True, False, False]
Current timestep = 5539. State = [[ 0.08910175 -0.26146075]]. Action = [[ 0.24593657 -0.2363806  -0.19694889  0.7277906 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 5539 is [False, False, True, True, False, False]
Human Feedback received at timestep 5539 of -1
Current timestep = 5540. State = [[ 0.08977089 -0.26165143]]. Action = [[ 0.24924272 -0.20835292 -0.18353625  0.5570146 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 5540 is [False, False, True, True, False, False]
Current timestep = 5541. State = [[ 0.08997384 -0.2617544 ]]. Action = [[ 0.2451928  -0.23297775 -0.12593722  0.68050456]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 5541 is [False, False, True, True, False, False]
Scene graph at timestep 5541 is [False, False, True, True, False, False]
State prediction error at timestep 5541 is tensor(3.9091e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5542. State = [[ 0.09015884 -0.26172808]]. Action = [[ 0.21229422 -0.23732775 -0.18555756  0.8507582 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 5542 is [False, False, True, True, False, False]
Scene graph at timestep 5542 is [False, False, True, True, False, False]
State prediction error at timestep 5542 is tensor(1.2209e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5543. State = [[ 0.09005477 -0.26168966]]. Action = [[ 0.24611437 -0.22343057 -0.17280458  0.6152185 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 5543 is [False, False, True, True, False, False]
Human Feedback received at timestep 5543 of -1
Current timestep = 5544. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.20618492 -0.23529007 -0.1591104   0.84037447]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 5544 is [False, False, True, True, False, False]
Scene graph at timestep 5544 is [False, False, True, True, False, False]
State prediction error at timestep 5544 is tensor(3.1886e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5545. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.24266863 -0.14713912 -0.09508684  0.7812145 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 5545 is [False, False, True, True, False, False]
Scene graph at timestep 5545 is [False, False, True, True, False, False]
State prediction error at timestep 5545 is tensor(2.3578e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5546. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.22915488 -0.212525   -0.11715239  0.86630297]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 5546 is [False, False, True, True, False, False]
Current timestep = 5547. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.24855489 -0.21801828 -0.16257927  0.49756312]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 5547 is [False, False, True, True, False, False]
Current timestep = 5548. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.23839596 -0.2243174  -0.08810593  0.7977512 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 5548 is [False, False, True, True, False, False]
Current timestep = 5549. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.17888257 -0.18326011 -0.14394026  0.58667374]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 5549 is [False, False, True, True, False, False]
Current timestep = 5550. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.24932861 -0.18978554 -0.10401523  0.34662724]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 5550 is [False, False, True, True, False, False]
Current timestep = 5551. State = [[ 0.09009665 -0.26169825]]. Action = [[ 0.2472423  -0.21251808 -0.13596858  0.47442985]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 5551 is [False, False, True, True, False, False]
Scene graph at timestep 5551 is [False, False, True, True, False, False]
State prediction error at timestep 5551 is tensor(4.0385e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5552. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24906155 -0.21343568 -0.10443178  0.4429598 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 5552 is [False, False, True, True, False, False]
Current timestep = 5553. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23828572 -0.11484042 -0.12874302  0.49618912]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 5553 is [False, False, True, True, False, False]
Current timestep = 5554. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.06654483 -0.22519244 -0.15175483  0.75810504]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 5554 is [False, False, True, True, False, False]
Scene graph at timestep 5554 is [False, False, True, True, False, False]
State prediction error at timestep 5554 is tensor(1.8781e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5555. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23048294 -0.21442287 -0.13767044  0.62382185]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 5555 is [False, False, True, True, False, False]
Scene graph at timestep 5555 is [False, False, True, True, False, False]
State prediction error at timestep 5555 is tensor(2.7448e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5556. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23502702 -0.21411975 -0.1619194   0.08154678]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 5556 is [False, False, True, True, False, False]
Current timestep = 5557. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24602184 -0.20065954 -0.12611112  0.62085366]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 5557 is [False, False, True, True, False, False]
Current timestep = 5558. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21971107 -0.21778814 -0.16834016  0.48832703]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 5558 is [False, False, True, True, False, False]
Current timestep = 5559. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20993584 -0.22406155 -0.16138007  0.45016122]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 5559 is [False, False, True, True, False, False]
Scene graph at timestep 5559 is [False, False, True, True, False, False]
State prediction error at timestep 5559 is tensor(2.2713e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5560. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23860565 -0.15525846 -0.17414254  0.06338811]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 5560 is [False, False, True, True, False, False]
Current timestep = 5561. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24692842 -0.21684413 -0.19931647  0.45287526]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 5561 is [False, False, True, True, False, False]
Scene graph at timestep 5561 is [False, False, True, True, False, False]
State prediction error at timestep 5561 is tensor(2.5220e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5562. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24777398 -0.10225752 -0.15765128  0.48459792]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 5562 is [False, False, True, True, False, False]
Scene graph at timestep 5562 is [False, False, True, True, False, False]
State prediction error at timestep 5562 is tensor(1.6360e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5563. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2389814  -0.07181233 -0.1891249   0.25318658]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 5563 is [False, False, True, True, False, False]
Scene graph at timestep 5563 is [False, False, True, True, False, False]
State prediction error at timestep 5563 is tensor(5.5683e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5564. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23347676 -0.09613723 -0.18935218  0.26350546]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 5564 is [False, False, True, True, False, False]
Current timestep = 5565. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23896188 -0.18045115 -0.1845729   0.22393954]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 5565 is [False, False, True, True, False, False]
Current timestep = 5566. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24784964 -0.16153194 -0.17021284  0.2851733 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 5566 is [False, False, True, True, False, False]
Current timestep = 5567. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24674544 -0.07361296 -0.1988395   0.13878655]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 5567 is [False, False, True, True, False, False]
Scene graph at timestep 5567 is [False, False, True, True, False, False]
State prediction error at timestep 5567 is tensor(2.9071e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5568. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24934417 -0.09850635 -0.16566563  0.32101274]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 5568 is [False, False, True, True, False, False]
Current timestep = 5569. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24394155 -0.10888672 -0.15299611 -0.07927215]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 5569 is [False, False, True, True, False, False]
Current timestep = 5570. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24841374 -0.18188848 -0.11664906  0.19165349]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 5570 is [False, False, True, True, False, False]
Scene graph at timestep 5570 is [False, False, True, True, False, False]
State prediction error at timestep 5570 is tensor(6.4358e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5571. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24058348 -0.1413138  -0.11660871 -0.14653504]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 5571 is [False, False, True, True, False, False]
Current timestep = 5572. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22348255 -0.09602281 -0.17383084 -0.05013585]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 5572 is [False, False, True, True, False, False]
Current timestep = 5573. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21441531 -0.08872074 -0.13634907  0.02105927]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 5573 is [False, False, True, True, False, False]
Scene graph at timestep 5573 is [False, False, True, True, False, False]
State prediction error at timestep 5573 is tensor(1.7454e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5574. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23443827 -0.09079312 -0.18867098 -0.11125106]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 5574 is [False, False, True, True, False, False]
Current timestep = 5575. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24713764 -0.12484583 -0.10136856 -0.25902128]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 5575 is [False, False, True, True, False, False]
Current timestep = 5576. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24241722 -0.03474872 -0.12135707  0.08255696]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 5576 is [False, False, True, True, False, False]
Current timestep = 5577. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24891609 -0.08841775 -0.15963028  0.02455342]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 5577 is [False, False, True, True, False, False]
Current timestep = 5578. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24892968 -0.1591085  -0.15889312 -0.15700305]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 5578 is [False, False, True, True, False, False]
Current timestep = 5579. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24636984 -0.06979404 -0.18470329 -0.05504179]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 5579 is [False, False, True, True, False, False]
Current timestep = 5580. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24889457 -0.09221889 -0.15294234 -0.1835599 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 5580 is [False, False, True, True, False, False]
Current timestep = 5581. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24975085 -0.10560052 -0.18333347 -0.04725003]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 5581 is [False, False, True, True, False, False]
Current timestep = 5582. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24582094 -0.13108398 -0.172954   -0.05345339]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 5582 is [False, False, True, True, False, False]
Scene graph at timestep 5582 is [False, False, True, True, False, False]
State prediction error at timestep 5582 is tensor(3.3003e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5583. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2488864  -0.17309734 -0.16623498  0.06276536]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 5583 is [False, False, True, True, False, False]
Current timestep = 5584. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24917728 -0.14985475 -0.16231208  0.08787525]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 5584 is [False, False, True, True, False, False]
Current timestep = 5585. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24965608 -0.1715929  -0.16392964  0.12996125]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 5585 is [False, False, True, True, False, False]
Current timestep = 5586. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24896726 -0.17293482 -0.15339695  0.19167972]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 5586 is [False, False, True, True, False, False]
Human Feedback received at timestep 5586 of -1
Current timestep = 5587. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24677408 -0.1682024  -0.16239981  0.13142729]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 5587 is [False, False, True, True, False, False]
Current timestep = 5588. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24918991 -0.17605276 -0.16257995  0.07258546]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 5588 is [False, False, True, True, False, False]
Scene graph at timestep 5588 is [False, False, True, True, False, False]
State prediction error at timestep 5588 is tensor(1.0029e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5588 of -1
Current timestep = 5589. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24941021 -0.16927144 -0.13862771  0.01277769]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 5589 is [False, False, True, True, False, False]
Scene graph at timestep 5589 is [False, False, True, True, False, False]
State prediction error at timestep 5589 is tensor(2.1828e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5589 of -1
Current timestep = 5590. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24936754 -0.16209461 -0.16082565  0.02726531]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 5590 is [False, False, True, True, False, False]
Current timestep = 5591. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2493416  -0.14780366 -0.16171129  0.00695145]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 5591 is [False, False, True, True, False, False]
Human Feedback received at timestep 5591 of -1
Current timestep = 5592. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24889302 -0.13930589 -0.15194069  0.11179614]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 5592 is [False, False, True, True, False, False]
Human Feedback received at timestep 5592 of -1
Current timestep = 5593. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24640572 -0.16239755 -0.14089422  0.10668528]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 5593 is [False, False, True, True, False, False]
Current timestep = 5594. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24933225 -0.15600179 -0.16204734  0.04137993]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 5594 is [False, False, True, True, False, False]
Human Feedback received at timestep 5594 of -1
Current timestep = 5595. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24929792 -0.14832278 -0.13499968  0.00814247]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 5595 is [False, False, True, True, False, False]
Scene graph at timestep 5595 is [False, False, True, True, False, False]
State prediction error at timestep 5595 is tensor(2.2027e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5596. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2494325  -0.14549352 -0.1567452   0.00419343]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 5596 is [False, False, True, True, False, False]
Scene graph at timestep 5596 is [False, False, True, True, False, False]
State prediction error at timestep 5596 is tensor(1.9629e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5597. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24295026 -0.14164932 -0.17389567 -0.03345674]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 5597 is [False, False, True, True, False, False]
Current timestep = 5598. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24798745 -0.16233516 -0.13995224 -0.07787538]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 5598 is [False, False, True, True, False, False]
Current timestep = 5599. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24767968 -0.126168   -0.15182851 -0.01669401]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 5599 is [False, False, True, True, False, False]
Current timestep = 5600. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24872833 -0.1337065  -0.10796559 -0.07611263]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 5600 is [False, False, True, True, False, False]
Scene graph at timestep 5600 is [False, False, True, True, False, False]
State prediction error at timestep 5600 is tensor(3.9173e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5601. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24220783 -0.16477706 -0.11273155 -0.07350039]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 5601 is [False, False, True, True, False, False]
Current timestep = 5602. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2469756  -0.1408331  -0.11636476 -0.02636778]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 5602 is [False, False, True, True, False, False]
Scene graph at timestep 5602 is [False, False, True, True, False, False]
State prediction error at timestep 5602 is tensor(2.6049e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5603. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24920452 -0.12790409 -0.13594088 -0.02277911]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 5603 is [False, False, True, True, False, False]
Scene graph at timestep 5603 is [False, False, True, True, False, False]
State prediction error at timestep 5603 is tensor(3.1741e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5604. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24593323 -0.13486183 -0.13701984 -0.01203221]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 5604 is [False, False, True, True, False, False]
Scene graph at timestep 5604 is [False, False, True, True, False, False]
State prediction error at timestep 5604 is tensor(2.6671e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5605. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24897015 -0.10834986 -0.14610398 -0.02205497]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 5605 is [False, False, True, True, False, False]
Current timestep = 5606. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2413047  -0.14150691 -0.13887419 -0.12728912]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 5606 is [False, False, True, True, False, False]
Current timestep = 5607. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24857542 -0.14472976 -0.12986575 -0.10356569]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 5607 is [False, False, True, True, False, False]
Current timestep = 5608. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23651108 -0.13226613 -0.14156099 -0.04280305]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 5608 is [False, False, True, True, False, False]
Current timestep = 5609. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24742258 -0.13276033 -0.12736791 -0.10196865]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 5609 is [False, False, True, True, False, False]
Current timestep = 5610. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2485713  -0.12704751 -0.12223576 -0.07901728]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 5610 is [False, False, True, True, False, False]
Current timestep = 5611. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24561852 -0.13561217 -0.10177232 -0.10581183]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 5611 is [False, False, True, True, False, False]
Scene graph at timestep 5611 is [False, False, True, True, False, False]
State prediction error at timestep 5611 is tensor(3.9561e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5612. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24102432 -0.13408826 -0.0944531  -0.0246976 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 5612 is [False, False, True, True, False, False]
Current timestep = 5613. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23829538 -0.13549192 -0.13230443 -0.07304311]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 5613 is [False, False, True, True, False, False]
Current timestep = 5614. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23890686 -0.12385775 -0.08215415 -0.07569897]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 5614 is [False, False, True, True, False, False]
Current timestep = 5615. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24772257 -0.10184374 -0.14281712 -0.15530825]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 5615 is [False, False, True, True, False, False]
Current timestep = 5616. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24792036 -0.09509997 -0.07618317 -0.18565273]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 5616 is [False, False, True, True, False, False]
Current timestep = 5617. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24139512 -0.0831266  -0.10170667 -0.12486821]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 5617 is [False, False, True, True, False, False]
Scene graph at timestep 5617 is [False, False, True, True, False, False]
State prediction error at timestep 5617 is tensor(5.5063e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5618. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24654895 -0.15166707 -0.08812776 -0.21990567]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 5618 is [False, False, True, True, False, False]
Scene graph at timestep 5618 is [False, False, True, True, False, False]
State prediction error at timestep 5618 is tensor(5.2768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5619. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22636828 -0.0837823  -0.13440354 -0.15227503]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 5619 is [False, False, True, True, False, False]
Human Feedback received at timestep 5619 of -1
Current timestep = 5620. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24391404 -0.11353192 -0.15102375 -0.18680811]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 5620 is [False, False, True, True, False, False]
Current timestep = 5621. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22902858 -0.13392977 -0.07389364 -0.18601972]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 5621 is [False, False, True, True, False, False]
Current timestep = 5622. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22607484 -0.06695431 -0.09381491 -0.14157116]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 5622 is [False, False, True, True, False, False]
Current timestep = 5623. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24430004 -0.07565328 -0.10686798 -0.13562763]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 5623 is [False, False, True, True, False, False]
Current timestep = 5624. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24579692 -0.09339988 -0.13130456 -0.12748396]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 5624 is [False, False, True, True, False, False]
Scene graph at timestep 5624 is [False, False, True, True, False, False]
State prediction error at timestep 5624 is tensor(4.3004e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5625. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23557907 -0.08813739 -0.13291796 -0.20117247]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 5625 is [False, False, True, True, False, False]
Scene graph at timestep 5625 is [False, False, True, True, False, False]
State prediction error at timestep 5625 is tensor(4.5955e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5626. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23983824 -0.12582359 -0.10321042 -0.17622817]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 5626 is [False, False, True, True, False, False]
Scene graph at timestep 5626 is [False, False, True, True, False, False]
State prediction error at timestep 5626 is tensor(5.8989e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5627. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23408121 -0.08268628 -0.14952895 -0.18253839]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 5627 is [False, False, True, True, False, False]
Current timestep = 5628. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23948759 -0.10687307 -0.0431795  -0.15612268]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 5628 is [False, False, True, True, False, False]
Scene graph at timestep 5628 is [False, False, True, True, False, False]
State prediction error at timestep 5628 is tensor(4.1042e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5629. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21939567 -0.04027592 -0.14745505 -0.1502968 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 5629 is [False, False, True, True, False, False]
Current timestep = 5630. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24510825 -0.12239242 -0.04057314 -0.13119519]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 5630 is [False, False, True, True, False, False]
Current timestep = 5631. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24634045 -0.07669166 -0.11908916 -0.23531473]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 5631 is [False, False, True, True, False, False]
Current timestep = 5632. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24387154 -0.10721801 -0.11410981 -0.13221371]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 5632 is [False, False, True, True, False, False]
Scene graph at timestep 5632 is [False, False, True, True, False, False]
State prediction error at timestep 5632 is tensor(4.3410e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5633. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23068577 -0.09453759 -0.058424   -0.22844344]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 5633 is [False, False, True, True, False, False]
Current timestep = 5634. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24196658 -0.07810947 -0.11873299 -0.15361482]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 5634 is [False, False, True, True, False, False]
Current timestep = 5635. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23873836 -0.11812365 -0.13485703 -0.2617734 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 5635 is [False, False, True, True, False, False]
Current timestep = 5636. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2097232  -0.06481935 -0.12133312 -0.13831311]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 5636 is [False, False, True, True, False, False]
Scene graph at timestep 5636 is [False, False, True, True, False, False]
State prediction error at timestep 5636 is tensor(3.6752e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5637. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23870099 -0.07351936 -0.0762184  -0.14113057]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 5637 is [False, False, True, True, False, False]
Current timestep = 5638. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24856347 -0.1210293  -0.11656553 -0.22002423]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 5638 is [False, False, True, True, False, False]
Current timestep = 5639. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2460888  -0.04152924 -0.11045839 -0.23468852]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 5639 is [False, False, True, True, False, False]
Current timestep = 5640. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23815697 -0.07114457 -0.02505304 -0.24956763]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 5640 is [False, False, True, True, False, False]
Current timestep = 5641. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24182963 -0.13338442 -0.13058823 -0.21634871]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 5641 is [False, False, True, True, False, False]
Scene graph at timestep 5641 is [False, False, True, True, False, False]
State prediction error at timestep 5641 is tensor(3.9542e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5642. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23761886 -0.07243916 -0.12498786 -0.21541637]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 5642 is [False, False, True, True, False, False]
Current timestep = 5643. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24148071 -0.10356256 -0.06887573 -0.2272833 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 5643 is [False, False, True, True, False, False]
Scene graph at timestep 5643 is [False, False, True, True, False, False]
State prediction error at timestep 5643 is tensor(4.4135e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5644. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24559277 -0.10519288 -0.15198813 -0.10682487]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 5644 is [False, False, True, True, False, False]
Current timestep = 5645. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23086172 -0.06302215 -0.12098816 -0.17000985]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 5645 is [False, False, True, True, False, False]
Current timestep = 5646. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22984818 -0.10316084 -0.0809757  -0.17271078]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 5646 is [False, False, True, True, False, False]
Current timestep = 5647. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23437497 -0.07753181 -0.10189797 -0.18210214]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 5647 is [False, False, True, True, False, False]
Current timestep = 5648. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24775815 -0.10521507 -0.07863647 -0.24342668]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 5648 is [False, False, True, True, False, False]
Current timestep = 5649. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24153987 -0.06465593 -0.1465213  -0.15747714]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 5649 is [False, False, True, True, False, False]
Current timestep = 5650. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22580683 -0.08834079 -0.16465382 -0.2464301 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 5650 is [False, False, True, True, False, False]
Scene graph at timestep 5650 is [False, False, True, True, False, False]
State prediction error at timestep 5650 is tensor(3.6553e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5651. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24185759 -0.11119366 -0.13422059 -0.19743347]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 5651 is [False, False, True, True, False, False]
Human Feedback received at timestep 5651 of -1
Current timestep = 5652. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22409946 -0.04520734 -0.12495317 -0.23096222]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 5652 is [False, False, True, True, False, False]
Current timestep = 5653. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23466206 -0.09104878 -0.14689942 -0.19707757]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 5653 is [False, False, True, True, False, False]
Scene graph at timestep 5653 is [False, False, True, True, False, False]
State prediction error at timestep 5653 is tensor(3.7619e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5653 of -1
Current timestep = 5654. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24221465 -0.01762912 -0.09492123 -0.21416295]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 5654 is [False, False, True, True, False, False]
Current timestep = 5655. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21502227 -0.05690628 -0.09565958 -0.2149601 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 5655 is [False, False, True, True, False, False]
Current timestep = 5656. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24735308 -0.0797897  -0.08730303 -0.19899726]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 5656 is [False, False, True, True, False, False]
Scene graph at timestep 5656 is [False, False, True, True, False, False]
State prediction error at timestep 5656 is tensor(3.4364e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5657. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23030078 -0.08361658 -0.07879978 -0.22750413]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 5657 is [False, False, True, True, False, False]
Current timestep = 5658. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23937824 -0.06782106 -0.14614011 -0.27883625]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 5658 is [False, False, True, True, False, False]
Current timestep = 5659. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24164978 -0.12921253 -0.08222666 -0.3446797 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 5659 is [False, False, True, True, False, False]
Human Feedback received at timestep 5659 of -1
Current timestep = 5660. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24015051 -0.10179576 -0.0808181  -0.30461347]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 5660 is [False, False, True, True, False, False]
Current timestep = 5661. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.1918993  -0.01678354 -0.11990567 -0.41020745]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 5661 is [False, False, True, True, False, False]
Scene graph at timestep 5661 is [False, False, True, True, False, False]
State prediction error at timestep 5661 is tensor(2.9855e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5662. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23474002 -0.03162335 -0.09388953 -0.2510618 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 5662 is [False, False, True, True, False, False]
Current timestep = 5663. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2459054  -0.07741463 -0.10934934 -0.18891513]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 5663 is [False, False, True, True, False, False]
Current timestep = 5664. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24589002 -0.10704359 -0.08654648 -0.20446306]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 5664 is [False, False, True, True, False, False]
Current timestep = 5665. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24162754 -0.10577843 -0.12298748 -0.25858998]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 5665 is [False, False, True, True, False, False]
Current timestep = 5666. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24119353 -0.04835013 -0.13455108 -0.12191498]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 5666 is [False, False, True, True, False, False]
Scene graph at timestep 5666 is [False, False, True, True, False, False]
State prediction error at timestep 5666 is tensor(3.8684e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5667. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18036306 -0.08141893 -0.11506349 -0.21497184]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 5667 is [False, False, True, True, False, False]
Current timestep = 5668. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22977042 -0.06181353 -0.0391919  -0.17255497]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 5668 is [False, False, True, True, False, False]
Current timestep = 5669. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2449562  -0.06312323 -0.1040273  -0.21545559]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 5669 is [False, False, True, True, False, False]
Current timestep = 5670. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23379236 -0.13133985 -0.13880365 -0.21601194]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 5670 is [False, False, True, True, False, False]
Scene graph at timestep 5670 is [False, False, True, True, False, False]
State prediction error at timestep 5670 is tensor(4.0140e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5671. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24395353 -0.12540953 -0.11619316 -0.26548266]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 5671 is [False, False, True, True, False, False]
Current timestep = 5672. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21521887 -0.09759736 -0.0903801  -0.26471555]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 5672 is [False, False, True, True, False, False]
Current timestep = 5673. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22065645 -0.0886246  -0.13451841 -0.26183128]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 5673 is [False, False, True, True, False, False]
Current timestep = 5674. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21445674 -0.13390587 -0.04476488 -0.2097019 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 5674 is [False, False, True, True, False, False]
Scene graph at timestep 5674 is [False, False, True, True, False, False]
State prediction error at timestep 5674 is tensor(4.2287e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5675. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22415286 -0.09417096 -0.13153213 -0.20406854]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 5675 is [False, False, True, True, False, False]
Current timestep = 5676. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23724237 -0.04389495 -0.13381657 -0.2021538 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 5676 is [False, False, True, True, False, False]
Current timestep = 5677. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23708168 -0.13320449 -0.11270621 -0.1423313 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 5677 is [False, False, True, True, False, False]
Current timestep = 5678. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24698097 -0.05758572 -0.12572141 -0.20371145]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 5678 is [False, False, True, True, False, False]
Scene graph at timestep 5678 is [False, False, True, True, False, False]
State prediction error at timestep 5678 is tensor(3.8296e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5679. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23806155 -0.07338929 -0.11278082 -0.20452434]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 5679 is [False, False, True, True, False, False]
Current timestep = 5680. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19120204 -0.05320646 -0.11161295 -0.11837268]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 5680 is [False, False, True, True, False, False]
Current timestep = 5681. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24309361 -0.08693051 -0.14050326 -0.08915406]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 5681 is [False, False, True, True, False, False]
Current timestep = 5682. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.17997909 -0.10433689 -0.09855083 -0.12282854]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 5682 is [False, False, True, True, False, False]
Scene graph at timestep 5682 is [False, False, True, True, False, False]
State prediction error at timestep 5682 is tensor(3.3392e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5683. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24369842 -0.07814461 -0.13596144 -0.31010616]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 5683 is [False, False, True, True, False, False]
Scene graph at timestep 5683 is [False, False, True, True, False, False]
State prediction error at timestep 5683 is tensor(3.4982e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5684. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23459595 -0.03897385 -0.09148571 -0.24658024]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 5684 is [False, False, True, True, False, False]
Current timestep = 5685. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21387905 -0.04420264 -0.08269624 -0.17388356]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 5685 is [False, False, True, True, False, False]
Scene graph at timestep 5685 is [False, False, True, True, False, False]
State prediction error at timestep 5685 is tensor(3.2463e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5686. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23515046 -0.07800296 -0.11501291 -0.21065408]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 5686 is [False, False, True, True, False, False]
Scene graph at timestep 5686 is [False, False, True, True, False, False]
State prediction error at timestep 5686 is tensor(4.5297e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5687. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24438229 -0.11775106 -0.11710677 -0.21659082]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 5687 is [False, False, True, True, False, False]
Current timestep = 5688. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24456239 -0.08704621 -0.14137799 -0.31788158]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 5688 is [False, False, True, True, False, False]
Current timestep = 5689. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23638922 -0.1065053  -0.15581027 -0.2100228 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 5689 is [False, False, True, True, False, False]
Scene graph at timestep 5689 is [False, False, True, True, False, False]
State prediction error at timestep 5689 is tensor(4.3984e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5690. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24627349 -0.08790193 -0.14518124 -0.21933508]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 5690 is [False, False, True, True, False, False]
Current timestep = 5691. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2454085  -0.02275242 -0.05140561 -0.09231776]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 5691 is [False, False, True, True, False, False]
Current timestep = 5692. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24439713 -0.08250344 -0.13730507 -0.29307342]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 5692 is [False, False, True, True, False, False]
Current timestep = 5693. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2475192  -0.05646071 -0.11238045 -0.21357238]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 5693 is [False, False, True, True, False, False]
Current timestep = 5694. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24147436 -0.10335147 -0.10875776 -0.04393679]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 5694 is [False, False, True, True, False, False]
Current timestep = 5695. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24578741 -0.03193578 -0.1313035  -0.33663714]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 5695 is [False, False, True, True, False, False]
Current timestep = 5696. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2100712  -0.12090382 -0.12858497 -0.24870163]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 5696 is [False, False, True, True, False, False]
Scene graph at timestep 5696 is [False, False, True, True, False, False]
State prediction error at timestep 5696 is tensor(4.5068e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5697. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22998562 -0.01791368 -0.12131175 -0.24105859]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 5697 is [False, False, True, True, False, False]
Current timestep = 5698. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24489832 -0.05309534 -0.11279681 -0.30331945]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 5698 is [False, False, True, True, False, False]
Current timestep = 5699. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24451989 -0.05510327 -0.1387317  -0.3225869 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 5699 is [False, False, True, True, False, False]
Scene graph at timestep 5699 is [False, False, True, True, False, False]
State prediction error at timestep 5699 is tensor(3.2241e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5700. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2311235  -0.00945069 -0.11519502 -0.1556822 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 5700 is [False, False, True, True, False, False]
Current timestep = 5701. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23820823 -0.08971298 -0.1015043  -0.16344422]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 5701 is [False, False, True, True, False, False]
Current timestep = 5702. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2139923  -0.12087592 -0.10378656 -0.18462777]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 5702 is [False, False, True, True, False, False]
Current timestep = 5703. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24637881 -0.07615784 -0.06556207 -0.2913245 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 5703 is [False, False, True, True, False, False]
Current timestep = 5704. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24459559 -0.06463124 -0.14179952 -0.21028113]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 5704 is [False, False, True, True, False, False]
Current timestep = 5705. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23693761 -0.1163674  -0.11057413 -0.15518486]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 5705 is [False, False, True, True, False, False]
Current timestep = 5706. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24175751 -0.08699545 -0.11767182 -0.2095874 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 5706 is [False, False, True, True, False, False]
Current timestep = 5707. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23160297 -0.13706738 -0.04827927 -0.11269099]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 5707 is [False, False, True, True, False, False]
Current timestep = 5708. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18839648 -0.15849361 -0.10189158 -0.21337384]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 5708 is [False, False, True, True, False, False]
Scene graph at timestep 5708 is [False, False, True, True, False, False]
State prediction error at timestep 5708 is tensor(4.6498e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5709. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2427673  -0.06620586 -0.10959616 -0.21845901]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 5709 is [False, False, True, True, False, False]
Current timestep = 5710. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24328369 -0.07735252 -0.11536661 -0.17997599]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 5710 is [False, False, True, True, False, False]
Current timestep = 5711. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22729802 -0.06283483 -0.11131942 -0.18852127]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 5711 is [False, False, True, True, False, False]
Current timestep = 5712. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2459932  -0.09814599 -0.12114835 -0.2779498 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 5712 is [False, False, True, True, False, False]
Current timestep = 5713. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24426061 -0.0617203  -0.08551189 -0.20039874]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 5713 is [False, False, True, True, False, False]
Scene graph at timestep 5713 is [False, False, True, True, False, False]
State prediction error at timestep 5713 is tensor(3.2593e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5714. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23626488 -0.09189761 -0.11926436 -0.30129576]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 5714 is [False, False, True, True, False, False]
Current timestep = 5715. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22287881 -0.0328642  -0.15005437 -0.2709341 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 5715 is [False, False, True, True, False, False]
Scene graph at timestep 5715 is [False, False, True, True, False, False]
State prediction error at timestep 5715 is tensor(3.1985e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5716. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2386688  -0.07210031 -0.109616   -0.36815453]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 5716 is [False, False, True, True, False, False]
Current timestep = 5717. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23396248 -0.06536314 -0.13192019 -0.224347  ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 5717 is [False, False, True, True, False, False]
Scene graph at timestep 5717 is [False, False, True, True, False, False]
State prediction error at timestep 5717 is tensor(3.8847e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5718. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23602036 -0.05127645 -0.16837768 -0.25464797]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 5718 is [False, False, True, True, False, False]
Current timestep = 5719. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24186832 -0.09759003 -0.07943037 -0.23571658]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 5719 is [False, False, True, True, False, False]
Current timestep = 5720. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23344463 -0.06036896 -0.15144368 -0.23748446]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 5720 is [False, False, True, True, False, False]
Scene graph at timestep 5720 is [False, False, True, True, False, False]
State prediction error at timestep 5720 is tensor(4.1072e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5721. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23779061 -0.08003691 -0.13233845 -0.3005731 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 5721 is [False, False, True, True, False, False]
Current timestep = 5722. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23571658 -0.11624265 -0.14937256 -0.21391952]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 5722 is [False, False, True, True, False, False]
Current timestep = 5723. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2435067  -0.07158932 -0.12634902 -0.28922498]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 5723 is [False, False, True, True, False, False]
Current timestep = 5724. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24358475 -0.10151839 -0.10675488 -0.1752069 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 5724 is [False, False, True, True, False, False]
Current timestep = 5725. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23376358 -0.08335516 -0.09804279 -0.32706887]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 5725 is [False, False, True, True, False, False]
Current timestep = 5726. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24385989 -0.07456397 -0.06503756 -0.17046785]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 5726 is [False, False, True, True, False, False]
Scene graph at timestep 5726 is [False, False, True, True, False, False]
State prediction error at timestep 5726 is tensor(4.4812e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5727. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23374873 -0.08824858 -0.14835764 -0.13703227]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 5727 is [False, False, True, True, False, False]
Current timestep = 5728. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19992366 -0.09777561 -0.12102623 -0.25609505]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 5728 is [False, False, True, True, False, False]
Scene graph at timestep 5728 is [False, False, True, True, False, False]
State prediction error at timestep 5728 is tensor(4.3449e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5729. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21464998 -0.09302294 -0.14339821 -0.04944235]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 5729 is [False, False, True, True, False, False]
Current timestep = 5730. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23551732 -0.02738285 -0.10705614 -0.3258953 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 5730 is [False, False, True, True, False, False]
Current timestep = 5731. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22928265 -0.09277296 -0.07722729 -0.26837116]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 5731 is [False, False, True, True, False, False]
Scene graph at timestep 5731 is [False, False, True, True, False, False]
State prediction error at timestep 5731 is tensor(4.2129e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5732. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24307632 -0.01028913 -0.12672648 -0.24339736]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 5732 is [False, False, True, True, False, False]
Current timestep = 5733. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22791278 -0.07623027 -0.07381803 -0.33450615]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 5733 is [False, False, True, True, False, False]
Current timestep = 5734. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24837792 -0.06535128 -0.13041824 -0.17357755]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 5734 is [False, False, True, True, False, False]
Scene graph at timestep 5734 is [False, False, True, True, False, False]
State prediction error at timestep 5734 is tensor(3.7916e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5735. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23918426 -0.10673723 -0.08729818 -0.22080755]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 5735 is [False, False, True, True, False, False]
Current timestep = 5736. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24123526 -0.1178945  -0.09872258 -0.21461874]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 5736 is [False, False, True, True, False, False]
Current timestep = 5737. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24304062 -0.07252741 -0.13385777 -0.27320588]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 5737 is [False, False, True, True, False, False]
Current timestep = 5738. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22868249 -0.08433878 -0.07644114 -0.25118178]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 5738 is [False, False, True, True, False, False]
Current timestep = 5739. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24785584 -0.06205392 -0.14021549 -0.33776093]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 5739 is [False, False, True, True, False, False]
Current timestep = 5740. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2440731  -0.09989592 -0.10171929 -0.24970096]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 5740 is [False, False, True, True, False, False]
Scene graph at timestep 5740 is [False, False, True, True, False, False]
State prediction error at timestep 5740 is tensor(3.6873e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5741. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22107023 -0.07969674 -0.0942519  -0.23723578]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 5741 is [False, False, True, True, False, False]
Current timestep = 5742. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23834774 -0.12074663 -0.12246487 -0.2606722 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 5742 is [False, False, True, True, False, False]
Human Feedback received at timestep 5742 of -1
Current timestep = 5743. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24177983 -0.12742653 -0.09418795 -0.27536392]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 5743 is [False, False, True, True, False, False]
Current timestep = 5744. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23750779  0.03188625 -0.1109322  -0.14149016]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 5744 is [False, False, True, True, False, False]
Current timestep = 5745. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21750036 -0.10629536 -0.1228583  -0.27469838]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 5745 is [False, False, True, True, False, False]
Human Feedback received at timestep 5745 of -1
Current timestep = 5746. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22510737 -0.13812664 -0.12303275 -0.21418524]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 5746 is [False, False, True, True, False, False]
Current timestep = 5747. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24190187 -0.16018902 -0.11097065 -0.25395525]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 5747 is [False, False, True, True, False, False]
Current timestep = 5748. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2439335  -0.10086939 -0.14548248 -0.23073727]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 5748 is [False, False, True, True, False, False]
Current timestep = 5749. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2206504  -0.07924113 -0.09092355 -0.24345881]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 5749 is [False, False, True, True, False, False]
Current timestep = 5750. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2468546  -0.05133232 -0.08960737 -0.23280156]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 5750 is [False, False, True, True, False, False]
Scene graph at timestep 5750 is [False, False, True, True, False, False]
State prediction error at timestep 5750 is tensor(3.5709e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5751. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2447201  -0.06734802 -0.14609426 -0.12427694]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 5751 is [False, False, True, True, False, False]
Human Feedback received at timestep 5751 of -1
Current timestep = 5752. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24514142 -0.14382258 -0.12127256 -0.2422347 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 5752 is [False, False, True, True, False, False]
Current timestep = 5753. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21801019 -0.04435927 -0.10543825 -0.27047008]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 5753 is [False, False, True, True, False, False]
Scene graph at timestep 5753 is [False, False, True, True, False, False]
State prediction error at timestep 5753 is tensor(3.4252e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5754. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24456817 -0.08881299 -0.10359544 -0.12569618]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 5754 is [False, False, True, True, False, False]
Current timestep = 5755. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23311579 -0.09690826 -0.107526   -0.30432773]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 5755 is [False, False, True, True, False, False]
Current timestep = 5756. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21372616 -0.07421157 -0.10645828 -0.23872584]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 5756 is [False, False, True, True, False, False]
Current timestep = 5757. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24537295 -0.09072202 -0.11570847 -0.21878284]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 5757 is [False, False, True, True, False, False]
Current timestep = 5758. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2458713  -0.05527964 -0.14274423 -0.17818469]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 5758 is [False, False, True, True, False, False]
Current timestep = 5759. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24257162 -0.07073393 -0.09828335 -0.19862694]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 5759 is [False, False, True, True, False, False]
Current timestep = 5760. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21530315 -0.05436343 -0.07813114 -0.0642097 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 5760 is [False, False, True, True, False, False]
Scene graph at timestep 5760 is [False, False, True, True, False, False]
State prediction error at timestep 5760 is tensor(3.8429e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5761. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22928187 -0.05839869 -0.11716366 -0.22648138]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 5761 is [False, False, True, True, False, False]
Current timestep = 5762. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24842113 -0.03267518 -0.12915131 -0.19261044]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 5762 is [False, False, True, True, False, False]
Current timestep = 5763. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24690631 -0.06969175 -0.1775324  -0.14772493]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 5763 is [False, False, True, True, False, False]
Scene graph at timestep 5763 is [False, False, True, True, False, False]
State prediction error at timestep 5763 is tensor(3.7634e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5764. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24439347 -0.11497243 -0.09984349 -0.15596849]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 5764 is [False, False, True, True, False, False]
Current timestep = 5765. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20927313 -0.01359645 -0.1279793  -0.29986918]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 5765 is [False, False, True, True, False, False]
Current timestep = 5766. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23891503 -0.02609345 -0.1286153  -0.17931485]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 5766 is [False, False, True, True, False, False]
Current timestep = 5767. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21720958 -0.07398963 -0.07519817 -0.38930964]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 5767 is [False, False, True, True, False, False]
Scene graph at timestep 5767 is [False, False, True, True, False, False]
State prediction error at timestep 5767 is tensor(2.4641e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5768. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22193524 -0.06513271 -0.0948759  -0.08984578]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 5768 is [False, False, True, True, False, False]
Current timestep = 5769. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2409184  -0.06541054 -0.10942444 -0.30144572]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 5769 is [False, False, True, True, False, False]
Current timestep = 5770. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.16262335 -0.07172465 -0.05438153 -0.18838644]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 5770 is [False, False, True, True, False, False]
Current timestep = 5771. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24289876 -0.09150861 -0.09748904 -0.2682134 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 5771 is [False, False, True, True, False, False]
Scene graph at timestep 5771 is [False, False, True, True, False, False]
State prediction error at timestep 5771 is tensor(3.3962e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5772. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21639895 -0.11874659 -0.12858595 -0.2631029 ]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 5772 is [False, False, True, True, False, False]
Current timestep = 5773. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24269271 -0.0435341  -0.15850592 -0.3046236 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 5773 is [False, False, True, True, False, False]
Scene graph at timestep 5773 is [False, False, True, True, False, False]
State prediction error at timestep 5773 is tensor(2.7408e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5774. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22336137 -0.04517251 -0.17775586 -0.18949896]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 5774 is [False, False, True, True, False, False]
Current timestep = 5775. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24078822 -0.09681036 -0.09702557 -0.15875125]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 5775 is [False, False, True, True, False, False]
Scene graph at timestep 5775 is [False, False, True, True, False, False]
State prediction error at timestep 5775 is tensor(4.2960e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5776. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23890728 -0.05918887 -0.07705566 -0.29440188]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 5776 is [False, False, True, True, False, False]
Current timestep = 5777. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24530894 -0.04977816 -0.0780938  -0.31939775]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 5777 is [False, False, True, True, False, False]
Scene graph at timestep 5777 is [False, False, True, True, False, False]
State prediction error at timestep 5777 is tensor(2.4801e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5778. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23231024 -0.08630325 -0.02219839 -0.18375814]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 5778 is [False, False, True, True, False, False]
Scene graph at timestep 5778 is [False, False, True, True, False, False]
State prediction error at timestep 5778 is tensor(4.4446e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5779. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19240475 -0.08174831 -0.09905222 -0.19045365]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 5779 is [False, False, True, True, False, False]
Current timestep = 5780. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24354827 -0.10471594 -0.14844899 -0.1508326 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 5780 is [False, False, True, True, False, False]
Current timestep = 5781. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23310092 -0.05091369 -0.10512951 -0.19373727]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 5781 is [False, False, True, True, False, False]
Current timestep = 5782. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24180764 -0.08419418 -0.02110113 -0.22236788]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 5782 is [False, False, True, True, False, False]
Current timestep = 5783. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23959649 -0.1006297  -0.17772837 -0.16740125]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 5783 is [False, False, True, True, False, False]
Scene graph at timestep 5783 is [False, False, True, True, False, False]
State prediction error at timestep 5783 is tensor(2.9314e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5784. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18742156 -0.11212696 -0.1100681  -0.257699  ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 5784 is [False, False, True, True, False, False]
Current timestep = 5785. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22927383 -0.05048323 -0.06977303 -0.23033929]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 5785 is [False, False, True, True, False, False]
Scene graph at timestep 5785 is [False, False, True, True, False, False]
State prediction error at timestep 5785 is tensor(2.6613e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5786. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21457285 -0.05977821 -0.14291312 -0.24320161]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 5786 is [False, False, True, True, False, False]
Current timestep = 5787. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24509764 -0.08516374 -0.16190684 -0.09792495]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 5787 is [False, False, True, True, False, False]
Current timestep = 5788. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.15315872 -0.09076586 -0.09735654 -0.26134872]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 5788 is [False, False, True, True, False, False]
Current timestep = 5789. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22973055 -0.09242636 -0.1237886  -0.20338643]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 5789 is [False, False, True, True, False, False]
Current timestep = 5790. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.16702741 -0.05432346 -0.12116578 -0.20036405]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 5790 is [False, False, True, True, False, False]
Scene graph at timestep 5790 is [False, False, True, True, False, False]
State prediction error at timestep 5790 is tensor(3.4118e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5791. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24200267 -0.06494889 -0.12300134 -0.263757  ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 5791 is [False, False, True, True, False, False]
Current timestep = 5792. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21930262 -0.08026889 -0.05370882 -0.24983108]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 5792 is [False, False, True, True, False, False]
Current timestep = 5793. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24473247 -0.03302436 -0.15204102 -0.21461552]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 5793 is [False, False, True, True, False, False]
Current timestep = 5794. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23966268 -0.10518199 -0.14753485 -0.1949377 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 5794 is [False, False, True, True, False, False]
Scene graph at timestep 5794 is [False, False, True, True, False, False]
State prediction error at timestep 5794 is tensor(3.7159e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5795. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22375369 -0.09562606 -0.11813763 -0.2003128 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 5795 is [False, False, True, True, False, False]
Current timestep = 5796. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22209781  0.01093325 -0.12358937 -0.18374896]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 5796 is [False, False, True, True, False, False]
Current timestep = 5797. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23898101 -0.11185215 -0.16252308 -0.24860072]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 5797 is [False, False, True, True, False, False]
Current timestep = 5798. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23024613 -0.02866103 -0.17671745 -0.25668418]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 5798 is [False, False, True, True, False, False]
Current timestep = 5799. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23965907 -0.09090695 -0.15330824 -0.25745404]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 5799 is [False, False, True, True, False, False]
Current timestep = 5800. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22078651 -0.14125355 -0.13425578 -0.02217019]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 5800 is [False, False, True, True, False, False]
Current timestep = 5801. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24267334 -0.11366385 -0.11385743 -0.19073641]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 5801 is [False, False, True, True, False, False]
Current timestep = 5802. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22905308 -0.06347652 -0.13015814 -0.27515453]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 5802 is [False, False, True, True, False, False]
Current timestep = 5803. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23112348 -0.09791192 -0.12695317 -0.3050542 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 5803 is [False, False, True, True, False, False]
Current timestep = 5804. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24007228 -0.05492926 -0.10261706 -0.26063836]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 5804 is [False, False, True, True, False, False]
Scene graph at timestep 5804 is [False, False, True, True, False, False]
State prediction error at timestep 5804 is tensor(2.8069e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5805. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23486245 -0.0751937  -0.09882981 -0.38198578]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 5805 is [False, False, True, True, False, False]
Current timestep = 5806. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24820703  0.01173192 -0.16934082 -0.20321703]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 5806 is [False, False, True, True, False, False]
Current timestep = 5807. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2417151  -0.08881581 -0.12363103 -0.27087188]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 5807 is [False, False, True, True, False, False]
Scene graph at timestep 5807 is [False, False, True, True, False, False]
State prediction error at timestep 5807 is tensor(2.8774e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5808. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24476051  0.05314904 -0.10452583 -0.30212295]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 5808 is [False, False, True, True, False, False]
Current timestep = 5809. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24168289 -0.05125129 -0.15014851 -0.14362383]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 5809 is [False, False, True, True, False, False]
Current timestep = 5810. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21826863 -0.07241964 -0.08960253 -0.2372272 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 5810 is [False, False, True, True, False, False]
Current timestep = 5811. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21002227 -0.05403277 -0.13409048 -0.21214396]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 5811 is [False, False, True, True, False, False]
Scene graph at timestep 5811 is [False, False, True, True, False, False]
State prediction error at timestep 5811 is tensor(4.4193e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5812. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19903094 -0.060506   -0.12835929 -0.29904282]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 5812 is [False, False, True, True, False, False]
Current timestep = 5813. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20557335 -0.02796985 -0.05886604 -0.26192033]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 5813 is [False, False, True, True, False, False]
Current timestep = 5814. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19423309 -0.06762922 -0.13936496 -0.1651628 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 5814 is [False, False, True, True, False, False]
Current timestep = 5815. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23348671 -0.04951033 -0.16171487 -0.20076245]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 5815 is [False, False, True, True, False, False]
Current timestep = 5816. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22081497 -0.08166805 -0.06413296 -0.0639866 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 5816 is [False, False, True, True, False, False]
Current timestep = 5817. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24261898 -0.09551743 -0.04602154 -0.32071203]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 5817 is [False, False, True, True, False, False]
Current timestep = 5818. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22712702 -0.05089067 -0.10857618 -0.34481633]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 5818 is [False, False, True, True, False, False]
Current timestep = 5819. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24393985 -0.07256454 -0.11413497 -0.2768432 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 5819 is [False, False, True, True, False, False]
Current timestep = 5820. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24201736 -0.05336773 -0.08073342 -0.3155449 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 5820 is [False, False, True, True, False, False]
Current timestep = 5821. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22472513 -0.03581892 -0.13714428 -0.22977763]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 5821 is [False, False, True, True, False, False]
Scene graph at timestep 5821 is [False, False, True, True, False, False]
State prediction error at timestep 5821 is tensor(2.8437e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5822. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20872551 -0.13124572 -0.17020121 -0.13942873]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 5822 is [False, False, True, True, False, False]
Current timestep = 5823. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24261522 -0.0927847  -0.11644578 -0.26135612]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 5823 is [False, False, True, True, False, False]
Current timestep = 5824. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2011478  -0.05796766 -0.12083471 -0.25194204]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 5824 is [False, False, True, True, False, False]
Current timestep = 5825. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24855268 -0.02995789 -0.11306751 -0.15849555]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 5825 is [False, False, True, True, False, False]
Current timestep = 5826. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2211551   0.00726092 -0.12902482 -0.23581761]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 5826 is [False, False, True, True, False, False]
Scene graph at timestep 5826 is [False, False, True, True, False, False]
State prediction error at timestep 5826 is tensor(2.8706e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5827. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21562046 -0.10672739 -0.08664203 -0.10748672]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 5827 is [False, False, True, True, False, False]
Current timestep = 5828. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2412116  -0.07648185 -0.05996335 -0.25543642]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 5828 is [False, False, True, True, False, False]
Scene graph at timestep 5828 is [False, False, True, True, False, False]
State prediction error at timestep 5828 is tensor(3.4005e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5829. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2388705  -0.10960457 -0.13807923 -0.15900367]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 5829 is [False, False, True, True, False, False]
Current timestep = 5830. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19065434 -0.03967711 -0.08205122 -0.17356128]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 5830 is [False, False, True, True, False, False]
Current timestep = 5831. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24002403 -0.01813725 -0.08732557 -0.05612671]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 5831 is [False, False, True, True, False, False]
Current timestep = 5832. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23620173 -0.01235479 -0.07404184 -0.30520022]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 5832 is [False, False, True, True, False, False]
Current timestep = 5833. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20000851 -0.08685413 -0.1002872  -0.14491475]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 5833 is [False, False, True, True, False, False]
Current timestep = 5834. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24518955 -0.02844532 -0.1579818  -0.09889513]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 5834 is [False, False, True, True, False, False]
Current timestep = 5835. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20892411 -0.06043597 -0.10835773 -0.09980386]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 5835 is [False, False, True, True, False, False]
Current timestep = 5836. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.17705065 -0.16166061 -0.09845129 -0.22968102]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 5836 is [False, False, True, True, False, False]
Current timestep = 5837. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23365122 -0.07382627 -0.15712254 -0.07566011]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 5837 is [False, False, True, True, False, False]
Current timestep = 5838. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24559158 -0.09331378 -0.12737457 -0.13186556]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 5838 is [False, False, True, True, False, False]
Current timestep = 5839. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.17080355 -0.03303649 -0.12385897 -0.22776556]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 5839 is [False, False, True, True, False, False]
Current timestep = 5840. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2184771  -0.08661151 -0.13042606 -0.18752563]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 5840 is [False, False, True, True, False, False]
Scene graph at timestep 5840 is [False, False, True, True, False, False]
State prediction error at timestep 5840 is tensor(4.0020e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5841. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22839561 -0.07393856 -0.13668494 -0.15612864]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 5841 is [False, False, True, True, False, False]
Current timestep = 5842. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23336929 -0.05719107 -0.1502179  -0.27198195]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 5842 is [False, False, True, True, False, False]
Scene graph at timestep 5842 is [False, False, True, True, False, False]
State prediction error at timestep 5842 is tensor(3.1660e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5843. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22856706 -0.08969936 -0.0744209  -0.20897114]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 5843 is [False, False, True, True, False, False]
Current timestep = 5844. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24199295 -0.07043324 -0.08317381 -0.2928778 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 5844 is [False, False, True, True, False, False]
Current timestep = 5845. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20445216 -0.04441725 -0.16359472 -0.32531065]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 5845 is [False, False, True, True, False, False]
Current timestep = 5846. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22457665 -0.11410002 -0.10495582 -0.2205689 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 5846 is [False, False, True, True, False, False]
Current timestep = 5847. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23225647  0.01365176 -0.06206974 -0.26770085]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 5847 is [False, False, True, True, False, False]
Scene graph at timestep 5847 is [False, False, True, True, False, False]
State prediction error at timestep 5847 is tensor(2.2307e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5848. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23315835 -0.0836271  -0.02256224 -0.25141042]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 5848 is [False, False, True, True, False, False]
Current timestep = 5849. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23504055 -0.0527496  -0.14938276 -0.17123193]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 5849 is [False, False, True, True, False, False]
Current timestep = 5850. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2188114  -0.08809364 -0.0663407  -0.20769984]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 5850 is [False, False, True, True, False, False]
Scene graph at timestep 5850 is [False, False, True, True, False, False]
State prediction error at timestep 5850 is tensor(3.6577e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5851. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2113038   0.01691043 -0.10780615 -0.10099727]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 5851 is [False, False, True, True, False, False]
Current timestep = 5852. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23089543 -0.02365802 -0.1352792  -0.29914808]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 5852 is [False, False, True, True, False, False]
Current timestep = 5853. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20439076 -0.02107245 -0.15387328 -0.25837088]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 5853 is [False, False, True, True, False, False]
Current timestep = 5854. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21540797  0.0074763  -0.12459038 -0.24452734]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 5854 is [False, False, True, True, False, False]
Current timestep = 5855. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21035361 -0.02787137 -0.11684118 -0.15101457]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 5855 is [False, False, True, True, False, False]
Current timestep = 5856. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2125079  -0.07736275 -0.05461396 -0.25637686]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 5856 is [False, False, True, True, False, False]
Current timestep = 5857. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24050498  0.00050235 -0.12154318 -0.19107723]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 5857 is [False, False, True, True, False, False]
Current timestep = 5858. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23387545 -0.10989763 -0.12498549 -0.25297493]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 5858 is [False, False, True, True, False, False]
Scene graph at timestep 5858 is [False, False, True, True, False, False]
State prediction error at timestep 5858 is tensor(3.3246e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5859. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23564178  0.01341596 -0.07304049 -0.11549824]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 5859 is [False, False, True, True, False, False]
Scene graph at timestep 5859 is [False, False, True, True, False, False]
State prediction error at timestep 5859 is tensor(3.3432e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5860. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2386663  -0.09889869 -0.14787589 -0.1137141 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 5860 is [False, False, True, True, False, False]
Scene graph at timestep 5860 is [False, False, True, True, False, False]
State prediction error at timestep 5860 is tensor(3.3999e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5861. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23826858 -0.0590889  -0.13817252 -0.24438477]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 5861 is [False, False, True, True, False, False]
Current timestep = 5862. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18095827 -0.03061934 -0.12890676 -0.23594093]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 5862 is [False, False, True, True, False, False]
Scene graph at timestep 5862 is [False, False, True, True, False, False]
State prediction error at timestep 5862 is tensor(3.6265e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5863. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20446339 -0.03978255 -0.15455957 -0.11731571]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 5863 is [False, False, True, True, False, False]
Current timestep = 5864. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23512352 -0.04892659 -0.10519662 -0.13789028]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 5864 is [False, False, True, True, False, False]
Scene graph at timestep 5864 is [False, False, True, True, False, False]
State prediction error at timestep 5864 is tensor(3.2649e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5865. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22619286 -0.01785803 -0.17524609 -0.14517516]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 5865 is [False, False, True, True, False, False]
Current timestep = 5866. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18873447 -0.06112802 -0.15755895 -0.05555212]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 5866 is [False, False, True, True, False, False]
Scene graph at timestep 5866 is [False, False, True, True, False, False]
State prediction error at timestep 5866 is tensor(2.5956e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5867. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20498222 -0.0971688  -0.12406614 -0.27750397]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 5867 is [False, False, True, True, False, False]
Current timestep = 5868. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23910725 -0.01110265 -0.16015068 -0.18779033]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 5868 is [False, False, True, True, False, False]
Current timestep = 5869. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20662194 -0.03498417 -0.10357675 -0.24049002]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 5869 is [False, False, True, True, False, False]
Current timestep = 5870. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2180368   0.06260097 -0.0547801  -0.21500629]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 5870 is [False, False, True, True, False, False]
Current timestep = 5871. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21718079 -0.01218694 -0.15394169 -0.25581074]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 5871 is [False, False, True, True, False, False]
Scene graph at timestep 5871 is [False, False, True, True, False, False]
State prediction error at timestep 5871 is tensor(3.2792e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5872. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21802813 -0.05617696 -0.11073169 -0.01148647]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 5872 is [False, False, True, True, False, False]
Current timestep = 5873. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20147389 -0.05097808 -0.14042102 -0.29616237]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 5873 is [False, False, True, True, False, False]
Current timestep = 5874. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.246993   -0.10898998 -0.10779296 -0.2505697 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 5874 is [False, False, True, True, False, False]
Current timestep = 5875. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22561997 -0.0060409  -0.14094813  0.03631783]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 5875 is [False, False, True, True, False, False]
Current timestep = 5876. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23938501 -0.09132786 -0.13893715 -0.2531377 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 5876 is [False, False, True, True, False, False]
Scene graph at timestep 5876 is [False, False, True, True, False, False]
State prediction error at timestep 5876 is tensor(3.7033e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5877. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24376294  0.00425518 -0.17854753 -0.18329352]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 5877 is [False, False, True, True, False, False]
Current timestep = 5878. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.1707493  -0.08002719 -0.10006246 -0.05521089]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 5878 is [False, False, True, True, False, False]
Current timestep = 5879. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2483165  -0.05424091 -0.11161472 -0.24838477]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 5879 is [False, False, True, True, False, False]
Scene graph at timestep 5879 is [False, False, True, True, False, False]
State prediction error at timestep 5879 is tensor(3.2161e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5880. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24332577  0.02053654 -0.13743095 -0.15673554]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 5880 is [False, False, True, True, False, False]
Current timestep = 5881. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23565674 -0.05510643 -0.12388653 -0.09025019]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 5881 is [False, False, True, True, False, False]
Current timestep = 5882. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22217011 -0.05189314 -0.12244597 -0.23903567]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 5882 is [False, False, True, True, False, False]
Current timestep = 5883. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23485065 -0.03668606 -0.16841483 -0.07166892]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 5883 is [False, False, True, True, False, False]
Current timestep = 5884. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22197735 -0.00538319 -0.12264574 -0.1216327 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 5884 is [False, False, True, True, False, False]
Current timestep = 5885. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2358832   0.00361463 -0.05885892 -0.30087733]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 5885 is [False, False, True, True, False, False]
Current timestep = 5886. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23767674 -0.09192669 -0.11193693 -0.11205912]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 5886 is [False, False, True, True, False, False]
Current timestep = 5887. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24343032  0.00516772 -0.04437143 -0.2407623 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 5887 is [False, False, True, True, False, False]
Current timestep = 5888. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23787075  0.00470778 -0.08214231 -0.17381269]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 5888 is [False, False, True, True, False, False]
Current timestep = 5889. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22869885  0.01694337 -0.10035291 -0.08044547]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 5889 is [False, False, True, True, False, False]
Scene graph at timestep 5889 is [False, False, True, True, False, False]
State prediction error at timestep 5889 is tensor(3.1247e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5890. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2422961  -0.0323772  -0.16724353 -0.22777492]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 5890 is [False, False, True, True, False, False]
Current timestep = 5891. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.219948    0.01781693 -0.04781038 -0.01062065]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 5891 is [False, False, True, True, False, False]
Current timestep = 5892. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20631245 -0.03877151 -0.09488103  0.02612603]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 5892 is [False, False, True, True, False, False]
Current timestep = 5893. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22478998 -0.05669303 -0.08795691 -0.09620094]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 5893 is [False, False, True, True, False, False]
Current timestep = 5894. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23932761 -0.02967143 -0.00462957 -0.20559949]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 5894 is [False, False, True, True, False, False]
Current timestep = 5895. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23378682 -0.07135931 -0.13493961 -0.13984191]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 5895 is [False, False, True, True, False, False]
Current timestep = 5896. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22738391 -0.04054293 -0.08376914 -0.24946952]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 5896 is [False, False, True, True, False, False]
Scene graph at timestep 5896 is [False, False, True, True, False, False]
State prediction error at timestep 5896 is tensor(2.9717e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5897. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24225554 -0.07447946 -0.06083645  0.0056088 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 5897 is [False, False, True, True, False, False]
Current timestep = 5898. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23397207 -0.06834427 -0.16702488 -0.10716128]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 5898 is [False, False, True, True, False, False]
Current timestep = 5899. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22645694 -0.0732137  -0.18002343 -0.18545043]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 5899 is [False, False, True, True, False, False]
Current timestep = 5900. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23778534  0.02366176 -0.09832339 -0.17757547]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 5900 is [False, False, True, True, False, False]
Current timestep = 5901. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22581422  0.03175661 -0.16697611 -0.14587635]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 5901 is [False, False, True, True, False, False]
Current timestep = 5902. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19107497 -0.07897332 -0.08582813 -0.19440198]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 5902 is [False, False, True, True, False, False]
Scene graph at timestep 5902 is [False, False, True, True, False, False]
State prediction error at timestep 5902 is tensor(3.4752e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5903. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22229594  0.03655821 -0.10812066 -0.29156268]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 5903 is [False, False, True, True, False, False]
Scene graph at timestep 5903 is [False, False, True, True, False, False]
State prediction error at timestep 5903 is tensor(2.3206e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5904. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2234653  -0.04991704 -0.15692534  0.04059076]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 5904 is [False, False, True, True, False, False]
Current timestep = 5905. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18807387 -0.05244373 -0.10558873 -0.16984695]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 5905 is [False, False, True, True, False, False]
Current timestep = 5906. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23579484  0.03187823 -0.17464887 -0.3010124 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 5906 is [False, False, True, True, False, False]
Scene graph at timestep 5906 is [False, False, True, True, False, False]
State prediction error at timestep 5906 is tensor(2.1726e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5907. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.17868298 -0.07190184 -0.11529385 -0.08600479]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 5907 is [False, False, True, True, False, False]
Current timestep = 5908. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21870196 -0.12542993 -0.14387114 -0.25946665]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 5908 is [False, False, True, True, False, False]
Current timestep = 5909. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.03483716 -0.12527017 -0.13727306 -0.03450316]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 5909 is [False, False, True, True, False, False]
Scene graph at timestep 5909 is [False, False, True, True, False, False]
State prediction error at timestep 5909 is tensor(1.5115e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5910. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24210864 -0.0037231  -0.09095466 -0.22680545]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 5910 is [False, False, True, True, False, False]
Current timestep = 5911. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22832763  0.01161131 -0.09769467 -0.11318135]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 5911 is [False, False, True, True, False, False]
Current timestep = 5912. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2214823  -0.01934047 -0.1269865  -0.13573813]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 5912 is [False, False, True, True, False, False]
Current timestep = 5913. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19928366  0.02941495 -0.1183646  -0.18046618]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 5913 is [False, False, True, True, False, False]
Scene graph at timestep 5913 is [False, False, True, True, False, False]
State prediction error at timestep 5913 is tensor(2.6084e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5914. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24603271  0.00396967 -0.0490436  -0.14573574]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 5914 is [False, False, True, True, False, False]
Current timestep = 5915. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.141249   -0.0062241  -0.07341506  0.00153065]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 5915 is [False, False, True, True, False, False]
Scene graph at timestep 5915 is [False, False, True, True, False, False]
State prediction error at timestep 5915 is tensor(1.8969e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5916. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23539579 -0.05814525 -0.11380911  0.05647659]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 5916 is [False, False, True, True, False, False]
Human Feedback received at timestep 5916 of -1
Current timestep = 5917. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22555164 -0.00217763 -0.11458403 -0.20974666]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 5917 is [False, False, True, True, False, False]
Current timestep = 5918. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22277582 -0.05052313 -0.0853039  -0.0646767 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 5918 is [False, False, True, True, False, False]
Current timestep = 5919. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20296046 -0.00522472 -0.09525508  0.047508  ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 5919 is [False, False, True, True, False, False]
Current timestep = 5920. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2334336  -0.02677083 -0.11070549 -0.1328913 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 5920 is [False, False, True, True, False, False]
Current timestep = 5921. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23913556  0.01770759 -0.09393016 -0.13301593]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 5921 is [False, False, True, True, False, False]
Scene graph at timestep 5921 is [False, False, True, True, False, False]
State prediction error at timestep 5921 is tensor(2.8927e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5922. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21056736  0.05217779 -0.16322578 -0.12325877]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 5922 is [False, False, True, True, False, False]
Current timestep = 5923. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22675812 -0.11275321 -0.1780052  -0.16257638]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 5923 is [False, False, True, True, False, False]
Current timestep = 5924. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23686773 -0.08879849 -0.16300778 -0.0806058 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 5924 is [False, False, True, True, False, False]
Current timestep = 5925. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.228822    0.04223782 -0.19008929 -0.0436455 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 5925 is [False, False, True, True, False, False]
Current timestep = 5926. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21481416 -0.10036875 -0.09725629 -0.09255838]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 5926 is [False, False, True, True, False, False]
Current timestep = 5927. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23440197 -0.03160983 -0.15528652 -0.02586043]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 5927 is [False, False, True, True, False, False]
Current timestep = 5928. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24318844  0.04671913 -0.07776088 -0.14818162]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 5928 is [False, False, True, True, False, False]
Current timestep = 5929. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2433942  -0.03060883 -0.1716569  -0.0901075 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 5929 is [False, False, True, True, False, False]
Scene graph at timestep 5929 is [False, False, True, True, False, False]
State prediction error at timestep 5929 is tensor(2.3341e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5930. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22685787 -0.03174403 -0.09995937 -0.08540016]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 5930 is [False, False, True, True, False, False]
Current timestep = 5931. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24147439 -0.0388889  -0.1115253  -0.08707374]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 5931 is [False, False, True, True, False, False]
Current timestep = 5932. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19742185 -0.03002396 -0.06722274  0.03879547]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 5932 is [False, False, True, True, False, False]
Current timestep = 5933. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23563209 -0.08154121 -0.13996218 -0.3154804 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 5933 is [False, False, True, True, False, False]
Current timestep = 5934. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18825114  0.00749585 -0.10250209  0.01861405]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 5934 is [False, False, True, True, False, False]
Current timestep = 5935. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2419321  -0.02984168 -0.12975603 -0.29236674]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 5935 is [False, False, True, True, False, False]
Current timestep = 5936. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23815021 -0.02823669 -0.14684196 -0.01723242]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 5936 is [False, False, True, True, False, False]
Current timestep = 5937. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20581508 -0.04684204 -0.11663789 -0.1791982 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 5937 is [False, False, True, True, False, False]
Current timestep = 5938. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23592329 -0.10612711 -0.16338632  0.00691283]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 5938 is [False, False, True, True, False, False]
Current timestep = 5939. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.2453678  -0.05500732 -0.01828377 -0.17151397]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 5939 is [False, False, True, True, False, False]
Scene graph at timestep 5939 is [False, False, True, True, False, False]
State prediction error at timestep 5939 is tensor(3.4237e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5940. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23585957 -0.11367863 -0.06082545 -0.27753043]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 5940 is [False, False, True, True, False, False]
Current timestep = 5941. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24629685 -0.03499246 -0.09838426  0.01071477]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 5941 is [False, False, True, True, False, False]
Scene graph at timestep 5941 is [False, False, True, True, False, False]
State prediction error at timestep 5941 is tensor(2.0752e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5942. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22835088 -0.0883282  -0.1002668  -0.12216753]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 5942 is [False, False, True, True, False, False]
Scene graph at timestep 5942 is [False, False, True, True, False, False]
State prediction error at timestep 5942 is tensor(3.0968e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5943. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.20010483 -0.01734325 -0.13892186 -0.1385923 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 5943 is [False, False, True, True, False, False]
Current timestep = 5944. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18866956 -0.07226837 -0.17929976 -0.23516071]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 5944 is [False, False, True, True, False, False]
Current timestep = 5945. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24764377 -0.0389127  -0.14761272  0.04920018]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 5945 is [False, False, True, True, False, False]
Current timestep = 5946. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.13746482 -0.10061541 -0.11924794 -0.19389307]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 5946 is [False, False, True, True, False, False]
Scene graph at timestep 5946 is [False, False, True, True, False, False]
State prediction error at timestep 5946 is tensor(3.1175e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5947. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24721232 -0.08172193 -0.00131814 -0.06284523]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 5947 is [False, False, True, True, False, False]
Current timestep = 5948. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.15145126 -0.0157426  -0.13849351 -0.15747893]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 5948 is [False, False, True, True, False, False]
Current timestep = 5949. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.18429285 -0.03978856 -0.13458091  0.09467578]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 5949 is [False, False, True, True, False, False]
Scene graph at timestep 5949 is [False, False, True, True, False, False]
State prediction error at timestep 5949 is tensor(6.8609e-06, grad_fn=<MseLossBackward0>)
Current timestep = 5950. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23617172  0.01318291 -0.12877192 -0.20968586]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 5950 is [False, False, True, True, False, False]
Current timestep = 5951. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22407001 -0.06924751 -0.13178687 -0.17889363]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 5951 is [False, False, True, True, False, False]
Current timestep = 5952. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21912807 -0.09813832 -0.15069197  0.00473237]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 5952 is [False, False, True, True, False, False]
Current timestep = 5953. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.22022378  0.02458602 -0.11888398 -0.01664275]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 5953 is [False, False, True, True, False, False]
Current timestep = 5954. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.19462574 -0.02299953 -0.12024406 -0.4610783 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 5954 is [False, False, True, True, False, False]
Current timestep = 5955. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.21945351 -0.07800768 -0.1199255  -0.08900487]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 5955 is [False, False, True, True, False, False]
Current timestep = 5956. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23962265 -0.06177521 -0.05333132 -0.32646263]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 5956 is [False, False, True, True, False, False]
Scene graph at timestep 5956 is [False, False, True, True, False, False]
State prediction error at timestep 5956 is tensor(2.3505e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5957. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.1774815   0.03074655 -0.17335132  0.00890601]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 5957 is [False, False, True, True, False, False]
Scene graph at timestep 5957 is [False, False, True, True, False, False]
State prediction error at timestep 5957 is tensor(2.0625e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5958. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.17104787 -0.00400858 -0.09847151 -0.19822568]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 5958 is [False, False, True, True, False, False]
Current timestep = 5959. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.24466762 -0.06889533 -0.10571158  0.05439639]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 5959 is [False, False, True, True, False, False]
Current timestep = 5960. State = [[ 0.09007087 -0.261688  ]]. Action = [[ 0.23604286 -0.03238161 -0.19712445 -0.09264886]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 5960 is [False, False, True, True, False, False]
Scene graph at timestep 5960 is [False, False, True, True, False, False]
State prediction error at timestep 5960 is tensor(2.8061e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5961. State = [[-0.2260235   0.07543395]]. Action = [[ 0.15692538 -0.06067698 -0.15686327 -0.19666648]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 5961 is [False, False, True, True, False, False]
Current timestep = 5962. State = [[-0.21838078  0.0826568 ]]. Action = [[ 0.22895151 -0.17536035 -0.15227096 -0.1037882 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 5962 is [True, False, False, False, True, False]
Current timestep = 5963. State = [[-0.21648778  0.08210263]]. Action = [[ 0.23968291 -0.15783298 -0.1368681  -0.01791906]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 5963 is [True, False, False, False, True, False]
Current timestep = 5964. State = [[-0.2123453   0.07966616]]. Action = [[ 0.1992715  -0.15132436 -0.13393328 -0.09817219]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 5964 is [True, False, False, False, True, False]
Current timestep = 5965. State = [[-0.20728965  0.07688523]]. Action = [[ 0.24240789 -0.1287795  -0.13105595  0.00668788]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 5965 is [True, False, False, False, True, False]
Current timestep = 5966. State = [[-0.20134509  0.07416784]]. Action = [[ 0.23644072 -0.13915609 -0.17270288  0.03178084]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 5966 is [True, False, False, False, True, False]
Scene graph at timestep 5966 is [True, False, False, False, True, False]
State prediction error at timestep 5966 is tensor(1.6447e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5967. State = [[-0.19218545  0.06965528]]. Action = [[ 0.20843595 -0.13083704 -0.15094574 -0.12709129]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5967 is [True, False, False, False, True, False]
Scene graph at timestep 5967 is [True, False, False, False, True, False]
State prediction error at timestep 5967 is tensor(3.2514e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5968. State = [[-0.18405268  0.06496353]]. Action = [[ 0.15867165 -0.14787033 -0.14520842 -0.0416069 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 5968 is [True, False, False, False, True, False]
Human Feedback received at timestep 5968 of 1
Current timestep = 5969. State = [[-0.17728047  0.06053059]]. Action = [[ 0.18981355 -0.16998993 -0.15799941 -0.00579935]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 5969 is [True, False, False, False, True, False]
Current timestep = 5970. State = [[-0.17010723  0.05532357]]. Action = [[ 0.23766625 -0.1661362  -0.17180943 -0.01132405]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 5970 is [True, False, False, False, True, False]
Current timestep = 5971. State = [[-0.1632073   0.05024477]]. Action = [[ 0.23590556 -0.133329   -0.13701001 -0.01689893]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 5971 is [True, False, False, False, True, False]
Current timestep = 5972. State = [[-0.15593547  0.04489107]]. Action = [[ 0.22937545 -0.08778486 -0.15230058 -0.13415372]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 5972 is [True, False, False, False, True, False]
Human Feedback received at timestep 5972 of 1
Current timestep = 5973. State = [[-0.14771658  0.0404568 ]]. Action = [[ 0.21259165 -0.14690752 -0.14358623  0.01597548]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 5973 is [True, False, False, False, True, False]
Current timestep = 5974. State = [[-0.13884668  0.03593315]]. Action = [[ 0.22732803 -0.11367989 -0.14465542 -0.2361983 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 5974 is [True, False, False, False, True, False]
Current timestep = 5975. State = [[-0.13146158  0.03181771]]. Action = [[ 0.23475653 -0.12012413 -0.19872059 -0.11483437]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 5975 is [True, False, False, False, True, False]
Current timestep = 5976. State = [[-0.12363361  0.02736441]]. Action = [[ 0.22688055 -0.10595629 -0.16907473 -0.03759903]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 5976 is [True, False, False, False, True, False]
Human Feedback received at timestep 5976 of 1
Current timestep = 5977. State = [[-0.11464451  0.02289732]]. Action = [[ 0.2003246  -0.10118037 -0.14372854 -0.0572449 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 5977 is [True, False, False, False, True, False]
Scene graph at timestep 5977 is [True, False, False, False, True, False]
State prediction error at timestep 5977 is tensor(3.4629e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5978. State = [[-0.10536628  0.01834036]]. Action = [[ 0.22863236 -0.14025152 -0.17167231 -0.0567826 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 5978 is [True, False, False, False, True, False]
Scene graph at timestep 5978 is [True, False, False, False, True, False]
State prediction error at timestep 5978 is tensor(3.5519e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5979. State = [[-0.09666979  0.01388807]]. Action = [[ 0.24487045 -0.09083948 -0.13799372  0.04234254]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 5979 is [True, False, False, False, True, False]
Current timestep = 5980. State = [[-0.08963627  0.01015407]]. Action = [[ 0.21325815 -0.14364946 -0.14873669 -0.05621821]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 5980 is [True, False, False, False, True, False]
Human Feedback received at timestep 5980 of 1
Current timestep = 5981. State = [[-0.08162421  0.00563589]]. Action = [[ 0.23749739 -0.11980063 -0.14583895 -0.03663498]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 5981 is [True, False, False, False, True, False]
Scene graph at timestep 5981 is [True, False, False, False, True, False]
State prediction error at timestep 5981 is tensor(3.0312e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5982. State = [[-0.07194182  0.00057625]]. Action = [[ 0.24373102 -0.08537678 -0.17503636 -0.17884326]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 5982 is [True, False, False, False, True, False]
Current timestep = 5983. State = [[-0.0642241  -0.00330901]]. Action = [[ 0.17100048 -0.1414124  -0.1509599  -0.11830425]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 5983 is [True, False, False, False, True, False]
Current timestep = 5984. State = [[-0.05716354 -0.00723474]]. Action = [[ 0.22215003 -0.10132521 -0.15839903  0.00236785]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 5984 is [True, False, False, False, True, False]
Current timestep = 5985. State = [[-0.04928731 -0.01129068]]. Action = [[ 0.23072803 -0.10182849 -0.12659791 -0.19048774]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 5985 is [True, False, False, False, True, False]
Human Feedback received at timestep 5985 of 1
Current timestep = 5986. State = [[-0.04115281 -0.01544297]]. Action = [[ 0.22146344 -0.12901212 -0.12526143 -0.2245608 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 5986 is [False, True, False, False, True, False]
Current timestep = 5987. State = [[-0.232539    0.10255955]]. Action = [[ 0.20872378 -0.10635322 -0.15304577 -0.10844851]]. Reward = [100.]
Curr episode timestep = 25
Scene graph at timestep 5987 is [False, True, False, False, True, False]
Human Feedback received at timestep 5987 of 1
Current timestep = 5988. State = [[-0.22715576  0.11221989]]. Action = [[ 0.2410168  -0.12069708 -0.14813307 -0.00240546]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 5988 is [True, False, False, False, True, False]
Current timestep = 5989. State = [[-0.22546744  0.11203241]]. Action = [[ 0.23055667 -0.12515579 -0.14689288 -0.14384925]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 5989 is [True, False, False, False, True, False]
Scene graph at timestep 5989 is [True, False, False, False, True, False]
State prediction error at timestep 5989 is tensor(3.8588e-08, grad_fn=<MseLossBackward0>)
Current timestep = 5990. State = [[-0.22127561  0.11014654]]. Action = [[ 0.2130222  -0.1494187  -0.14900388  0.08212316]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 5990 is [True, False, False, False, True, False]
Current timestep = 5991. State = [[-0.21619636  0.1074459 ]]. Action = [[ 0.21186829 -0.14185768 -0.14259227  0.08373666]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 5991 is [True, False, False, False, True, False]
Current timestep = 5992. State = [[-0.20969923  0.10382546]]. Action = [[ 0.21727312 -0.11120273 -0.1428359  -0.11792207]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 5992 is [True, False, False, False, True, False]
Current timestep = 5993. State = [[-0.20252076  0.10062813]]. Action = [[ 0.21785319 -0.11195338 -0.13830002  0.00323164]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 5993 is [True, False, False, False, True, False]
Scene graph at timestep 5993 is [True, False, False, False, True, False]
State prediction error at timestep 5993 is tensor(2.6323e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5994. State = [[-0.19494852  0.09699104]]. Action = [[ 0.18580073 -0.14976375 -0.13335387  0.06505477]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 5994 is [True, False, False, False, True, False]
Scene graph at timestep 5994 is [True, False, False, False, True, False]
State prediction error at timestep 5994 is tensor(1.9788e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5995. State = [[-0.18690203  0.09185307]]. Action = [[ 0.22639441 -0.1483148  -0.15384646 -0.01047629]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 5995 is [True, False, False, False, True, False]
Human Feedback received at timestep 5995 of 1
Current timestep = 5996. State = [[-0.17878579  0.08681933]]. Action = [[ 0.24022236 -0.14937267 -0.13566299 -0.00920069]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 5996 is [True, False, False, False, True, False]
Current timestep = 5997. State = [[-0.17152421  0.08223909]]. Action = [[ 0.20991272 -0.1782937  -0.14058097 -0.01521051]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 5997 is [True, False, False, False, True, False]
Scene graph at timestep 5997 is [True, False, False, False, True, False]
State prediction error at timestep 5997 is tensor(2.1145e-05, grad_fn=<MseLossBackward0>)
Current timestep = 5998. State = [[-0.16277088  0.07629093]]. Action = [[ 0.22666538 -0.14880313 -0.10431056 -0.17417282]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 5998 is [True, False, False, False, True, False]
Scene graph at timestep 5998 is [True, False, False, False, True, False]
State prediction error at timestep 5998 is tensor(4.7743e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5998 of 1
Current timestep = 5999. State = [[-0.15367027  0.07103637]]. Action = [[ 0.18997267 -0.15048164 -0.13435583 -0.15678442]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 5999 is [True, False, False, False, True, False]
Current timestep = 6000. State = [[-0.14574431  0.06561903]]. Action = [[ 0.23690319 -0.13459276 -0.11901662 -0.03639704]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 6000 is [True, False, False, False, True, False]
Current timestep = 6001. State = [[-0.13884832  0.06086923]]. Action = [[ 0.24223912 -0.14182524 -0.14775172 -0.04002994]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 6001 is [True, False, False, False, True, False]
Current timestep = 6002. State = [[-0.13089758  0.05615555]]. Action = [[ 0.22160864 -0.12524277 -0.12757973 -0.19687444]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 6002 is [True, False, False, False, True, False]
Human Feedback received at timestep 6002 of 1
Current timestep = 6003. State = [[-0.12291279  0.05162558]]. Action = [[ 0.22379178 -0.1500454  -0.13451709  0.00546205]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 6003 is [True, False, False, False, True, False]
Current timestep = 6004. State = [[-0.11461369  0.04652666]]. Action = [[ 0.22015041 -0.12250051 -0.11108381 -0.09503931]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 6004 is [True, False, False, False, True, False]
Current timestep = 6005. State = [[-0.10673831  0.04181292]]. Action = [[ 0.2317599  -0.11888899 -0.12696409 -0.22624892]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 6005 is [True, False, False, False, True, False]
Human Feedback received at timestep 6005 of 1
Current timestep = 6006. State = [[-0.09833598  0.03708909]]. Action = [[ 0.1945495  -0.12265858 -0.1513696  -0.18827444]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 6006 is [True, False, False, False, True, False]
Current timestep = 6007. State = [[-0.09037717  0.0322498 ]]. Action = [[ 0.2379762  -0.12409937 -0.14702512 -0.05090588]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 6007 is [True, False, False, False, True, False]
Current timestep = 6008. State = [[-0.08225834  0.02758119]]. Action = [[ 0.22901905 -0.11116862 -0.1655102   0.13597918]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 6008 is [True, False, False, False, True, False]
Scene graph at timestep 6008 is [True, False, False, False, True, False]
State prediction error at timestep 6008 is tensor(2.8664e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6008 of 1
Current timestep = 6009. State = [[-0.07321358  0.02270554]]. Action = [[ 0.2222881  -0.11193126 -0.15530582 -0.13433558]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 6009 is [True, False, False, False, True, False]
Scene graph at timestep 6009 is [True, False, False, False, True, False]
State prediction error at timestep 6009 is tensor(4.2143e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6010. State = [[-0.06457479  0.01856183]]. Action = [[ 0.21770382 -0.09732309 -0.13982585 -0.01762849]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 6010 is [True, False, False, False, True, False]
Current timestep = 6011. State = [[-0.05748282  0.01460247]]. Action = [[ 0.21415663 -0.10919084 -0.1819035  -0.07066882]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 6011 is [True, False, False, False, True, False]
Current timestep = 6012. State = [[-0.04992238  0.01063763]]. Action = [[ 0.23648894 -0.13953747 -0.1651858  -0.12704921]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 6012 is [True, False, False, False, True, False]
Current timestep = 6013. State = [[-0.04254791  0.00642973]]. Action = [[ 0.23415697 -0.14268997 -0.12161809  0.02321994]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 6013 is [False, True, False, False, True, False]
Human Feedback received at timestep 6013 of 1
Current timestep = 6014. State = [[-0.22298291  0.09678996]]. Action = [[ 0.18990144 -0.10674915 -0.13941906 -0.09331304]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 6014 is [False, True, False, False, True, False]
Current timestep = 6015. State = [[-0.22057047  0.10545886]]. Action = [[ 0.22150493 -0.14603885 -0.16518404 -0.1302861 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 6015 is [True, False, False, False, True, False]
Current timestep = 6016. State = [[-0.21851283  0.1047371 ]]. Action = [[ 0.24191123 -0.15334825 -0.14123729 -0.01477158]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 6016 is [True, False, False, False, True, False]
Current timestep = 6017. State = [[-0.21499972  0.10275672]]. Action = [[ 0.2298395  -0.14256375 -0.16202624 -0.07383573]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 6017 is [True, False, False, False, True, False]
Current timestep = 6018. State = [[-0.20893812  0.0998636 ]]. Action = [[ 0.22791922 -0.15112193 -0.16069226 -0.06091553]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 6018 is [True, False, False, False, True, False]
Scene graph at timestep 6018 is [True, False, False, False, True, False]
State prediction error at timestep 6018 is tensor(1.2647e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6019. State = [[-0.2013385   0.09587785]]. Action = [[ 0.23481533 -0.15665641 -0.1519983  -0.051817  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 6019 is [True, False, False, False, True, False]
Human Feedback received at timestep 6019 of 1
Current timestep = 6020. State = [[-0.19339836  0.09164207]]. Action = [[ 0.23353153 -0.14673053 -0.15537842 -0.07701635]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 6020 is [True, False, False, False, True, False]
Current timestep = 6021. State = [[-0.18549761  0.08683726]]. Action = [[ 0.23572937 -0.14953893 -0.1467783   0.02235699]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6021 is [True, False, False, False, True, False]
Scene graph at timestep 6021 is [True, False, False, False, True, False]
State prediction error at timestep 6021 is tensor(2.9437e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6022. State = [[-0.17691301  0.08137702]]. Action = [[ 0.22950256 -0.12342432 -0.09481898 -0.09358609]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 6022 is [True, False, False, False, True, False]
Current timestep = 6023. State = [[-0.1687013   0.07663604]]. Action = [[ 0.21809557 -0.14283465 -0.13359182  0.00915766]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 6023 is [True, False, False, False, True, False]
Current timestep = 6024. State = [[-0.16156757  0.07226668]]. Action = [[ 0.20228016 -0.14036322 -0.1599284  -0.10822833]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 6024 is [True, False, False, False, True, False]
Human Feedback received at timestep 6024 of 1
Current timestep = 6025. State = [[-0.15258434  0.06698626]]. Action = [[ 0.23937708 -0.13641962 -0.12127014 -0.12423301]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 6025 is [True, False, False, False, True, False]
Current timestep = 6026. State = [[-0.1444365   0.06217687]]. Action = [[ 0.22641334 -0.13539039 -0.15459354 -0.12230742]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 6026 is [True, False, False, False, True, False]
Current timestep = 6027. State = [[-0.1365774   0.05798404]]. Action = [[ 0.2002548  -0.15428819 -0.15306036 -0.03648329]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 6027 is [True, False, False, False, True, False]
Scene graph at timestep 6027 is [True, False, False, False, True, False]
State prediction error at timestep 6027 is tensor(2.0910e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6028. State = [[-0.12774312  0.0525455 ]]. Action = [[ 0.19081175 -0.13467659 -0.1746913  -0.03689218]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 6028 is [True, False, False, False, True, False]
Scene graph at timestep 6028 is [True, False, False, False, True, False]
State prediction error at timestep 6028 is tensor(4.7851e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6028 of 1
Current timestep = 6029. State = [[-0.11932793  0.04708729]]. Action = [[ 0.22573942 -0.1186399  -0.15140088  0.00770295]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 6029 is [True, False, False, False, True, False]
Scene graph at timestep 6029 is [True, False, False, False, True, False]
State prediction error at timestep 6029 is tensor(2.9345e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6030. State = [[-0.11183041  0.04279763]]. Action = [[ 0.23479053 -0.12388562 -0.14830817 -0.10514235]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 6030 is [True, False, False, False, True, False]
Scene graph at timestep 6030 is [True, False, False, False, True, False]
State prediction error at timestep 6030 is tensor(1.6481e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6031. State = [[-0.10322489  0.03846751]]. Action = [[ 0.20471162 -0.10887058 -0.12173381 -0.04625911]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 6031 is [True, False, False, False, True, False]
Human Feedback received at timestep 6031 of 1
Current timestep = 6032. State = [[-0.09509065  0.03400917]]. Action = [[ 0.23186451 -0.14969519 -0.1358334   0.04057515]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 6032 is [True, False, False, False, True, False]
Current timestep = 6033. State = [[-0.08757596  0.02911072]]. Action = [[ 0.22868636 -0.08290502 -0.09729505  0.04076314]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 6033 is [True, False, False, False, True, False]
Current timestep = 6034. State = [[-0.07925755  0.02498202]]. Action = [[ 0.22658122 -0.16471076 -0.11911628 -0.05073947]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 6034 is [True, False, False, False, True, False]
Current timestep = 6035. State = [[-0.07188395  0.0202326 ]]. Action = [[ 0.24251881 -0.12637044 -0.08318605 -0.07912534]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 6035 is [True, False, False, False, True, False]
Current timestep = 6036. State = [[-0.06439238  0.01550874]]. Action = [[ 0.2401374  -0.12729207 -0.12892537 -0.12552202]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 6036 is [True, False, False, False, True, False]
Human Feedback received at timestep 6036 of 1
Current timestep = 6037. State = [[-0.05522912  0.01007941]]. Action = [[ 0.23817551 -0.16283128 -0.15086551 -0.00287324]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 6037 is [True, False, False, False, True, False]
Current timestep = 6038. State = [[-0.04651588  0.00492164]]. Action = [[ 0.22910964 -0.13023056 -0.1174448  -0.07458419]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 6038 is [True, False, False, False, True, False]
Scene graph at timestep 6038 is [False, True, False, False, True, False]
State prediction error at timestep 6038 is tensor(3.0469e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6039. State = [[-3.7807077e-02 -5.7080611e-05]]. Action = [[ 0.1799021  -0.11088884 -0.14425288  0.11392188]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 6039 is [False, True, False, False, True, False]
Current timestep = 6040. State = [[-0.25555882 -0.0150346 ]]. Action = [[ 0.22478613 -0.08466594 -0.10762373 -0.04910672]]. Reward = [100.]
Curr episode timestep = 25
Scene graph at timestep 6040 is [False, True, False, False, True, False]
Scene graph at timestep 6040 is [True, False, False, False, True, False]
State prediction error at timestep 6040 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6040 of 1
Current timestep = 6041. State = [[-0.25578564 -0.01722353]]. Action = [[ 0.20758206 -0.13028315 -0.17628317  0.01455557]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 6041 is [True, False, False, False, True, False]
Current timestep = 6042. State = [[-0.25339365 -0.01863251]]. Action = [[ 0.23442829 -0.11150718 -0.16024694 -0.05680877]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 6042 is [True, False, False, False, True, False]
Current timestep = 6043. State = [[-0.24916323 -0.02128482]]. Action = [[ 0.23167211 -0.11077431 -0.15517235 -0.07746315]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 6043 is [True, False, False, False, True, False]
Current timestep = 6044. State = [[-0.2434158  -0.02424817]]. Action = [[ 0.23144457 -0.12141365 -0.1326956   0.07262313]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 6044 is [True, False, False, False, True, False]
Current timestep = 6045. State = [[-0.23623975 -0.02790058]]. Action = [[ 0.23284554 -0.10343894 -0.1492424  -0.03027886]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 6045 is [True, False, False, False, True, False]
Current timestep = 6046. State = [[-0.2296237  -0.03117914]]. Action = [[ 0.23894405 -0.11816537 -0.14994787  0.09500992]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 6046 is [True, False, False, False, True, False]
Current timestep = 6047. State = [[-0.2220812  -0.03506192]]. Action = [[ 0.1905725  -0.1334975  -0.16978481  0.03509164]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6047 is [True, False, False, False, True, False]
Current timestep = 6048. State = [[-0.21485515 -0.03937496]]. Action = [[ 0.235688   -0.14124256 -0.15738955  0.06903219]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 6048 is [True, False, False, False, True, False]
Current timestep = 6049. State = [[-0.20712785 -0.04426752]]. Action = [[ 0.20159495 -0.12159175 -0.15688474 -0.00238341]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 6049 is [True, False, False, False, True, False]
Current timestep = 6050. State = [[-0.1996912  -0.04942925]]. Action = [[ 0.19863874 -0.13707086 -0.15163803 -0.05209059]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 6050 is [True, False, False, False, True, False]
Current timestep = 6051. State = [[-0.19200408 -0.05407529]]. Action = [[ 0.21220326 -0.13508745 -0.15995118  0.11547995]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 6051 is [True, False, False, False, True, False]
Current timestep = 6052. State = [[-0.18480244 -0.05866031]]. Action = [[ 0.216088   -0.04689822 -0.16992372 -0.13348514]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 6052 is [True, False, False, False, True, False]
Scene graph at timestep 6052 is [True, False, False, False, True, False]
State prediction error at timestep 6052 is tensor(2.8448e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6053. State = [[-0.1770556  -0.06266569]]. Action = [[ 0.19101483 -0.13579789 -0.16494292  0.01764727]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 6053 is [True, False, False, False, True, False]
Current timestep = 6054. State = [[-0.17005634 -0.06686659]]. Action = [[ 0.20790017 -0.10601258 -0.15306497  0.0266602 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 6054 is [True, False, False, False, True, False]
Scene graph at timestep 6054 is [True, False, False, False, True, False]
State prediction error at timestep 6054 is tensor(2.7913e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6055. State = [[-0.16183352 -0.07105133]]. Action = [[ 0.19974458 -0.10485521 -0.14754792  0.0652746 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 6055 is [True, False, False, False, True, False]
Current timestep = 6056. State = [[-0.15422767 -0.07501882]]. Action = [[ 0.23476312 -0.03289419 -0.12049887 -0.29031104]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 6056 is [True, False, False, False, True, False]
Scene graph at timestep 6056 is [True, False, False, False, True, False]
State prediction error at timestep 6056 is tensor(2.9692e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6057. State = [[-0.14617957 -0.07815736]]. Action = [[ 0.18698257 -0.12167406 -0.13745509 -0.13113332]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 6057 is [True, False, False, False, True, False]
Scene graph at timestep 6057 is [True, False, False, False, True, False]
State prediction error at timestep 6057 is tensor(2.6087e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6058. State = [[-0.13813825 -0.08182303]]. Action = [[ 0.21121538 -0.0762307  -0.13265555 -0.1391694 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 6058 is [True, False, False, False, True, False]
Scene graph at timestep 6058 is [True, False, False, False, True, False]
State prediction error at timestep 6058 is tensor(2.6567e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6059. State = [[-0.1299036  -0.08515255]]. Action = [[ 0.21779901 -0.07936126 -0.1213187  -0.02039486]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 6059 is [True, False, False, False, True, False]
Scene graph at timestep 6059 is [True, False, False, False, True, False]
State prediction error at timestep 6059 is tensor(3.1218e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6060. State = [[-0.12278126 -0.08770917]]. Action = [[ 0.18446589 -0.07229087 -0.16368149 -0.02872884]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 6060 is [True, False, False, False, True, False]
Scene graph at timestep 6060 is [True, False, False, False, True, False]
State prediction error at timestep 6060 is tensor(1.7467e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6061. State = [[-0.11469006 -0.09026497]]. Action = [[ 0.22075242 -0.105876   -0.10345322  0.06788385]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 6061 is [True, False, False, False, True, False]
Current timestep = 6062. State = [[-0.10793433 -0.09314679]]. Action = [[ 0.20699978 -0.05083922 -0.16335623 -0.12789035]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 6062 is [True, False, False, False, True, False]
Current timestep = 6063. State = [[-0.1007573  -0.09578791]]. Action = [[ 0.21105316 -0.02949172 -0.12881836  0.11972272]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 6063 is [True, False, False, False, True, False]
Scene graph at timestep 6063 is [True, False, False, False, True, False]
State prediction error at timestep 6063 is tensor(1.7139e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6064. State = [[-0.09196369 -0.09776489]]. Action = [[ 0.16213903 -0.07222265 -0.16004501 -0.09561765]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 6064 is [True, False, False, False, True, False]
Current timestep = 6065. State = [[-0.08496848 -0.10007729]]. Action = [[ 0.21666312 -0.09306681 -0.1734568  -0.10540605]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 6065 is [True, False, False, False, True, False]
Current timestep = 6066. State = [[-0.07849266 -0.10267448]]. Action = [[ 0.22032562 -0.00114006 -0.12945995 -0.11494726]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 6066 is [True, False, False, False, True, False]
Scene graph at timestep 6066 is [True, False, False, False, True, False]
State prediction error at timestep 6066 is tensor(1.7220e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6067. State = [[-0.07159283 -0.10455386]]. Action = [[ 0.23185521 -0.10304743 -0.11848105 -0.06001991]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 6067 is [True, False, False, False, True, False]
Scene graph at timestep 6067 is [True, False, False, False, True, False]
State prediction error at timestep 6067 is tensor(1.9642e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6067 of 1
Current timestep = 6068. State = [[-0.06277262 -0.10739224]]. Action = [[ 0.19233322 -0.09725717 -0.18002543 -0.30289292]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 6068 is [True, False, False, False, True, False]
Current timestep = 6069. State = [[-0.05546912 -0.11047234]]. Action = [[ 0.21157652 -0.1013937  -0.12227035 -0.10925764]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 6069 is [True, False, False, False, True, False]
Current timestep = 6070. State = [[-0.04885577 -0.11386195]]. Action = [[ 0.24123499 -0.10460541 -0.15924352  0.0080204 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 6070 is [True, False, False, False, True, False]
Scene graph at timestep 6070 is [False, True, False, False, True, False]
State prediction error at timestep 6070 is tensor(1.9239e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6071. State = [[-0.04108268 -0.11771808]]. Action = [[ 0.2167547  -0.0994779  -0.15399323 -0.05148005]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 6071 is [False, True, False, False, True, False]
Scene graph at timestep 6071 is [False, True, False, False, True, False]
State prediction error at timestep 6071 is tensor(3.6828e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6072. State = [[-0.03184073 -0.12183419]]. Action = [[ 0.23739678 -0.12766515 -0.20306136  0.14937961]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 6072 is [False, True, False, False, True, False]
Current timestep = 6073. State = [[-0.02471951 -0.12555625]]. Action = [[ 0.2406219  -0.06344432 -0.12409514  0.1285367 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 6073 is [False, True, False, False, True, False]
Scene graph at timestep 6073 is [False, True, False, True, False, False]
State prediction error at timestep 6073 is tensor(2.1474e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6074. State = [[-0.0159802  -0.12899396]]. Action = [[ 0.2303226  -0.06705853 -0.1511276   0.13563728]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 6074 is [False, True, False, True, False, False]
Current timestep = 6075. State = [[-0.00851455 -0.13167992]]. Action = [[ 0.2175177   0.05100757 -0.07713786  0.0293479 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 6075 is [False, True, False, True, False, False]
Current timestep = 6076. State = [[-0.00019784 -0.13306367]]. Action = [[ 0.22847402 -0.0750494  -0.1499343   0.11313605]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 6076 is [False, True, False, True, False, False]
Human Feedback received at timestep 6076 of 1
Current timestep = 6077. State = [[ 0.00843414 -0.13474564]]. Action = [[ 0.219563   -0.07715707 -0.14596424 -0.09365654]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 6077 is [False, True, False, True, False, False]
Current timestep = 6078. State = [[ 0.02245442 -0.13784635]]. Action = [[ 0.22795546 -0.03932588 -0.14757386 -0.19715065]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 6078 is [False, True, False, True, False, False]
Scene graph at timestep 6078 is [False, True, False, True, False, False]
State prediction error at timestep 6078 is tensor(9.1675e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6079. State = [[ 0.03015972 -0.13951357]]. Action = [[ 0.14242375 -0.05561875 -0.18793592 -0.2368533 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 6079 is [False, True, False, True, False, False]
Human Feedback received at timestep 6079 of -1
Current timestep = 6080. State = [[ 0.03726569 -0.14096676]]. Action = [[ 0.12497562  0.0343067  -0.05517054  0.3939352 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 6080 is [False, True, False, True, False, False]
Scene graph at timestep 6080 is [False, True, False, True, False, False]
State prediction error at timestep 6080 is tensor(3.9443e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6080 of -1
Current timestep = 6081. State = [[ 0.04399437 -0.14191507]]. Action = [[ 0.15446195 -0.12425056 -0.11602452  0.01270688]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 6081 is [False, True, False, True, False, False]
Current timestep = 6082. State = [[ 0.05038182 -0.14366288]]. Action = [[ 0.2310881   0.00950629 -0.03263605  0.05530059]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 6082 is [False, True, False, True, False, False]
Scene graph at timestep 6082 is [False, False, True, True, False, False]
State prediction error at timestep 6082 is tensor(3.2900e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6083. State = [[ 0.05744675 -0.14486402]]. Action = [[ 0.23318493  0.04452726 -0.08322406 -0.09987605]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 6083 is [False, False, True, True, False, False]
Human Feedback received at timestep 6083 of -1
Current timestep = 6084. State = [[ 0.06465702 -0.14540756]]. Action = [[ 0.20544747  0.00277692 -0.10958773 -0.16585863]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 6084 is [False, False, True, True, False, False]
Human Feedback received at timestep 6084 of -1
Current timestep = 6085. State = [[ 0.07013719 -0.1457372 ]]. Action = [[ 0.23348182  0.03752702 -0.17465211 -0.16416848]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 6085 is [False, False, True, True, False, False]
Human Feedback received at timestep 6085 of -1
Current timestep = 6086. State = [[ 0.07450213 -0.1460562 ]]. Action = [[ 0.22706258 -0.00362909 -0.2016682   0.02658176]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 6086 is [False, False, True, True, False, False]
Current timestep = 6087. State = [[ 0.07767612 -0.14621656]]. Action = [[ 0.19145101 -0.05679557 -0.1393881   0.22256601]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 6087 is [False, False, True, True, False, False]
Scene graph at timestep 6087 is [False, False, True, True, False, False]
State prediction error at timestep 6087 is tensor(5.9212e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6087 of -1
Current timestep = 6088. State = [[ 0.08075849 -0.14638999]]. Action = [[ 0.24308082  0.04979637 -0.17913772  0.08848143]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 6088 is [False, False, True, True, False, False]
Human Feedback received at timestep 6088 of -1
Current timestep = 6089. State = [[ 0.08144518 -0.14639162]]. Action = [[0.23488355 0.01986817 0.0755851  0.19897282]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 6089 is [False, False, True, True, False, False]
Current timestep = 6090. State = [[ 0.08137282 -0.1463969 ]]. Action = [[ 0.07743132 -0.06683728  0.07189545  0.14080775]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 6090 is [False, False, True, True, False, False]
Scene graph at timestep 6090 is [False, False, True, True, False, False]
State prediction error at timestep 6090 is tensor(1.6411e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6090 of -1
Current timestep = 6091. State = [[ 0.08139079 -0.14637545]]. Action = [[ 0.16903383  0.08751273 -0.07210475  0.20334196]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 6091 is [False, False, True, True, False, False]
Human Feedback received at timestep 6091 of -1
Current timestep = 6092. State = [[ 0.08139079 -0.14637545]]. Action = [[ 0.10406798  0.01258251 -0.09581107  0.36790884]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 6092 is [False, False, True, True, False, False]
Scene graph at timestep 6092 is [False, False, True, True, False, False]
State prediction error at timestep 6092 is tensor(8.2315e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6093. State = [[ 0.08139079 -0.14637545]]. Action = [[ 0.19920889  0.00522584 -0.15583952  0.11232972]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 6093 is [False, False, True, True, False, False]
Human Feedback received at timestep 6093 of -1
Current timestep = 6094. State = [[ 0.08139079 -0.14637545]]. Action = [[ 0.17969775  0.0098137   0.01709273 -0.06337756]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 6094 is [False, False, True, True, False, False]
Scene graph at timestep 6094 is [False, False, True, True, False, False]
State prediction error at timestep 6094 is tensor(4.6951e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6095. State = [[ 0.08139079 -0.14637545]]. Action = [[ 0.14760679 -0.06323022 -0.15244046 -0.01953977]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 6095 is [False, False, True, True, False, False]
Scene graph at timestep 6095 is [False, False, True, True, False, False]
State prediction error at timestep 6095 is tensor(1.0850e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6096. State = [[ 0.08139079 -0.14637545]]. Action = [[ 0.22884327 -0.06498066 -0.10006237  0.2602576 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 6096 is [False, False, True, True, False, False]
Scene graph at timestep 6096 is [False, False, True, True, False, False]
State prediction error at timestep 6096 is tensor(4.2483e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6097. State = [[ 0.08139079 -0.14637545]]. Action = [[ 0.21289796 -0.07567365 -0.19664577  0.22111237]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 6097 is [False, False, True, True, False, False]
Human Feedback received at timestep 6097 of -1
Current timestep = 6098. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17557463  0.01652315 -0.03191583  0.23308063]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 6098 is [False, False, True, True, False, False]
Human Feedback received at timestep 6098 of -1
Current timestep = 6099. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15905893 -0.07875888 -0.16608651 -0.27977818]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 6099 is [False, False, True, True, False, False]
Current timestep = 6100. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22076407 -0.08175117 -0.02461068 -0.08078629]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 6100 is [False, False, True, True, False, False]
Current timestep = 6101. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21294272 -0.08065069 -0.17012168  0.16040444]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 6101 is [False, False, True, True, False, False]
Scene graph at timestep 6101 is [False, False, True, True, False, False]
State prediction error at timestep 6101 is tensor(5.4315e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6102. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.16770762 -0.11218047 -0.20913997  0.0380547 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 6102 is [False, False, True, True, False, False]
Scene graph at timestep 6102 is [False, False, True, True, False, False]
State prediction error at timestep 6102 is tensor(9.3952e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6103. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.12025657 -0.08891383 -0.04975662 -0.2302717 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 6103 is [False, False, True, True, False, False]
Current timestep = 6104. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19452679 -0.04550663 -0.01492813 -0.11983263]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 6104 is [False, False, True, True, False, False]
Current timestep = 6105. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24665314 -0.08038366 -0.11421224  0.2017132 ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 6105 is [False, False, True, True, False, False]
Current timestep = 6106. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.09525979 -0.11857556 -0.15901482 -0.19234562]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 6106 is [False, False, True, True, False, False]
Current timestep = 6107. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19296825 -0.01545539  0.04793897 -0.1294834 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 6107 is [False, False, True, True, False, False]
Current timestep = 6108. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23513252 -0.05506763 -0.09562485 -0.04831457]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 6108 is [False, False, True, True, False, False]
Current timestep = 6109. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22074074 -0.14788957 -0.21421093  0.11226511]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 6109 is [False, False, True, True, False, False]
Scene graph at timestep 6109 is [False, False, True, True, False, False]
State prediction error at timestep 6109 is tensor(1.0206e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6110. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24170434 -0.08202808 -0.14186262 -0.04319715]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 6110 is [False, False, True, True, False, False]
Current timestep = 6111. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22637719 -0.12815109 -0.07499713 -0.14662099]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 6111 is [False, False, True, True, False, False]
Current timestep = 6112. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21992415 -0.02861679 -0.08401927 -0.00295448]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 6112 is [False, False, True, True, False, False]
Scene graph at timestep 6112 is [False, False, True, True, False, False]
State prediction error at timestep 6112 is tensor(2.7015e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6113. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.11930451 -0.14773637 -0.04922806 -0.01082188]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 6113 is [False, False, True, True, False, False]
Scene graph at timestep 6113 is [False, False, True, True, False, False]
State prediction error at timestep 6113 is tensor(1.0664e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6114. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18619418  0.04041117 -0.18419255 -0.15439653]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 6114 is [False, False, True, True, False, False]
Scene graph at timestep 6114 is [False, False, True, True, False, False]
State prediction error at timestep 6114 is tensor(1.1784e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6115. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22713023 -0.04205132 -0.13239183  0.31488085]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 6115 is [False, False, True, True, False, False]
Current timestep = 6116. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21579677 -0.16015378 -0.0777674   0.18099844]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 6116 is [False, False, True, True, False, False]
Scene graph at timestep 6116 is [False, False, True, True, False, False]
State prediction error at timestep 6116 is tensor(2.2527e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6117. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2310291  -0.09270778 -0.19879174  0.15125978]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 6117 is [False, False, True, True, False, False]
Scene graph at timestep 6117 is [False, False, True, True, False, False]
State prediction error at timestep 6117 is tensor(7.7669e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6118. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21786273 -0.07707563 -0.14412922  0.12977505]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 6118 is [False, False, True, True, False, False]
Scene graph at timestep 6118 is [False, False, True, True, False, False]
State prediction error at timestep 6118 is tensor(2.5921e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6119. State = [[ 0.08140874 -0.14637414]]. Action = [[0.23533845 0.05122468 0.04865417 0.04630637]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 6119 is [False, False, True, True, False, False]
Current timestep = 6120. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18411556 -0.01259464 -0.18295527 -0.06850046]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 6120 is [False, False, True, True, False, False]
Current timestep = 6121. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23384959 -0.12484831 -0.16303515 -0.11109257]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 6121 is [False, False, True, True, False, False]
Current timestep = 6122. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22374827 -0.05612591  0.0080708   0.20496595]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 6122 is [False, False, True, True, False, False]
Scene graph at timestep 6122 is [False, False, True, True, False, False]
State prediction error at timestep 6122 is tensor(2.2993e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6123. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22663686 -0.04225776 -0.12264901  0.13680851]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 6123 is [False, False, True, True, False, False]
Current timestep = 6124. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1805979  -0.14098413  0.05576774  0.42387056]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 6124 is [False, False, True, True, False, False]
Scene graph at timestep 6124 is [False, False, True, True, False, False]
State prediction error at timestep 6124 is tensor(6.4867e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6125. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23768061 -0.04698285 -0.11561222 -0.13353634]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 6125 is [False, False, True, True, False, False]
Current timestep = 6126. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.16722694  0.03907609 -0.08208206  0.19468522]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 6126 is [False, False, True, True, False, False]
Current timestep = 6127. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20200706 -0.02105606 -0.12569962  0.25789392]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 6127 is [False, False, True, True, False, False]
Current timestep = 6128. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20279771 -0.05203058 -0.10877962  0.22187603]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 6128 is [False, False, True, True, False, False]
Current timestep = 6129. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.0493148  -0.05935481 -0.1612555   0.01679635]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 6129 is [False, False, True, True, False, False]
Current timestep = 6130. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21530217 -0.14421281  0.01220945  0.16744101]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 6130 is [False, False, True, True, False, False]
Current timestep = 6131. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2017526  -0.01760624 -0.2238368  -0.11902314]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 6131 is [False, False, True, True, False, False]
Current timestep = 6132. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19359255 -0.05938125 -0.07558411  0.14622152]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 6132 is [False, False, True, True, False, False]
Current timestep = 6133. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24445885 -0.14359315 -0.08226606 -0.19131285]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 6133 is [False, False, True, True, False, False]
Scene graph at timestep 6133 is [False, False, True, True, False, False]
State prediction error at timestep 6133 is tensor(1.3224e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6134. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22530377 -0.14039838 -0.19338141 -0.26638418]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 6134 is [False, False, True, True, False, False]
Scene graph at timestep 6134 is [False, False, True, True, False, False]
State prediction error at timestep 6134 is tensor(2.2037e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6135. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.08028781 -0.14294522 -0.14387421  0.0952481 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 6135 is [False, False, True, True, False, False]
Current timestep = 6136. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.11498731 -0.03786376  0.01887977 -0.04251188]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 6136 is [False, False, True, True, False, False]
Current timestep = 6137. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24612382 -0.05444077  0.15015632 -0.00046849]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 6137 is [False, False, True, True, False, False]
Current timestep = 6138. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2369166   0.09205878 -0.14688939 -0.33419263]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 6138 is [False, False, True, True, False, False]
Scene graph at timestep 6138 is [False, False, True, True, False, False]
State prediction error at timestep 6138 is tensor(5.3551e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6139. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.13852617 -0.00746956 -0.13760526 -0.14716142]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 6139 is [False, False, True, True, False, False]
Current timestep = 6140. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2072173  -0.01895505 -0.02282988 -0.04892784]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 6140 is [False, False, True, True, False, False]
Current timestep = 6141. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23181486 -0.02212936 -0.1383612   0.00419557]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 6141 is [False, False, True, True, False, False]
Scene graph at timestep 6141 is [False, False, True, True, False, False]
State prediction error at timestep 6141 is tensor(9.6371e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6142. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2360956  -0.0236135  -0.17028636  0.12942088]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 6142 is [False, False, True, True, False, False]
Current timestep = 6143. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15455511 -0.07082987 -0.12005593  0.21824908]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 6143 is [False, False, True, True, False, False]
Current timestep = 6144. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2286762  -0.051792   -0.11971328  0.25519836]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 6144 is [False, False, True, True, False, False]
Scene graph at timestep 6144 is [False, False, True, True, False, False]
State prediction error at timestep 6144 is tensor(6.1709e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6145. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18903047  0.01348701 -0.148617    0.42082047]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 6145 is [False, False, True, True, False, False]
Scene graph at timestep 6145 is [False, False, True, True, False, False]
State prediction error at timestep 6145 is tensor(1.0421e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6146. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19660074 -0.05854371 -0.1397203   0.513137  ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 6146 is [False, False, True, True, False, False]
Current timestep = 6147. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19318372 -0.10922594 -0.1432223   0.16261911]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 6147 is [False, False, True, True, False, False]
Current timestep = 6148. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23452008 -0.08660544 -0.13234867 -0.27737832]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 6148 is [False, False, True, True, False, False]
Scene graph at timestep 6148 is [False, False, True, True, False, False]
State prediction error at timestep 6148 is tensor(1.5373e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6149. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23283863  0.06293067 -0.0553003   0.21649373]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 6149 is [False, False, True, True, False, False]
Current timestep = 6150. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22049701 -0.01781656 -0.11018905 -0.21757847]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 6150 is [False, False, True, True, False, False]
Current timestep = 6151. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24044937 -0.08861002 -0.10786754  0.02067304]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 6151 is [False, False, True, True, False, False]
Scene graph at timestep 6151 is [False, False, True, True, False, False]
State prediction error at timestep 6151 is tensor(7.0430e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6152. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23526913 -0.09450643 -0.05440272  0.17466259]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 6152 is [False, False, True, True, False, False]
Current timestep = 6153. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23499098  0.05209437 -0.07364081  0.31054044]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 6153 is [False, False, True, True, False, False]
Current timestep = 6154. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23821744 -0.04978646 -0.17484461 -0.06570411]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 6154 is [False, False, True, True, False, False]
Current timestep = 6155. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22415435 -0.05174902 -0.11059161 -0.25354886]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 6155 is [False, False, True, True, False, False]
Current timestep = 6156. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.09579062  0.03961682 -0.14592148  0.28864408]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 6156 is [False, False, True, True, False, False]
Scene graph at timestep 6156 is [False, False, True, True, False, False]
State prediction error at timestep 6156 is tensor(3.6150e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6157. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24643064  0.15515119 -0.16117278 -0.16043442]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 6157 is [False, False, True, True, False, False]
Current timestep = 6158. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24491197 -0.02917151 -0.08145776 -0.20779991]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 6158 is [False, False, True, True, False, False]
Current timestep = 6159. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20435768 -0.09033185 -0.20024154  0.04067087]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 6159 is [False, False, True, True, False, False]
Current timestep = 6160. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.16673237 -0.03729814 -0.09264025 -0.17919052]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 6160 is [False, False, True, True, False, False]
Current timestep = 6161. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22783592 -0.09665528 -0.22970778  0.14007413]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 6161 is [False, False, True, True, False, False]
Current timestep = 6162. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22637391 -0.11699976 -0.05641419 -0.45568848]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 6162 is [False, False, True, True, False, False]
Current timestep = 6163. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19601703  0.0371823  -0.17109317  0.30001903]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 6163 is [False, False, True, True, False, False]
Current timestep = 6164. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15092283 -0.08212537 -0.06330031  0.02240217]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 6164 is [False, False, True, True, False, False]
Current timestep = 6165. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.12821984 -0.11274052 -0.21066748 -0.07171404]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 6165 is [False, False, True, True, False, False]
Current timestep = 6166. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24243957 -0.19436201 -0.10964052 -0.07827139]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 6166 is [False, False, True, True, False, False]
Current timestep = 6167. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17934111 -0.14091606  0.10601568  0.07578266]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 6167 is [False, False, True, True, False, False]
Current timestep = 6168. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23282546 -0.03154776 -0.18694742 -0.38239944]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 6168 is [False, False, True, True, False, False]
Current timestep = 6169. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21738899 -0.05680421 -0.06554672 -0.5610751 ]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 6169 is [False, False, True, True, False, False]
Current timestep = 6170. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22353473  0.05863753 -0.05987757  0.2659899 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 6170 is [False, False, True, True, False, False]
Current timestep = 6171. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.03237244  0.0235239  -0.16369827  0.03147459]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 6171 is [False, False, True, True, False, False]
Current timestep = 6172. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.09560907 -0.09387884 -0.06347397 -0.08749759]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 6172 is [False, False, True, True, False, False]
Scene graph at timestep 6172 is [False, False, True, True, False, False]
State prediction error at timestep 6172 is tensor(9.0233e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6173. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17364311 -0.06896269 -0.17728159 -0.35447   ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 6173 is [False, False, True, True, False, False]
Current timestep = 6174. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24316245 -0.06760156 -0.03802995 -0.31787217]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 6174 is [False, False, True, True, False, False]
Current timestep = 6175. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18876994 -0.06434031  0.0150291  -0.16098166]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 6175 is [False, False, True, True, False, False]
Current timestep = 6176. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22665668  0.00179198 -0.16157143  0.07752001]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 6176 is [False, False, True, True, False, False]
Scene graph at timestep 6176 is [False, False, True, True, False, False]
State prediction error at timestep 6176 is tensor(8.1830e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6177. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19699192 -0.05968626 -0.08046828 -0.10414588]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 6177 is [False, False, True, True, False, False]
Scene graph at timestep 6177 is [False, False, True, True, False, False]
State prediction error at timestep 6177 is tensor(1.0027e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6178. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.14878988  0.00591016  0.04137453 -0.13776296]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 6178 is [False, False, True, True, False, False]
Scene graph at timestep 6178 is [False, False, True, True, False, False]
State prediction error at timestep 6178 is tensor(3.6726e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6179. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19938803 -0.03048317 -0.01291206 -0.14521283]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 6179 is [False, False, True, True, False, False]
Scene graph at timestep 6179 is [False, False, True, True, False, False]
State prediction error at timestep 6179 is tensor(7.4605e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6180. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22016841 -0.09594679  0.0703162   0.06442237]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 6180 is [False, False, True, True, False, False]
Current timestep = 6181. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24326783 -0.10967445 -0.10063785 -0.28990424]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 6181 is [False, False, True, True, False, False]
Current timestep = 6182. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21835285 -0.17099679 -0.06896672 -0.12754017]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 6182 is [False, False, True, True, False, False]
Current timestep = 6183. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20814711 -0.01879054 -0.1242491  -0.24882388]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 6183 is [False, False, True, True, False, False]
Current timestep = 6184. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22820276 -0.10258837 -0.11925223 -0.05161053]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 6184 is [False, False, True, True, False, False]
Scene graph at timestep 6184 is [False, False, True, True, False, False]
State prediction error at timestep 6184 is tensor(1.1916e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6185. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.13388845 -0.150389   -0.0962819  -0.34365684]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 6185 is [False, False, True, True, False, False]
Current timestep = 6186. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23877102 -0.10063462 -0.07010756  0.21727729]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 6186 is [False, False, True, True, False, False]
Scene graph at timestep 6186 is [False, False, True, True, False, False]
State prediction error at timestep 6186 is tensor(9.7636e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6187. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22012329  0.0254201  -0.0547438  -0.31081945]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 6187 is [False, False, True, True, False, False]
Scene graph at timestep 6187 is [False, False, True, True, False, False]
State prediction error at timestep 6187 is tensor(5.0670e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6188. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22524488 -0.09048416 -0.16042805 -0.17698044]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 6188 is [False, False, True, True, False, False]
Current timestep = 6189. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22900033 -0.03468448 -0.13186303 -0.3942157 ]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 6189 is [False, False, True, True, False, False]
Current timestep = 6190. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18193924  0.01060456 -0.04279062  0.19302213]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 6190 is [False, False, True, True, False, False]
Current timestep = 6191. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2363913  -0.09880307 -0.01919194 -0.05807257]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 6191 is [False, False, True, True, False, False]
Scene graph at timestep 6191 is [False, False, True, True, False, False]
State prediction error at timestep 6191 is tensor(4.8640e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6192. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24351618 -0.03912586 -0.07928422 -0.22219849]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 6192 is [False, False, True, True, False, False]
Current timestep = 6193. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19048512 -0.03548902  0.0085713  -0.20293498]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 6193 is [False, False, True, True, False, False]
Current timestep = 6194. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24008062 -0.13316087 -0.0346628  -0.18269974]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 6194 is [False, False, True, True, False, False]
Current timestep = 6195. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23636058 -0.11609095 -0.09923112 -0.37624753]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 6195 is [False, False, True, True, False, False]
Current timestep = 6196. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.13845831 -0.01139817 -0.01933882 -0.17298722]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 6196 is [False, False, True, True, False, False]
Current timestep = 6197. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23076302  0.02045915 -0.14476022 -0.09568286]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 6197 is [False, False, True, True, False, False]
Current timestep = 6198. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23877037 -0.0808861  -0.1152128   0.02763176]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 6198 is [False, False, True, True, False, False]
Current timestep = 6199. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23422056 -0.00875558 -0.09318374  0.02614689]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 6199 is [False, False, True, True, False, False]
Scene graph at timestep 6199 is [False, False, True, True, False, False]
State prediction error at timestep 6199 is tensor(5.2475e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6200. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22962469 -0.06028157 -0.13754942 -0.05130935]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 6200 is [False, False, True, True, False, False]
Current timestep = 6201. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.05324808 -0.04734612 -0.12281954 -0.2880947 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 6201 is [False, False, True, True, False, False]
Current timestep = 6202. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21195155 -0.01270469 -0.04642279 -0.05713779]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 6202 is [False, False, True, True, False, False]
Current timestep = 6203. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20494828 -0.07258844 -0.04165667 -0.25963068]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 6203 is [False, False, True, True, False, False]
Current timestep = 6204. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2479887  -0.10400882 -0.11952874  0.11801004]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 6204 is [False, False, True, True, False, False]
Current timestep = 6205. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20469582 -0.04654503 -0.03695787 -0.0389207 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 6205 is [False, False, True, True, False, False]
Current timestep = 6206. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21910447  0.04436645 -0.15749508 -0.24568343]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 6206 is [False, False, True, True, False, False]
Scene graph at timestep 6206 is [False, False, True, True, False, False]
State prediction error at timestep 6206 is tensor(6.3124e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6207. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1998483  -0.11361292 -0.10055855  0.04387975]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 6207 is [False, False, True, True, False, False]
Current timestep = 6208. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2404412   0.07199156 -0.05565938  0.15746129]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 6208 is [False, False, True, True, False, False]
Current timestep = 6209. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1461016   0.03025901 -0.14404297  0.11083984]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 6209 is [False, False, True, True, False, False]
Current timestep = 6210. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23274726 -0.00206779 -0.08324252 -0.39497232]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 6210 is [False, False, True, True, False, False]
Current timestep = 6211. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20265514 -0.06070773 -0.06614938 -0.07446563]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 6211 is [False, False, True, True, False, False]
Current timestep = 6212. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21387285 -0.11378226 -0.09209339 -0.16641241]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 6212 is [False, False, True, True, False, False]
Scene graph at timestep 6212 is [False, False, True, True, False, False]
State prediction error at timestep 6212 is tensor(6.5265e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6213. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24459183 -0.13348302 -0.13839814 -0.33516765]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 6213 is [False, False, True, True, False, False]
Current timestep = 6214. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23590022 -0.0181275   0.01382414 -0.5740889 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 6214 is [False, False, True, True, False, False]
Current timestep = 6215. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24242955 -0.08347714 -0.04493913 -0.29761183]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 6215 is [False, False, True, True, False, False]
Scene graph at timestep 6215 is [False, False, True, True, False, False]
State prediction error at timestep 6215 is tensor(8.1069e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6216. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22323495  0.03252047 -0.0250867  -0.08160925]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 6216 is [False, False, True, True, False, False]
Current timestep = 6217. State = [[ 0.08140874 -0.14637414]]. Action = [[0.24113315 0.06849155 0.119672   0.05221784]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 6217 is [False, False, True, True, False, False]
Current timestep = 6218. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1333479  -0.04200001 -0.15912452  0.15923119]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 6218 is [False, False, True, True, False, False]
Current timestep = 6219. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17756957 -0.04721938 -0.00647952  0.02337778]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 6219 is [False, False, True, True, False, False]
Current timestep = 6220. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17806312 -0.05045736 -0.06711811  0.139184  ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 6220 is [False, False, True, True, False, False]
Current timestep = 6221. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.14631301 -0.11547717 -0.13427703 -0.17811638]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 6221 is [False, False, True, True, False, False]
Current timestep = 6222. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24611652 -0.00923583 -0.04995076 -0.36248946]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 6222 is [False, False, True, True, False, False]
Current timestep = 6223. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1961869  -0.13797612 -0.04789236  0.31164932]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 6223 is [False, False, True, True, False, False]
Current timestep = 6224. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17969954 -0.16099522 -0.16662788  0.02977324]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 6224 is [False, False, True, True, False, False]
Current timestep = 6225. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24692133 -0.05617492 -0.09341273 -0.25976175]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 6225 is [False, False, True, True, False, False]
Current timestep = 6226. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22414768 -0.07723099 -0.01460889  0.23266852]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 6226 is [False, False, True, True, False, False]
Current timestep = 6227. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19236517 -0.0384998  -0.0590291  -0.13456333]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 6227 is [False, False, True, True, False, False]
Current timestep = 6228. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24662149 -0.01917256 -0.09651227  0.12454474]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 6228 is [False, False, True, True, False, False]
Current timestep = 6229. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21401906 -0.06769958 -0.03428604 -0.02520579]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 6229 is [False, False, True, True, False, False]
Current timestep = 6230. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23278293 -0.05562884 -0.03480463 -0.33649695]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 6230 is [False, False, True, True, False, False]
Scene graph at timestep 6230 is [False, False, True, True, False, False]
State prediction error at timestep 6230 is tensor(5.5119e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6231. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20488447 -0.01705173 -0.12794656 -0.09278458]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 6231 is [False, False, True, True, False, False]
Scene graph at timestep 6231 is [False, False, True, True, False, False]
State prediction error at timestep 6231 is tensor(6.0468e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6232. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24408266 -0.1291748  -0.00396016 -0.48051453]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 6232 is [False, False, True, True, False, False]
Current timestep = 6233. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18693191  0.01665452 -0.05517587 -0.376536  ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 6233 is [False, False, True, True, False, False]
Current timestep = 6234. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23642614 -0.11804523 -0.06823866 -0.18964046]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 6234 is [False, False, True, True, False, False]
Current timestep = 6235. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2042394  -0.11129171 -0.16726235 -0.14484984]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 6235 is [False, False, True, True, False, False]
Scene graph at timestep 6235 is [False, False, True, True, False, False]
State prediction error at timestep 6235 is tensor(1.3778e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6236. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23866016 -0.07276303  0.015596   -0.37135875]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 6236 is [False, False, True, True, False, False]
Current timestep = 6237. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2301406  -0.04595076 -0.13555217 -0.37157035]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 6237 is [False, False, True, True, False, False]
Current timestep = 6238. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.12346998 -0.0555705  -0.15710992 -0.42991412]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 6238 is [False, False, True, True, False, False]
Scene graph at timestep 6238 is [False, False, True, True, False, False]
State prediction error at timestep 6238 is tensor(1.1070e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6239. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24548638 -0.09460977  0.02488366 -0.39548463]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 6239 is [False, False, True, True, False, False]
Current timestep = 6240. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24371108 -0.09955475 -0.16795611 -0.40354443]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 6240 is [False, False, True, True, False, False]
Current timestep = 6241. State = [[ 0.08140874 -0.14637414]]. Action = [[ 1.9177651e-01 -1.1981748e-01  8.4638596e-06 -5.4708540e-01]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 6241 is [False, False, True, True, False, False]
Current timestep = 6242. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22089797 -0.11825922 -0.04667923 -0.18701828]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 6242 is [False, False, True, True, False, False]
Current timestep = 6243. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19871894 -0.00969312 -0.13129243 -0.02623075]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 6243 is [False, False, True, True, False, False]
Scene graph at timestep 6243 is [False, False, True, True, False, False]
State prediction error at timestep 6243 is tensor(1.0093e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6244. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19721344 -0.0799588  -0.06791794 -0.20531094]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 6244 is [False, False, True, True, False, False]
Current timestep = 6245. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20775506 -0.05872263 -0.07364948  0.20949376]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 6245 is [False, False, True, True, False, False]
Current timestep = 6246. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20879376 -0.0303115  -0.04953797 -0.23391318]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 6246 is [False, False, True, True, False, False]
Current timestep = 6247. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17474234 -0.06178451 -0.13457513 -0.13569105]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 6247 is [False, False, True, True, False, False]
Scene graph at timestep 6247 is [False, False, True, True, False, False]
State prediction error at timestep 6247 is tensor(7.2775e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6248. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.11381286 -0.07260299 -0.10032278  0.29422474]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 6248 is [False, False, True, True, False, False]
Current timestep = 6249. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.07510185 -0.08200043 -0.13029915 -0.19740152]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 6249 is [False, False, True, True, False, False]
Current timestep = 6250. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23656267 -0.05603227 -0.10383105 -0.37792134]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 6250 is [False, False, True, True, False, False]
Scene graph at timestep 6250 is [False, False, True, True, False, False]
State prediction error at timestep 6250 is tensor(3.0164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6251. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24460304 -0.05526918 -0.04716374 -0.28383613]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 6251 is [False, False, True, True, False, False]
Current timestep = 6252. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2380966  -0.09332019 -0.05214074 -0.24839127]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 6252 is [False, False, True, True, False, False]
Current timestep = 6253. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17102125 -0.15594527 -0.08314006 -0.14815694]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 6253 is [False, False, True, True, False, False]
Current timestep = 6254. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2076002  -0.1579334  -0.1360631   0.16499901]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 6254 is [False, False, True, True, False, False]
Current timestep = 6255. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.08389977  0.01674756 -0.10446784  0.16573095]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 6255 is [False, False, True, True, False, False]
Current timestep = 6256. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21761769 -0.09795879 -0.20604038  0.11159813]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 6256 is [False, False, True, True, False, False]
Current timestep = 6257. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2340397  -0.09604859 -0.07179397 -0.16607201]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 6257 is [False, False, True, True, False, False]
Current timestep = 6258. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1941441  -0.04842897 -0.07404912  0.25516987]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 6258 is [False, False, True, True, False, False]
Current timestep = 6259. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23739237 -0.04658739 -0.12129967 -0.20326185]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 6259 is [False, False, True, True, False, False]
Current timestep = 6260. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23792863 -0.0925111  -0.14732593 -0.24832976]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 6260 is [False, False, True, True, False, False]
Current timestep = 6261. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20935416 -0.07238498 -0.16828255  0.1681037 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 6261 is [False, False, True, True, False, False]
Current timestep = 6262. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24080318 -0.02402906 -0.09895805 -0.03156358]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 6262 is [False, False, True, True, False, False]
Current timestep = 6263. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23077488 -0.07767227 -0.13045529 -0.02610826]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 6263 is [False, False, True, True, False, False]
Current timestep = 6264. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23216897 -0.01738216 -0.02752928 -0.21052599]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 6264 is [False, False, True, True, False, False]
Scene graph at timestep 6264 is [False, False, True, True, False, False]
State prediction error at timestep 6264 is tensor(2.0926e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6265. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.183927   -0.0128013  -0.12221701 -0.43146586]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 6265 is [False, False, True, True, False, False]
Current timestep = 6266. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23915187 -0.07822926 -0.08167341  0.03630972]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 6266 is [False, False, True, True, False, False]
Current timestep = 6267. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.14333898 -0.07781017 -0.02518217 -0.2575971 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 6267 is [False, False, True, True, False, False]
Current timestep = 6268. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.07347375 -0.18890542 -0.16556558 -0.20898968]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 6268 is [False, False, True, True, False, False]
Current timestep = 6269. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23158121 -0.06510794 -0.04012024 -0.09941328]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 6269 is [False, False, True, True, False, False]
Current timestep = 6270. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23504305 -0.02108207 -0.02755347 -0.23904282]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 6270 is [False, False, True, True, False, False]
Current timestep = 6271. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20499942 -0.0005032  -0.10140565 -0.33458865]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 6271 is [False, False, True, True, False, False]
Current timestep = 6272. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23956841 -0.08104174 -0.08038057 -0.16644269]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 6272 is [False, False, True, True, False, False]
Current timestep = 6273. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2357035  -0.00123733  0.09177977 -0.09946281]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 6273 is [False, False, True, True, False, False]
Current timestep = 6274. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24604642 -0.13885237 -0.19274837 -0.15542948]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 6274 is [False, False, True, True, False, False]
Current timestep = 6275. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22284812 -0.12075874 -0.06764933  0.20228839]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 6275 is [False, False, True, True, False, False]
Current timestep = 6276. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24104261 -0.07383351 -0.13083878  0.11620712]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 6276 is [False, False, True, True, False, False]
Current timestep = 6277. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20710927 -0.00538503 -0.05320771 -0.2341075 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 6277 is [False, False, True, True, False, False]
Current timestep = 6278. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22940451 -0.04088569 -0.08470058  0.1217674 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 6278 is [False, False, True, True, False, False]
Current timestep = 6279. State = [[ 0.08140874 -0.14637414]]. Action = [[ 2.4351552e-01 -1.7136335e-04 -1.3648812e-01 -2.5718367e-01]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 6279 is [False, False, True, True, False, False]
Scene graph at timestep 6279 is [False, False, True, True, False, False]
State prediction error at timestep 6279 is tensor(5.0547e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6280. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18973136 -0.03368971 -0.07810342 -0.23016763]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 6280 is [False, False, True, True, False, False]
Current timestep = 6281. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24592152 -0.01204309 -0.10694101 -0.3971784 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 6281 is [False, False, True, True, False, False]
Current timestep = 6282. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2342282  -0.12955116 -0.14092276  0.3345729 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 6282 is [False, False, True, True, False, False]
Scene graph at timestep 6282 is [False, False, True, True, False, False]
State prediction error at timestep 6282 is tensor(2.8040e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6283. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23374936 -0.0885902  -0.03617854  0.07202601]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 6283 is [False, False, True, True, False, False]
Current timestep = 6284. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24315089 -0.09622282  0.04484338 -0.12971342]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 6284 is [False, False, True, True, False, False]
Scene graph at timestep 6284 is [False, False, True, True, False, False]
State prediction error at timestep 6284 is tensor(1.8214e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6285. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24066398 -0.1017348  -0.05832298 -0.3160066 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 6285 is [False, False, True, True, False, False]
Current timestep = 6286. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21961528 -0.10460815 -0.13799568  0.13660693]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 6286 is [False, False, True, True, False, False]
Scene graph at timestep 6286 is [False, False, True, True, False, False]
State prediction error at timestep 6286 is tensor(2.5446e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6287. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18791407 -0.07182616  0.02376807  0.3124156 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 6287 is [False, False, True, True, False, False]
Current timestep = 6288. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2189033  -0.12333727 -0.12065229 -0.11647093]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 6288 is [False, False, True, True, False, False]
Scene graph at timestep 6288 is [False, False, True, True, False, False]
State prediction error at timestep 6288 is tensor(5.7716e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6289. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2385633  -0.11254716 -0.03279915 -0.259382  ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 6289 is [False, False, True, True, False, False]
Current timestep = 6290. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22176042 -0.06625794 -0.07623482 -0.31369972]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 6290 is [False, False, True, True, False, False]
Current timestep = 6291. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22489804 -0.07020369 -0.13485697 -0.19165546]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 6291 is [False, False, True, True, False, False]
Scene graph at timestep 6291 is [False, False, True, True, False, False]
State prediction error at timestep 6291 is tensor(7.5168e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6292. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18676537 -0.15039456 -0.05060114 -0.36383206]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 6292 is [False, False, True, True, False, False]
Current timestep = 6293. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21108067 -0.01484115 -0.0402019   0.11128891]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 6293 is [False, False, True, True, False, False]
Current timestep = 6294. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21752241 -0.04140353  0.03447458  0.4349606 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 6294 is [False, False, True, True, False, False]
Current timestep = 6295. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23468417 -0.01220912  0.00098181  0.01833498]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 6295 is [False, False, True, True, False, False]
Current timestep = 6296. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23968473 -0.08513346  0.02590707 -0.14142066]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 6296 is [False, False, True, True, False, False]
Current timestep = 6297. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20210665  0.02331817 -0.10513026  0.13264048]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 6297 is [False, False, True, True, False, False]
Current timestep = 6298. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.07062572 -0.05807799 -0.03849351 -0.1402533 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 6298 is [False, False, True, True, False, False]
Scene graph at timestep 6298 is [False, False, True, True, False, False]
State prediction error at timestep 6298 is tensor(4.9775e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6299. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18507332 -0.14076823 -0.05704188 -0.13367361]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 6299 is [False, False, True, True, False, False]
Current timestep = 6300. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18235582 -0.030596   -0.09554861 -0.16383505]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 6300 is [False, False, True, True, False, False]
Current timestep = 6301. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23467842 -0.12538251 -0.0953944  -0.15657675]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 6301 is [False, False, True, True, False, False]
Current timestep = 6302. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21096063 -0.02432042 -0.053482   -0.12042904]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 6302 is [False, False, True, True, False, False]
Current timestep = 6303. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21456271 -0.11626896 -0.093282   -0.30086124]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 6303 is [False, False, True, True, False, False]
Scene graph at timestep 6303 is [False, False, True, True, False, False]
State prediction error at timestep 6303 is tensor(9.1202e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6304. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24453104 -0.13215292 -0.14164744 -0.01898551]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 6304 is [False, False, True, True, False, False]
Scene graph at timestep 6304 is [False, False, True, True, False, False]
State prediction error at timestep 6304 is tensor(5.6312e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6305. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23861095 -0.07477504  0.05332273 -0.21810728]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 6305 is [False, False, True, True, False, False]
Current timestep = 6306. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1855309  -0.0522622  -0.10986394 -0.21311533]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 6306 is [False, False, True, True, False, False]
Current timestep = 6307. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1898933  -0.10886952  0.10814783  0.07848871]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 6307 is [False, False, True, True, False, False]
Current timestep = 6308. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23327982 -0.07952574 -0.12255429 -0.35728693]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 6308 is [False, False, True, True, False, False]
Scene graph at timestep 6308 is [False, False, True, True, False, False]
State prediction error at timestep 6308 is tensor(4.7219e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6309. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.091611   -0.07359456 -0.17026068  0.39119697]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 6309 is [False, False, True, True, False, False]
Current timestep = 6310. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24677461 -0.16832472 -0.039417   -0.03996444]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 6310 is [False, False, True, True, False, False]
Current timestep = 6311. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23504084 -0.09798759 -0.08550747  0.06298053]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 6311 is [False, False, True, True, False, False]
Current timestep = 6312. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2042064   0.04852673 -0.04307579 -0.03989202]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 6312 is [False, False, True, True, False, False]
Current timestep = 6313. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24332368 -0.05689169 -0.09063536 -0.03791237]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 6313 is [False, False, True, True, False, False]
Current timestep = 6314. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2238521   0.02811098 -0.08169025 -0.16953719]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 6314 is [False, False, True, True, False, False]
Current timestep = 6315. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23948294 -0.0213744  -0.06580228  0.01225078]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 6315 is [False, False, True, True, False, False]
Current timestep = 6316. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23991424  0.00133306 -0.13241798  0.01477242]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 6316 is [False, False, True, True, False, False]
Current timestep = 6317. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23602957 -0.05559886 -0.02847125 -0.25602305]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 6317 is [False, False, True, True, False, False]
Current timestep = 6318. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24471939 -0.03132221 -0.07238671 -0.5619434 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 6318 is [False, False, True, True, False, False]
Current timestep = 6319. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24388844 -0.05991709  0.00903833 -0.23359019]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 6319 is [False, False, True, True, False, False]
Current timestep = 6320. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18409818 -0.05985446 -0.03388931 -0.15930128]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 6320 is [False, False, True, True, False, False]
Current timestep = 6321. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22448403  0.05583054 -0.12850836 -0.0796926 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 6321 is [False, False, True, True, False, False]
Scene graph at timestep 6321 is [False, False, True, True, False, False]
State prediction error at timestep 6321 is tensor(2.3653e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6322. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24269694 -0.07316101 -0.15223673  0.2779808 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 6322 is [False, False, True, True, False, False]
Scene graph at timestep 6322 is [False, False, True, True, False, False]
State prediction error at timestep 6322 is tensor(1.7680e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6323. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23849547 -0.08215377 -0.1284227  -0.167333  ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 6323 is [False, False, True, True, False, False]
Current timestep = 6324. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21331108 -0.08050333 -0.04128462 -0.01799703]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 6324 is [False, False, True, True, False, False]
Current timestep = 6325. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.14294767 -0.07146731 -0.1562972   0.03039157]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 6325 is [False, False, True, True, False, False]
Current timestep = 6326. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22710776 -0.03227024 -0.01980685 -0.20796943]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 6326 is [False, False, True, True, False, False]
Current timestep = 6327. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15160063 -0.0471905  -0.06264398 -0.10388237]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 6327 is [False, False, True, True, False, False]
Current timestep = 6328. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22879165 -0.06864686 -0.11408038  0.08834195]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 6328 is [False, False, True, True, False, False]
Scene graph at timestep 6328 is [False, False, True, True, False, False]
State prediction error at timestep 6328 is tensor(3.0019e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6329. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23674321 -0.14642923 -0.00796279 -0.23632765]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 6329 is [False, False, True, True, False, False]
Scene graph at timestep 6329 is [False, False, True, True, False, False]
State prediction error at timestep 6329 is tensor(4.1441e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6330. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21644282 -0.07308264 -0.09257731 -0.21202385]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 6330 is [False, False, True, True, False, False]
Current timestep = 6331. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24614805 -0.05869591 -0.16810732 -0.12363899]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 6331 is [False, False, True, True, False, False]
Current timestep = 6332. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23722708 -0.0777007  -0.0818411  -0.26752204]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 6332 is [False, False, True, True, False, False]
Current timestep = 6333. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21738255 -0.09285685 -0.10637759 -0.21174574]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 6333 is [False, False, True, True, False, False]
Current timestep = 6334. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18949488 -0.07355705 -0.08466342 -0.35193837]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 6334 is [False, False, True, True, False, False]
Current timestep = 6335. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24637839 -0.05039622 -0.07519762 -0.06535375]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 6335 is [False, False, True, True, False, False]
Current timestep = 6336. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23670569 -0.05150843 -0.09876215 -0.20265222]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 6336 is [False, False, True, True, False, False]
Scene graph at timestep 6336 is [False, False, True, True, False, False]
State prediction error at timestep 6336 is tensor(3.6625e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6337. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.13108996 -0.01717095 -0.09178047 -0.5316742 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 6337 is [False, False, True, True, False, False]
Scene graph at timestep 6337 is [False, False, True, True, False, False]
State prediction error at timestep 6337 is tensor(3.4065e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6338. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23929954 -0.09220257 -0.1506896  -0.4326085 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 6338 is [False, False, True, True, False, False]
Scene graph at timestep 6338 is [False, False, True, True, False, False]
State prediction error at timestep 6338 is tensor(3.3583e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6339. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2354089  -0.01603396 -0.09245825 -0.35149312]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 6339 is [False, False, True, True, False, False]
Current timestep = 6340. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23747921 -0.05349249 -0.0920635  -0.28830707]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 6340 is [False, False, True, True, False, False]
Current timestep = 6341. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23903283 -0.09267631 -0.0143871  -0.22128999]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 6341 is [False, False, True, True, False, False]
Current timestep = 6342. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24341726 -0.07339494 -0.04875645 -0.26505554]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 6342 is [False, False, True, True, False, False]
Current timestep = 6343. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22223684 -0.02082558 -0.05254838 -0.07242966]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 6343 is [False, False, True, True, False, False]
Current timestep = 6344. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.16912585 -0.10345261 -0.05450925 -0.01031917]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 6344 is [False, False, True, True, False, False]
Scene graph at timestep 6344 is [False, False, True, True, False, False]
State prediction error at timestep 6344 is tensor(2.1462e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6345. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24273455 -0.07254766 -0.10464415 -0.09416348]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 6345 is [False, False, True, True, False, False]
Scene graph at timestep 6345 is [False, False, True, True, False, False]
State prediction error at timestep 6345 is tensor(3.7629e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6346. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22410887 -0.08294307 -0.11720055 -0.27123475]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 6346 is [False, False, True, True, False, False]
Scene graph at timestep 6346 is [False, False, True, True, False, False]
State prediction error at timestep 6346 is tensor(4.0665e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6347. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23964721 -0.05792236 -0.07609843 -0.28344595]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 6347 is [False, False, True, True, False, False]
Current timestep = 6348. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23839694 -0.09494229 -0.11456388 -0.31166804]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 6348 is [False, False, True, True, False, False]
Current timestep = 6349. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21449965 -0.04944524 -0.0913128  -0.12385046]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 6349 is [False, False, True, True, False, False]
Current timestep = 6350. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15404129 -0.0492865  -0.08215082 -0.43328637]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 6350 is [False, False, True, True, False, False]
Scene graph at timestep 6350 is [False, False, True, True, False, False]
State prediction error at timestep 6350 is tensor(4.5899e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6351. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23356372 -0.03917737 -0.06439421 -0.14396763]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 6351 is [False, False, True, True, False, False]
Current timestep = 6352. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2248449  -0.07413903 -0.09178925  0.10609114]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 6352 is [False, False, True, True, False, False]
Current timestep = 6353. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21461898 -0.04241845 -0.1082074  -0.09941268]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 6353 is [False, False, True, True, False, False]
Current timestep = 6354. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.13148522 -0.11795223 -0.16070484 -0.12192488]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 6354 is [False, False, True, True, False, False]
Scene graph at timestep 6354 is [False, False, True, True, False, False]
State prediction error at timestep 6354 is tensor(7.9528e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6355. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24497479 -0.07024148 -0.09049302 -0.304991  ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 6355 is [False, False, True, True, False, False]
Current timestep = 6356. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21760446 -0.096091   -0.07203168 -0.25222737]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 6356 is [False, False, True, True, False, False]
Scene graph at timestep 6356 is [False, False, True, True, False, False]
State prediction error at timestep 6356 is tensor(3.7729e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6357. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23162812 -0.11588944 -0.04530281 -0.264193  ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 6357 is [False, False, True, True, False, False]
Current timestep = 6358. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22236282 -0.08909383 -0.09167537 -0.04215467]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 6358 is [False, False, True, True, False, False]
Current timestep = 6359. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23886865 -0.12303507 -0.12752531 -0.37693095]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 6359 is [False, False, True, True, False, False]
Current timestep = 6360. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24500793 -0.04485767 -0.11590248 -0.02118695]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 6360 is [False, False, True, True, False, False]
Current timestep = 6361. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24456635 -0.11636396 -0.09878667 -0.05085468]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 6361 is [False, False, True, True, False, False]
Current timestep = 6362. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23410341 -0.0922866  -0.04158184 -0.2894265 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 6362 is [False, False, True, True, False, False]
Current timestep = 6363. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21831864 -0.10300605 -0.06253926  0.04512084]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 6363 is [False, False, True, True, False, False]
Scene graph at timestep 6363 is [False, False, True, True, False, False]
State prediction error at timestep 6363 is tensor(1.0964e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6364. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22025776 -0.14116798 -0.07493167 -0.3587963 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 6364 is [False, False, True, True, False, False]
Current timestep = 6365. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22481519 -0.04725367 -0.03252405 -0.15395117]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 6365 is [False, False, True, True, False, False]
Current timestep = 6366. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.14125657 -0.06258489 -0.09311482 -0.3022583 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 6366 is [False, False, True, True, False, False]
Scene graph at timestep 6366 is [False, False, True, True, False, False]
State prediction error at timestep 6366 is tensor(6.0641e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6367. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24371374 -0.05232686 -0.1438068   0.3745885 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 6367 is [False, False, True, True, False, False]
Scene graph at timestep 6367 is [False, False, True, True, False, False]
State prediction error at timestep 6367 is tensor(3.3133e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6368. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23876983 -0.06640065  0.05034965 -0.29981273]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 6368 is [False, False, True, True, False, False]
Current timestep = 6369. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21509185 -0.00390963 -0.11947751 -0.19413233]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 6369 is [False, False, True, True, False, False]
Current timestep = 6370. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1306009  -0.08404332 -0.07519078 -0.13340014]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 6370 is [False, False, True, True, False, False]
Current timestep = 6371. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23925525 -0.12197766 -0.08114944 -0.1304692 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 6371 is [False, False, True, True, False, False]
Current timestep = 6372. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22104001 -0.07077992 -0.17729808 -0.07812917]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 6372 is [False, False, True, True, False, False]
Current timestep = 6373. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24593979  0.01293081 -0.05635929 -0.23047441]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 6373 is [False, False, True, True, False, False]
Current timestep = 6374. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18076122 -0.06471479  0.05046248 -0.18560147]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 6374 is [False, False, True, True, False, False]
Current timestep = 6375. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23707747 -0.12375399 -0.14148265 -0.08339185]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 6375 is [False, False, True, True, False, False]
Current timestep = 6376. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23969403 -0.08852398 -0.09633853 -0.08013415]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 6376 is [False, False, True, True, False, False]
Scene graph at timestep 6376 is [False, False, True, True, False, False]
State prediction error at timestep 6376 is tensor(2.7737e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6377. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2089608  -0.05690446 -0.01763102  0.05258214]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 6377 is [False, False, True, True, False, False]
Scene graph at timestep 6377 is [False, False, True, True, False, False]
State prediction error at timestep 6377 is tensor(6.9492e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6378. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22909412 -0.09838516  0.0271349  -0.16038483]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 6378 is [False, False, True, True, False, False]
Current timestep = 6379. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22574085 -0.13156818 -0.03737937 -0.18304282]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 6379 is [False, False, True, True, False, False]
Current timestep = 6380. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21293592 -0.06921333 -0.01439448 -0.09261483]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 6380 is [False, False, True, True, False, False]
Current timestep = 6381. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21276116 -0.10068634 -0.02040493 -0.1635285 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 6381 is [False, False, True, True, False, False]
Current timestep = 6382. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23130402 -0.09860697 -0.05396369 -0.16603595]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 6382 is [False, False, True, True, False, False]
Current timestep = 6383. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23381957 -0.0391307  -0.10686114 -0.11018777]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 6383 is [False, False, True, True, False, False]
Scene graph at timestep 6383 is [False, False, True, True, False, False]
State prediction error at timestep 6383 is tensor(1.4780e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6384. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22461969 -0.07857816 -0.03872168 -0.27434427]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 6384 is [False, False, True, True, False, False]
Current timestep = 6385. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23185384 -0.09012254 -0.09727599 -0.07769006]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 6385 is [False, False, True, True, False, False]
Scene graph at timestep 6385 is [False, False, True, True, False, False]
State prediction error at timestep 6385 is tensor(1.0124e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6386. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18776065 -0.12656924 -0.13906819 -0.07483232]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 6386 is [False, False, True, True, False, False]
Scene graph at timestep 6386 is [False, False, True, True, False, False]
State prediction error at timestep 6386 is tensor(7.7255e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6387. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17061162 -0.07647943 -0.07196657 -0.3186062 ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 6387 is [False, False, True, True, False, False]
Current timestep = 6388. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15381667 -0.0780206  -0.08562115 -0.04634291]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 6388 is [False, False, True, True, False, False]
Current timestep = 6389. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.16334909 -0.06910554 -0.00451465 -0.08432508]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 6389 is [False, False, True, True, False, False]
Current timestep = 6390. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21449995 -0.05993617 -0.1364059   0.38024962]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 6390 is [False, False, True, True, False, False]
Current timestep = 6391. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1964941  -0.06792122 -0.09787656 -0.17487788]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 6391 is [False, False, True, True, False, False]
Current timestep = 6392. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21972868 -0.09140053 -0.14548247 -0.03268212]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 6392 is [False, False, True, True, False, False]
Current timestep = 6393. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22258276 -0.04643646 -0.05713969  0.16726923]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 6393 is [False, False, True, True, False, False]
Current timestep = 6394. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24289364 -0.02312948 -0.10551184 -0.14159226]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 6394 is [False, False, True, True, False, False]
Current timestep = 6395. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23994559 -0.0447156  -0.09200424 -0.35276592]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 6395 is [False, False, True, True, False, False]
Current timestep = 6396. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.09184644 -0.10188544 -0.10134393 -0.2180394 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 6396 is [False, False, True, True, False, False]
Current timestep = 6397. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.16035163 -0.07846677 -0.07557422 -0.12013531]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 6397 is [False, False, True, True, False, False]
Current timestep = 6398. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17731392  0.02406392 -0.06308734 -0.2917142 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 6398 is [False, False, True, True, False, False]
Current timestep = 6399. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21125633 -0.06966436 -0.00239831 -0.15963757]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 6399 is [False, False, True, True, False, False]
Current timestep = 6400. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24528092 -0.08167958 -0.09279799 -0.21315563]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 6400 is [False, False, True, True, False, False]
Current timestep = 6401. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.14057744 -0.08322087 -0.10323094 -0.44582576]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 6401 is [False, False, True, True, False, False]
Current timestep = 6402. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17735118 -0.08530465 -0.00026999  0.14594293]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 6402 is [False, False, True, True, False, False]
Current timestep = 6403. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2002916  -0.03094956 -0.11884329 -0.13660336]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 6403 is [False, False, True, True, False, False]
Scene graph at timestep 6403 is [False, False, True, True, False, False]
State prediction error at timestep 6403 is tensor(1.9628e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6404. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24525312 -0.12195878 -0.09220403 -0.24931031]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 6404 is [False, False, True, True, False, False]
Current timestep = 6405. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22811902 -0.13208118  0.04167753 -0.23827857]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 6405 is [False, False, True, True, False, False]
Current timestep = 6406. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22473335 -0.09164056 -0.13881394  0.00676119]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 6406 is [False, False, True, True, False, False]
Current timestep = 6407. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24212879 -0.07130001 -0.10497019 -0.15137827]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 6407 is [False, False, True, True, False, False]
Current timestep = 6408. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22975394 -0.12041502 -0.11719629 -0.12539148]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 6408 is [False, False, True, True, False, False]
Current timestep = 6409. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2418443  -0.06934398 -0.15668407 -0.16820502]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 6409 is [False, False, True, True, False, False]
Current timestep = 6410. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24051535 -0.09092093 -0.08275761 -0.20842952]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 6410 is [False, False, True, True, False, False]
Current timestep = 6411. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18616328 -0.11371452 -0.10394621 -0.15326971]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 6411 is [False, False, True, True, False, False]
Scene graph at timestep 6411 is [False, False, True, True, False, False]
State prediction error at timestep 6411 is tensor(3.4829e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6412. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23726225 -0.07233952 -0.12073493 -0.2750336 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 6412 is [False, False, True, True, False, False]
Current timestep = 6413. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24229413 -0.09256737 -0.01796165 -0.2305637 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 6413 is [False, False, True, True, False, False]
Current timestep = 6414. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24386752 -0.11692318 -0.16981895 -0.2969494 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 6414 is [False, False, True, True, False, False]
Current timestep = 6415. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23761809 -0.07446453 -0.02631347 -0.05895394]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 6415 is [False, False, True, True, False, False]
Current timestep = 6416. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22665298 -0.06391728 -0.07733248 -0.35026664]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 6416 is [False, False, True, True, False, False]
Current timestep = 6417. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20408866 -0.11942092 -0.10470852  0.17713046]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 6417 is [False, False, True, True, False, False]
Scene graph at timestep 6417 is [False, False, True, True, False, False]
State prediction error at timestep 6417 is tensor(2.0287e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6418. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2287615  -0.11426356 -0.14171089  0.13460064]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 6418 is [False, False, True, True, False, False]
Current timestep = 6419. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24396169 -0.03937757 -0.12588245 -0.0605365 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 6419 is [False, False, True, True, False, False]
Current timestep = 6420. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22683564 -0.09832937 -0.08383578 -0.07065165]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 6420 is [False, False, True, True, False, False]
Current timestep = 6421. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.13683459 -0.01985517 -0.09397313 -0.05647308]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 6421 is [False, False, True, True, False, False]
Current timestep = 6422. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20699546 -0.09821126 -0.10733297  0.06715572]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 6422 is [False, False, True, True, False, False]
Current timestep = 6423. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23423415 -0.12212496 -0.07884803 -0.3147031 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 6423 is [False, False, True, True, False, False]
Current timestep = 6424. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23861319 -0.07342072 -0.08488344 -0.0233441 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 6424 is [False, False, True, True, False, False]
Current timestep = 6425. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2114313  -0.13528416 -0.08015504 -0.48165667]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 6425 is [False, False, True, True, False, False]
Scene graph at timestep 6425 is [False, False, True, True, False, False]
State prediction error at timestep 6425 is tensor(5.4816e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6426. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23812604 -0.02622865 -0.05213669 -0.04614711]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 6426 is [False, False, True, True, False, False]
Current timestep = 6427. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19818652 -0.10410193 -0.04195644 -0.18430936]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 6427 is [False, False, True, True, False, False]
Scene graph at timestep 6427 is [False, False, True, True, False, False]
State prediction error at timestep 6427 is tensor(4.3966e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6428. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24341682 -0.07541287 -0.06249014  0.01397038]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 6428 is [False, False, True, True, False, False]
Scene graph at timestep 6428 is [False, False, True, True, False, False]
State prediction error at timestep 6428 is tensor(6.0144e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6429. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23263413 -0.05723928 -0.08231732 -0.02795899]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 6429 is [False, False, True, True, False, False]
Current timestep = 6430. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23708257 -0.09031643 -0.10241683 -0.23170322]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 6430 is [False, False, True, True, False, False]
Current timestep = 6431. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24310875 -0.09424582 -0.10235891 -0.42046285]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 6431 is [False, False, True, True, False, False]
Current timestep = 6432. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20884514 -0.03009586 -0.1461407  -0.2563051 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 6432 is [False, False, True, True, False, False]
Current timestep = 6433. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23020628 -0.08259362 -0.03187592 -0.06959057]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 6433 is [False, False, True, True, False, False]
Current timestep = 6434. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22323656 -0.12327808 -0.0878844  -0.344432  ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 6434 is [False, False, True, True, False, False]
Current timestep = 6435. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21232349 -0.15592623 -0.1453487  -0.21969628]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 6435 is [False, False, True, True, False, False]
Scene graph at timestep 6435 is [False, False, True, True, False, False]
State prediction error at timestep 6435 is tensor(4.8674e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6436. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21428764 -0.113704   -0.11861195 -0.09856212]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 6436 is [False, False, True, True, False, False]
Current timestep = 6437. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24292487 -0.1640038  -0.08048633 -0.01618445]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 6437 is [False, False, True, True, False, False]
Current timestep = 6438. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18889982 -0.13435668 -0.11153135 -0.01770139]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 6438 is [False, False, True, True, False, False]
Current timestep = 6439. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24442223 -0.14827262 -0.08262333 -0.09284884]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 6439 is [False, False, True, True, False, False]
Current timestep = 6440. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22832388 -0.06810537 -0.15027316 -0.25987303]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 6440 is [False, False, True, True, False, False]
Current timestep = 6441. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19789532 -0.01503961 -0.14501037 -0.02495527]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 6441 is [False, False, True, True, False, False]
Current timestep = 6442. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19744182 -0.05949813 -0.13045888 -0.14416748]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 6442 is [False, False, True, True, False, False]
Current timestep = 6443. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2387071  -0.07021725 -0.06880404 -0.25805163]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 6443 is [False, False, True, True, False, False]
Current timestep = 6444. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15778214 -0.0653735  -0.076772   -0.04757196]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 6444 is [False, False, True, True, False, False]
Current timestep = 6445. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24404871 -0.03191829 -0.08592626  0.04451466]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 6445 is [False, False, True, True, False, False]
Current timestep = 6446. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2448067  -0.04069813 -0.02822469 -0.15863407]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 6446 is [False, False, True, True, False, False]
Current timestep = 6447. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23966742 -0.09903994 -0.08188456 -0.05483025]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 6447 is [False, False, True, True, False, False]
Current timestep = 6448. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2392121  -0.08117081 -0.07837564 -0.09704149]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 6448 is [False, False, True, True, False, False]
Current timestep = 6449. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19587606 -0.03873616 -0.11863278 -0.12205923]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 6449 is [False, False, True, True, False, False]
Current timestep = 6450. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17677665 -0.07419053 -0.07133484 -0.21705002]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 6450 is [False, False, True, True, False, False]
Current timestep = 6451. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22607523 -0.08948264 -0.02776426 -0.12468743]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 6451 is [False, False, True, True, False, False]
Current timestep = 6452. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24354717 -0.0771282  -0.08041319 -0.230923  ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 6452 is [False, False, True, True, False, False]
Current timestep = 6453. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21750402 -0.13230887 -0.13288711  0.11664748]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 6453 is [False, False, True, True, False, False]
Scene graph at timestep 6453 is [False, False, True, True, False, False]
State prediction error at timestep 6453 is tensor(1.9007e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6454. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24160081 -0.07442206 -0.09377049 -0.13613206]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 6454 is [False, False, True, True, False, False]
Scene graph at timestep 6454 is [False, False, True, True, False, False]
State prediction error at timestep 6454 is tensor(1.3135e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6455. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.07793629 -0.09158149 -0.15671162  0.09205639]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 6455 is [False, False, True, True, False, False]
Current timestep = 6456. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23725083 -0.10413435 -0.08247894  0.03067279]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 6456 is [False, False, True, True, False, False]
Current timestep = 6457. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24573779 -0.1015102  -0.06221355  0.07852948]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 6457 is [False, False, True, True, False, False]
Current timestep = 6458. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23682961 -0.07679017 -0.14525683 -0.09397519]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 6458 is [False, False, True, True, False, False]
Scene graph at timestep 6458 is [False, False, True, True, False, False]
State prediction error at timestep 6458 is tensor(6.3205e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6459. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24260074 -0.12583733 -0.04679587 -0.39087713]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 6459 is [False, False, True, True, False, False]
Current timestep = 6460. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15208113 -0.07378265 -0.00171115 -0.29971564]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 6460 is [False, False, True, True, False, False]
Current timestep = 6461. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.11936373  0.01045704 -0.08002685 -0.13873231]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 6461 is [False, False, True, True, False, False]
Current timestep = 6462. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24239796 -0.11783995 -0.14088476 -0.30909383]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 6462 is [False, False, True, True, False, False]
Scene graph at timestep 6462 is [False, False, True, True, False, False]
State prediction error at timestep 6462 is tensor(1.5463e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6463. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23234701 -0.15363124 -0.04777332 -0.1963172 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 6463 is [False, False, True, True, False, False]
Scene graph at timestep 6463 is [False, False, True, True, False, False]
State prediction error at timestep 6463 is tensor(4.9507e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6464. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.13243932 -0.02742165 -0.13929752  0.09643626]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 6464 is [False, False, True, True, False, False]
Current timestep = 6465. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18918669 -0.07535118 -0.1214698  -0.516057  ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 6465 is [False, False, True, True, False, False]
Current timestep = 6466. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23197916 -0.10191211 -0.08002777 -0.38142073]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 6466 is [False, False, True, True, False, False]
Current timestep = 6467. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18094116 -0.1110442  -0.09111044  0.02320838]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 6467 is [False, False, True, True, False, False]
Current timestep = 6468. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22680843 -0.05191582 -0.15764594 -0.1405071 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 6468 is [False, False, True, True, False, False]
Current timestep = 6469. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22455797 -0.09610583 -0.18421824 -0.12982422]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 6469 is [False, False, True, True, False, False]
Current timestep = 6470. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20538229 -0.08362229 -0.18198884 -0.1219117 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 6470 is [False, False, True, True, False, False]
Current timestep = 6471. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2281276  -0.00716102 -0.00751996 -0.3076979 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 6471 is [False, False, True, True, False, False]
Current timestep = 6472. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23540342 -0.04919462 -0.15165827 -0.12727928]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 6472 is [False, False, True, True, False, False]
Current timestep = 6473. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21692705 -0.06031598 -0.0739435  -0.16394365]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 6473 is [False, False, True, True, False, False]
Current timestep = 6474. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22172523 -0.10646842 -0.12316102 -0.08970481]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 6474 is [False, False, True, True, False, False]
Current timestep = 6475. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23747179 -0.04231685 -0.09772801 -0.2758677 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 6475 is [False, False, True, True, False, False]
Current timestep = 6476. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23167768  0.00140685 -0.1227054   0.17511344]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 6476 is [False, False, True, True, False, False]
Current timestep = 6477. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22913659  0.00772703 -0.15510091  0.19257414]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 6477 is [False, False, True, True, False, False]
Current timestep = 6478. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18683928 -0.06860074 -0.02978037  0.08344948]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 6478 is [False, False, True, True, False, False]
Scene graph at timestep 6478 is [False, False, True, True, False, False]
State prediction error at timestep 6478 is tensor(7.7498e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6479. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2407943  -0.06448567 -0.07511012  0.05144858]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 6479 is [False, False, True, True, False, False]
Current timestep = 6480. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18986022 -0.03552899 -0.05087978  0.02288628]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 6480 is [False, False, True, True, False, False]
Scene graph at timestep 6480 is [False, False, True, True, False, False]
State prediction error at timestep 6480 is tensor(2.2841e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6481. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22053683 -0.11177966 -0.06857565 -0.20662397]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 6481 is [False, False, True, True, False, False]
Current timestep = 6482. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22713402 -0.08495447 -0.09085803 -0.04251647]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 6482 is [False, False, True, True, False, False]
Current timestep = 6483. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22114968 -0.03539082 -0.06563132 -0.41784984]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 6483 is [False, False, True, True, False, False]
Current timestep = 6484. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2351942  -0.04566321 -0.0231979  -0.0113793 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 6484 is [False, False, True, True, False, False]
Scene graph at timestep 6484 is [False, False, True, True, False, False]
State prediction error at timestep 6484 is tensor(2.4984e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6485. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2303918   0.01511642 -0.10434461 -0.0835191 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 6485 is [False, False, True, True, False, False]
Current timestep = 6486. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22504416 -0.12185572 -0.09089586 -0.14771318]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 6486 is [False, False, True, True, False, False]
Current timestep = 6487. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22607264 -0.11025769 -0.13190457 -0.1733827 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 6487 is [False, False, True, True, False, False]
Current timestep = 6488. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.17631376 -0.06779605 -0.08930326 -0.14860338]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 6488 is [False, False, True, True, False, False]
Current timestep = 6489. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15801114 -0.08943728 -0.02823535 -0.09849489]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 6489 is [False, False, True, True, False, False]
Current timestep = 6490. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19084662 -0.09600213 -0.12851292 -0.04833072]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 6490 is [False, False, True, True, False, False]
Current timestep = 6491. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.08550251 -0.02447867 -0.05072933 -0.44905353]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 6491 is [False, False, True, True, False, False]
Scene graph at timestep 6491 is [False, False, True, True, False, False]
State prediction error at timestep 6491 is tensor(7.5293e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6492. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23673415 -0.10535169 -0.01638862 -0.02940029]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 6492 is [False, False, True, True, False, False]
Current timestep = 6493. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24124146 -0.08429804 -0.09095094 -0.3101338 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 6493 is [False, False, True, True, False, False]
Scene graph at timestep 6493 is [False, False, True, True, False, False]
State prediction error at timestep 6493 is tensor(3.5238e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6494. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2278713  -0.05030498 -0.0219937  -0.32809544]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 6494 is [False, False, True, True, False, False]
Current timestep = 6495. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2438448  -0.11703038 -0.05421431  0.23482656]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 6495 is [False, False, True, True, False, False]
Current timestep = 6496. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23870236 -0.12055109 -0.12050515  0.03224778]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 6496 is [False, False, True, True, False, False]
Current timestep = 6497. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18532813 -0.02512628 -0.11861044  0.199391  ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 6497 is [False, False, True, True, False, False]
Scene graph at timestep 6497 is [False, False, True, True, False, False]
State prediction error at timestep 6497 is tensor(9.3601e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6498. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23110294 -0.08286241 -0.03938924 -0.22152483]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 6498 is [False, False, True, True, False, False]
Current timestep = 6499. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18112552 -0.07788604 -0.09195954 -0.45397443]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 6499 is [False, False, True, True, False, False]
Current timestep = 6500. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21519375 -0.06622942 -0.06824589 -0.16377664]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 6500 is [False, False, True, True, False, False]
Scene graph at timestep 6500 is [False, False, True, True, False, False]
State prediction error at timestep 6500 is tensor(8.2943e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6501. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22980106 -0.09838177 -0.05853692 -0.1260246 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 6501 is [False, False, True, True, False, False]
Current timestep = 6502. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2352367  -0.02686927 -0.07584664 -0.36101627]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 6502 is [False, False, True, True, False, False]
Current timestep = 6503. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21458897 -0.06309837 -0.14482369 -0.27435315]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 6503 is [False, False, True, True, False, False]
Current timestep = 6504. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20770717 -0.05502737 -0.11793688  0.06513035]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 6504 is [False, False, True, True, False, False]
Current timestep = 6505. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.15944803 -0.07670861 -0.11691335 -0.18743122]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 6505 is [False, False, True, True, False, False]
Current timestep = 6506. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23649424 -0.059062   -0.07452197 -0.09532225]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 6506 is [False, False, True, True, False, False]
Scene graph at timestep 6506 is [False, False, True, True, False, False]
State prediction error at timestep 6506 is tensor(1.2084e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6507. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21760154 -0.02060242 -0.16925746 -0.19786435]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 6507 is [False, False, True, True, False, False]
Current timestep = 6508. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2263827  -0.11130263 -0.11540177 -0.18474728]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 6508 is [False, False, True, True, False, False]
Scene graph at timestep 6508 is [False, False, True, True, False, False]
State prediction error at timestep 6508 is tensor(1.4488e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6509. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23896047 -0.1036431  -0.17353027 -0.3484553 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 6509 is [False, False, True, True, False, False]
Current timestep = 6510. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24208277 -0.12817025 -0.08994359 -0.10552251]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 6510 is [False, False, True, True, False, False]
Current timestep = 6511. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22378159 -0.09214671 -0.14657757 -0.19017637]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 6511 is [False, False, True, True, False, False]
Current timestep = 6512. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20797157 -0.04553431 -0.13293533  0.06359363]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 6512 is [False, False, True, True, False, False]
Current timestep = 6513. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22425577 -0.10893597 -0.08942619 -0.09160519]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 6513 is [False, False, True, True, False, False]
Scene graph at timestep 6513 is [False, False, True, True, False, False]
State prediction error at timestep 6513 is tensor(1.0824e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6514. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18053663 -0.05688463 -0.09869215 -0.18957901]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 6514 is [False, False, True, True, False, False]
Current timestep = 6515. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23753011 -0.04923625 -0.14706305 -0.3459617 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 6515 is [False, False, True, True, False, False]
Current timestep = 6516. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24500918 -0.05917107 -0.04349491 -0.3560195 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 6516 is [False, False, True, True, False, False]
Current timestep = 6517. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19593441 -0.02806999 -0.07433648 -0.10159379]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 6517 is [False, False, True, True, False, False]
Current timestep = 6518. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20231056 -0.11606318 -0.09913349 -0.23378742]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 6518 is [False, False, True, True, False, False]
Current timestep = 6519. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.19332114 -0.04962367 -0.12713957 -0.01942551]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 6519 is [False, False, True, True, False, False]
Current timestep = 6520. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24073035 -0.09050499 -0.05969702 -0.2735579 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 6520 is [False, False, True, True, False, False]
Current timestep = 6521. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22369874 -0.10030568 -0.12286547 -0.08275956]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 6521 is [False, False, True, True, False, False]
Scene graph at timestep 6521 is [False, False, True, True, False, False]
State prediction error at timestep 6521 is tensor(2.3709e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6522. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22529846 -0.09152728 -0.04301316 -0.28504717]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 6522 is [False, False, True, True, False, False]
Current timestep = 6523. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21665496 -0.04483065 -0.12437192 -0.01720101]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 6523 is [False, False, True, True, False, False]
Current timestep = 6524. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24186677 -0.11334112 -0.09001341 -0.19243681]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 6524 is [False, False, True, True, False, False]
Current timestep = 6525. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23622096 -0.0311112  -0.09300712 -0.16062522]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 6525 is [False, False, True, True, False, False]
Current timestep = 6526. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21998352 -0.08687292 -0.1577279  -0.28329146]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 6526 is [False, False, True, True, False, False]
Current timestep = 6527. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20197374 -0.06866466 -0.12405986 -0.10463005]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 6527 is [False, False, True, True, False, False]
Current timestep = 6528. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23523343 -0.12823993 -0.10622427 -0.07969207]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 6528 is [False, False, True, True, False, False]
Current timestep = 6529. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24615264 -0.10842296 -0.03584972 -0.08583999]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 6529 is [False, False, True, True, False, False]
Scene graph at timestep 6529 is [False, False, True, True, False, False]
State prediction error at timestep 6529 is tensor(2.9849e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6530. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.21241373 -0.09300581 -0.11261512  0.03583074]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 6530 is [False, False, True, True, False, False]
Scene graph at timestep 6530 is [False, False, True, True, False, False]
State prediction error at timestep 6530 is tensor(7.8674e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6531. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23136318 -0.03494151 -0.12764804 -0.16039765]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 6531 is [False, False, True, True, False, False]
Current timestep = 6532. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.20055264 -0.11753315 -0.13567987 -0.13007134]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 6532 is [False, False, True, True, False, False]
Current timestep = 6533. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18036014 -0.08891407 -0.05780074 -0.16531277]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 6533 is [False, False, True, True, False, False]
Current timestep = 6534. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.07790947 -0.10111633 -0.13486576 -0.3237635 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 6534 is [False, False, True, True, False, False]
Current timestep = 6535. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.24466541 -0.08563313 -0.03449392 -0.0834136 ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 6535 is [False, False, True, True, False, False]
Current timestep = 6536. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.22560182 -0.08137038 -0.09189206 -0.26312685]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 6536 is [False, False, True, True, False, False]
Current timestep = 6537. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.1914733  -0.07973671 -0.11569136 -0.2838148 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 6537 is [False, False, True, True, False, False]
Current timestep = 6538. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.0147101   0.01150519 -0.07528168 -0.05311853]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 6538 is [False, False, True, True, False, False]
Scene graph at timestep 6538 is [False, False, True, True, False, False]
State prediction error at timestep 6538 is tensor(1.4209e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6539. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.2332418  -0.13565679 -0.13008955 -0.26556313]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 6539 is [False, False, True, True, False, False]
Scene graph at timestep 6539 is [False, False, True, True, False, False]
State prediction error at timestep 6539 is tensor(2.5680e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6540. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.18016201 -0.06030464 -0.15488882  0.24170351]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 6540 is [False, False, True, True, False, False]
Current timestep = 6541. State = [[ 0.08140874 -0.14637414]]. Action = [[ 0.23991093 -0.12984435  0.00760442  0.10312557]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 6541 is [False, False, True, True, False, False]
Scene graph at timestep 6541 is [False, False, True, True, False, False]
State prediction error at timestep 6541 is tensor(1.3907e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6542. State = [[-0.21222086 -0.06238867]]. Action = [[ 0.23527625 -0.0886406  -0.09785724  0.13343704]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 6542 is [False, False, True, True, False, False]
Current timestep = 6543. State = [[-0.2074366  -0.06735002]]. Action = [[ 0.23715249 -0.0613616  -0.13385253 -0.23610425]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 6543 is [True, False, False, False, True, False]
Current timestep = 6544. State = [[-0.20513593 -0.06761727]]. Action = [[ 0.18732116 -0.08888528 -0.14117694 -0.11052322]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 6544 is [True, False, False, False, True, False]
Current timestep = 6545. State = [[-0.20200866 -0.06898702]]. Action = [[ 0.24810207 -0.06523392 -0.1228067  -0.06527007]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 6545 is [True, False, False, False, True, False]
Current timestep = 6546. State = [[-0.19665857 -0.07091196]]. Action = [[ 0.24130073 -0.08521591 -0.134765   -0.1831851 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 6546 is [True, False, False, False, True, False]
Current timestep = 6547. State = [[-0.18908577 -0.07320268]]. Action = [[ 0.05192804 -0.0480639  -0.13694929 -0.34524655]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 6547 is [True, False, False, False, True, False]
Current timestep = 6548. State = [[-0.18265943 -0.07527789]]. Action = [[ 0.21705014 -0.03660384 -0.10365321 -0.12153989]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 6548 is [True, False, False, False, True, False]
Scene graph at timestep 6548 is [True, False, False, False, True, False]
State prediction error at timestep 6548 is tensor(9.7050e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6549. State = [[-0.1761485  -0.07747264]]. Action = [[ 0.22495252 -0.0652504  -0.1252129  -0.09168398]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 6549 is [True, False, False, False, True, False]
Current timestep = 6550. State = [[-0.16962215 -0.07956483]]. Action = [[ 0.23317063 -0.06892133 -0.14738883 -0.0911085 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 6550 is [True, False, False, False, True, False]
Human Feedback received at timestep 6550 of 1
Current timestep = 6551. State = [[-0.16109861 -0.0820554 ]]. Action = [[ 0.22173062 -0.06352334 -0.10967858 -0.16072261]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 6551 is [True, False, False, False, True, False]
Current timestep = 6552. State = [[-0.15352988 -0.08469418]]. Action = [[ 0.24312341 -0.04616934 -0.09080246 -0.14585435]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 6552 is [True, False, False, False, True, False]
Current timestep = 6553. State = [[-0.14661895 -0.08694982]]. Action = [[ 0.23406601 -0.07723755 -0.16140918 -0.27301842]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 6553 is [True, False, False, False, True, False]
Current timestep = 6554. State = [[-0.13872607 -0.08926395]]. Action = [[ 0.1983062  -0.10195436 -0.08021507 -0.2112844 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 6554 is [True, False, False, False, True, False]
Current timestep = 6555. State = [[-0.13086174 -0.09236192]]. Action = [[ 0.2333537  -0.09286349 -0.14256021 -0.16929865]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 6555 is [True, False, False, False, True, False]
Human Feedback received at timestep 6555 of 1
Current timestep = 6556. State = [[-0.1227548  -0.09573539]]. Action = [[ 0.15891224 -0.09057638 -0.11796564 -0.17585886]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 6556 is [True, False, False, False, True, False]
Current timestep = 6557. State = [[-0.11491881 -0.0988804 ]]. Action = [[ 0.23984724 -0.07532805 -0.12603873 -0.18886536]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 6557 is [True, False, False, False, True, False]
Current timestep = 6558. State = [[-0.10720257 -0.10195812]]. Action = [[ 0.20221442 -0.06638986 -0.1401659  -0.2723546 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 6558 is [True, False, False, False, True, False]
Current timestep = 6559. State = [[-0.09982222 -0.10482053]]. Action = [[ 0.22026765 -0.12885256 -0.12222308 -0.24793619]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 6559 is [True, False, False, False, True, False]
Current timestep = 6560. State = [[-0.092752   -0.10824194]]. Action = [[ 0.23131016 -0.08721277 -0.08311374 -0.19755179]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 6560 is [True, False, False, False, True, False]
Current timestep = 6561. State = [[-0.08456315 -0.11218139]]. Action = [[ 0.23780727 -0.10450724 -0.08398446 -0.33226138]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 6561 is [True, False, False, False, True, False]
Scene graph at timestep 6561 is [True, False, False, False, True, False]
State prediction error at timestep 6561 is tensor(3.3340e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6562. State = [[-0.07575855 -0.11589047]]. Action = [[ 0.14978978 -0.07130119 -0.12537384 -0.05759197]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 6562 is [True, False, False, False, True, False]
Current timestep = 6563. State = [[-0.06861762 -0.118873  ]]. Action = [[ 0.23513693 -0.06896804 -0.139321   -0.17255956]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 6563 is [True, False, False, False, True, False]
Human Feedback received at timestep 6563 of -1
Current timestep = 6564. State = [[-0.06006125 -0.12193284]]. Action = [[ 0.23948103 -0.09243295 -0.11598748 -0.19847888]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 6564 is [True, False, False, False, True, False]
Scene graph at timestep 6564 is [True, False, False, False, True, False]
State prediction error at timestep 6564 is tensor(3.3691e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6565. State = [[-0.05166724 -0.12499714]]. Action = [[ 0.2310459  -0.10295299 -0.1247744   0.04325604]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 6565 is [True, False, False, False, True, False]
Scene graph at timestep 6565 is [True, False, False, False, True, False]
State prediction error at timestep 6565 is tensor(2.5156e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6566. State = [[-0.0435462  -0.12814759]]. Action = [[ 0.24339005 -0.06988814 -0.13897382 -0.18720001]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 6566 is [True, False, False, False, True, False]
Scene graph at timestep 6566 is [False, True, False, True, False, False]
State prediction error at timestep 6566 is tensor(3.3841e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6566 of -1
Current timestep = 6567. State = [[-0.03457779 -0.13128974]]. Action = [[ 0.0430555  -0.12192155 -0.08952801 -0.00488305]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 6567 is [False, True, False, True, False, False]
Scene graph at timestep 6567 is [False, True, False, True, False, False]
State prediction error at timestep 6567 is tensor(1.8300e-05, grad_fn=<MseLossBackward0>)
Current timestep = 6568. State = [[-0.02683071 -0.13536195]]. Action = [[ 0.24607816 -0.08721647 -0.06787276 -0.25026274]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 6568 is [False, True, False, True, False, False]
Human Feedback received at timestep 6568 of -1
Current timestep = 6569. State = [[-0.02032005 -0.13900739]]. Action = [[ 0.23298466 -0.06918427 -0.11808509  0.17945778]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 6569 is [False, True, False, True, False, False]
Current timestep = 6570. State = [[-0.01368312 -0.1421217 ]]. Action = [[ 0.23175249 -0.07349835 -0.08266914  0.02063251]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 6570 is [False, True, False, True, False, False]
Human Feedback received at timestep 6570 of -1
Current timestep = 6571. State = [[-0.00620156 -0.14499708]]. Action = [[ 0.2332632  -0.03139478 -0.11519454 -0.11300796]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 6571 is [False, True, False, True, False, False]
Current timestep = 6572. State = [[ 0.00217173 -0.14688498]]. Action = [[ 0.22284812 -0.02569075 -0.18420368 -0.30832124]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 6572 is [False, True, False, True, False, False]
Human Feedback received at timestep 6572 of -1
Current timestep = 6573. State = [[ 0.01023187 -0.14855538]]. Action = [[ 0.21094981 -0.04400602 -0.04840697 -0.223001  ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 6573 is [False, True, False, True, False, False]
Human Feedback received at timestep 6573 of -1
Current timestep = 6574. State = [[ 0.01872041 -0.14981414]]. Action = [[ 0.2283963  -0.05436066 -0.03215891 -0.33841288]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 6574 is [False, True, False, True, False, False]
Current timestep = 6575. State = [[ 0.02739736 -0.1512197 ]]. Action = [[ 0.24047089 -0.07653806 -0.0542635  -0.22641897]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 6575 is [False, True, False, True, False, False]
Human Feedback received at timestep 6575 of -1
Current timestep = 6576. State = [[ 0.03563619 -0.15300646]]. Action = [[ 0.23040867 -0.10874617 -0.11363536 -0.31998998]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 6576 is [False, True, False, True, False, False]
Human Feedback received at timestep 6576 of -1
Current timestep = 6577. State = [[ 0.04371724 -0.15584308]]. Action = [[ 0.24326885 -0.0880889  -0.11220865 -0.21874309]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 6577 is [False, True, False, True, False, False]
Human Feedback received at timestep 6577 of -1
Current timestep = 6578. State = [[ 0.05298444 -0.159405  ]]. Action = [[ 0.24214071 -0.07187879 -0.03381757 -0.31428766]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 6578 is [False, True, False, True, False, False]
Current timestep = 6579. State = [[ 0.06131765 -0.1625256 ]]. Action = [[ 0.17198294 -0.08727258 -0.10525885  0.0098747 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 6579 is [False, False, True, True, False, False]
Human Feedback received at timestep 6579 of -1
Current timestep = 6580. State = [[ 0.06981438 -0.165707  ]]. Action = [[ 0.18411261 -0.08671191 -0.13116594 -0.5109005 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 6580 is [False, False, True, True, False, False]
Human Feedback received at timestep 6580 of -1
Current timestep = 6581. State = [[ 0.07640326 -0.16770019]]. Action = [[ 0.23643562 -0.10179089 -0.11160763 -0.00457871]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 6581 is [False, False, True, True, False, False]
Current timestep = 6582. State = [[ 0.081558   -0.16891755]]. Action = [[ 0.1846346   0.00074527 -0.10612562  0.0107398 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 6582 is [False, False, True, True, False, False]
Scene graph at timestep 6582 is [False, False, True, True, False, False]
State prediction error at timestep 6582 is tensor(1.6159e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6582 of -1
Current timestep = 6583. State = [[ 0.08590896 -0.17001832]]. Action = [[ 0.19455084 -0.02420284 -0.0772019  -0.2162162 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 6583 is [False, False, True, True, False, False]
Human Feedback received at timestep 6583 of -1
Current timestep = 6584. State = [[ 0.08860438 -0.17062224]]. Action = [[ 0.18809375 -0.07329011 -0.1367882  -0.22979724]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 6584 is [False, False, True, True, False, False]
Current timestep = 6585. State = [[ 0.09064043 -0.17115192]]. Action = [[ 0.23094177 -0.08734471 -0.03444593  0.13341439]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 6585 is [False, False, True, True, False, False]
Human Feedback received at timestep 6585 of -1
Current timestep = 6586. State = [[ 0.09180199 -0.17149357]]. Action = [[ 0.24518889 -0.06441751 -0.10133243 -0.15067011]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 6586 is [False, False, True, True, False, False]
Scene graph at timestep 6586 is [False, False, True, True, False, False]
State prediction error at timestep 6586 is tensor(5.5585e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6587. State = [[ 0.0918161  -0.17145526]]. Action = [[ 0.234797   -0.11783752 -0.09093414 -0.38617277]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 6587 is [False, False, True, True, False, False]
Scene graph at timestep 6587 is [False, False, True, True, False, False]
State prediction error at timestep 6587 is tensor(3.4443e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6588. State = [[ 0.09175856 -0.17163645]]. Action = [[ 0.22205356 -0.0972099   0.01631221 -0.14513421]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 6588 is [False, False, True, True, False, False]
Scene graph at timestep 6588 is [False, False, True, True, False, False]
State prediction error at timestep 6588 is tensor(4.2296e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6589. State = [[ 0.09171018 -0.17181946]]. Action = [[ 0.14278218 -0.11401367 -0.12776437 -0.09943783]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 6589 is [False, False, True, True, False, False]
Scene graph at timestep 6589 is [False, False, True, True, False, False]
State prediction error at timestep 6589 is tensor(3.5734e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6590. State = [[ 0.09169404 -0.17188047]]. Action = [[ 0.19383332 -0.07831666 -0.03910302 -0.07764292]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 6590 is [False, False, True, True, False, False]
Human Feedback received at timestep 6590 of -1
Current timestep = 6591. State = [[ 0.09164572 -0.17206304]]. Action = [[ 0.17246246 -0.13946761 -0.08844225 -0.22031361]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 6591 is [False, False, True, True, False, False]
Current timestep = 6592. State = [[ 0.09162956 -0.17212403]]. Action = [[ 0.23326015 -0.0399548  -0.09720901 -0.10438299]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 6592 is [False, False, True, True, False, False]
Current timestep = 6593. State = [[ 0.09159724 -0.17224601]]. Action = [[ 0.19499731 -0.13104977 -0.14341107 -0.1943844 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 6593 is [False, False, True, True, False, False]
Current timestep = 6594. State = [[ 0.09158108 -0.17230698]]. Action = [[ 0.18851706 -0.0861724  -0.0718935  -0.13811111]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 6594 is [False, False, True, True, False, False]
Current timestep = 6595. State = [[ 0.09158108 -0.17230698]]. Action = [[ 0.20201471 -0.14983718 -0.03594524  0.17466545]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 6595 is [False, False, True, True, False, False]
Scene graph at timestep 6595 is [False, False, True, True, False, False]
State prediction error at timestep 6595 is tensor(6.0168e-09, grad_fn=<MseLossBackward0>)
Current timestep = 6596. State = [[ 0.09153267 -0.17248951]]. Action = [[ 0.17110693 -0.04307875 -0.13374838  0.21512389]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 6596 is [False, False, True, True, False, False]
Current timestep = 6597. State = [[ 0.09153267 -0.17248951]]. Action = [[ 0.21274596 -0.10298178 -0.11152962 -0.12708414]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 6597 is [False, False, True, True, False, False]
Current timestep = 6598. State = [[ 0.09153267 -0.17248951]]. Action = [[ 0.23968443 -0.14790587 -0.12072894  0.03289163]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 6598 is [False, False, True, True, False, False]
Current timestep = 6599. State = [[ 0.09153267 -0.17248951]]. Action = [[ 0.15136749 -0.09626266  0.01969925 -0.29205704]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 6599 is [False, False, True, True, False, False]
Current timestep = 6600. State = [[ 0.09153267 -0.17248951]]. Action = [[ 0.20073438 -0.16671059 -0.0981116  -0.16270852]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 6600 is [False, False, True, True, False, False]
Current timestep = 6601. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.18243992  0.04214072 -0.14722484 -0.14766878]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 6601 is [False, False, True, True, False, False]
Current timestep = 6602. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.19495434 -0.0840475  -0.0585126   0.3027097 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 6602 is [False, False, True, True, False, False]
Scene graph at timestep 6602 is [False, False, True, True, False, False]
State prediction error at timestep 6602 is tensor(1.3829e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6603. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.20607132 -0.06521559 -0.05868566 -0.19103909]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 6603 is [False, False, True, True, False, False]
Scene graph at timestep 6603 is [False, False, True, True, False, False]
State prediction error at timestep 6603 is tensor(7.6030e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6604. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.23115844 -0.07797411 -0.10404116 -0.13553488]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 6604 is [False, False, True, True, False, False]
Current timestep = 6605. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.23156142 -0.12752825 -0.00752179 -0.22361296]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 6605 is [False, False, True, True, False, False]
Scene graph at timestep 6605 is [False, False, True, True, False, False]
State prediction error at timestep 6605 is tensor(8.0011e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6606. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.1534493  -0.13243335 -0.08581284  0.14808583]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 6606 is [False, False, True, True, False, False]
Scene graph at timestep 6606 is [False, False, True, True, False, False]
State prediction error at timestep 6606 is tensor(1.3053e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6607. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.23428994 -0.0615737  -0.07422781 -0.02173406]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 6607 is [False, False, True, True, False, False]
Current timestep = 6608. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.21208876 -0.11407989 -0.05428126  0.05286646]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 6608 is [False, False, True, True, False, False]
Current timestep = 6609. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.22227272 -0.12123454 -0.12303215 -0.01486319]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 6609 is [False, False, True, True, False, False]
Current timestep = 6610. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24222088 -0.11620942 -0.02229717 -0.01512474]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 6610 is [False, False, True, True, False, False]
Current timestep = 6611. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24650392 -0.10453883 -0.14004791 -0.24844956]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 6611 is [False, False, True, True, False, False]
Current timestep = 6612. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.19557679 -0.11911923 -0.13732225 -0.12037611]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 6612 is [False, False, True, True, False, False]
Current timestep = 6613. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.1400249  -0.10502085 -0.09062642 -0.0484637 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 6613 is [False, False, True, True, False, False]
Current timestep = 6614. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.18815994 -0.13187052 -0.14733924 -0.12032211]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 6614 is [False, False, True, True, False, False]
Current timestep = 6615. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.19138515 -0.10660939 -0.09826066 -0.3876633 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 6615 is [False, False, True, True, False, False]
Current timestep = 6616. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.20844778 -0.1241297  -0.07401693  0.11230743]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 6616 is [False, False, True, True, False, False]
Scene graph at timestep 6616 is [False, False, True, True, False, False]
State prediction error at timestep 6616 is tensor(9.0522e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6617. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.1110431   0.02951473 -0.06498477  0.1105144 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 6617 is [False, False, True, True, False, False]
Current timestep = 6618. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.14546776 -0.04545309 -0.07179737 -0.3714338 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 6618 is [False, False, True, True, False, False]
Current timestep = 6619. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.16969728 -0.14117119 -0.17233059  0.24384105]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 6619 is [False, False, True, True, False, False]
Current timestep = 6620. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24293804 -0.01754934  0.02871117 -0.01842338]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 6620 is [False, False, True, True, False, False]
Scene graph at timestep 6620 is [False, False, True, True, False, False]
State prediction error at timestep 6620 is tensor(1.0402e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6621. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24069405 -0.00414513 -0.11323069  0.17428112]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 6621 is [False, False, True, True, False, False]
Current timestep = 6622. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.09746397 -0.11159432 -0.05506381 -0.14439392]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 6622 is [False, False, True, True, False, False]
Current timestep = 6623. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.00535911 -0.07650319 -0.03926685 -0.24701029]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 6623 is [False, False, True, True, False, False]
Current timestep = 6624. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.16607028  0.05518693 -0.10968646 -0.16418976]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 6624 is [False, False, True, True, False, False]
Current timestep = 6625. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.07906002 -0.10482687 -0.1117146   0.06788075]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 6625 is [False, False, True, True, False, False]
Current timestep = 6626. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.12300515 -0.06671382 -0.08648069 -0.43977547]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 6626 is [False, False, True, True, False, False]
Current timestep = 6627. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.20654005 -0.09458186 -0.05972204 -0.07000905]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 6627 is [False, False, True, True, False, False]
Scene graph at timestep 6627 is [False, False, True, True, False, False]
State prediction error at timestep 6627 is tensor(4.7688e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6628. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.22112569 -0.08899783 -0.06014143 -0.2030313 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 6628 is [False, False, True, True, False, False]
Scene graph at timestep 6628 is [False, False, True, True, False, False]
State prediction error at timestep 6628 is tensor(2.8575e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6629. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.19894579 -0.08398953 -0.04399201  0.13409507]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 6629 is [False, False, True, True, False, False]
Current timestep = 6630. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.23189574 -0.11777088 -0.07871304  0.18952048]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 6630 is [False, False, True, True, False, False]
Current timestep = 6631. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.22306901 -0.15879627  0.03734431  0.15399039]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 6631 is [False, False, True, True, False, False]
Current timestep = 6632. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.14398071 -0.01817998  0.06367725 -0.31417465]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 6632 is [False, False, True, True, False, False]
Current timestep = 6633. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.23682654  0.0212976  -0.09861718  0.02652633]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 6633 is [False, False, True, True, False, False]
Current timestep = 6634. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.10673687 -0.07147598 -0.02240235  0.24463522]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 6634 is [False, False, True, True, False, False]
Scene graph at timestep 6634 is [False, False, True, True, False, False]
State prediction error at timestep 6634 is tensor(6.4147e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6635. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24017853 -0.14378601 -0.04354729  0.03093779]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 6635 is [False, False, True, True, False, False]
Current timestep = 6636. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.08868107  0.00226972 -0.10348991 -0.07770443]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 6636 is [False, False, True, True, False, False]
Current timestep = 6637. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.16867924 -0.15547492 -0.12784766 -0.2058729 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 6637 is [False, False, True, True, False, False]
Current timestep = 6638. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.20443714 -0.09313822 -0.10859612  0.12717319]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 6638 is [False, False, True, True, False, False]
Current timestep = 6639. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.20855081 -0.1357179  -0.13090365 -0.10147637]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 6639 is [False, False, True, True, False, False]
Current timestep = 6640. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24305743 -0.08632249 -0.11022863 -0.22702467]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 6640 is [False, False, True, True, False, False]
Current timestep = 6641. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.13390112 -0.05658028 -0.09597686  0.16413498]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 6641 is [False, False, True, True, False, False]
Current timestep = 6642. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.20180506 -0.12110811 -0.0740217   0.15384495]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 6642 is [False, False, True, True, False, False]
Current timestep = 6643. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.15872586 -0.04444    -0.02304332 -0.4850747 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 6643 is [False, False, True, True, False, False]
Scene graph at timestep 6643 is [False, False, True, True, False, False]
State prediction error at timestep 6643 is tensor(2.3359e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6644. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.1443305  -0.17540592 -0.11150561 -0.5346716 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 6644 is [False, False, True, True, False, False]
Scene graph at timestep 6644 is [False, False, True, True, False, False]
State prediction error at timestep 6644 is tensor(4.0963e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6645. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.1616301   0.00309262 -0.19964223 -0.09004939]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 6645 is [False, False, True, True, False, False]
Current timestep = 6646. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.21503508 -0.07130092 -0.13405244 -0.4902004 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 6646 is [False, False, True, True, False, False]
Current timestep = 6647. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.18397766 -0.13097507  0.00486207  0.45879555]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 6647 is [False, False, True, True, False, False]
Scene graph at timestep 6647 is [False, False, True, True, False, False]
State prediction error at timestep 6647 is tensor(2.8451e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6648. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.2074041  -0.04186815 -0.13658297  0.16140401]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 6648 is [False, False, True, True, False, False]
Scene graph at timestep 6648 is [False, False, True, True, False, False]
State prediction error at timestep 6648 is tensor(1.0471e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6649. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.07027042 -0.08734417 -0.19379188 -0.536079  ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 6649 is [False, False, True, True, False, False]
Current timestep = 6650. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.22031856 -0.07695428  0.07699353  0.04999042]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 6650 is [False, False, True, True, False, False]
Current timestep = 6651. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.18117565 -0.16617467  0.11643878  0.12489796]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 6651 is [False, False, True, True, False, False]
Current timestep = 6652. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.14236593 -0.10330833 -0.10534942  0.24209976]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 6652 is [False, False, True, True, False, False]
Scene graph at timestep 6652 is [False, False, True, True, False, False]
State prediction error at timestep 6652 is tensor(2.0591e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6653. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.22127804 -0.18366437 -0.02650139 -0.0194025 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 6653 is [False, False, True, True, False, False]
Current timestep = 6654. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.18604976 -0.12947184 -0.1471196   0.37008393]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 6654 is [False, False, True, True, False, False]
Current timestep = 6655. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.0728597  -0.13327259 -0.13266367 -0.3177904 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 6655 is [False, False, True, True, False, False]
Scene graph at timestep 6655 is [False, False, True, True, False, False]
State prediction error at timestep 6655 is tensor(2.6282e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6656. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.09803095 -0.00464217 -0.12553945 -0.03855395]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 6656 is [False, False, True, True, False, False]
Current timestep = 6657. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.16908187 -0.06645995 -0.01526411  0.19671714]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 6657 is [False, False, True, True, False, False]
Scene graph at timestep 6657 is [False, False, True, True, False, False]
State prediction error at timestep 6657 is tensor(3.2265e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6658. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.18224937 -0.01834765 -0.14859188 -0.28909063]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 6658 is [False, False, True, True, False, False]
Current timestep = 6659. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.07939306 -0.17380182 -0.12037377  0.35650182]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 6659 is [False, False, True, True, False, False]
Current timestep = 6660. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.2180936  -0.01978594 -0.10722116 -0.4422127 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 6660 is [False, False, True, True, False, False]
Current timestep = 6661. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.01586252 -0.13019809 -0.14970076 -0.40394223]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 6661 is [False, False, True, True, False, False]
Current timestep = 6662. State = [[ 0.09151649 -0.17255048]]. Action = [[-0.02451251 -0.12373091 -0.0603947   0.2507794 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 6662 is [False, False, True, True, False, False]
Scene graph at timestep 6662 is [False, False, True, True, False, False]
State prediction error at timestep 6662 is tensor(5.2606e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6663. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.23505563 -0.03449024 -0.21796887 -0.08776951]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 6663 is [False, False, True, True, False, False]
Current timestep = 6664. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.00877604 -0.11005054 -0.14673351 -0.13990837]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 6664 is [False, False, True, True, False, False]
Current timestep = 6665. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24104679  0.08358404 -0.07405138 -0.13941306]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 6665 is [False, False, True, True, False, False]
Current timestep = 6666. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.2098     -0.04830033 -0.00182949 -0.73834085]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 6666 is [False, False, True, True, False, False]
Current timestep = 6667. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.24503988 -0.14036724 -0.09496561  0.09085834]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 6667 is [False, False, True, True, False, False]
Current timestep = 6668. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.21111709 -0.15365455 -0.16196795 -0.23218411]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 6668 is [False, False, True, True, False, False]
Scene graph at timestep 6668 is [False, False, True, True, False, False]
State prediction error at timestep 6668 is tensor(3.1989e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6669. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.11769232 -0.01765108 -0.07316722  0.14244246]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 6669 is [False, False, True, True, False, False]
Current timestep = 6670. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.22503585  0.08693087 -0.07096446  0.32641327]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 6670 is [False, False, True, True, False, False]
Current timestep = 6671. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.19341055 -0.13596927 -0.01480247  0.14208126]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 6671 is [False, False, True, True, False, False]
Current timestep = 6672. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.22050685 -0.05071715 -0.06816176 -0.2001726 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 6672 is [False, False, True, True, False, False]
Current timestep = 6673. State = [[ 0.09151649 -0.17255048]]. Action = [[-0.09174815 -0.06819367 -0.08924164 -0.01450115]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 6673 is [False, False, True, True, False, False]
Current timestep = 6674. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.14336392 -0.10924158  0.06025097  0.03921378]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 6674 is [False, False, True, True, False, False]
Current timestep = 6675. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.06598726 -0.04495479 -0.22834742 -0.55936414]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 6675 is [False, False, True, True, False, False]
Current timestep = 6676. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.16406977 -0.10846034 -0.0836499   0.05436909]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 6676 is [False, False, True, True, False, False]
Current timestep = 6677. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.10435453 -0.0535621  -0.1843838   0.11394155]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 6677 is [False, False, True, True, False, False]
Current timestep = 6678. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.20724475 -0.11519988 -0.11236182  0.19107783]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 6678 is [False, False, True, True, False, False]
Current timestep = 6679. State = [[ 0.09151649 -0.17255048]]. Action = [[ 0.13301921 -0.05754983  0.10070595  0.13210094]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 6679 is [False, False, True, True, False, False]
Current timestep = 6680. State = [[ 0.09166507 -0.17253904]]. Action = [[ 0.22635171 -0.07322097  0.01022539 -0.19375485]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 6680 is [False, False, True, True, False, False]
Current timestep = 6681. State = [[ 0.09159256 -0.17254612]]. Action = [[ 0.21528876 -0.07032438 -0.08406073 -0.25598896]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 6681 is [False, False, True, True, False, False]
Current timestep = 6682. State = [[ 0.09159256 -0.17254612]]. Action = [[ 0.1591348   0.01163906 -0.09347689 -0.21863687]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 6682 is [False, False, True, True, False, False]
Scene graph at timestep 6682 is [False, False, True, True, False, False]
State prediction error at timestep 6682 is tensor(6.4522e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6683. State = [[ 0.09159256 -0.17254612]]. Action = [[ 0.08190683 -0.0945396  -0.03332269  0.43427968]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 6683 is [False, False, True, True, False, False]
Current timestep = 6684. State = [[ 0.09159256 -0.17254612]]. Action = [[-0.10690582 -0.04130179 -0.08770111 -0.46315086]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 6684 is [False, False, True, True, False, False]
Current timestep = 6685. State = [[ 0.09159256 -0.17254612]]. Action = [[ 0.22676319 -0.12808247 -0.18803151 -0.3253367 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 6685 is [False, False, True, True, False, False]
Scene graph at timestep 6685 is [False, False, True, True, False, False]
State prediction error at timestep 6685 is tensor(1.5536e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6686. State = [[ 0.09159256 -0.17254612]]. Action = [[ 0.20964861 -0.10434955  0.01177827  0.3615682 ]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 6686 is [False, False, True, True, False, False]
Current timestep = 6687. State = [[ 0.09159256 -0.17254612]]. Action = [[ 0.19667932 -0.03631091 -0.07160729  0.2354809 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 6687 is [False, False, True, True, False, False]
Scene graph at timestep 6687 is [False, False, True, True, False, False]
State prediction error at timestep 6687 is tensor(1.2406e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6688. State = [[ 0.0916701 -0.1725499]]. Action = [[ 0.22844815  0.0157789   0.09682843 -0.2392646 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 6688 is [False, False, True, True, False, False]
Current timestep = 6689. State = [[ 0.0916248  -0.17252752]]. Action = [[ 0.17183858 -0.03725336  0.07799485 -0.35375893]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 6689 is [False, False, True, True, False, False]
Current timestep = 6690. State = [[ 0.09159819 -0.17251964]]. Action = [[ 0.17979449  0.03839746  0.14317042 -0.31465113]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 6690 is [False, False, True, True, False, False]
Current timestep = 6691. State = [[ 0.09159819 -0.17251964]]. Action = [[ 0.15889177  0.02643597 -0.09592752 -0.26420915]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 6691 is [False, False, True, True, False, False]
Current timestep = 6692. State = [[ 0.09159819 -0.17251964]]. Action = [[ 0.03636533  0.01372868 -0.12677346  0.08166528]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 6692 is [False, False, True, True, False, False]
Current timestep = 6693. State = [[ 0.09159819 -0.17251964]]. Action = [[ 0.17601237 -0.1290245  -0.15803084 -0.3150857 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 6693 is [False, False, True, True, False, False]
Current timestep = 6694. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.18687993 -0.01506831 -0.11808303  0.10785174]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 6694 is [False, False, True, True, False, False]
Current timestep = 6695. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.17572588 -0.06979537 -0.05097122 -0.10259199]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 6695 is [False, False, True, True, False, False]
Scene graph at timestep 6695 is [False, False, True, True, False, False]
State prediction error at timestep 6695 is tensor(5.6545e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6696. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.19746792 -0.06008959 -0.08228943 -0.43884313]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 6696 is [False, False, True, True, False, False]
Scene graph at timestep 6696 is [False, False, True, True, False, False]
State prediction error at timestep 6696 is tensor(1.2068e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6697. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21484658 -0.06304654 -0.02875999  0.32260227]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 6697 is [False, False, True, True, False, False]
Current timestep = 6698. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.1657477  -0.06297719 -0.11382754 -0.3473475 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 6698 is [False, False, True, True, False, False]
Current timestep = 6699. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.17635477 -0.05154295 -0.22511947 -0.6638571 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 6699 is [False, False, True, True, False, False]
Current timestep = 6700. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.04830542 -0.10552229 -0.10118222  0.0866282 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 6700 is [False, False, True, True, False, False]
Current timestep = 6701. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.00434889 -0.06682199 -0.05987421  0.0015434 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 6701 is [False, False, True, True, False, False]
Scene graph at timestep 6701 is [False, False, True, True, False, False]
State prediction error at timestep 6701 is tensor(8.6818e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6702. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.01598227 -0.13053586 -0.15846488 -0.1869592 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 6702 is [False, False, True, True, False, False]
Scene graph at timestep 6702 is [False, False, True, True, False, False]
State prediction error at timestep 6702 is tensor(8.4909e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6703. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21470612 -0.06846058 -0.10858491  0.08048141]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 6703 is [False, False, True, True, False, False]
Scene graph at timestep 6703 is [False, False, True, True, False, False]
State prediction error at timestep 6703 is tensor(1.9792e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6704. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.0504975  -0.16410805 -0.03290518 -0.36281806]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 6704 is [False, False, True, True, False, False]
Scene graph at timestep 6704 is [False, False, True, True, False, False]
State prediction error at timestep 6704 is tensor(9.2045e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6705. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.00668433 -0.1235939   0.06570244  0.00090969]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 6705 is [False, False, True, True, False, False]
Current timestep = 6706. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.2042427   0.07410103 -0.1485285  -0.30858648]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 6706 is [False, False, True, True, False, False]
Scene graph at timestep 6706 is [False, False, True, True, False, False]
State prediction error at timestep 6706 is tensor(1.8922e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6707. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.13346887 -0.02048348  0.12267676  0.2958454 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 6707 is [False, False, True, True, False, False]
Current timestep = 6708. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.18859845 -0.13802019 -0.21948223 -0.13310045]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 6708 is [False, False, True, True, False, False]
Current timestep = 6709. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.16948226 -0.09572396 -0.03749639 -0.86353606]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 6709 is [False, False, True, True, False, False]
Current timestep = 6710. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.02903378 -0.15707164 -0.0769721  -0.4102708 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 6710 is [False, False, True, True, False, False]
Current timestep = 6711. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.16453832 -0.14084092 -0.08307554 -0.01810402]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 6711 is [False, False, True, True, False, False]
Scene graph at timestep 6711 is [False, False, True, True, False, False]
State prediction error at timestep 6711 is tensor(1.6292e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6712. State = [[ 0.09151849 -0.17249602]]. Action = [[0.22025436 0.0491116  0.0187932  0.11587775]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 6712 is [False, False, True, True, False, False]
Current timestep = 6713. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.22240913 -0.15526815 -0.1512821   0.30293012]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 6713 is [False, False, True, True, False, False]
Scene graph at timestep 6713 is [False, False, True, True, False, False]
State prediction error at timestep 6713 is tensor(4.0012e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6714. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.17256325  0.02469102 -0.21815728 -0.46383965]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 6714 is [False, False, True, True, False, False]
Current timestep = 6715. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21802646 -0.06199758 -0.01670834  0.40460265]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 6715 is [False, False, True, True, False, False]
Scene graph at timestep 6715 is [False, False, True, True, False, False]
State prediction error at timestep 6715 is tensor(1.1372e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6716. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.08943039 -0.16983993 -0.24341154  0.3205155 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 6716 is [False, False, True, True, False, False]
Scene graph at timestep 6716 is [False, False, True, True, False, False]
State prediction error at timestep 6716 is tensor(3.8537e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6717. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.12266558 -0.0403823   0.01432183 -0.10602379]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 6717 is [False, False, True, True, False, False]
Current timestep = 6718. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.23351276 -0.13638566 -0.23695233 -0.13492554]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 6718 is [False, False, True, True, False, False]
Scene graph at timestep 6718 is [False, False, True, True, False, False]
State prediction error at timestep 6718 is tensor(1.6494e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6719. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.00498791 -0.16698742  0.01219895 -0.41545653]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 6719 is [False, False, True, True, False, False]
Current timestep = 6720. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.10182729 -0.12849596 -0.08531985 -0.29026437]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 6720 is [False, False, True, True, False, False]
Current timestep = 6721. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.0373055   0.01235604 -0.0281706   0.00085795]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 6721 is [False, False, True, True, False, False]
Current timestep = 6722. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.10477093  0.0185954  -0.08721043 -0.17924678]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 6722 is [False, False, True, True, False, False]
Current timestep = 6723. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.03510565  0.06162131 -0.00062172 -0.0240705 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 6723 is [False, False, True, True, False, False]
Current timestep = 6724. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.16396168 -0.03329261 -0.10969602  0.5348792 ]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 6724 is [False, False, True, True, False, False]
Current timestep = 6725. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.1904946  -0.00811999 -0.13696393 -0.01820809]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 6725 is [False, False, True, True, False, False]
Current timestep = 6726. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.23526165 -0.08508077  0.08980536 -0.12495422]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 6726 is [False, False, True, True, False, False]
Current timestep = 6727. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.13658458 -0.12981872 -0.12302005  0.8438219 ]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 6727 is [False, False, True, True, False, False]
Current timestep = 6728. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.01084465  0.03771111 -0.02184203  0.24698317]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 6728 is [False, False, True, True, False, False]
Scene graph at timestep 6728 is [False, False, True, True, False, False]
State prediction error at timestep 6728 is tensor(2.6403e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6729. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.05094042 -0.11718167 -0.24473186 -0.68137866]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 6729 is [False, False, True, True, False, False]
Scene graph at timestep 6729 is [False, False, True, True, False, False]
State prediction error at timestep 6729 is tensor(2.8137e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6730. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21059567 -0.02744938 -0.15156122  0.19779909]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 6730 is [False, False, True, True, False, False]
Current timestep = 6731. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.13492537  0.02286947 -0.09003288  0.4120133 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 6731 is [False, False, True, True, False, False]
Current timestep = 6732. State = [[ 0.09151849 -0.17249602]]. Action = [[0.1386795  0.04110843 0.07134575 0.56213605]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 6732 is [False, False, True, True, False, False]
Current timestep = 6733. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.20541781 -0.12089735 -0.08715022 -0.1302675 ]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 6733 is [False, False, True, True, False, False]
Scene graph at timestep 6733 is [False, False, True, True, False, False]
State prediction error at timestep 6733 is tensor(7.2933e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6734. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.05410761  0.02657124 -0.07589789  0.70066094]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 6734 is [False, False, True, True, False, False]
Current timestep = 6735. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.04651658 -0.00019288  0.18490127  0.08948326]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 6735 is [False, False, True, True, False, False]
Scene graph at timestep 6735 is [False, False, True, True, False, False]
State prediction error at timestep 6735 is tensor(1.9311e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6736. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.23961419 -0.1454081  -0.20503218  0.49690747]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 6736 is [False, False, True, True, False, False]
Scene graph at timestep 6736 is [False, False, True, True, False, False]
State prediction error at timestep 6736 is tensor(2.3936e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6737. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.14546347 -0.16004625 -0.05935147 -0.62545294]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 6737 is [False, False, True, True, False, False]
Scene graph at timestep 6737 is [False, False, True, True, False, False]
State prediction error at timestep 6737 is tensor(2.7210e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6738. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.08377564 -0.2119759   0.20320863  0.50452447]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 6738 is [False, False, True, True, False, False]
Current timestep = 6739. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21223578 -0.07062     0.12255216 -0.4152341 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 6739 is [False, False, True, True, False, False]
Current timestep = 6740. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.03636323 -0.17757146  0.23338515  0.07013881]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 6740 is [False, False, True, True, False, False]
Current timestep = 6741. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.12623942 -0.08717489  0.08920333 -0.8971637 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 6741 is [False, False, True, True, False, False]
Current timestep = 6742. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21606666 -0.10603364 -0.01340152  0.10257924]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 6742 is [False, False, True, True, False, False]
Current timestep = 6743. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.01619624 -0.20812774  0.19305009  0.6796185 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 6743 is [False, False, True, True, False, False]
Current timestep = 6744. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21301347 -0.16788337 -0.02263935  0.8471651 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 6744 is [False, False, True, True, False, False]
Current timestep = 6745. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.14012885 -0.10168457  0.19366509  0.22476804]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 6745 is [False, False, True, True, False, False]
Current timestep = 6746. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.10324752 -0.0745105  -0.07921925  0.3365562 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 6746 is [False, False, True, True, False, False]
Current timestep = 6747. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.13459283 -0.18178172  0.1066688   0.33579826]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 6747 is [False, False, True, True, False, False]
Current timestep = 6748. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.21656048 -0.14996119  0.02162269  0.6681931 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 6748 is [False, False, True, True, False, False]
Current timestep = 6749. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.14361742  0.02420801  0.03036872 -0.4919936 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 6749 is [False, False, True, True, False, False]
Current timestep = 6750. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.15168196 -0.04621634  0.22099423 -0.0047214 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 6750 is [False, False, True, True, False, False]
Current timestep = 6751. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.22334954 -0.16993695 -0.1598585  -0.3052746 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 6751 is [False, False, True, True, False, False]
Scene graph at timestep 6751 is [False, False, True, True, False, False]
State prediction error at timestep 6751 is tensor(3.8290e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6752. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.21689144 -0.14858091 -0.0714303  -0.04249066]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 6752 is [False, False, True, True, False, False]
Scene graph at timestep 6752 is [False, False, True, True, False, False]
State prediction error at timestep 6752 is tensor(8.5026e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6753. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.14113052 -0.06586842 -0.17696722 -0.5246298 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 6753 is [False, False, True, True, False, False]
Scene graph at timestep 6753 is [False, False, True, True, False, False]
State prediction error at timestep 6753 is tensor(1.3097e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6754. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.12774254 -0.12490724 -0.16117986  0.00722778]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 6754 is [False, False, True, True, False, False]
Scene graph at timestep 6754 is [False, False, True, True, False, False]
State prediction error at timestep 6754 is tensor(3.6504e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6755. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.03825459 -0.18756786 -0.20125383 -0.81955487]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 6755 is [False, False, True, True, False, False]
Current timestep = 6756. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.17314667 -0.11596796  0.07336837  0.8102801 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 6756 is [False, False, True, True, False, False]
Current timestep = 6757. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.17209512  0.08891103 -0.04820874  0.577584  ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 6757 is [False, False, True, True, False, False]
Scene graph at timestep 6757 is [False, False, True, True, False, False]
State prediction error at timestep 6757 is tensor(1.5667e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6758. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.03155887 -0.05663563  0.14625937 -0.41068703]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 6758 is [False, False, True, True, False, False]
Current timestep = 6759. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.07557058 -0.04823856  0.07744053  0.10625517]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 6759 is [False, False, True, True, False, False]
Current timestep = 6760. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.13486123 -0.22139683 -0.05190033 -0.05764574]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 6760 is [False, False, True, True, False, False]
Current timestep = 6761. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.08415624 -0.1881325  -0.14080213  0.55536795]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 6761 is [False, False, True, True, False, False]
Current timestep = 6762. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.03993854 -0.11396149 -0.16610032  0.6497102 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 6762 is [False, False, True, True, False, False]
Current timestep = 6763. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.11057988 -0.17987409  0.1317311  -0.39016533]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 6763 is [False, False, True, True, False, False]
Scene graph at timestep 6763 is [False, False, True, True, False, False]
State prediction error at timestep 6763 is tensor(5.0022e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6764. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.01245683 -0.10292429  0.11271766  0.88166165]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 6764 is [False, False, True, True, False, False]
Current timestep = 6765. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.12170005 -0.15280865  0.17719114  0.65785456]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 6765 is [False, False, True, True, False, False]
Current timestep = 6766. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.1968357  -0.03794995 -0.07101481 -0.13843906]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 6766 is [False, False, True, True, False, False]
Current timestep = 6767. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.03213543 -0.15435752 -0.21881871  0.9622431 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 6767 is [False, False, True, True, False, False]
Scene graph at timestep 6767 is [False, False, True, True, False, False]
State prediction error at timestep 6767 is tensor(2.4737e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6768. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.03015038 -0.09546363  0.03830245  0.17575204]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 6768 is [False, False, True, True, False, False]
Scene graph at timestep 6768 is [False, False, True, True, False, False]
State prediction error at timestep 6768 is tensor(1.4618e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6769. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.01905864 -0.04592538 -0.10889164  0.30897915]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 6769 is [False, False, True, True, False, False]
Scene graph at timestep 6769 is [False, False, True, True, False, False]
State prediction error at timestep 6769 is tensor(3.5098e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6770. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.0132381  -0.17258745  0.18164518 -0.81810987]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 6770 is [False, False, True, True, False, False]
Current timestep = 6771. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.18555227  0.02274454 -0.23728566  0.14420426]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 6771 is [False, False, True, True, False, False]
Current timestep = 6772. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.01190087 -0.10850902  0.15528464  0.51230645]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 6772 is [False, False, True, True, False, False]
Current timestep = 6773. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.08697188 -0.21445832  0.1881519   0.5828396 ]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 6773 is [False, False, True, True, False, False]
Current timestep = 6774. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.18976837 -0.08837408 -0.10999674  0.31774735]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 6774 is [False, False, True, True, False, False]
Current timestep = 6775. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.1804018  -0.17844406  0.1390655  -0.25187325]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 6775 is [False, False, True, True, False, False]
Scene graph at timestep 6775 is [False, False, True, True, False, False]
State prediction error at timestep 6775 is tensor(8.4216e-09, grad_fn=<MseLossBackward0>)
Current timestep = 6776. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.15428835 -0.14975798  0.24290574 -0.39162707]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 6776 is [False, False, True, True, False, False]
Current timestep = 6777. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.16107884 -0.11822811  0.10481492 -0.02809447]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 6777 is [False, False, True, True, False, False]
Current timestep = 6778. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.08386353 -0.19278437  0.23406112  0.7754903 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 6778 is [False, False, True, True, False, False]
Scene graph at timestep 6778 is [False, False, True, True, False, False]
State prediction error at timestep 6778 is tensor(1.6871e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6779. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.10088956 -0.05297506  0.17825389 -0.26408505]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 6779 is [False, False, True, True, False, False]
Scene graph at timestep 6779 is [False, False, True, True, False, False]
State prediction error at timestep 6779 is tensor(3.5514e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6780. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.02158387 -0.11720258 -0.17984214 -0.7392601 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 6780 is [False, False, True, True, False, False]
Current timestep = 6781. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.12015933 -0.14079867 -0.23573723  0.7602279 ]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 6781 is [False, False, True, True, False, False]
Current timestep = 6782. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.0590722  -0.00976676  0.24825203 -0.90329456]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 6782 is [False, False, True, True, False, False]
Current timestep = 6783. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.0397865  -0.2125282   0.13893312  0.775923  ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 6783 is [False, False, True, True, False, False]
Current timestep = 6784. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.14871562 -0.13382693  0.1836896  -0.33057785]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 6784 is [False, False, True, True, False, False]
Current timestep = 6785. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.01302603 -0.21677268  0.01573059  0.58549404]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 6785 is [False, False, True, True, False, False]
Current timestep = 6786. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.12837376 -0.07686417  0.01418138  0.9487283 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 6786 is [False, False, True, True, False, False]
Current timestep = 6787. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.07966319 -0.06418656 -0.24518487  0.6497607 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 6787 is [False, False, True, True, False, False]
Current timestep = 6788. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.07265463 -0.0758349  -0.08563933  0.8683536 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 6788 is [False, False, True, True, False, False]
Current timestep = 6789. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.12001991 -0.04093851 -0.21476841  0.92020535]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 6789 is [False, False, True, True, False, False]
Current timestep = 6790. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.10237232 -0.13577795 -0.05482019  0.61293817]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 6790 is [False, False, True, True, False, False]
Current timestep = 6791. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.06601855 -0.03248395 -0.1262882  -0.6575956 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 6791 is [False, False, True, True, False, False]
Current timestep = 6792. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.03289461 -0.14413257 -0.21559389 -0.1710875 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 6792 is [False, False, True, True, False, False]
Current timestep = 6793. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.14452296 -0.21893153  0.10102963  0.03921485]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 6793 is [False, False, True, True, False, False]
Current timestep = 6794. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.01767123 -0.08925352 -0.23370521  0.49428606]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 6794 is [False, False, True, True, False, False]
Current timestep = 6795. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.11493906 -0.12194988  0.05084175  0.4935875 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 6795 is [False, False, True, True, False, False]
Current timestep = 6796. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.19652905 -0.20230955 -0.14594765 -0.273085  ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 6796 is [False, False, True, True, False, False]
Current timestep = 6797. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.00548647 -0.21815063 -0.0851935   0.28067446]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 6797 is [False, False, True, True, False, False]
Current timestep = 6798. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.10368815 -0.14532018 -0.20455001  0.69491076]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 6798 is [False, False, True, True, False, False]
Current timestep = 6799. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.1280956  -0.1770431  -0.06364048 -0.25665045]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 6799 is [False, False, True, True, False, False]
Scene graph at timestep 6799 is [False, False, True, True, False, False]
State prediction error at timestep 6799 is tensor(5.8105e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6800. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.09295574 -0.19132797  0.2472049  -0.15438354]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 6800 is [False, False, True, True, False, False]
Current timestep = 6801. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.09386952 -0.11380628 -0.24630907  0.5909362 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 6801 is [False, False, True, True, False, False]
Current timestep = 6802. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.19305618 -0.22087018 -0.22065118  0.19664013]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 6802 is [False, False, True, True, False, False]
Current timestep = 6803. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.19646136 -0.18804638  0.24901462  0.9275322 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 6803 is [False, False, True, True, False, False]
Current timestep = 6804. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.20559537 -0.17314118 -0.08281144 -0.31676096]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 6804 is [False, False, True, True, False, False]
Current timestep = 6805. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.03075725 -0.228944    0.16877764  0.84970427]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 6805 is [False, False, True, True, False, False]
Current timestep = 6806. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.03092189 -0.1889211   0.2411449  -0.4901682 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 6806 is [False, False, True, True, False, False]
Current timestep = 6807. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.00177528 -0.12564147 -0.24517396  0.6860472 ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 6807 is [False, False, True, True, False, False]
Scene graph at timestep 6807 is [False, False, True, True, False, False]
State prediction error at timestep 6807 is tensor(1.9228e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6808. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.12426108 -0.22235313 -0.17054392  0.42984533]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 6808 is [False, False, True, True, False, False]
Current timestep = 6809. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.00602877 -0.20826522  0.242342    0.22887397]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 6809 is [False, False, True, True, False, False]
Scene graph at timestep 6809 is [False, False, True, True, False, False]
State prediction error at timestep 6809 is tensor(6.1460e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6810. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.12706842 -0.21216723 -0.10704398  0.52718043]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 6810 is [False, False, True, True, False, False]
Current timestep = 6811. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.09115922 -0.2166459  -0.20741239  0.82257044]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 6811 is [False, False, True, True, False, False]
Scene graph at timestep 6811 is [False, False, True, True, False, False]
State prediction error at timestep 6811 is tensor(2.2285e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6812. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.01396571 -0.13982704  0.24805152  0.2276119 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 6812 is [False, False, True, True, False, False]
Scene graph at timestep 6812 is [False, False, True, True, False, False]
State prediction error at timestep 6812 is tensor(5.0142e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6813. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.09180376 -0.16642533  0.22057915  0.35555232]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 6813 is [False, False, True, True, False, False]
Current timestep = 6814. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.08643258 -0.2225686  -0.23318554  0.75060105]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 6814 is [False, False, True, True, False, False]
Current timestep = 6815. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.01624526 -0.22923468 -0.23338495 -0.34794497]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 6815 is [False, False, True, True, False, False]
Current timestep = 6816. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.05796054 -0.18854922  0.24860525 -0.47468448]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 6816 is [False, False, True, True, False, False]
Current timestep = 6817. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.01806383  0.06059062 -0.16700596  0.3018782 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 6817 is [False, False, True, True, False, False]
Current timestep = 6818. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.03912565 -0.1497652   0.24911517 -0.39510787]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 6818 is [False, False, True, True, False, False]
Scene graph at timestep 6818 is [False, False, True, True, False, False]
State prediction error at timestep 6818 is tensor(9.0233e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6819. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.05530089 -0.19666982  0.15680352  0.40013218]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 6819 is [False, False, True, True, False, False]
Current timestep = 6820. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.0077168  -0.22673874  0.24201149  0.7575705 ]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 6820 is [False, False, True, True, False, False]
Current timestep = 6821. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.20425704 -0.1608966   0.24312922  0.08081722]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 6821 is [False, False, True, True, False, False]
Current timestep = 6822. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.06820261 -0.12490003 -0.17567529 -0.53425264]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 6822 is [False, False, True, True, False, False]
Current timestep = 6823. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.11373326 -0.12222245  0.1999133   0.23646867]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 6823 is [False, False, True, True, False, False]
Current timestep = 6824. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.11369306 -0.21451417  0.2126768  -0.31911635]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 6824 is [False, False, True, True, False, False]
Current timestep = 6825. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.02802378 -0.21061637 -0.24996112  0.44331098]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 6825 is [False, False, True, True, False, False]
Current timestep = 6826. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.04901484 -0.08968067 -0.24533579  0.43866444]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 6826 is [False, False, True, True, False, False]
Current timestep = 6827. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.06110778 -0.19023734  0.24601817  0.6824722 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 6827 is [False, False, True, True, False, False]
Current timestep = 6828. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.07877541 -0.23197816 -0.09970593  0.1815362 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 6828 is [False, False, True, True, False, False]
Current timestep = 6829. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.00204191 -0.15556657  0.24882579  0.4100535 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 6829 is [False, False, True, True, False, False]
Current timestep = 6830. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.04322544 -0.17598596  0.21259809  0.42489028]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 6830 is [False, False, True, True, False, False]
Current timestep = 6831. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.04327847 -0.19204503  0.24759406  0.59411204]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 6831 is [False, False, True, True, False, False]
Current timestep = 6832. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.02076817 -0.15089148 -0.13540305  0.5276656 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 6832 is [False, False, True, True, False, False]
Scene graph at timestep 6832 is [False, False, True, True, False, False]
State prediction error at timestep 6832 is tensor(2.5088e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6833. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.0252929  -0.16657779  0.21554524  0.64452744]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 6833 is [False, False, True, True, False, False]
Scene graph at timestep 6833 is [False, False, True, True, False, False]
State prediction error at timestep 6833 is tensor(1.1285e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6834. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.04162383 -0.2419777  -0.24966462  0.9026556 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 6834 is [False, False, True, True, False, False]
Current timestep = 6835. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.05469467 -0.19494742 -0.24774313  0.6038518 ]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 6835 is [False, False, True, True, False, False]
Scene graph at timestep 6835 is [False, False, True, True, False, False]
State prediction error at timestep 6835 is tensor(6.7392e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6836. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.02301958 -0.16863714  0.21682444  0.2838695 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 6836 is [False, False, True, True, False, False]
Current timestep = 6837. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.01259756 -0.12698738 -0.23972102  0.61893106]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 6837 is [False, False, True, True, False, False]
Current timestep = 6838. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.06571995 -0.20712914  0.07922983  0.35457587]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 6838 is [False, False, True, True, False, False]
Current timestep = 6839. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.11326328 -0.13629287  0.24954373  0.27915633]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 6839 is [False, False, True, True, False, False]
Scene graph at timestep 6839 is [False, False, True, True, False, False]
State prediction error at timestep 6839 is tensor(2.7488e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6840. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.08049941 -0.10351121  0.2370922   0.21499217]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 6840 is [False, False, True, True, False, False]
Scene graph at timestep 6840 is [False, False, True, True, False, False]
State prediction error at timestep 6840 is tensor(2.0164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6841. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.05339143 -0.18104129  0.22893035  0.46565974]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 6841 is [False, False, True, True, False, False]
Scene graph at timestep 6841 is [False, False, True, True, False, False]
State prediction error at timestep 6841 is tensor(8.0119e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6842. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.09649104 -0.2005259   0.04556543  0.88591254]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 6842 is [False, False, True, True, False, False]
Current timestep = 6843. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.04393977 -0.1980758   0.14956713  0.5407429 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 6843 is [False, False, True, True, False, False]
Current timestep = 6844. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.06377262 -0.17680256 -0.24988881  0.12838137]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 6844 is [False, False, True, True, False, False]
Scene graph at timestep 6844 is [False, False, True, True, False, False]
State prediction error at timestep 6844 is tensor(6.3555e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6845. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.08283994 -0.13107012 -0.08658674  0.32938957]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 6845 is [False, False, True, True, False, False]
Current timestep = 6846. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.1477337  -0.17346472  0.22727737  0.06201923]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 6846 is [False, False, True, True, False, False]
Current timestep = 6847. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.05675863 -0.20260309 -0.11451173  0.79727006]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 6847 is [False, False, True, True, False, False]
Scene graph at timestep 6847 is [False, False, True, True, False, False]
State prediction error at timestep 6847 is tensor(3.4632e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6848. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.03610221 -0.16659892  0.16413045  0.54419374]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 6848 is [False, False, True, True, False, False]
Scene graph at timestep 6848 is [False, False, True, True, False, False]
State prediction error at timestep 6848 is tensor(2.8528e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6849. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.18074581 -0.11054422  0.20050722  0.3139391 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 6849 is [False, False, True, True, False, False]
Current timestep = 6850. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.01316801 -0.21225871  0.23883611  0.42348468]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 6850 is [False, False, True, True, False, False]
Current timestep = 6851. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.0275321  -0.03606644  0.11791882  0.46858263]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 6851 is [False, False, True, True, False, False]
Scene graph at timestep 6851 is [False, False, True, True, False, False]
State prediction error at timestep 6851 is tensor(1.5294e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6852. State = [[ 0.09151849 -0.17249602]]. Action = [[ 0.00640729 -0.2065976  -0.24981146  0.62305033]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 6852 is [False, False, True, True, False, False]
Scene graph at timestep 6852 is [False, False, True, True, False, False]
State prediction error at timestep 6852 is tensor(2.6211e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6853. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.03486252 -0.15240778 -0.23817162  0.6197449 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 6853 is [False, False, True, True, False, False]
Current timestep = 6854. State = [[ 0.09151849 -0.17249602]]. Action = [[-0.11769196 -0.20471816 -0.2486912   0.7361798 ]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 6854 is [False, False, True, True, False, False]
Current timestep = 6855. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06198414 -0.19558024 -0.23536028  0.53522277]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 6855 is [False, False, True, True, False, False]
Current timestep = 6856. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07676515 -0.19064325 -0.15551858  0.46371925]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 6856 is [False, False, True, True, False, False]
Scene graph at timestep 6856 is [False, False, True, True, False, False]
State prediction error at timestep 6856 is tensor(2.4311e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6857. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.09533218 -0.23393416  0.21410692  0.05455077]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 6857 is [False, False, True, True, False, False]
Current timestep = 6858. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01643366 -0.14012527 -0.01878326  0.39047754]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 6858 is [False, False, True, True, False, False]
Current timestep = 6859. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03264743 -0.20830548  0.20539677  0.551301  ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 6859 is [False, False, True, True, False, False]
Scene graph at timestep 6859 is [False, False, True, True, False, False]
State prediction error at timestep 6859 is tensor(3.6449e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6860. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02375458 -0.15254681 -0.2499887   0.838222  ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 6860 is [False, False, True, True, False, False]
Current timestep = 6861. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07012573 -0.11138615 -0.2499885   0.7135421 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 6861 is [False, False, True, True, False, False]
Current timestep = 6862. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.1192722  -0.15278082  0.22402734  0.04793608]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 6862 is [False, False, True, True, False, False]
Current timestep = 6863. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05920956 -0.10056999 -0.10176235  0.588477  ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 6863 is [False, False, True, True, False, False]
Current timestep = 6864. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.0281589  -0.02451725  0.24521351  0.4282869 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 6864 is [False, False, True, True, False, False]
Current timestep = 6865. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07515851 -0.20415099 -0.24994896  0.29388964]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 6865 is [False, False, True, True, False, False]
Current timestep = 6866. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02166429 -0.00594199 -0.24103904  0.5991187 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 6866 is [False, False, True, True, False, False]
Current timestep = 6867. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.0638184  -0.19242574 -0.04556122  0.571656  ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 6867 is [False, False, True, True, False, False]
Current timestep = 6868. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.09576523 -0.18678561  0.21082997  0.5258056 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 6868 is [False, False, True, True, False, False]
Scene graph at timestep 6868 is [False, False, True, True, False, False]
State prediction error at timestep 6868 is tensor(2.6231e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6869. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05035996 -0.1787058   0.12264025  0.5199976 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 6869 is [False, False, True, True, False, False]
Scene graph at timestep 6869 is [False, False, True, True, False, False]
State prediction error at timestep 6869 is tensor(3.9348e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6870. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.07552454  0.06541303 -0.24998173  0.29632223]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 6870 is [False, False, True, True, False, False]
Current timestep = 6871. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05435869 -0.10253105 -0.24415202  0.55143523]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 6871 is [False, False, True, True, False, False]
Current timestep = 6872. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03933233 -0.16946213 -0.24594769 -0.03989983]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 6872 is [False, False, True, True, False, False]
Current timestep = 6873. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01242661 -0.1271161  -0.10851812  0.6453059 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 6873 is [False, False, True, True, False, False]
Current timestep = 6874. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.08645654 -0.18009932  0.230667    0.5712166 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 6874 is [False, False, True, True, False, False]
Scene graph at timestep 6874 is [False, False, True, True, False, False]
State prediction error at timestep 6874 is tensor(9.3431e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6875. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.08733597 -0.18636946 -0.1910466   0.46553302]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 6875 is [False, False, True, True, False, False]
Scene graph at timestep 6875 is [False, False, True, True, False, False]
State prediction error at timestep 6875 is tensor(1.6885e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6876. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01963997 -0.17834286 -0.1078386   0.80609417]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 6876 is [False, False, True, True, False, False]
Current timestep = 6877. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.06254369 -0.14735946  0.22570986  0.23881507]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 6877 is [False, False, True, True, False, False]
Current timestep = 6878. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02393481 -0.19275177 -0.24551037  0.33108377]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 6878 is [False, False, True, True, False, False]
Current timestep = 6879. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05223948 -0.19052227 -0.23022921  0.68946433]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 6879 is [False, False, True, True, False, False]
Scene graph at timestep 6879 is [False, False, True, True, False, False]
State prediction error at timestep 6879 is tensor(1.2919e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6880. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.06167662 -0.2035502  -0.15731317  0.52725244]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 6880 is [False, False, True, True, False, False]
Current timestep = 6881. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05167931 -0.14186211 -0.23022749 -0.06049126]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 6881 is [False, False, True, True, False, False]
Current timestep = 6882. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02291527 -0.19919005  0.22576258  0.24068356]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 6882 is [False, False, True, True, False, False]
Current timestep = 6883. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03337498 -0.21718942  0.13387215  0.70390296]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 6883 is [False, False, True, True, False, False]
Current timestep = 6884. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.06151187 -0.23871353  0.13292429  0.4140265 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 6884 is [False, False, True, True, False, False]
Scene graph at timestep 6884 is [False, False, True, True, False, False]
State prediction error at timestep 6884 is tensor(2.8416e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6885. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02190161 -0.18186294 -0.17057957  0.33922458]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 6885 is [False, False, True, True, False, False]
Current timestep = 6886. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01685463 -0.17465898  0.22791004  0.5802932 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 6886 is [False, False, True, True, False, False]
Current timestep = 6887. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.11986935 -0.17051785  0.07335138  0.36708093]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 6887 is [False, False, True, True, False, False]
Scene graph at timestep 6887 is [False, False, True, True, False, False]
State prediction error at timestep 6887 is tensor(5.2207e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6888. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00811833 -0.15970172  0.20269424  0.43078506]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 6888 is [False, False, True, True, False, False]
Current timestep = 6889. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02318579 -0.10515332  0.23767072  0.41762936]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 6889 is [False, False, True, True, False, False]
Current timestep = 6890. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05505729 -0.18645836  0.24960953  0.67717075]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 6890 is [False, False, True, True, False, False]
Current timestep = 6891. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00140232 -0.20054142  0.23725072  0.5963483 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 6891 is [False, False, True, True, False, False]
Current timestep = 6892. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00051659 -0.19785982 -0.24667099  0.20949459]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 6892 is [False, False, True, True, False, False]
Scene graph at timestep 6892 is [False, False, True, True, False, False]
State prediction error at timestep 6892 is tensor(1.7100e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6893. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04167612 -0.21687949 -0.24873196  0.6823361 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 6893 is [False, False, True, True, False, False]
Current timestep = 6894. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00120254 -0.19081889  0.02891713  0.4050299 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 6894 is [False, False, True, True, False, False]
Current timestep = 6895. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.0209564  -0.17751987  0.24994552  0.51006436]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 6895 is [False, False, True, True, False, False]
Current timestep = 6896. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02354503 -0.10526901 -0.19483855  0.48646605]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 6896 is [False, False, True, True, False, False]
Scene graph at timestep 6896 is [False, False, True, True, False, False]
State prediction error at timestep 6896 is tensor(1.2195e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6897. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.10702616 -0.1641204  -0.23196761  0.38803697]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 6897 is [False, False, True, True, False, False]
Current timestep = 6898. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01655105 -0.1488424  -0.21897735  0.3350191 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 6898 is [False, False, True, True, False, False]
Current timestep = 6899. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04730818 -0.12173411 -0.24345589  0.5516082 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 6899 is [False, False, True, True, False, False]
Current timestep = 6900. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00436327 -0.13842635  0.24972117  0.31713676]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 6900 is [False, False, True, True, False, False]
Current timestep = 6901. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.0460217  -0.1973252   0.18302837  0.38945472]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 6901 is [False, False, True, True, False, False]
Scene graph at timestep 6901 is [False, False, True, True, False, False]
State prediction error at timestep 6901 is tensor(1.4623e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6902. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05538929 -0.20411532 -0.07694235  0.25307178]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 6902 is [False, False, True, True, False, False]
Current timestep = 6903. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02707902 -0.10429682  0.24776882  0.7058921 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 6903 is [False, False, True, True, False, False]
Current timestep = 6904. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01314099 -0.11105534 -0.21657863  0.5158552 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 6904 is [False, False, True, True, False, False]
Scene graph at timestep 6904 is [False, False, True, True, False, False]
State prediction error at timestep 6904 is tensor(1.1566e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6905. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06936449 -0.21238011  0.24061406  0.57051015]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 6905 is [False, False, True, True, False, False]
Current timestep = 6906. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00128323 -0.195135   -0.24404189  0.6853924 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 6906 is [False, False, True, True, False, False]
Current timestep = 6907. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.09022003 -0.12916096 -0.2235983   0.65883756]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 6907 is [False, False, True, True, False, False]
Current timestep = 6908. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00739859 -0.14786223  0.15072468  0.43062305]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 6908 is [False, False, True, True, False, False]
Current timestep = 6909. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05404855 -0.18910994 -0.15759088  0.50079274]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 6909 is [False, False, True, True, False, False]
Current timestep = 6910. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.12496343 -0.21443628  0.15864107  0.27951348]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 6910 is [False, False, True, True, False, False]
Current timestep = 6911. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01934657 -0.17586032  0.16802788  0.03068638]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 6911 is [False, False, True, True, False, False]
Current timestep = 6912. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06231578 -0.15692204 -0.08703639  0.5868974 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 6912 is [False, False, True, True, False, False]
Scene graph at timestep 6912 is [False, False, True, True, False, False]
State prediction error at timestep 6912 is tensor(1.3859e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6913. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00901413 -0.17744526 -0.00225805  0.47245395]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 6913 is [False, False, True, True, False, False]
Scene graph at timestep 6913 is [False, False, True, True, False, False]
State prediction error at timestep 6913 is tensor(2.7588e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6914. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01338334 -0.1309977   0.23871624  0.1939143 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 6914 is [False, False, True, True, False, False]
Scene graph at timestep 6914 is [False, False, True, True, False, False]
State prediction error at timestep 6914 is tensor(1.5624e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6915. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02332571 -0.17460328 -0.2425308   0.6620612 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 6915 is [False, False, True, True, False, False]
Current timestep = 6916. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05282225 -0.19961852 -0.2419004   0.32084823]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 6916 is [False, False, True, True, False, False]
Current timestep = 6917. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0489179  -0.17362235  0.05158809  0.41553986]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 6917 is [False, False, True, True, False, False]
Scene graph at timestep 6917 is [False, False, True, True, False, False]
State prediction error at timestep 6917 is tensor(1.4529e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6918. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.04959354 -0.16936865 -0.21805978  0.37987757]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 6918 is [False, False, True, True, False, False]
Current timestep = 6919. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00443453 -0.17177343 -0.13817587  0.526299  ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 6919 is [False, False, True, True, False, False]
Current timestep = 6920. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03437293 -0.13803665 -0.0498502   0.49506247]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 6920 is [False, False, True, True, False, False]
Scene graph at timestep 6920 is [False, False, True, True, False, False]
State prediction error at timestep 6920 is tensor(8.8362e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6921. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07476115 -0.1784445   0.22208792  0.4283154 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 6921 is [False, False, True, True, False, False]
Scene graph at timestep 6921 is [False, False, True, True, False, False]
State prediction error at timestep 6921 is tensor(1.6642e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6922. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0423924  -0.0584574   0.19884396  0.45913804]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 6922 is [False, False, True, True, False, False]
Current timestep = 6923. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00200388 -0.12475391 -0.10455295  0.45324194]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 6923 is [False, False, True, True, False, False]
Current timestep = 6924. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01818952 -0.2048985   0.24226868  0.41216278]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 6924 is [False, False, True, True, False, False]
Current timestep = 6925. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07533798 -0.16585837  0.1556254   0.5622628 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 6925 is [False, False, True, True, False, False]
Current timestep = 6926. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0615564  -0.19187656 -0.1581777   0.35554147]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 6926 is [False, False, True, True, False, False]
Scene graph at timestep 6926 is [False, False, True, True, False, False]
State prediction error at timestep 6926 is tensor(6.4206e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6927. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01807007 -0.16108315 -0.23278123  0.6348256 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 6927 is [False, False, True, True, False, False]
Current timestep = 6928. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02611074 -0.18246682  0.23849088  0.42944062]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 6928 is [False, False, True, True, False, False]
Current timestep = 6929. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.04309624 -0.18881448  0.02008793  0.19398856]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 6929 is [False, False, True, True, False, False]
Current timestep = 6930. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06436767 -0.15783235  0.1857121   0.45238698]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 6930 is [False, False, True, True, False, False]
Current timestep = 6931. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05972505 -0.20663029  0.23890254  0.35610604]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 6931 is [False, False, True, True, False, False]
Current timestep = 6932. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01511204 -0.18929836 -0.2455348   0.15828705]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 6932 is [False, False, True, True, False, False]
Current timestep = 6933. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01240925 -0.19181037  0.2098524   0.64454174]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 6933 is [False, False, True, True, False, False]
Current timestep = 6934. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01683457 -0.17746647  0.05030409  0.38636672]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 6934 is [False, False, True, True, False, False]
Current timestep = 6935. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02448182 -0.18458952  0.09385654  0.1172322 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 6935 is [False, False, True, True, False, False]
Current timestep = 6936. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0455164  -0.19004227 -0.19386227  0.6341771 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 6936 is [False, False, True, True, False, False]
Current timestep = 6937. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06562953 -0.11654425 -0.04603609  0.32881796]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 6937 is [False, False, True, True, False, False]
Current timestep = 6938. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.04933429 -0.15796338 -0.10896358  0.7397859 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 6938 is [False, False, True, True, False, False]
Scene graph at timestep 6938 is [False, False, True, True, False, False]
State prediction error at timestep 6938 is tensor(1.8307e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6939. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00170888 -0.19673765  0.0426082   0.5852926 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 6939 is [False, False, True, True, False, False]
Current timestep = 6940. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06577888 -0.15831442  0.12963542  0.47219634]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 6940 is [False, False, True, True, False, False]
Current timestep = 6941. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03678364 -0.2265176   0.2178866   0.49897802]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 6941 is [False, False, True, True, False, False]
Current timestep = 6942. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.08905149 -0.19857106 -0.22258744  0.13238919]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 6942 is [False, False, True, True, False, False]
Current timestep = 6943. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06796935 -0.16661114 -0.13120243  0.2961774 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 6943 is [False, False, True, True, False, False]
Current timestep = 6944. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07129169 -0.10692564 -0.09965661  0.5414958 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 6944 is [False, False, True, True, False, False]
Scene graph at timestep 6944 is [False, False, True, True, False, False]
State prediction error at timestep 6944 is tensor(1.8866e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6945. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02491206 -0.2080364   0.03070983  0.28929257]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 6945 is [False, False, True, True, False, False]
Current timestep = 6946. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00380187 -0.18513784  0.21831685  0.17708623]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 6946 is [False, False, True, True, False, False]
Current timestep = 6947. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05256343 -0.17502624  0.22199407  0.35525382]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 6947 is [False, False, True, True, False, False]
Current timestep = 6948. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00275293 -0.18812951 -0.212447    0.01898789]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 6948 is [False, False, True, True, False, False]
Current timestep = 6949. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.07691637 -0.21332516  0.1393081   0.14518905]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 6949 is [False, False, True, True, False, False]
Current timestep = 6950. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06049204 -0.10553087  0.039399    0.65353703]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 6950 is [False, False, True, True, False, False]
Current timestep = 6951. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03431076 -0.15734127  0.20744199  0.597813  ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 6951 is [False, False, True, True, False, False]
Current timestep = 6952. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01869361 -0.14275627 -0.02439597  0.48130226]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 6952 is [False, False, True, True, False, False]
Scene graph at timestep 6952 is [False, False, True, True, False, False]
State prediction error at timestep 6952 is tensor(3.4223e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6953. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03702298 -0.22054838 -0.24593726  0.31427717]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 6953 is [False, False, True, True, False, False]
Current timestep = 6954. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0196043  -0.11379673 -0.24321155  0.3055272 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 6954 is [False, False, True, True, False, False]
Scene graph at timestep 6954 is [False, False, True, True, False, False]
State prediction error at timestep 6954 is tensor(1.9622e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6955. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05364287 -0.14100218 -0.2334165   0.59177303]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 6955 is [False, False, True, True, False, False]
Current timestep = 6956. State = [[ 0.09156204 -0.17249864]]. Action = [[-2.8564036e-04 -1.6427360e-01 -3.3887342e-02  4.1028416e-01]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 6956 is [False, False, True, True, False, False]
Current timestep = 6957. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01672986 -0.17074862 -0.24805228  0.27109015]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 6957 is [False, False, True, True, False, False]
Current timestep = 6958. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.04502627 -0.23180482 -0.11881921  0.29653502]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 6958 is [False, False, True, True, False, False]
Scene graph at timestep 6958 is [False, False, True, True, False, False]
State prediction error at timestep 6958 is tensor(1.5744e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6959. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07941833 -0.21350902 -0.20530708  0.36778903]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 6959 is [False, False, True, True, False, False]
Current timestep = 6960. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0051493  -0.16621836  0.1560044   0.17168367]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 6960 is [False, False, True, True, False, False]
Current timestep = 6961. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05846691 -0.10842958 -0.06742641  0.22449529]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 6961 is [False, False, True, True, False, False]
Scene graph at timestep 6961 is [False, False, True, True, False, False]
State prediction error at timestep 6961 is tensor(2.1738e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6962. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.04131025 -0.21784258  0.23545024  0.6359931 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 6962 is [False, False, True, True, False, False]
Current timestep = 6963. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06145817 -0.17728613 -0.15966041  0.45194435]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 6963 is [False, False, True, True, False, False]
Current timestep = 6964. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04180568 -0.14121144 -0.1578634   0.46615553]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 6964 is [False, False, True, True, False, False]
Current timestep = 6965. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00955799 -0.063704    0.16398835  0.4542203 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 6965 is [False, False, True, True, False, False]
Scene graph at timestep 6965 is [False, False, True, True, False, False]
State prediction error at timestep 6965 is tensor(2.5712e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6966. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05485569 -0.18526314 -0.2499059   0.46327543]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 6966 is [False, False, True, True, False, False]
Scene graph at timestep 6966 is [False, False, True, True, False, False]
State prediction error at timestep 6966 is tensor(3.9039e-07, grad_fn=<MseLossBackward0>)
Current timestep = 6967. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.13548802 -0.20101602 -0.20031542  0.2008568 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 6967 is [False, False, True, True, False, False]
Current timestep = 6968. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01684743 -0.1911566   0.0829187   0.46841836]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 6968 is [False, False, True, True, False, False]
Current timestep = 6969. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0048237  -0.17096187 -0.09618443  0.3294233 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 6969 is [False, False, True, True, False, False]
Scene graph at timestep 6969 is [False, False, True, True, False, False]
State prediction error at timestep 6969 is tensor(5.4862e-08, grad_fn=<MseLossBackward0>)
Current timestep = 6970. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00295794 -0.03106493 -0.16615187  0.19518077]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 6970 is [False, False, True, True, False, False]
Current timestep = 6971. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02763565 -0.19324383  0.04799175  0.46659875]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 6971 is [False, False, True, True, False, False]
Current timestep = 6972. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03196692 -0.17580011 -0.01480444  0.35159814]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 6972 is [False, False, True, True, False, False]
Current timestep = 6973. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03811985 -0.20040579 -0.24313691  0.42801607]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 6973 is [False, False, True, True, False, False]
Scene graph at timestep 6973 is [False, False, True, True, False, False]
State prediction error at timestep 6973 is tensor(1.6996e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6974. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01916549 -0.15988322  0.24442887  0.60111856]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 6974 is [False, False, True, True, False, False]
Scene graph at timestep 6974 is [False, False, True, True, False, False]
State prediction error at timestep 6974 is tensor(1.6782e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6975. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.08422397 -0.16706087  0.2343558   0.63306904]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 6975 is [False, False, True, True, False, False]
Current timestep = 6976. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03476839 -0.145813   -0.2149592   0.48911262]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 6976 is [False, False, True, True, False, False]
Current timestep = 6977. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02857877 -0.15791982 -0.17562915  0.3067354 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 6977 is [False, False, True, True, False, False]
Current timestep = 6978. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02080512 -0.21536191 -0.09018838  0.17919207]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 6978 is [False, False, True, True, False, False]
Scene graph at timestep 6978 is [False, False, True, True, False, False]
State prediction error at timestep 6978 is tensor(2.2542e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6979. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05162686 -0.18768929 -0.20740776  0.52181435]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 6979 is [False, False, True, True, False, False]
Current timestep = 6980. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02441749 -0.15837973  0.19857025  0.20687485]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 6980 is [False, False, True, True, False, False]
Scene graph at timestep 6980 is [False, False, True, True, False, False]
State prediction error at timestep 6980 is tensor(2.9496e-06, grad_fn=<MseLossBackward0>)
Current timestep = 6981. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.07813865 -0.20131719  0.01739261  0.45977092]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 6981 is [False, False, True, True, False, False]
Current timestep = 6982. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03415272 -0.19718203 -0.24361724  0.72396874]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 6982 is [False, False, True, True, False, False]
Current timestep = 6983. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01194423 -0.20057873 -0.03742726  0.4077518 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 6983 is [False, False, True, True, False, False]
Current timestep = 6984. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01451851 -0.21746454  0.19047749  0.02233541]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 6984 is [False, False, True, True, False, False]
Current timestep = 6985. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.0410625  -0.20704679 -0.23046033  0.2650268 ]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 6985 is [False, False, True, True, False, False]
Current timestep = 6986. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01526058 -0.12612171 -0.11896758  0.4506979 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 6986 is [False, False, True, True, False, False]
Current timestep = 6987. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00939175 -0.17889673 -0.0087907   0.5096388 ]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 6987 is [False, False, True, True, False, False]
Current timestep = 6988. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0381304  -0.18052216  0.01332441  0.42502403]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 6988 is [False, False, True, True, False, False]
Current timestep = 6989. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04070361 -0.21316431 -0.17203096  0.2810117 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 6989 is [False, False, True, True, False, False]
Current timestep = 6990. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03277868 -0.11636299  0.03722396  0.3262397 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 6990 is [False, False, True, True, False, False]
Current timestep = 6991. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0259324  -0.20931832  0.05681577  0.41120553]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 6991 is [False, False, True, True, False, False]
Current timestep = 6992. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00807235 -0.1415422  -0.18231802  0.35015154]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 6992 is [False, False, True, True, False, False]
Current timestep = 6993. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03468859 -0.18839146  0.1615698  -0.14783168]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 6993 is [False, False, True, True, False, False]
Current timestep = 6994. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04666294 -0.17766799 -0.24649723  0.49724627]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 6994 is [False, False, True, True, False, False]
Current timestep = 6995. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03484741 -0.13291441 -0.24910802  0.1789267 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 6995 is [False, False, True, True, False, False]
Current timestep = 6996. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01348239 -0.20810813  0.23711058  0.04476547]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 6996 is [False, False, True, True, False, False]
Current timestep = 6997. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00504178 -0.16606146 -0.00832751  0.39531803]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 6997 is [False, False, True, True, False, False]
Current timestep = 6998. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05736533 -0.13731924 -0.12558004  0.30564666]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 6998 is [False, False, True, True, False, False]
Current timestep = 6999. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00414692 -0.16061217  0.24107713  0.5243125 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 6999 is [False, False, True, True, False, False]
Scene graph at timestep 6999 is [False, False, True, True, False, False]
State prediction error at timestep 6999 is tensor(1.3814e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7000. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02046803 -0.1776587   0.23281157 -0.06025374]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 7000 is [False, False, True, True, False, False]
Current timestep = 7001. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00090419 -0.20265196  0.15514868  0.41581047]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 7001 is [False, False, True, True, False, False]
Current timestep = 7002. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.05196634 -0.13651505 -0.15110053  0.46351492]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 7002 is [False, False, True, True, False, False]
Current timestep = 7003. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05117023 -0.17250296  0.02107167  0.59264517]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 7003 is [False, False, True, True, False, False]
Current timestep = 7004. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04818162 -0.20926745  0.16252187  0.4600023 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 7004 is [False, False, True, True, False, False]
Scene graph at timestep 7004 is [False, False, True, True, False, False]
State prediction error at timestep 7004 is tensor(1.6593e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7005. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01507023 -0.19900085 -0.08395484  0.11546886]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 7005 is [False, False, True, True, False, False]
Current timestep = 7006. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.0036487  -0.18381833  0.22243607  0.28743637]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 7006 is [False, False, True, True, False, False]
Current timestep = 7007. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.06124216 -0.11645496 -0.1972519   0.26136887]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 7007 is [False, False, True, True, False, False]
Scene graph at timestep 7007 is [False, False, True, True, False, False]
State prediction error at timestep 7007 is tensor(2.2720e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7008. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01907556 -0.18215649 -0.21140859 -0.12391561]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 7008 is [False, False, True, True, False, False]
Scene graph at timestep 7008 is [False, False, True, True, False, False]
State prediction error at timestep 7008 is tensor(9.2450e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7009. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00264445 -0.11633605 -0.17652467  0.3454119 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 7009 is [False, False, True, True, False, False]
Current timestep = 7010. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02157341 -0.19419026 -0.24482544  0.521106  ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 7010 is [False, False, True, True, False, False]
Current timestep = 7011. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.10473639 -0.13693465  0.22239527  0.32497275]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 7011 is [False, False, True, True, False, False]
Current timestep = 7012. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.07946718 -0.16233483 -0.15696862  0.4227035 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 7012 is [False, False, True, True, False, False]
Current timestep = 7013. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03662467 -0.14473823 -0.14631008  0.40321136]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 7013 is [False, False, True, True, False, False]
Current timestep = 7014. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.05147967 -0.16847305  0.11612391  0.06559825]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 7014 is [False, False, True, True, False, False]
Current timestep = 7015. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00533026 -0.11274344 -0.24281543  0.40039194]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 7015 is [False, False, True, True, False, False]
Current timestep = 7016. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.092693   -0.18635108  0.17045149  0.43483245]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 7016 is [False, False, True, True, False, False]
Current timestep = 7017. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01051503 -0.20537089  0.05442131  0.26141453]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 7017 is [False, False, True, True, False, False]
Current timestep = 7018. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.09940436 -0.13678898 -0.23663718  0.5536468 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 7018 is [False, False, True, True, False, False]
Current timestep = 7019. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00606525 -0.1580987   0.14667904  0.50177956]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 7019 is [False, False, True, True, False, False]
Current timestep = 7020. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.10011852 -0.19330662 -0.17925638  0.30310297]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 7020 is [False, False, True, True, False, False]
Current timestep = 7021. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04021257 -0.1589162   0.04054269  0.66836166]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 7021 is [False, False, True, True, False, False]
Current timestep = 7022. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02242631 -0.17658256  0.23186564  0.44802225]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 7022 is [False, False, True, True, False, False]
Current timestep = 7023. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02623618 -0.15713042 -0.22418156  0.3042624 ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 7023 is [False, False, True, True, False, False]
Current timestep = 7024. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04189491 -0.21900077 -0.14729084  0.2525655 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 7024 is [False, False, True, True, False, False]
Current timestep = 7025. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.124616   -0.20066217 -0.12375495  0.3678012 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 7025 is [False, False, True, True, False, False]
Scene graph at timestep 7025 is [False, False, True, True, False, False]
State prediction error at timestep 7025 is tensor(5.9962e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7026. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.04113288 -0.17166363  0.10057074  0.21835136]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 7026 is [False, False, True, True, False, False]
Scene graph at timestep 7026 is [False, False, True, True, False, False]
State prediction error at timestep 7026 is tensor(2.1339e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7027. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01495072 -0.17522869 -0.23731378  0.5499456 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 7027 is [False, False, True, True, False, False]
Current timestep = 7028. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03398117 -0.1795601   0.06100023  0.4992273 ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 7028 is [False, False, True, True, False, False]
Scene graph at timestep 7028 is [False, False, True, True, False, False]
State prediction error at timestep 7028 is tensor(1.6611e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7029. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.07166031 -0.13243833 -0.23628175  0.5298958 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 7029 is [False, False, True, True, False, False]
Current timestep = 7030. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.03644408 -0.1744242   0.00869277  0.58833504]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 7030 is [False, False, True, True, False, False]
Current timestep = 7031. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.0562399  -0.14285028 -0.19118828  0.6005571 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 7031 is [False, False, True, True, False, False]
Scene graph at timestep 7031 is [False, False, True, True, False, False]
State prediction error at timestep 7031 is tensor(2.0921e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7032. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.007191   -0.11835077 -0.20627636  0.04954243]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 7032 is [False, False, True, True, False, False]
Scene graph at timestep 7032 is [False, False, True, True, False, False]
State prediction error at timestep 7032 is tensor(3.4181e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7033. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01877449 -0.19027205 -0.23986249  0.4505571 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 7033 is [False, False, True, True, False, False]
Current timestep = 7034. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.06726116 -0.20407546 -0.24898408  0.17914605]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 7034 is [False, False, True, True, False, False]
Current timestep = 7035. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.08360356 -0.2005599  -0.22367613 -0.10269254]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 7035 is [False, False, True, True, False, False]
Current timestep = 7036. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03599826 -0.18491906  0.15801638  0.48509133]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 7036 is [False, False, True, True, False, False]
Current timestep = 7037. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.01546712 -0.14816329  0.2473816   0.39948213]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 7037 is [False, False, True, True, False, False]
Current timestep = 7038. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.00422192 -0.210399   -0.113205    0.4751904 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 7038 is [False, False, True, True, False, False]
Current timestep = 7039. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.01891464 -0.19371094 -0.24774463  0.23330653]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 7039 is [False, False, True, True, False, False]
Current timestep = 7040. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.02917188 -0.17320703  0.23413712  0.39334965]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 7040 is [False, False, True, True, False, False]
Scene graph at timestep 7040 is [False, False, True, True, False, False]
State prediction error at timestep 7040 is tensor(1.2285e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7041. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.03415596 -0.21750763  0.11685902  0.3998816 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 7041 is [False, False, True, True, False, False]
Current timestep = 7042. State = [[ 0.09156204 -0.17249864]]. Action = [[ 0.02829775 -0.21934678  0.22512406  0.12221837]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 7042 is [False, False, True, True, False, False]
Scene graph at timestep 7042 is [False, False, True, True, False, False]
State prediction error at timestep 7042 is tensor(5.2607e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7043. State = [[ 0.09156204 -0.17249864]]. Action = [[-0.00403325 -0.13035706 -0.10182151  0.19219089]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 7043 is [False, False, True, True, False, False]
Scene graph at timestep 7043 is [False, False, True, True, False, False]
State prediction error at timestep 7043 is tensor(4.2589e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7044. State = [[-0.22403833  0.05167016]]. Action = [[-0.04417166 -0.1671567   0.03747794  0.50806475]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 7044 is [False, False, True, True, False, False]
Current timestep = 7045. State = [[-0.22202836  0.05691506]]. Action = [[ 0.2444183  -0.10879602 -0.1595033  -0.2923482 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 7045 is [True, False, False, False, True, False]
Current timestep = 7046. State = [[-0.22022897  0.05695569]]. Action = [[ 0.22918242 -0.10518323 -0.14528392 -0.18277681]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 7046 is [True, False, False, False, True, False]
Current timestep = 7047. State = [[-0.21638401  0.05603069]]. Action = [[ 0.2321029  -0.10705045 -0.12770335 -0.38298368]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 7047 is [True, False, False, False, True, False]
Current timestep = 7048. State = [[-0.21073006  0.05445601]]. Action = [[ 0.23487842 -0.06581131 -0.1479834  -0.28242016]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 7048 is [True, False, False, False, True, False]
Current timestep = 7049. State = [[-0.20287444  0.05298011]]. Action = [[ 0.23963845 -0.11958551 -0.13163869 -0.2900144 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 7049 is [True, False, False, False, True, False]
Scene graph at timestep 7049 is [True, False, False, False, True, False]
State prediction error at timestep 7049 is tensor(1.1046e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7050. State = [[-0.19447938  0.05065824]]. Action = [[ 0.24885565 -0.0917953  -0.14180236 -0.3366921 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 7050 is [True, False, False, False, True, False]
Human Feedback received at timestep 7050 of 1
Current timestep = 7051. State = [[-0.18598846  0.04790848]]. Action = [[ 0.23558438 -0.10407197 -0.13703376 -0.33361906]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 7051 is [True, False, False, False, True, False]
Scene graph at timestep 7051 is [True, False, False, False, True, False]
State prediction error at timestep 7051 is tensor(2.0470e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7052. State = [[-0.17572646  0.04467418]]. Action = [[ 0.24922842 -0.09004603 -0.13303776 -0.38596034]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7052 is [True, False, False, False, True, False]
Current timestep = 7053. State = [[-0.16843355  0.04172859]]. Action = [[ 0.24556771 -0.10204467 -0.13321197 -0.20808417]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 7053 is [True, False, False, False, True, False]
Human Feedback received at timestep 7053 of 1
Current timestep = 7054. State = [[-0.15980676  0.03856084]]. Action = [[ 0.2470775  -0.11477646 -0.11919469 -0.24560732]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 7054 is [True, False, False, False, True, False]
Scene graph at timestep 7054 is [True, False, False, False, True, False]
State prediction error at timestep 7054 is tensor(1.7884e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7055. State = [[-0.14950599  0.03450394]]. Action = [[ 0.24465784 -0.11804113 -0.12152827 -0.25863785]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 7055 is [True, False, False, False, True, False]
Current timestep = 7056. State = [[-0.14105141  0.03063546]]. Action = [[ 0.24267936 -0.10068704 -0.1669687  -0.24391991]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 7056 is [True, False, False, False, True, False]
Human Feedback received at timestep 7056 of 1
Current timestep = 7057. State = [[-0.13215245  0.026886  ]]. Action = [[ 0.23985079 -0.06793143 -0.14481515 -0.32990062]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 7057 is [True, False, False, False, True, False]
Scene graph at timestep 7057 is [True, False, False, False, True, False]
State prediction error at timestep 7057 is tensor(1.8236e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7058. State = [[-0.12250969  0.02347184]]. Action = [[ 0.22632283 -0.11835584 -0.16408855 -0.28987122]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 7058 is [True, False, False, False, True, False]
Current timestep = 7059. State = [[-0.11501709  0.02033442]]. Action = [[ 0.22082293 -0.08338028 -0.14421153 -0.23842096]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 7059 is [True, False, False, False, True, False]
Human Feedback received at timestep 7059 of 1
Current timestep = 7060. State = [[-0.1067484   0.01706384]]. Action = [[ 0.21598971 -0.07567266 -0.14180472 -0.2251612 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 7060 is [True, False, False, False, True, False]
Current timestep = 7061. State = [[-0.09786446  0.01386136]]. Action = [[ 0.23668677 -0.09810846 -0.144596    0.01517642]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 7061 is [True, False, False, False, True, False]
Current timestep = 7062. State = [[-0.09001164  0.01094489]]. Action = [[ 0.23444018 -0.1338064  -0.12505306 -0.09233171]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 7062 is [True, False, False, False, True, False]
Human Feedback received at timestep 7062 of 1
Current timestep = 7063. State = [[-0.08199205  0.00685045]]. Action = [[ 0.24388236 -0.07587671 -0.12862863 -0.21878982]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 7063 is [True, False, False, False, True, False]
Current timestep = 7064. State = [[-0.07303012  0.00321054]]. Action = [[ 0.19947755 -0.07710244 -0.09464049 -0.04743123]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 7064 is [True, False, False, False, True, False]
Current timestep = 7065. State = [[-0.06494171 -0.00046124]]. Action = [[ 0.24233681 -0.14901823 -0.16086437 -0.00392503]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 7065 is [True, False, False, False, True, False]
Current timestep = 7066. State = [[-0.05655221 -0.00453236]]. Action = [[ 0.18336326 -0.05897856 -0.1166181   0.07393312]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 7066 is [True, False, False, False, True, False]
Current timestep = 7067. State = [[-0.04893501 -0.00757604]]. Action = [[ 0.15441757 -0.13513003 -0.09134071  0.2141155 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 7067 is [True, False, False, False, True, False]
Current timestep = 7068. State = [[-0.04138171 -0.01142322]]. Action = [[ 0.17659804 -0.12848438 -0.09988555  0.0245713 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 7068 is [False, True, False, False, True, False]
Human Feedback received at timestep 7068 of 1
Current timestep = 7069. State = [[-0.23577324 -0.10143919]]. Action = [[ 0.1522432  -0.11050852 -0.10915057 -0.05145085]]. Reward = [100.]
Curr episode timestep = 24
Scene graph at timestep 7069 is [False, True, False, False, True, False]
Current timestep = 7070. State = [[-0.23401682 -0.10728176]]. Action = [[ 0.23910171 -0.05895332 -0.14597186 -0.12435293]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 7070 is [True, False, False, False, True, False]
Current timestep = 7071. State = [[-0.2319432 -0.1077852]]. Action = [[ 0.22736281 -0.05420263 -0.15017737 -0.103257  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 7071 is [True, False, False, False, True, False]
Current timestep = 7072. State = [[-0.22845998 -0.10869472]]. Action = [[ 0.22123307 -0.04793242 -0.1362523  -0.18937385]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 7072 is [True, False, False, False, True, False]
Current timestep = 7073. State = [[-0.2230579  -0.10997389]]. Action = [[ 0.16328311 -0.00773984 -0.10867241 -0.17806739]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 7073 is [True, False, False, False, True, False]
Current timestep = 7074. State = [[-0.216626   -0.11114474]]. Action = [[ 0.24151173 -0.07102373 -0.14433402 -0.265109  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 7074 is [True, False, False, False, True, False]
Current timestep = 7075. State = [[-0.20957318 -0.11269465]]. Action = [[ 0.15247187 -0.07076469 -0.13800718 -0.18082237]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 7075 is [True, False, False, False, True, False]
Current timestep = 7076. State = [[-0.20217785 -0.11455574]]. Action = [[ 0.22727019 -0.08133003 -0.11502875 -0.15180427]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 7076 is [True, False, False, False, True, False]
Human Feedback received at timestep 7076 of 1
Current timestep = 7077. State = [[-0.19509327 -0.11642561]]. Action = [[ 0.22355878 -0.06958024 -0.10461682 -0.08652842]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7077 is [True, False, False, False, True, False]
Current timestep = 7078. State = [[-0.1876899 -0.1189495]]. Action = [[ 0.20831698 -0.02659625 -0.14765371 -0.3028351 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 7078 is [True, False, False, False, True, False]
Scene graph at timestep 7078 is [True, False, False, False, True, False]
State prediction error at timestep 7078 is tensor(7.2979e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7079. State = [[-0.18012984 -0.12114464]]. Action = [[ 0.22932208 -0.09075898 -0.12142748 -0.15902907]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 7079 is [True, False, False, False, True, False]
Current timestep = 7080. State = [[-0.17262848 -0.12370095]]. Action = [[ 0.23992887 -0.0461401  -0.15074591 -0.32330024]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 7080 is [True, False, False, False, True, False]
Current timestep = 7081. State = [[-0.16564322 -0.12592758]]. Action = [[ 0.23409948 -0.0905837  -0.12271491 -0.23207295]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 7081 is [True, False, False, False, True, False]
Current timestep = 7082. State = [[-0.15706368 -0.12890573]]. Action = [[ 0.23129392 -0.05137374 -0.14993557 -0.01857811]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 7082 is [True, False, False, True, False, False]
Current timestep = 7083. State = [[-0.14894822 -0.13151136]]. Action = [[ 0.2220105  -0.05977204 -0.11013177 -0.18841493]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 7083 is [True, False, False, True, False, False]
Current timestep = 7084. State = [[-0.1409412  -0.13394171]]. Action = [[ 0.2131918  -0.07405676 -0.1742254  -0.10933536]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 7084 is [True, False, False, True, False, False]
Current timestep = 7085. State = [[-0.13221847 -0.13660362]]. Action = [[ 0.21413451 -0.11929049 -0.09371579 -0.2734803 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 7085 is [True, False, False, True, False, False]
Current timestep = 7086. State = [[-0.12461464 -0.13972867]]. Action = [[ 0.20961896 -0.10789254 -0.08972074 -0.05720139]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 7086 is [True, False, False, True, False, False]
Current timestep = 7087. State = [[-0.1165488  -0.14342192]]. Action = [[ 0.18348044 -0.09160347 -0.13136236 -0.15804029]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 7087 is [True, False, False, True, False, False]
Current timestep = 7088. State = [[-0.10955332 -0.14665185]]. Action = [[ 0.21619281 -0.11368126 -0.00253199 -0.05012357]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 7088 is [True, False, False, True, False, False]
Human Feedback received at timestep 7088 of -1
Current timestep = 7089. State = [[-0.10159083 -0.15055025]]. Action = [[ 0.22798663 -0.1044818  -0.07760967 -0.3163702 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 7089 is [True, False, False, True, False, False]
Current timestep = 7090. State = [[-0.09360686 -0.15435812]]. Action = [[ 0.13617587 -0.07614586 -0.14779672  0.18258023]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 7090 is [True, False, False, True, False, False]
Current timestep = 7091. State = [[-0.08534124 -0.15840809]]. Action = [[ 0.20386803 -0.12732902 -0.12566303 -0.03806907]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 7091 is [True, False, False, True, False, False]
Scene graph at timestep 7091 is [True, False, False, True, False, False]
State prediction error at timestep 7091 is tensor(1.9810e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7091 of -1
Current timestep = 7092. State = [[-0.0772014  -0.16251811]]. Action = [[ 0.1567604  -0.12006715 -0.09005073  0.21339023]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 7092 is [True, False, False, True, False, False]
Current timestep = 7093. State = [[-0.07053575 -0.16676453]]. Action = [[ 0.16667032 -0.08972496 -0.19671595 -0.09191793]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 7093 is [True, False, False, True, False, False]
Scene graph at timestep 7093 is [True, False, False, True, False, False]
State prediction error at timestep 7093 is tensor(1.7299e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7093 of -1
Current timestep = 7094. State = [[-0.06391352 -0.17057584]]. Action = [[ 0.1195699  -0.14850652 -0.05107363  0.2494942 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 7094 is [True, False, False, True, False, False]
Current timestep = 7095. State = [[-0.05823507 -0.17494704]]. Action = [[ 1.5129656e-01 -1.3596345e-01 -2.1547079e-05  1.4110386e-01]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 7095 is [True, False, False, True, False, False]
Current timestep = 7096. State = [[-0.05284677 -0.17946933]]. Action = [[ 0.05813867 -0.18972614 -0.0806651   0.02758777]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 7096 is [True, False, False, True, False, False]
Scene graph at timestep 7096 is [True, False, False, True, False, False]
State prediction error at timestep 7096 is tensor(1.9239e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7096 of -1
Current timestep = 7097. State = [[-0.04780179 -0.18583104]]. Action = [[ 0.03421748 -0.12328464 -0.172614    0.16297233]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 7097 is [True, False, False, True, False, False]
Human Feedback received at timestep 7097 of -1
Current timestep = 7098. State = [[-0.04400317 -0.19109891]]. Action = [[ 0.06274289 -0.13512865 -0.04721406  0.17217124]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 7098 is [False, True, False, True, False, False]
Scene graph at timestep 7098 is [False, True, False, True, False, False]
State prediction error at timestep 7098 is tensor(2.3246e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7099. State = [[-0.04162056 -0.19641496]]. Action = [[ 0.1707663  -0.15081753 -0.09846446  0.46561098]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 7099 is [False, True, False, True, False, False]
Current timestep = 7100. State = [[-0.03838507 -0.20209199]]. Action = [[ 0.09819645 -0.11551216 -0.07662669  0.20497441]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 7100 is [False, True, False, True, False, False]
Current timestep = 7101. State = [[-0.0347366  -0.20629965]]. Action = [[ 0.1561907  -0.15794975 -0.17976062  0.2043122 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 7101 is [False, True, False, True, False, False]
Scene graph at timestep 7101 is [False, True, False, True, False, False]
State prediction error at timestep 7101 is tensor(1.6631e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7102. State = [[-0.02975531 -0.21156898]]. Action = [[ 0.04655463 -0.14403614 -0.01034716  0.06129467]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 7102 is [False, True, False, True, False, False]
Current timestep = 7103. State = [[-0.02638208 -0.21709849]]. Action = [[-0.00816904 -0.1683389  -0.17261212  0.13261104]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 7103 is [False, True, False, True, False, False]
Scene graph at timestep 7103 is [False, True, False, True, False, False]
State prediction error at timestep 7103 is tensor(1.7433e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7104. State = [[-0.02360059 -0.22307427]]. Action = [[ 0.09586507 -0.13597515 -0.11009552  0.24559093]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 7104 is [False, True, False, True, False, False]
Scene graph at timestep 7104 is [False, True, False, True, False, False]
State prediction error at timestep 7104 is tensor(2.7002e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7105. State = [[-0.020446   -0.22847748]]. Action = [[ 0.02821109 -0.1980634  -0.17380641  0.35987616]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 7105 is [False, True, False, True, False, False]
Human Feedback received at timestep 7105 of -1
Current timestep = 7106. State = [[-0.01853049 -0.23418842]]. Action = [[ 0.1112631  -0.14713083 -0.02523439  0.39411604]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 7106 is [False, True, False, True, False, False]
Scene graph at timestep 7106 is [False, True, False, True, False, False]
State prediction error at timestep 7106 is tensor(2.2309e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7107. State = [[-0.01530388 -0.24054435]]. Action = [[ 0.06932774 -0.1803879   0.04977816  0.18627787]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 7107 is [False, True, False, True, False, False]
Current timestep = 7108. State = [[-0.0126692  -0.24698971]]. Action = [[ 0.07547319 -0.19081573 -0.02290516  0.31355953]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 7108 is [False, True, False, True, False, False]
Current timestep = 7109. State = [[-0.01002599 -0.2529497 ]]. Action = [[ 0.04885262 -0.16898744 -0.13993567  0.22504234]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 7109 is [False, True, False, True, False, False]
Current timestep = 7110. State = [[-0.00796313 -0.25901452]]. Action = [[ 2.7775764e-05 -1.1500868e-01  1.2788242e-01  4.6701717e-01]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 7110 is [False, True, False, True, False, False]
Current timestep = 7111. State = [[-0.00547277 -0.2650019 ]]. Action = [[ 0.02221888 -0.19629495 -0.09326744  0.42632222]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 7111 is [False, True, False, True, False, False]
Current timestep = 7112. State = [[-0.00344654 -0.2710746 ]]. Action = [[ 0.03294668 -0.11519614 -0.20767915  0.22506785]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 7112 is [False, True, False, True, False, False]
Human Feedback received at timestep 7112 of -1
Current timestep = 7113. State = [[-0.00248034 -0.27698863]]. Action = [[ 0.01843959 -0.19869311 -0.08068128  0.48545206]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 7113 is [False, True, False, True, False, False]
Scene graph at timestep 7113 is [False, True, False, True, False, False]
State prediction error at timestep 7113 is tensor(2.2012e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7114. State = [[-0.00162123 -0.28377086]]. Action = [[ 0.03713045 -0.20049271 -0.01704529  0.02400208]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 7114 is [False, True, False, True, False, False]
Current timestep = 7115. State = [[ 0.00043763 -0.29052678]]. Action = [[ 0.01790187 -0.1361966  -0.17825496  0.36162353]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 7115 is [False, True, False, True, False, False]
Scene graph at timestep 7115 is [False, True, False, True, False, False]
State prediction error at timestep 7115 is tensor(3.3872e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7115 of -1
Current timestep = 7116. State = [[ 0.00240092 -0.29699606]]. Action = [[-0.03196174 -0.1284459  -0.21421577  0.3279829 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 7116 is [False, True, False, True, False, False]
Human Feedback received at timestep 7116 of -1
Current timestep = 7117. State = [[ 0.00343145 -0.30245432]]. Action = [[-0.09189616 -0.0367655  -0.24392536  0.32912648]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 7117 is [False, True, False, True, False, False]
Current timestep = 7118. State = [[ 0.00326271 -0.30629313]]. Action = [[ 0.04433626 -0.13141093  0.14206672  0.09113646]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 7118 is [False, True, False, True, False, False]
Human Feedback received at timestep 7118 of -1
Current timestep = 7119. State = [[ 0.00289402 -0.3102166 ]]. Action = [[ 0.01477087 -0.12642649 -0.04364143  0.02392113]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 7119 is [False, True, False, True, False, False]
Human Feedback received at timestep 7119 of -1
Current timestep = 7120. State = [[ 0.002487   -0.31267112]]. Action = [[ 0.06191406 -0.13113785 -0.21672301  0.49633634]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 7120 is [False, True, False, True, False, False]
Human Feedback received at timestep 7120 of -1
Current timestep = 7121. State = [[ 0.00218199 -0.31389993]]. Action = [[ 0.09208053 -0.12898801 -0.2469191  -0.1280284 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 7121 is [False, True, False, True, False, False]
Scene graph at timestep 7121 is [False, True, False, True, False, False]
State prediction error at timestep 7121 is tensor(1.5020e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7122. State = [[ 0.00202843 -0.3145356 ]]. Action = [[ 0.03020036 -0.16835503 -0.00572899  0.20262694]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 7122 is [False, True, False, True, False, False]
Scene graph at timestep 7122 is [False, True, False, True, False, False]
State prediction error at timestep 7122 is tensor(5.2026e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7123. State = [[ 0.00197267 -0.31491637]]. Action = [[ 0.04226279 -0.17997512 -0.02328473  0.18584406]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 7123 is [False, True, False, True, False, False]
Scene graph at timestep 7123 is [False, True, False, True, False, False]
State prediction error at timestep 7123 is tensor(1.9419e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7124. State = [[ 0.00195504 -0.31491023]]. Action = [[-0.08344564 -0.12010992  0.12384617  0.05462861]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 7124 is [False, True, False, True, False, False]
Scene graph at timestep 7124 is [False, True, False, True, False, False]
State prediction error at timestep 7124 is tensor(3.4758e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7124 of -1
Current timestep = 7125. State = [[ 0.00190269 -0.3149714 ]]. Action = [[ 0.01238972 -0.19999436 -0.00176662  0.08765662]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 7125 is [False, True, False, True, False, False]
Current timestep = 7126. State = [[ 0.00190269 -0.3149714 ]]. Action = [[ 0.01448986 -0.15279146 -0.18046327  0.5247505 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 7126 is [False, True, False, True, False, False]
Scene graph at timestep 7126 is [False, True, False, True, False, False]
State prediction error at timestep 7126 is tensor(6.5761e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7126 of -1
Current timestep = 7127. State = [[ 0.0018938 -0.3149683]]. Action = [[-0.03277384 -0.18465318 -0.02784027  0.3338269 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 7127 is [False, True, False, True, False, False]
Current timestep = 7128. State = [[ 0.00193371 -0.3149809 ]]. Action = [[-0.02876577 -0.19985436  0.18133187  0.26657152]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 7128 is [False, True, False, True, False, False]
Human Feedback received at timestep 7128 of -1
Current timestep = 7129. State = [[ 0.00217641 -0.31527403]]. Action = [[-0.05885637 -0.1508843  -0.07200105  0.41039097]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 7129 is [False, True, False, True, False, False]
Scene graph at timestep 7129 is [False, True, False, True, False, False]
State prediction error at timestep 7129 is tensor(9.3170e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7130. State = [[ 0.00217389 -0.31537077]]. Action = [[-0.09511846 -0.18454078 -0.24083963  0.41339767]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 7130 is [False, True, False, True, False, False]
Current timestep = 7131. State = [[ 0.00211587 -0.3155029 ]]. Action = [[ 0.0035257  -0.05644527 -0.07523657  0.40464354]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 7131 is [False, True, False, True, False, False]
Current timestep = 7132. State = [[ 0.00211587 -0.3155029 ]]. Action = [[ 0.04556865 -0.1204904  -0.1282877   0.36182308]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 7132 is [False, True, False, True, False, False]
Scene graph at timestep 7132 is [False, True, False, True, False, False]
State prediction error at timestep 7132 is tensor(1.9398e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7133. State = [[ 0.00222979 -0.31551978]]. Action = [[ 0.06143075 -0.1470558  -0.22697613  0.48500252]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 7133 is [False, True, False, True, False, False]
Scene graph at timestep 7133 is [False, True, False, True, False, False]
State prediction error at timestep 7133 is tensor(2.0969e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7134. State = [[ 0.00222662 -0.31553757]]. Action = [[-0.00364171 -0.06144717 -0.04903159  0.37909412]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 7134 is [False, True, False, True, False, False]
Current timestep = 7135. State = [[ 0.00225759 -0.31554705]]. Action = [[-0.06097035 -0.20911631 -0.22424428  0.352203  ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 7135 is [False, True, False, True, False, False]
Current timestep = 7136. State = [[ 0.00225759 -0.31554705]]. Action = [[-0.01986635 -0.1739471   0.18791232  0.06277359]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 7136 is [False, True, False, True, False, False]
Current timestep = 7137. State = [[ 0.00225759 -0.31554705]]. Action = [[ 0.00548458 -0.17704596 -0.24932359  0.31877112]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 7137 is [False, True, False, True, False, False]
Current timestep = 7138. State = [[ 0.00224876 -0.31554395]]. Action = [[ 0.09270275 -0.1376255  -0.24210888  0.19836473]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 7138 is [False, True, False, True, False, False]
Current timestep = 7139. State = [[ 0.00223984 -0.31554085]]. Action = [[ 0.01741481 -0.18487549  0.04235756  0.4756974 ]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 7139 is [False, True, False, True, False, False]
Current timestep = 7140. State = [[ 0.00222216 -0.31553468]]. Action = [[ 0.08793861 -0.19541143 -0.06780383  0.24606681]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 7140 is [False, True, False, True, False, False]
Scene graph at timestep 7140 is [False, True, False, True, False, False]
State prediction error at timestep 7140 is tensor(2.7638e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7141. State = [[ 0.00225275 -0.315544  ]]. Action = [[-0.03445114 -0.18558954 -0.1067524  -0.02692783]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 7141 is [False, True, False, True, False, False]
Scene graph at timestep 7141 is [False, True, False, True, False, False]
State prediction error at timestep 7141 is tensor(3.4681e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7142. State = [[ 0.00233185 -0.31571856]]. Action = [[ 0.00831437 -0.13511796 -0.24520312  0.5422517 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 7142 is [False, True, False, True, False, False]
Current timestep = 7143. State = [[ 0.00230517 -0.31570923]]. Action = [[-0.05338877 -0.19044213 -0.23708107  0.44029534]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 7143 is [False, True, False, True, False, False]
Current timestep = 7144. State = [[ 0.00243349 -0.31575   ]]. Action = [[-0.11161751 -0.16127865 -0.24953493  0.11177123]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 7144 is [False, True, False, True, False, False]
Current timestep = 7145. State = [[ 0.0024301  -0.31567657]]. Action = [[ 0.02777916 -0.17878547 -0.07798707  0.02046835]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 7145 is [False, True, False, True, False, False]
Current timestep = 7146. State = [[ 0.00256103 -0.31571817]]. Action = [[-0.01065066 -0.21286704 -0.12453979  0.33696115]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 7146 is [False, True, False, True, False, False]
Current timestep = 7147. State = [[ 0.00258296 -0.31572446]]. Action = [[ 0.00305906 -0.1737028  -0.12967841  0.09429216]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 7147 is [False, True, False, True, False, False]
Scene graph at timestep 7147 is [False, True, False, True, False, False]
State prediction error at timestep 7147 is tensor(2.4527e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7148. State = [[ 0.00256504 -0.3157182 ]]. Action = [[-0.02056803 -0.16642706 -0.13441336  0.4306805 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 7148 is [False, True, False, True, False, False]
Scene graph at timestep 7148 is [False, True, False, True, False, False]
State prediction error at timestep 7148 is tensor(1.4965e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7149. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.0788694  -0.19663279  0.05203065  0.2093364 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 7149 is [False, True, False, True, False, False]
Current timestep = 7150. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07581174 -0.17157619 -0.18250749  0.5310304 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 7150 is [False, True, False, True, False, False]
Current timestep = 7151. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02712798 -0.15389925 -0.22145396  0.3896284 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 7151 is [False, True, False, True, False, False]
Current timestep = 7152. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02807564 -0.14267938 -0.1775064   0.5182133 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 7152 is [False, True, False, True, False, False]
Current timestep = 7153. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06840064 -0.08679457 -0.24165717  0.49180603]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 7153 is [False, True, False, True, False, False]
Scene graph at timestep 7153 is [False, True, False, True, False, False]
State prediction error at timestep 7153 is tensor(6.7642e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7154. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04910007 -0.19808456 -0.06037502 -0.0808093 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 7154 is [False, True, False, True, False, False]
Current timestep = 7155. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02955662 -0.13387413 -0.21522318  0.19123316]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 7155 is [False, True, False, True, False, False]
Current timestep = 7156. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08358498 -0.09188253 -0.09745456  0.31011832]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 7156 is [False, True, False, True, False, False]
Current timestep = 7157. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.04185572 -0.13037115 -0.12839124  0.56461716]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 7157 is [False, True, False, True, False, False]
Current timestep = 7158. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.011024   -0.14974181 -0.21714501  0.30072582]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 7158 is [False, True, False, True, False, False]
Scene graph at timestep 7158 is [False, True, False, True, False, False]
State prediction error at timestep 7158 is tensor(2.1110e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7159. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03208725 -0.13109814 -0.14520563  0.43565643]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 7159 is [False, True, False, True, False, False]
Current timestep = 7160. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08774775 -0.1750731  -0.23150967  0.6153401 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 7160 is [False, True, False, True, False, False]
Current timestep = 7161. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05481672 -0.19305348  0.14147785 -0.02593619]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 7161 is [False, True, False, True, False, False]
Current timestep = 7162. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05105528 -0.13922612 -0.19354884  0.52800846]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 7162 is [False, True, False, True, False, False]
Current timestep = 7163. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03144589 -0.09197181 -0.22334962  0.73527694]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 7163 is [False, True, False, True, False, False]
Scene graph at timestep 7163 is [False, True, False, True, False, False]
State prediction error at timestep 7163 is tensor(1.7576e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7164. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05280578 -0.17007852 -0.22661097  0.11041296]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 7164 is [False, True, False, True, False, False]
Current timestep = 7165. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07913193 -0.14662553 -0.04263586  0.3251679 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 7165 is [False, True, False, True, False, False]
Scene graph at timestep 7165 is [False, True, False, True, False, False]
State prediction error at timestep 7165 is tensor(2.0033e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7166. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.03168839 -0.18694931 -0.24433017  0.1310494 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 7166 is [False, True, False, True, False, False]
Current timestep = 7167. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04389331 -0.17116487 -0.20943038  0.3302164 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 7167 is [False, True, False, True, False, False]
Current timestep = 7168. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02051446 -0.19435412 -0.1514465   0.5512146 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 7168 is [False, True, False, True, False, False]
Current timestep = 7169. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00983071 -0.10397586 -0.1153796   0.6463182 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 7169 is [False, True, False, True, False, False]
Current timestep = 7170. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.0066337  -0.19646753 -0.15400776  0.44634676]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 7170 is [False, True, False, True, False, False]
Current timestep = 7171. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.1031628  -0.18939148 -0.20859231  0.15996504]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 7171 is [False, True, False, True, False, False]
Current timestep = 7172. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03791291 -0.09406717 -0.2344169   0.39706063]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 7172 is [False, True, False, True, False, False]
Current timestep = 7173. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.03945482 -0.1410194   0.05263564  0.34354985]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 7173 is [False, True, False, True, False, False]
Current timestep = 7174. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02406138 -0.13567568  0.17598629  0.06838536]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 7174 is [False, True, False, True, False, False]
Scene graph at timestep 7174 is [False, True, False, True, False, False]
State prediction error at timestep 7174 is tensor(6.1034e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7175. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02538326 -0.20792948 -0.225214    0.549315  ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 7175 is [False, True, False, True, False, False]
Current timestep = 7176. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.12047949 -0.2079467  -0.1267851   0.5340003 ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 7176 is [False, True, False, True, False, False]
Current timestep = 7177. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.00200844 -0.22205947  0.1435515   0.6018884 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 7177 is [False, True, False, True, False, False]
Current timestep = 7178. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.00658399 -0.18539776 -0.206661   -0.05306447]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 7178 is [False, True, False, True, False, False]
Current timestep = 7179. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00276141 -0.20854491  0.23282483  0.3843987 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 7179 is [False, True, False, True, False, False]
Scene graph at timestep 7179 is [False, True, False, True, False, False]
State prediction error at timestep 7179 is tensor(1.7089e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7180. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08492541 -0.18794364 -0.24129912  0.29803586]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 7180 is [False, True, False, True, False, False]
Current timestep = 7181. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.09443367 -0.09211609 -0.11059496  0.6379883 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 7181 is [False, True, False, True, False, False]
Current timestep = 7182. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08733276 -0.15495068  0.08206668  0.61697865]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 7182 is [False, True, False, True, False, False]
Current timestep = 7183. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07443711 -0.19147173 -0.22110586  0.14986205]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 7183 is [False, True, False, True, False, False]
Current timestep = 7184. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04711589 -0.09470645 -0.13981037  0.45105422]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 7184 is [False, True, False, True, False, False]
Current timestep = 7185. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04899293 -0.19599877  0.00305149 -0.24147898]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 7185 is [False, True, False, True, False, False]
Current timestep = 7186. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.09975448 -0.20118693  0.16664445  0.3531989 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 7186 is [False, True, False, True, False, False]
Current timestep = 7187. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02032299 -0.1744437  -0.15929268  0.18248534]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 7187 is [False, True, False, True, False, False]
Current timestep = 7188. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07274124 -0.19498654 -0.24474247  0.60636544]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 7188 is [False, True, False, True, False, False]
Current timestep = 7189. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08363667 -0.17431222 -0.20709735  0.3864931 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 7189 is [False, True, False, True, False, False]
Scene graph at timestep 7189 is [False, True, False, True, False, False]
State prediction error at timestep 7189 is tensor(9.8298e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7190. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05715625 -0.19414367 -0.06692564  0.46892262]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 7190 is [False, True, False, True, False, False]
Current timestep = 7191. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06489736 -0.19052203 -0.17036909  0.33327746]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 7191 is [False, True, False, True, False, False]
Current timestep = 7192. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02680555 -0.18501234  0.23643285  0.25582588]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 7192 is [False, True, False, True, False, False]
Current timestep = 7193. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06519294 -0.19224459 -0.07463917  0.30888212]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 7193 is [False, True, False, True, False, False]
Current timestep = 7194. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.03063732 -0.1478978  -0.2232596   0.38434374]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 7194 is [False, True, False, True, False, False]
Scene graph at timestep 7194 is [False, True, False, True, False, False]
State prediction error at timestep 7194 is tensor(6.6729e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7195. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.01713714 -0.21706575 -0.14481823 -0.1226747 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 7195 is [False, True, False, True, False, False]
Scene graph at timestep 7195 is [False, True, False, True, False, False]
State prediction error at timestep 7195 is tensor(9.9570e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7196. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.0016766  -0.18755098 -0.16582853 -0.01812333]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 7196 is [False, True, False, True, False, False]
Current timestep = 7197. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.05886105 -0.16992332 -0.02742575  0.14968002]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 7197 is [False, True, False, True, False, False]
Current timestep = 7198. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06158814 -0.13028555 -0.24626581  0.06893575]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 7198 is [False, True, False, True, False, False]
Current timestep = 7199. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05135024 -0.0864623   0.0455474   0.2184453 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 7199 is [False, True, False, True, False, False]
Current timestep = 7200. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07509425 -0.20755559  0.09399247  0.3035078 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 7200 is [False, True, False, True, False, False]
Current timestep = 7201. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05920935 -0.16311136  0.23229077  0.4440447 ]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 7201 is [False, True, False, True, False, False]
Current timestep = 7202. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02130759 -0.14918894 -0.13056956  0.3378551 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 7202 is [False, True, False, True, False, False]
Current timestep = 7203. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05535606 -0.17632438 -0.22777891  0.3339913 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 7203 is [False, True, False, True, False, False]
Current timestep = 7204. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06085309 -0.18895371  0.02128741 -0.03782952]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 7204 is [False, True, False, True, False, False]
Current timestep = 7205. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.00139683 -0.1609917   0.09382808  0.46825743]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 7205 is [False, True, False, True, False, False]
Current timestep = 7206. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04601517 -0.17698263  0.04836339  0.48590326]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 7206 is [False, True, False, True, False, False]
Current timestep = 7207. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00156264 -0.17866403 -0.24031119  0.5020709 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 7207 is [False, True, False, True, False, False]
Scene graph at timestep 7207 is [False, True, False, True, False, False]
State prediction error at timestep 7207 is tensor(5.6531e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7208. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.03437591 -0.10503076 -0.04619572  0.20777571]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 7208 is [False, True, False, True, False, False]
Current timestep = 7209. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.00870639 -0.17759486 -0.07334206  0.46347892]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 7209 is [False, True, False, True, False, False]
Current timestep = 7210. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04060945 -0.21512996 -0.1738417   0.503893  ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 7210 is [False, True, False, True, False, False]
Current timestep = 7211. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.01645973 -0.17720948 -0.22985369  0.39512467]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 7211 is [False, True, False, True, False, False]
Current timestep = 7212. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.00679711 -0.08314615 -0.22531477  0.07882607]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 7212 is [False, True, False, True, False, False]
Scene graph at timestep 7212 is [False, True, False, True, False, False]
State prediction error at timestep 7212 is tensor(2.2393e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7213. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00547105 -0.1755926   0.0692122   0.44015014]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 7213 is [False, True, False, True, False, False]
Current timestep = 7214. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00861017 -0.18886514 -0.21226282  0.5075207 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 7214 is [False, True, False, True, False, False]
Current timestep = 7215. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01770623 -0.0626342   0.23324561  0.39323032]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 7215 is [False, True, False, True, False, False]
Scene graph at timestep 7215 is [False, True, False, True, False, False]
State prediction error at timestep 7215 is tensor(1.7225e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7216. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.11636159 -0.16435306  0.21523637  0.40152717]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 7216 is [False, True, False, True, False, False]
Current timestep = 7217. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.03236115 -0.21163636 -0.17660792  0.31435215]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 7217 is [False, True, False, True, False, False]
Current timestep = 7218. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03821272 -0.18620625 -0.2148558   0.48793733]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 7218 is [False, True, False, True, False, False]
Current timestep = 7219. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02006957 -0.18536673 -0.08188032 -0.15495819]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 7219 is [False, True, False, True, False, False]
Current timestep = 7220. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04478069 -0.2068156   0.05013531  0.27651894]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 7220 is [False, True, False, True, False, False]
Current timestep = 7221. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.04001309 -0.15771756 -0.20783962  0.34857583]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 7221 is [False, True, False, True, False, False]
Current timestep = 7222. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02179268 -0.18924706 -0.24267076  0.39673078]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 7222 is [False, True, False, True, False, False]
Current timestep = 7223. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07132517 -0.07984914 -0.03766716  0.37939143]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 7223 is [False, True, False, True, False, False]
Current timestep = 7224. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03463644 -0.01751688  0.00414008  0.371135  ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 7224 is [False, True, False, True, False, False]
Current timestep = 7225. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.11499614 -0.14161667  0.04684433  0.53324485]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 7225 is [False, True, False, True, False, False]
Current timestep = 7226. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.10060318 -0.13067937  0.08725077  0.5188608 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 7226 is [False, True, False, True, False, False]
Scene graph at timestep 7226 is [False, True, False, True, False, False]
State prediction error at timestep 7226 is tensor(2.6344e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7227. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.01454645 -0.1331387   0.00818238  0.39362204]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 7227 is [False, True, False, True, False, False]
Scene graph at timestep 7227 is [False, True, False, True, False, False]
State prediction error at timestep 7227 is tensor(3.0569e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7228. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07639852 -0.2110074  -0.13640253  0.4632963 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 7228 is [False, True, False, True, False, False]
Current timestep = 7229. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02706575 -0.17209414  0.0016444   0.5134249 ]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 7229 is [False, True, False, True, False, False]
Current timestep = 7230. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00716363 -0.18906501 -0.20568003  0.16789198]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 7230 is [False, True, False, True, False, False]
Current timestep = 7231. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00811785 -0.092453    0.23525491  0.53827405]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 7231 is [False, True, False, True, False, False]
Current timestep = 7232. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05151768 -0.1774895   0.22109848  0.37120903]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 7232 is [False, True, False, True, False, False]
Current timestep = 7233. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05080584 -0.15246727 -0.130979    0.5380962 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 7233 is [False, True, False, True, False, False]
Current timestep = 7234. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01204933 -0.19264908 -0.22809513  0.31364322]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 7234 is [False, True, False, True, False, False]
Current timestep = 7235. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03246909 -0.10378775 -0.2119878  -0.02822089]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 7235 is [False, True, False, True, False, False]
Scene graph at timestep 7235 is [False, True, False, True, False, False]
State prediction error at timestep 7235 is tensor(7.3851e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7236. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.06210983 -0.07468316 -0.2458435   0.20652866]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 7236 is [False, True, False, True, False, False]
Current timestep = 7237. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.005418   -0.20284452 -0.07178891  0.20502532]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 7237 is [False, True, False, True, False, False]
Current timestep = 7238. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.0385343  -0.15420336  0.110762    0.40922356]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 7238 is [False, True, False, True, False, False]
Scene graph at timestep 7238 is [False, True, False, True, False, False]
State prediction error at timestep 7238 is tensor(2.5721e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7239. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06091034 -0.19349717 -0.22051898  0.2794013 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 7239 is [False, True, False, True, False, False]
Current timestep = 7240. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03195162 -0.16943264 -0.22339606  0.2516353 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 7240 is [False, True, False, True, False, False]
Current timestep = 7241. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08368498 -0.12413374  0.08696687  0.20739913]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 7241 is [False, True, False, True, False, False]
Current timestep = 7242. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.14350118 -0.20101647 -0.24905996  0.33879519]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 7242 is [False, True, False, True, False, False]
Scene graph at timestep 7242 is [False, True, False, True, False, False]
State prediction error at timestep 7242 is tensor(4.5283e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7243. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03391767 -0.1226941  -0.08325113  0.40833783]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 7243 is [False, True, False, True, False, False]
Scene graph at timestep 7243 is [False, True, False, True, False, False]
State prediction error at timestep 7243 is tensor(2.5262e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7244. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08162582 -0.18307191 -0.00948642  0.31553662]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 7244 is [False, True, False, True, False, False]
Current timestep = 7245. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.00642899 -0.15029097 -0.04062578  0.21883607]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 7245 is [False, True, False, True, False, False]
Current timestep = 7246. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02518186 -0.21468519 -0.24803203  0.4975145 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 7246 is [False, True, False, True, False, False]
Current timestep = 7247. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.03529    -0.10900207  0.08756796  0.46105814]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 7247 is [False, True, False, True, False, False]
Scene graph at timestep 7247 is [False, True, False, True, False, False]
State prediction error at timestep 7247 is tensor(7.2531e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7248. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08663031 -0.08574319 -0.02394429  0.352005  ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 7248 is [False, True, False, True, False, False]
Current timestep = 7249. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06256863 -0.15991475 -0.1918079   0.33715177]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 7249 is [False, True, False, True, False, False]
Current timestep = 7250. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.0305025  -0.18594986  0.23395154  0.13000119]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 7250 is [False, True, False, True, False, False]
Current timestep = 7251. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07647252 -0.16729619  0.22467244  0.47622573]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 7251 is [False, True, False, True, False, False]
Scene graph at timestep 7251 is [False, True, False, True, False, False]
State prediction error at timestep 7251 is tensor(5.6687e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7252. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01109704 -0.13337925 -0.15363282  0.7999451 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 7252 is [False, True, False, True, False, False]
Current timestep = 7253. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08329438 -0.09056306 -0.00474925  0.0451262 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 7253 is [False, True, False, True, False, False]
Current timestep = 7254. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02623561 -0.17870273 -0.23186637  0.09158742]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 7254 is [False, True, False, True, False, False]
Current timestep = 7255. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01036289 -0.14467369 -0.12773015  0.69144773]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 7255 is [False, True, False, True, False, False]
Current timestep = 7256. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01034695 -0.18145248 -0.16241531  0.59814024]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 7256 is [False, True, False, True, False, False]
Current timestep = 7257. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06310007 -0.15950581 -0.23140556  0.12422323]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 7257 is [False, True, False, True, False, False]
Scene graph at timestep 7257 is [False, True, False, True, False, False]
State prediction error at timestep 7257 is tensor(1.3829e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7258. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03094375 -0.13828397 -0.24763821  0.04938555]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 7258 is [False, True, False, True, False, False]
Scene graph at timestep 7258 is [False, True, False, True, False, False]
State prediction error at timestep 7258 is tensor(7.7121e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7259. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.07604808 -0.19156227  0.21375102  0.06254947]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 7259 is [False, True, False, True, False, False]
Current timestep = 7260. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.06904626 -0.19970563 -0.23174486  0.23121595]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 7260 is [False, True, False, True, False, False]
Current timestep = 7261. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.01156205 -0.16720766  0.13717139  0.4695437 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 7261 is [False, True, False, True, False, False]
Current timestep = 7262. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.02522385 -0.17812513 -0.15012573  0.08402419]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 7262 is [False, True, False, True, False, False]
Current timestep = 7263. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01901805 -0.1712407   0.2397213   0.6527178 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 7263 is [False, True, False, True, False, False]
Current timestep = 7264. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.0258761  -0.22524084  0.0965665   0.56367946]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 7264 is [False, True, False, True, False, False]
Current timestep = 7265. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03849572 -0.12398766  0.00231397  0.14269543]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 7265 is [False, True, False, True, False, False]
Current timestep = 7266. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.0371775  -0.2068317  -0.00609899  0.27729726]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 7266 is [False, True, False, True, False, False]
Current timestep = 7267. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05108425 -0.16018602  0.20922053  0.6536889 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 7267 is [False, True, False, True, False, False]
Current timestep = 7268. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03537098 -0.19273016 -0.205106    0.30693078]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 7268 is [False, True, False, True, False, False]
Current timestep = 7269. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.09806092 -0.1347982  -0.18129505  0.39479923]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 7269 is [False, True, False, True, False, False]
Scene graph at timestep 7269 is [False, True, False, True, False, False]
State prediction error at timestep 7269 is tensor(1.8191e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7270. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.0304586  -0.16964306  0.0994972   0.7062267 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 7270 is [False, True, False, True, False, False]
Scene graph at timestep 7270 is [False, True, False, True, False, False]
State prediction error at timestep 7270 is tensor(1.2675e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7271. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.02523847 -0.12652901 -0.14522691  0.2964512 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 7271 is [False, True, False, True, False, False]
Scene graph at timestep 7271 is [False, True, False, True, False, False]
State prediction error at timestep 7271 is tensor(5.8925e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7272. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.12005763 -0.18430978 -0.23857616  0.2022059 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 7272 is [False, True, False, True, False, False]
Current timestep = 7273. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08108312 -0.17230438 -0.19225094  0.38940132]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 7273 is [False, True, False, True, False, False]
Current timestep = 7274. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.03331628 -0.23729424 -0.09046096  0.05614126]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 7274 is [False, True, False, True, False, False]
Scene graph at timestep 7274 is [False, True, False, True, False, False]
State prediction error at timestep 7274 is tensor(1.2383e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7275. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08108042 -0.17831792 -0.23302443  0.22482824]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 7275 is [False, True, False, True, False, False]
Current timestep = 7276. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.03476907 -0.16704407 -0.16027759  0.44194043]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 7276 is [False, True, False, True, False, False]
Current timestep = 7277. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01585007 -0.19928251  0.17330337  0.43447626]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 7277 is [False, True, False, True, False, False]
Scene graph at timestep 7277 is [False, True, False, True, False, False]
State prediction error at timestep 7277 is tensor(8.5703e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7278. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.11806279 -0.15201426 -0.20770031  0.5022656 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 7278 is [False, True, False, True, False, False]
Current timestep = 7279. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01534572 -0.15151224  0.06783819  0.72649956]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 7279 is [False, True, False, True, False, False]
Current timestep = 7280. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.00664052 -0.13822825 -0.21999133  0.30647314]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 7280 is [False, True, False, True, False, False]
Current timestep = 7281. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05484626 -0.22635639 -0.17067054 -0.24002445]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 7281 is [False, True, False, True, False, False]
Current timestep = 7282. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.077225   -0.18055928  0.01202756  0.14842653]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 7282 is [False, True, False, True, False, False]
Current timestep = 7283. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.01048903 -0.16244629  0.1952453   0.29275906]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 7283 is [False, True, False, True, False, False]
Current timestep = 7284. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.05236073 -0.16994162 -0.24582331  0.22907734]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 7284 is [False, True, False, True, False, False]
Current timestep = 7285. State = [[ 0.00255611 -0.31571507]]. Action = [[ 0.04177982 -0.17094709 -0.20306012  0.55188894]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 7285 is [False, True, False, True, False, False]
Current timestep = 7286. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.08156455 -0.19748837 -0.23048265  0.37690198]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 7286 is [False, True, False, True, False, False]
Current timestep = 7287. State = [[ 0.00255611 -0.31571507]]. Action = [[-0.00026125 -0.09948945 -0.22644623  0.15966547]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 7287 is [False, True, False, True, False, False]
Current timestep = 7288. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.03972387 -0.14510599 -0.21627091  0.46959877]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 7288 is [False, True, False, True, False, False]
Scene graph at timestep 7288 is [False, True, False, True, False, False]
State prediction error at timestep 7288 is tensor(3.7769e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7289. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03478864 -0.14260581 -0.17680614  0.37723994]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 7289 is [False, True, False, True, False, False]
Current timestep = 7290. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.06195265 -0.1615746  -0.17986923  0.6073313 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 7290 is [False, True, False, True, False, False]
Current timestep = 7291. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04086745 -0.18153997 -0.20353173  0.5616224 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 7291 is [False, True, False, True, False, False]
Current timestep = 7292. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.06790206 -0.16470042 -0.04459669  0.42863595]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 7292 is [False, True, False, True, False, False]
Current timestep = 7293. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.03529638 -0.14941159 -0.12374103 -0.01935291]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 7293 is [False, True, False, True, False, False]
Current timestep = 7294. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.01546912 -0.21641965  0.0409525   0.2873249 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 7294 is [False, True, False, True, False, False]
Current timestep = 7295. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.01926859 -0.22308178 -0.03175774  0.4172809 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 7295 is [False, True, False, True, False, False]
Current timestep = 7296. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0678786  -0.2235678   0.10285586  0.3967278 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 7296 is [False, True, False, True, False, False]
Current timestep = 7297. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03866713 -0.15097988 -0.06315643  0.4765861 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 7297 is [False, True, False, True, False, False]
Current timestep = 7298. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03570272 -0.14686312 -0.1832093   0.27926612]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 7298 is [False, True, False, True, False, False]
Scene graph at timestep 7298 is [False, True, False, True, False, False]
State prediction error at timestep 7298 is tensor(3.8036e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7299. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.00824668 -0.16562779  0.22715151  0.12323809]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 7299 is [False, True, False, True, False, False]
Current timestep = 7300. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04643978 -0.14509964 -0.24917433  0.43102622]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 7300 is [False, True, False, True, False, False]
Current timestep = 7301. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07686967 -0.15795809 -0.17497103  0.5145569 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 7301 is [False, True, False, True, False, False]
Current timestep = 7302. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.10145074 -0.17036553  0.09927604  0.35913277]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 7302 is [False, True, False, True, False, False]
Scene graph at timestep 7302 is [False, True, False, True, False, False]
State prediction error at timestep 7302 is tensor(3.6589e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7303. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.06484181 -0.1838474  -0.23499273 -0.01027751]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 7303 is [False, True, False, True, False, False]
Scene graph at timestep 7303 is [False, True, False, True, False, False]
State prediction error at timestep 7303 is tensor(9.6116e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7304. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08640867 -0.20790002 -0.09517021  0.20433712]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 7304 is [False, True, False, True, False, False]
Scene graph at timestep 7304 is [False, True, False, True, False, False]
State prediction error at timestep 7304 is tensor(1.4597e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7305. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.00561315 -0.13251926 -0.07000062  0.5292249 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 7305 is [False, True, False, True, False, False]
Current timestep = 7306. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04635641 -0.22324115  0.18462622  0.2442441 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 7306 is [False, True, False, True, False, False]
Scene graph at timestep 7306 is [False, True, False, True, False, False]
State prediction error at timestep 7306 is tensor(7.6605e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7307. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.00631478 -0.19318902 -0.23262066  0.1840775 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 7307 is [False, True, False, True, False, False]
Current timestep = 7308. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.04263076 -0.22998421 -0.22384697  0.59536195]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 7308 is [False, True, False, True, False, False]
Current timestep = 7309. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04358628 -0.14819448  0.03968474  0.7088046 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 7309 is [False, True, False, True, False, False]
Current timestep = 7310. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0871217  -0.14584447 -0.06718919  0.40771496]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 7310 is [False, True, False, True, False, False]
Scene graph at timestep 7310 is [False, True, False, True, False, False]
State prediction error at timestep 7310 is tensor(2.4137e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7311. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08288923 -0.18762022 -0.13721243  0.41940033]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 7311 is [False, True, False, True, False, False]
Scene graph at timestep 7311 is [False, True, False, True, False, False]
State prediction error at timestep 7311 is tensor(1.0214e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7312. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.06857561 -0.2041734  -0.20267916  0.2573148 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 7312 is [False, True, False, True, False, False]
Scene graph at timestep 7312 is [False, True, False, True, False, False]
State prediction error at timestep 7312 is tensor(1.5412e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7313. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04387128 -0.16193967 -0.12535156  0.48350203]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 7313 is [False, True, False, True, False, False]
Current timestep = 7314. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.05870017 -0.20894982  0.12146938  0.4491036 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 7314 is [False, True, False, True, False, False]
Current timestep = 7315. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03744274 -0.1590142  -0.14028488  0.3458848 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 7315 is [False, True, False, True, False, False]
Scene graph at timestep 7315 is [False, True, False, True, False, False]
State prediction error at timestep 7315 is tensor(1.1478e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7316. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.00432244 -0.11977646 -0.15639357  0.44774318]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 7316 is [False, True, False, True, False, False]
Current timestep = 7317. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.02061595 -0.08987479  0.0887942   0.45408058]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 7317 is [False, True, False, True, False, False]
Current timestep = 7318. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.01267035 -0.13416906  0.12227824  0.6161822 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 7318 is [False, True, False, True, False, False]
Current timestep = 7319. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07349782 -0.18440104 -0.24745844  0.6239743 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 7319 is [False, True, False, True, False, False]
Current timestep = 7320. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08231851 -0.11372602 -0.10523579  0.502367  ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 7320 is [False, True, False, True, False, False]
Current timestep = 7321. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04966938 -0.17154711  0.02150711  0.43913102]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 7321 is [False, True, False, True, False, False]
Current timestep = 7322. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.11134805 -0.1871718   0.09307459  0.2763548 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 7322 is [False, True, False, True, False, False]
Scene graph at timestep 7322 is [False, True, False, True, False, False]
State prediction error at timestep 7322 is tensor(1.9917e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7323. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.00769371 -0.15173008 -0.2388371   0.5500541 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 7323 is [False, True, False, True, False, False]
Scene graph at timestep 7323 is [False, True, False, True, False, False]
State prediction error at timestep 7323 is tensor(1.1999e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7324. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.11411372 -0.19740048 -0.12207747  0.2959168 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 7324 is [False, True, False, True, False, False]
Current timestep = 7325. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.05349869 -0.17937347 -0.23026542  0.5464444 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 7325 is [False, True, False, True, False, False]
Scene graph at timestep 7325 is [False, True, False, True, False, False]
State prediction error at timestep 7325 is tensor(2.2909e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7326. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.00511214 -0.1387869   0.00318113  0.34307158]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 7326 is [False, True, False, True, False, False]
Current timestep = 7327. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.05825958 -0.17269067 -0.24767968  0.6357899 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 7327 is [False, True, False, True, False, False]
Current timestep = 7328. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07445006 -0.15388799  0.07017657  0.32592988]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 7328 is [False, True, False, True, False, False]
Current timestep = 7329. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.02813026 -0.1040915   0.11946091  0.35510612]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 7329 is [False, True, False, True, False, False]
Current timestep = 7330. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.09442335 -0.1353306   0.14140046  0.62165046]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 7330 is [False, True, False, True, False, False]
Current timestep = 7331. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04605775 -0.1498689   0.1266213   0.39366496]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 7331 is [False, True, False, True, False, False]
Current timestep = 7332. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.01311143 -0.17611085 -0.20781106  0.35073793]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 7332 is [False, True, False, True, False, False]
Current timestep = 7333. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.05470128 -0.2087358  -0.24064282  0.45725226]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 7333 is [False, True, False, True, False, False]
Current timestep = 7334. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0038026  -0.1832921   0.09977809 -0.09565151]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 7334 is [False, True, False, True, False, False]
Scene graph at timestep 7334 is [False, True, False, True, False, False]
State prediction error at timestep 7334 is tensor(2.7008e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7335. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08085316 -0.20968564 -0.2339485   0.42331803]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 7335 is [False, True, False, True, False, False]
Scene graph at timestep 7335 is [False, True, False, True, False, False]
State prediction error at timestep 7335 is tensor(5.2929e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7336. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.05035895 -0.2128854   0.03805652  0.18149257]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 7336 is [False, True, False, True, False, False]
Scene graph at timestep 7336 is [False, True, False, True, False, False]
State prediction error at timestep 7336 is tensor(2.9083e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7337. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03506385 -0.08543375 -0.1647912   0.6531912 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 7337 is [False, True, False, True, False, False]
Current timestep = 7338. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.13484776 -0.18615952 -0.01229417  0.36869538]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 7338 is [False, True, False, True, False, False]
Current timestep = 7339. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0885981  -0.21974468  0.06974816  0.19547784]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 7339 is [False, True, False, True, False, False]
Scene graph at timestep 7339 is [False, True, False, True, False, False]
State prediction error at timestep 7339 is tensor(2.8672e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7340. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.01922944 -0.22054304 -0.21077421  0.29108334]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 7340 is [False, True, False, True, False, False]
Scene graph at timestep 7340 is [False, True, False, True, False, False]
State prediction error at timestep 7340 is tensor(1.0107e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7341. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.14128514 -0.20687349 -0.20756122  0.2616067 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 7341 is [False, True, False, True, False, False]
Current timestep = 7342. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03801739 -0.13743837 -0.19786446  0.6349505 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 7342 is [False, True, False, True, False, False]
Current timestep = 7343. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.02386461 -0.06252348 -0.21772435 -0.00056046]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 7343 is [False, True, False, True, False, False]
Current timestep = 7344. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04993859 -0.21536131 -0.24662957 -0.13421148]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 7344 is [False, True, False, True, False, False]
Current timestep = 7345. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07980239 -0.1274685   0.1523358   0.4662317 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 7345 is [False, True, False, True, False, False]
Scene graph at timestep 7345 is [False, True, False, True, False, False]
State prediction error at timestep 7345 is tensor(1.3606e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7346. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03923085 -0.17703946 -0.24750765  0.31869543]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 7346 is [False, True, False, True, False, False]
Current timestep = 7347. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07989731 -0.18916823 -0.1760923   0.22972071]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 7347 is [False, True, False, True, False, False]
Current timestep = 7348. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.11230628 -0.176469    0.05308494  0.65181816]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 7348 is [False, True, False, True, False, False]
Current timestep = 7349. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.06809962 -0.17517526  0.10534513  0.15046465]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 7349 is [False, True, False, True, False, False]
Current timestep = 7350. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07033159 -0.16075957 -0.20583868 -0.11170352]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 7350 is [False, True, False, True, False, False]
Scene graph at timestep 7350 is [False, True, False, True, False, False]
State prediction error at timestep 7350 is tensor(3.1856e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7351. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.05751072 -0.13074942 -0.24355698  0.5342014 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 7351 is [False, True, False, True, False, False]
Current timestep = 7352. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.02247548 -0.17908607  0.07053813  0.2739241 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 7352 is [False, True, False, True, False, False]
Scene graph at timestep 7352 is [False, True, False, True, False, False]
State prediction error at timestep 7352 is tensor(1.0699e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7353. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.13689908 -0.13309613  0.05496758  0.42808938]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 7353 is [False, True, False, True, False, False]
Current timestep = 7354. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03162409 -0.20617184 -0.19960557  0.5225885 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 7354 is [False, True, False, True, False, False]
Current timestep = 7355. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.14520851 -0.19904648 -0.07415716  0.37188816]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 7355 is [False, True, False, True, False, False]
Scene graph at timestep 7355 is [False, True, False, True, False, False]
State prediction error at timestep 7355 is tensor(3.3100e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7356. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.1052624   0.00458044  0.2217898   0.6168239 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 7356 is [False, True, False, True, False, False]
Current timestep = 7357. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04966743 -0.12557197 -0.24397688  0.47820127]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 7357 is [False, True, False, True, False, False]
Current timestep = 7358. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.12357119 -0.16535464 -0.02065676  0.07841611]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 7358 is [False, True, False, True, False, False]
Current timestep = 7359. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04414175 -0.21891545  0.23224902  0.20169318]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 7359 is [False, True, False, True, False, False]
Current timestep = 7360. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0549901  -0.18949758 -0.00733082  0.51028657]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 7360 is [False, True, False, True, False, False]
Current timestep = 7361. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0859119  -0.189616    0.06218684  0.5516958 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 7361 is [False, True, False, True, False, False]
Current timestep = 7362. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.13026407 -0.15706556 -0.1678763   0.21689391]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 7362 is [False, True, False, True, False, False]
Current timestep = 7363. State = [[ 0.00251488 -0.3157729 ]]. Action = [[-0.13662185 -0.21075575 -0.22389278  0.7477553 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 7363 is [False, True, False, True, False, False]
Current timestep = 7364. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08476177  0.08408898  0.00704542 -0.30747318]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 7364 is [False, True, False, True, False, False]
Current timestep = 7365. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.13177542 -0.1874032  -0.22973809  0.21716118]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 7365 is [False, True, False, True, False, False]
Scene graph at timestep 7365 is [False, True, False, True, False, False]
State prediction error at timestep 7365 is tensor(1.3148e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7366. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.06900704 -0.20356938 -0.08073875  0.3090353 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 7366 is [False, True, False, True, False, False]
Scene graph at timestep 7366 is [False, True, False, True, False, False]
State prediction error at timestep 7366 is tensor(4.1573e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7367. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.15984806 -0.13784578  0.11336616  0.67479765]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 7367 is [False, True, False, True, False, False]
Current timestep = 7368. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.00893703 -0.11842769 -0.16785778  0.49326587]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 7368 is [False, True, False, True, False, False]
Scene graph at timestep 7368 is [False, True, False, True, False, False]
State prediction error at timestep 7368 is tensor(9.2006e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7369. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07873946 -0.17592704 -0.17916885  0.59699726]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 7369 is [False, True, False, True, False, False]
Current timestep = 7370. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.06878707 -0.17703332 -0.24783279  0.41168058]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 7370 is [False, True, False, True, False, False]
Scene graph at timestep 7370 is [False, True, False, True, False, False]
State prediction error at timestep 7370 is tensor(8.0830e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7371. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08387834 -0.07438967  0.10906249  0.5627632 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 7371 is [False, True, False, True, False, False]
Current timestep = 7372. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.09629214 -0.182282   -0.13571255  0.14769137]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 7372 is [False, True, False, True, False, False]
Current timestep = 7373. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04933223 -0.04884318  0.19527417  0.55571175]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 7373 is [False, True, False, True, False, False]
Current timestep = 7374. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.11468588 -0.0550739  -0.23303407  0.32085395]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 7374 is [False, True, False, True, False, False]
Current timestep = 7375. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08705023 -0.2018156  -0.08792543  0.4253484 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 7375 is [False, True, False, True, False, False]
Current timestep = 7376. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.11218387 -0.19101311 -0.24354446 -0.3499099 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 7376 is [False, True, False, True, False, False]
Current timestep = 7377. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08643816 -0.18462877 -0.24903424  0.23545957]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 7377 is [False, True, False, True, False, False]
Scene graph at timestep 7377 is [False, True, False, True, False, False]
State prediction error at timestep 7377 is tensor(2.6338e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7378. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04067044 -0.19291727 -0.16917574  0.32450104]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 7378 is [False, True, False, True, False, False]
Current timestep = 7379. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.00797248 -0.07003859 -0.21190813  0.14173591]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 7379 is [False, True, False, True, False, False]
Scene graph at timestep 7379 is [False, True, False, True, False, False]
State prediction error at timestep 7379 is tensor(8.8032e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7380. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.14218669 -0.23101518 -0.03783679  0.5297046 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 7380 is [False, True, False, True, False, False]
Current timestep = 7381. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.14402066 -0.22430372  0.01633674  0.62037873]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 7381 is [False, True, False, True, False, False]
Current timestep = 7382. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0884144  -0.17440766  0.24271145  0.36422825]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 7382 is [False, True, False, True, False, False]
Current timestep = 7383. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.05325806  0.01357388 -0.24709462  0.19473577]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 7383 is [False, True, False, True, False, False]
Current timestep = 7384. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.07558146 -0.11094904 -0.24116307  0.07972491]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 7384 is [False, True, False, True, False, False]
Current timestep = 7385. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.00863177 -0.12513396 -0.18398705  0.15073597]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 7385 is [False, True, False, True, False, False]
Current timestep = 7386. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.06254745 -0.18981275 -0.2346439   0.2789197 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 7386 is [False, True, False, True, False, False]
Current timestep = 7387. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04742078 -0.13561502  0.2259644   0.6218375 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 7387 is [False, True, False, True, False, False]
Scene graph at timestep 7387 is [False, True, False, True, False, False]
State prediction error at timestep 7387 is tensor(2.9064e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7388. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.09361535 -0.16471756 -0.1602989   0.06573713]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 7388 is [False, True, False, True, False, False]
Current timestep = 7389. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.08412334 -0.21030626 -0.24045555  0.28878164]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 7389 is [False, True, False, True, False, False]
Current timestep = 7390. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.02413122 -0.22657734  0.08321398  0.15727198]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 7390 is [False, True, False, True, False, False]
Current timestep = 7391. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.07298088  0.03468779 -0.24463822  0.6072378 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 7391 is [False, True, False, True, False, False]
Scene graph at timestep 7391 is [False, True, False, True, False, False]
State prediction error at timestep 7391 is tensor(3.4112e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7392. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.01029629 -0.02436252 -0.2282547  -0.11471355]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 7392 is [False, True, False, True, False, False]
Current timestep = 7393. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.02829325 -0.09429079 -0.24766561  0.26264203]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 7393 is [False, True, False, True, False, False]
Current timestep = 7394. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.09656376 -0.16503344 -0.23874441  0.09355676]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 7394 is [False, True, False, True, False, False]
Scene graph at timestep 7394 is [False, True, False, True, False, False]
State prediction error at timestep 7394 is tensor(1.5953e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7395. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.00249368 -0.13327615 -0.18827534  0.53624237]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 7395 is [False, True, False, True, False, False]
Current timestep = 7396. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.12537356 -0.2164548  -0.11000936  0.42113793]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 7396 is [False, True, False, True, False, False]
Current timestep = 7397. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.03475429 -0.18154332 -0.1111625   0.49098217]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 7397 is [False, True, False, True, False, False]
Current timestep = 7398. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.00837737  0.06566608 -0.20210946  0.67853904]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 7398 is [False, True, False, True, False, False]
Current timestep = 7399. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.02759348 -0.21123023  0.06261721  0.6026645 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 7399 is [False, True, False, True, False, False]
Scene graph at timestep 7399 is [False, True, False, True, False, False]
State prediction error at timestep 7399 is tensor(4.2132e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7400. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.04836173 -0.20624772 -0.17260426  0.6730895 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 7400 is [False, True, False, True, False, False]
Scene graph at timestep 7400 is [False, True, False, True, False, False]
State prediction error at timestep 7400 is tensor(1.1118e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7401. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0316693  -0.18410277 -0.09767249  0.05593419]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 7401 is [False, True, False, True, False, False]
Current timestep = 7402. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.04619017 -0.17880508  0.0093655   0.3893684 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 7402 is [False, True, False, True, False, False]
Current timestep = 7403. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.09773293 -0.19471517 -0.11761236  0.36547232]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 7403 is [False, True, False, True, False, False]
Current timestep = 7404. State = [[ 0.00254709 -0.31571192]]. Action = [[ 0.01087528 -0.08986688 -0.12851413  0.48986518]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 7404 is [False, True, False, True, False, False]
Current timestep = 7405. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.09606639 -0.15612748 -0.18848072  0.19386506]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 7405 is [False, True, False, True, False, False]
Current timestep = 7406. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.1000258  -0.22877303 -0.07863286  0.17437887]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 7406 is [False, True, False, True, False, False]
Current timestep = 7407. State = [[ 0.00254709 -0.31571192]]. Action = [[-0.0806874  -0.05440895  0.15059778  0.73325706]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 7407 is [False, True, False, True, False, False]
Current timestep = 7408. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.11578889 -0.1804658  -0.21309909  0.05429053]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 7408 is [False, True, False, True, False, False]
Current timestep = 7409. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.12220132 -0.1313223  -0.22955132  0.35781097]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 7409 is [False, True, False, True, False, False]
Current timestep = 7410. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.03281452 -0.20175968  0.10730207  0.19258308]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 7410 is [False, True, False, True, False, False]
Current timestep = 7411. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.06911442 -0.13030562 -0.2328821   0.2879697 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 7411 is [False, True, False, True, False, False]
Current timestep = 7412. State = [[ 0.00252912 -0.31570563]]. Action = [[ 0.06512985 -0.10251701 -0.21425147  0.42300308]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 7412 is [False, True, False, True, False, False]
Current timestep = 7413. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.04473746 -0.17826098  0.01098564 -0.34148955]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 7413 is [False, True, False, True, False, False]
Scene graph at timestep 7413 is [False, True, False, True, False, False]
State prediction error at timestep 7413 is tensor(4.7213e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7414. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.06108363 -0.23420148 -0.23259048  0.01325262]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 7414 is [False, True, False, True, False, False]
Current timestep = 7415. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.06684145 -0.21447495  0.06338513  0.3032961 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 7415 is [False, True, False, True, False, False]
Current timestep = 7416. State = [[ 0.00252912 -0.31570563]]. Action = [[ 0.00403255 -0.1392193  -0.23775178  0.41549397]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 7416 is [False, True, False, True, False, False]
Scene graph at timestep 7416 is [False, True, False, True, False, False]
State prediction error at timestep 7416 is tensor(4.3250e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7417. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.00691774 -0.14666648 -0.23716535  0.20927715]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 7417 is [False, True, False, True, False, False]
Scene graph at timestep 7417 is [False, True, False, True, False, False]
State prediction error at timestep 7417 is tensor(1.9622e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7418. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.14071283 -0.11596856  0.18770462  0.28494143]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 7418 is [False, True, False, True, False, False]
Scene graph at timestep 7418 is [False, True, False, True, False, False]
State prediction error at timestep 7418 is tensor(2.3251e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7419. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.05066293 -0.10226682  0.14851317  0.05329132]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 7419 is [False, True, False, True, False, False]
Scene graph at timestep 7419 is [False, True, False, True, False, False]
State prediction error at timestep 7419 is tensor(8.7194e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7420. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.12605354 -0.1537965   0.07326815  0.29770732]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 7420 is [False, True, False, True, False, False]
Scene graph at timestep 7420 is [False, True, False, True, False, False]
State prediction error at timestep 7420 is tensor(6.2067e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7421. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.11135317  0.03813684 -0.24399315  0.0352726 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 7421 is [False, True, False, True, False, False]
Current timestep = 7422. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.1140154  -0.1810215  -0.14381032  0.7931025 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 7422 is [False, True, False, True, False, False]
Scene graph at timestep 7422 is [False, True, False, True, False, False]
State prediction error at timestep 7422 is tensor(1.1647e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7423. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.07781427  0.00500205 -0.01663144  0.3479079 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 7423 is [False, True, False, True, False, False]
Current timestep = 7424. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.0976575  -0.16756515 -0.18743962 -0.4323002 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 7424 is [False, True, False, True, False, False]
Current timestep = 7425. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.03168762 -0.1180671  -0.23522028 -0.01236123]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 7425 is [False, True, False, True, False, False]
Current timestep = 7426. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.09805889 -0.03520557  0.17184657  0.8345237 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 7426 is [False, True, False, True, False, False]
Current timestep = 7427. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.13561316 -0.02552436 -0.05485724  0.6447034 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 7427 is [False, True, False, True, False, False]
Scene graph at timestep 7427 is [False, True, False, True, False, False]
State prediction error at timestep 7427 is tensor(3.0354e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7428. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.1827511  -0.18899404 -0.15037876  0.28949738]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 7428 is [False, True, False, True, False, False]
Scene graph at timestep 7428 is [False, True, False, True, False, False]
State prediction error at timestep 7428 is tensor(1.8312e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7429. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.15195002 -0.11356577 -0.17834128  0.38246036]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 7429 is [False, True, False, True, False, False]
Current timestep = 7430. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.02852643 -0.13029896  0.18147936  0.5068188 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 7430 is [False, True, False, True, False, False]
Current timestep = 7431. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.02115844 -0.14176966 -0.13325141  0.46825528]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 7431 is [False, True, False, True, False, False]
Scene graph at timestep 7431 is [False, True, False, True, False, False]
State prediction error at timestep 7431 is tensor(5.6283e-09, grad_fn=<MseLossBackward0>)
Current timestep = 7432. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.02909279 -0.1170072  -0.0143292   0.09819841]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 7432 is [False, True, False, True, False, False]
Scene graph at timestep 7432 is [False, True, False, True, False, False]
State prediction error at timestep 7432 is tensor(2.8274e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7433. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.08183721 -0.13381208 -0.21735771  0.09697664]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 7433 is [False, True, False, True, False, False]
Scene graph at timestep 7433 is [False, True, False, True, False, False]
State prediction error at timestep 7433 is tensor(8.8940e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7434. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.09343338 -0.21144332  0.04963422  0.04487646]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 7434 is [False, True, False, True, False, False]
Current timestep = 7435. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.06778041 -0.12524895  0.0632458   0.20944893]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 7435 is [False, True, False, True, False, False]
Scene graph at timestep 7435 is [False, True, False, True, False, False]
State prediction error at timestep 7435 is tensor(4.3726e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7436. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.11758687 -0.21806775 -0.23852094 -0.01556861]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 7436 is [False, True, False, True, False, False]
Current timestep = 7437. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.06864476 -0.13699946  0.24577475  0.40296042]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 7437 is [False, True, False, True, False, False]
Current timestep = 7438. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.03597344 -0.16006291 -0.2213914   0.7176213 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 7438 is [False, True, False, True, False, False]
Scene graph at timestep 7438 is [False, True, False, True, False, False]
State prediction error at timestep 7438 is tensor(5.6465e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7439. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.00350128 -0.19826928 -0.02778649  0.42875147]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 7439 is [False, True, False, True, False, False]
Scene graph at timestep 7439 is [False, True, False, True, False, False]
State prediction error at timestep 7439 is tensor(1.7469e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7440. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.06710705 -0.21752176 -0.06272203 -0.05230242]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 7440 is [False, True, False, True, False, False]
Scene graph at timestep 7440 is [False, True, False, True, False, False]
State prediction error at timestep 7440 is tensor(1.5684e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7441. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.13503294 -0.15370093 -0.2225797   0.7133535 ]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 7441 is [False, True, False, True, False, False]
Scene graph at timestep 7441 is [False, True, False, True, False, False]
State prediction error at timestep 7441 is tensor(5.5232e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7442. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.02063547 -0.19023429 -0.17808577 -0.01646721]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 7442 is [False, True, False, True, False, False]
Current timestep = 7443. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.06946793 -0.23395322  0.14434546  0.50149846]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 7443 is [False, True, False, True, False, False]
Current timestep = 7444. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.10329396 -0.14685518  0.23231447  0.23152363]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 7444 is [False, True, False, True, False, False]
Current timestep = 7445. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.03092615 -0.130674    0.07689679  0.35852838]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 7445 is [False, True, False, True, False, False]
Current timestep = 7446. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.07867834  0.00545374 -0.06345223  0.62977433]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 7446 is [False, True, False, True, False, False]
Scene graph at timestep 7446 is [False, True, False, True, False, False]
State prediction error at timestep 7446 is tensor(2.8382e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7447. State = [[ 0.00252912 -0.31570563]]. Action = [[ 0.06854641 -0.21321014 -0.24538505  0.6246346 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 7447 is [False, True, False, True, False, False]
Scene graph at timestep 7447 is [False, True, False, True, False, False]
State prediction error at timestep 7447 is tensor(2.8128e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7448. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.04408884 -0.20649326 -0.23847578 -0.08759069]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 7448 is [False, True, False, True, False, False]
Current timestep = 7449. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.13987346 -0.17884181 -0.24881181  0.88984764]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 7449 is [False, True, False, True, False, False]
Current timestep = 7450. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.15932877 -0.06747341  0.19089687 -0.2913381 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 7450 is [False, True, False, True, False, False]
Scene graph at timestep 7450 is [False, True, False, True, False, False]
State prediction error at timestep 7450 is tensor(1.7098e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7451. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.12994857 -0.03183413 -0.11344472  0.75723577]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 7451 is [False, True, False, True, False, False]
Current timestep = 7452. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.14251448 -0.16103129 -0.24681532  0.3690616 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 7452 is [False, True, False, True, False, False]
Current timestep = 7453. State = [[ 0.00252912 -0.31570563]]. Action = [[ 0.01710305  0.06865436 -0.10248785  0.50676405]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 7453 is [False, True, False, True, False, False]
Current timestep = 7454. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.05799033 -0.147487   -0.21806511  0.639807  ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 7454 is [False, True, False, True, False, False]
Scene graph at timestep 7454 is [False, True, False, True, False, False]
State prediction error at timestep 7454 is tensor(5.1933e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7455. State = [[ 0.00252912 -0.31570563]]. Action = [[-0.12310362 -0.15775287 -0.22833851  0.43152714]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 7455 is [False, True, False, True, False, False]
Current timestep = 7456. State = [[ 0.00249691 -0.3157666 ]]. Action = [[-0.09319159  0.14512229  0.16713601  0.04726994]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 7456 is [False, True, False, True, False, False]
Current timestep = 7457. State = [[ 0.00251392 -0.3156938 ]]. Action = [[-0.00879532 -0.18828265  0.20672643  0.7197478 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 7457 is [False, True, False, True, False, False]
Scene graph at timestep 7457 is [False, True, False, True, False, False]
State prediction error at timestep 7457 is tensor(4.8174e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7458. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.15433522 -0.0558237   0.15280524  0.62399423]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 7458 is [False, True, False, True, False, False]
Current timestep = 7459. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.01568025 -0.06542312 -0.10012498  0.4230274 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 7459 is [False, True, False, True, False, False]
Current timestep = 7460. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.11352399 -0.1932777   0.01595861  0.12814677]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 7460 is [False, True, False, True, False, False]
Current timestep = 7461. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.12177613 -0.08530919  0.06928504  0.6262281 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 7461 is [False, True, False, True, False, False]
Current timestep = 7462. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.14122276 -0.22863017 -0.24402264  0.5875002 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 7462 is [False, True, False, True, False, False]
Current timestep = 7463. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.01585582 -0.23805782 -0.00899029  0.8751786 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 7463 is [False, True, False, True, False, False]
Current timestep = 7464. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.08942024 -0.24124688 -0.03005934  0.5897052 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 7464 is [False, True, False, True, False, False]
Current timestep = 7465. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.08580324 -0.19639452 -0.23912905  0.72453094]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 7465 is [False, True, False, True, False, False]
Scene graph at timestep 7465 is [False, True, False, True, False, False]
State prediction error at timestep 7465 is tensor(9.1175e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7466. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.12602155 -0.10341735 -0.04456656  0.4373517 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 7466 is [False, True, False, True, False, False]
Scene graph at timestep 7466 is [False, True, False, True, False, False]
State prediction error at timestep 7466 is tensor(2.5700e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7467. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.09268972 -0.05372381 -0.20805633  0.5073056 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 7467 is [False, True, False, True, False, False]
Current timestep = 7468. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.13737582 -0.12275571  0.14004803  0.44450665]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 7468 is [False, True, False, True, False, False]
Current timestep = 7469. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.10634905 -0.21415631  0.04525578  0.62357473]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 7469 is [False, True, False, True, False, False]
Scene graph at timestep 7469 is [False, True, False, True, False, False]
State prediction error at timestep 7469 is tensor(2.5404e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7470. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.15498562 -0.24126084 -0.22726567  0.41823924]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 7470 is [False, True, False, True, False, False]
Current timestep = 7471. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.1554244  -0.11116025 -0.22865501  0.7638948 ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 7471 is [False, True, False, True, False, False]
Current timestep = 7472. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.14289525 -0.21959448 -0.24800576  0.58813834]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 7472 is [False, True, False, True, False, False]
Current timestep = 7473. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.13169818 -0.12756942  0.01062408  0.50032127]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 7473 is [False, True, False, True, False, False]
Current timestep = 7474. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.11458698 -0.13285932  0.14150754  0.42192233]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 7474 is [False, True, False, True, False, False]
Scene graph at timestep 7474 is [False, True, False, True, False, False]
State prediction error at timestep 7474 is tensor(1.9642e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7475. State = [[ 0.00254613 -0.31563282]]. Action = [[-0.04106553 -0.17789622  0.18442369  0.54846275]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 7475 is [False, True, False, True, False, False]
Scene graph at timestep 7475 is [False, True, False, True, False, False]
State prediction error at timestep 7475 is tensor(2.0683e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7476. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.10227987 -0.23679022  0.22384566  0.64553785]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 7476 is [False, True, False, True, False, False]
Current timestep = 7477. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.09538287  0.0340054   0.17854416  0.87191415]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 7477 is [False, True, False, True, False, False]
Scene graph at timestep 7477 is [False, True, False, True, False, False]
State prediction error at timestep 7477 is tensor(7.4842e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7478. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.11749756 -0.11367011 -0.23864634  0.4226737 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 7478 is [False, True, False, True, False, False]
Current timestep = 7479. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.00440764 -0.19917922 -0.15434533  0.6699977 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 7479 is [False, True, False, True, False, False]
Current timestep = 7480. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.11556771 -0.21289086  0.12775138 -0.39432865]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 7480 is [False, True, False, True, False, False]
Current timestep = 7481. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.16164404  0.00613225  0.15192026  0.4754969 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 7481 is [False, True, False, True, False, False]
Scene graph at timestep 7481 is [False, True, False, True, False, False]
State prediction error at timestep 7481 is tensor(7.6752e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7482. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.12588869 -0.21780197 -0.24119586 -0.4031558 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 7482 is [False, True, False, True, False, False]
Current timestep = 7483. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.05283538 -0.21075502  0.19905895  0.21533561]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 7483 is [False, True, False, True, False, False]
Current timestep = 7484. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.11846964 -0.23427767  0.19858658  0.8285537 ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 7484 is [False, True, False, True, False, False]
Current timestep = 7485. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.1223397  -0.16509299  0.20338657  0.55011296]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 7485 is [False, True, False, True, False, False]
Current timestep = 7486. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.08544825 -0.21096377 -0.21715973  0.68681335]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 7486 is [False, True, False, True, False, False]
Current timestep = 7487. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.12280014 -0.14549698  0.12688285  0.18729436]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 7487 is [False, True, False, True, False, False]
Scene graph at timestep 7487 is [False, True, False, True, False, False]
State prediction error at timestep 7487 is tensor(9.1854e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7488. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.13940929 -0.20100714 -0.05977477  0.78667974]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 7488 is [False, True, False, True, False, False]
Scene graph at timestep 7488 is [False, True, False, True, False, False]
State prediction error at timestep 7488 is tensor(3.3429e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7489. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.06020138 -0.06559578  0.2455833   0.7136456 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 7489 is [False, True, False, True, False, False]
Current timestep = 7490. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.09128331 -0.21467148 -0.1576326  -0.02089024]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 7490 is [False, True, False, True, False, False]
Current timestep = 7491. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.13715523 -0.07080999 -0.23122738  0.7333602 ]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 7491 is [False, True, False, True, False, False]
Scene graph at timestep 7491 is [False, True, False, True, False, False]
State prediction error at timestep 7491 is tensor(1.8932e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7492. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.14136735 -0.23119602  0.14474958  0.49000633]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 7492 is [False, True, False, True, False, False]
Current timestep = 7493. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.06888814 -0.18344815 -0.11811867  0.7282038 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 7493 is [False, True, False, True, False, False]
Current timestep = 7494. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.13238104 -0.06878182 -0.11765996  0.73620415]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 7494 is [False, True, False, True, False, False]
Current timestep = 7495. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.12365961 -0.18110234 -0.21032788  0.30170918]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 7495 is [False, True, False, True, False, False]
Current timestep = 7496. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.04803471 -0.2271596  -0.23266265  0.23894095]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 7496 is [False, True, False, True, False, False]
Current timestep = 7497. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.08625197 -0.18588942 -0.04565948  0.6895857 ]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 7497 is [False, True, False, True, False, False]
Current timestep = 7498. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.11984339 -0.09772262 -0.2234913   0.16770959]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 7498 is [False, True, False, True, False, False]
Current timestep = 7499. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.05801733 -0.18404979 -0.19410361  0.44871926]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 7499 is [False, True, False, True, False, False]
Current timestep = 7500. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.10006079 -0.01477721 -0.24509664  0.530349  ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 7500 is [False, True, False, True, False, False]
Current timestep = 7501. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.08459432  0.00407273 -0.10958934  0.6683004 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 7501 is [False, True, False, True, False, False]
Current timestep = 7502. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.1031009  -0.229996   -0.14614685  0.6686628 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 7502 is [False, True, False, True, False, False]
Current timestep = 7503. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.12709147 -0.08618769 -0.01475939  0.64352214]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 7503 is [False, True, False, True, False, False]
Current timestep = 7504. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.12558249 -0.22800675 -0.14887883  0.7457197 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 7504 is [False, True, False, True, False, False]
Current timestep = 7505. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.02955818 -0.23905732 -0.07501912  0.67044926]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 7505 is [False, True, False, True, False, False]
Current timestep = 7506. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.077636   -0.23328888 -0.03973345  0.13910818]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 7506 is [False, True, False, True, False, False]
Current timestep = 7507. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.02233464 -0.23709644 -0.18417409 -0.1700908 ]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 7507 is [False, True, False, True, False, False]
Scene graph at timestep 7507 is [False, True, False, True, False, False]
State prediction error at timestep 7507 is tensor(3.7823e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7508. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.10514298  0.04077452  0.21035337  0.8739462 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 7508 is [False, True, False, True, False, False]
Scene graph at timestep 7508 is [False, True, False, True, False, False]
State prediction error at timestep 7508 is tensor(3.2902e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7509. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.1349617  -0.07509425 -0.07447402 -0.02707464]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 7509 is [False, True, False, True, False, False]
Scene graph at timestep 7509 is [False, True, False, True, False, False]
State prediction error at timestep 7509 is tensor(6.5749e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7510. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.05917688 -0.22258495 -0.12686498  0.82992435]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 7510 is [False, True, False, True, False, False]
Current timestep = 7511. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.12697095 -0.12031329 -0.21393196 -0.03435397]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 7511 is [False, True, False, True, False, False]
Current timestep = 7512. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.06561625 -0.05347857 -0.24514917  0.31173325]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 7512 is [False, True, False, True, False, False]
Current timestep = 7513. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.00807759 -0.12430429  0.14344797  0.5450913 ]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 7513 is [False, True, False, True, False, False]
Current timestep = 7514. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.1597959  -0.20246725 -0.24736097  0.62999344]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 7514 is [False, True, False, True, False, False]
Current timestep = 7515. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.1236192  -0.0615553  -0.24250904  0.5815127 ]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 7515 is [False, True, False, True, False, False]
Current timestep = 7516. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.05480851 -0.12569413  0.18818384  0.78933287]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 7516 is [False, True, False, True, False, False]
Current timestep = 7517. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.13632397 -0.06277698 -0.19599617  0.15949571]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 7517 is [False, True, False, True, False, False]
Current timestep = 7518. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.07485831 -0.19202965  0.01208866  0.7428671 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 7518 is [False, True, False, True, False, False]
Scene graph at timestep 7518 is [False, True, False, True, False, False]
State prediction error at timestep 7518 is tensor(1.9864e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7519. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.08909418 -0.0703091   0.22444588  0.5486369 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 7519 is [False, True, False, True, False, False]
Scene graph at timestep 7519 is [False, True, False, True, False, False]
State prediction error at timestep 7519 is tensor(1.9215e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7520. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.07642379 -0.12201491 -0.24017522  0.40557826]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 7520 is [False, True, False, True, False, False]
Current timestep = 7521. State = [[ 0.00253717 -0.3156297 ]]. Action = [[-0.00822112 -0.11081624  0.24960542  0.80124795]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 7521 is [False, True, False, True, False, False]
Current timestep = 7522. State = [[ 0.00249663 -0.31562862]]. Action = [[-0.09404245  0.13323784  0.22663116  0.08339047]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 7522 is [False, True, False, True, False, False]
Current timestep = 7523. State = [[ 0.00256093 -0.31526434]]. Action = [[-0.09385003 -0.05125846 -0.22209454  0.01679313]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 7523 is [False, True, False, True, False, False]
Current timestep = 7524. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.07502402 -0.2460764   0.21348995 -0.24648595]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 7524 is [False, True, False, True, False, False]
Current timestep = 7525. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.14813371 -0.15334114  0.21758276  0.3551755 ]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 7525 is [False, True, False, True, False, False]
Current timestep = 7526. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.01039162 -0.15217026 -0.23994091  0.6220627 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 7526 is [False, True, False, True, False, False]
Current timestep = 7527. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.12523787 -0.12371072 -0.00563221  0.16052854]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 7527 is [False, True, False, True, False, False]
Current timestep = 7528. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.12877436 -0.21355337  0.05484995  0.30027676]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 7528 is [False, True, False, True, False, False]
Current timestep = 7529. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.10257219 -0.03226352 -0.17350921  0.6175308 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 7529 is [False, True, False, True, False, False]
Current timestep = 7530. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.14812373 -0.17266683 -0.24404632  0.07663214]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 7530 is [False, True, False, True, False, False]
Current timestep = 7531. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.13577023 -0.24534406 -0.16923162 -0.05955315]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 7531 is [False, True, False, True, False, False]
Scene graph at timestep 7531 is [False, True, False, True, False, False]
State prediction error at timestep 7531 is tensor(7.8305e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7532. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.15189695  0.02750716 -0.24123363  0.11595893]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 7532 is [False, True, False, True, False, False]
Scene graph at timestep 7532 is [False, True, False, True, False, False]
State prediction error at timestep 7532 is tensor(1.0124e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7533. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.03689498 -0.15694974 -0.23938644  0.3267312 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 7533 is [False, True, False, True, False, False]
Current timestep = 7534. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.0633193  -0.18968576 -0.21941431  0.772081  ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 7534 is [False, True, False, True, False, False]
Current timestep = 7535. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.11417621 -0.14175195 -0.12899536  0.05611479]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 7535 is [False, True, False, True, False, False]
Scene graph at timestep 7535 is [False, True, False, True, False, False]
State prediction error at timestep 7535 is tensor(1.0580e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7536. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.04032663 -0.19522564 -0.24531253  0.36388302]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 7536 is [False, True, False, True, False, False]
Current timestep = 7537. State = [[ 0.00254622 -0.31510237]]. Action = [[-0.02873991 -0.06080443  0.04673404  0.44444788]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 7537 is [False, True, False, True, False, False]
Current timestep = 7538. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.0591625   0.09961635 -0.17950031  0.49750924]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 7538 is [False, True, False, True, False, False]
Current timestep = 7539. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.06371042 -0.18575716 -0.24756372  0.51437306]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 7539 is [False, True, False, True, False, False]
Current timestep = 7540. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.08283827 -0.18730332  0.14940464  0.20837295]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 7540 is [False, True, False, True, False, False]
Current timestep = 7541. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.12105456  0.00755823 -0.2454057   0.63114905]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 7541 is [False, True, False, True, False, False]
Scene graph at timestep 7541 is [False, True, False, True, False, False]
State prediction error at timestep 7541 is tensor(1.0429e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7542. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.1778593  -0.01200978 -0.24508487  0.8078357 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 7542 is [False, True, False, True, False, False]
Scene graph at timestep 7542 is [False, True, False, True, False, False]
State prediction error at timestep 7542 is tensor(1.2093e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7543. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.11435069 -0.10020712 -0.24009654  0.7658429 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 7543 is [False, True, False, True, False, False]
Scene graph at timestep 7543 is [False, True, False, True, False, False]
State prediction error at timestep 7543 is tensor(1.3719e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7544. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.0929032  -0.22387585  0.1359171  -0.13310117]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 7544 is [False, True, False, True, False, False]
Scene graph at timestep 7544 is [False, True, False, True, False, False]
State prediction error at timestep 7544 is tensor(2.5678e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7545. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.13552555 -0.11263412 -0.23957893  0.6050706 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 7545 is [False, True, False, True, False, False]
Scene graph at timestep 7545 is [False, True, False, True, False, False]
State prediction error at timestep 7545 is tensor(3.7134e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7546. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.07355845  0.03761885 -0.21343596  0.3479439 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 7546 is [False, True, False, True, False, False]
Current timestep = 7547. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.06510301 -0.11147383 -0.24497868  0.4452877 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 7547 is [False, True, False, True, False, False]
Current timestep = 7548. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.18193057 -0.22139607 -0.22947921  0.00639677]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 7548 is [False, True, False, True, False, False]
Current timestep = 7549. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.11856425 -0.23349582  0.09849215  0.70957685]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 7549 is [False, True, False, True, False, False]
Current timestep = 7550. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.15282497 -0.09639481  0.06909645  0.13400245]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 7550 is [False, True, False, True, False, False]
Current timestep = 7551. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.09553568 -0.15480407 -0.2138123  -0.05696487]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 7551 is [False, True, False, True, False, False]
Current timestep = 7552. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.15016338 -0.22776258  0.20001453  0.4182527 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 7552 is [False, True, False, True, False, False]
Current timestep = 7553. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.14423136 -0.24697654  0.1677593   0.6871363 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 7553 is [False, True, False, True, False, False]
Current timestep = 7554. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.13587397 -0.15426378 -0.22548869 -0.17261732]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 7554 is [False, True, False, True, False, False]
Current timestep = 7555. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.10621415  0.0580858  -0.0593279   0.08018899]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 7555 is [False, True, False, True, False, False]
Current timestep = 7556. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.12427583 -0.05242851  0.24566352  0.73837686]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 7556 is [False, True, False, True, False, False]
Current timestep = 7557. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.102107   -0.15809225  0.00418103  0.65288377]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 7557 is [False, True, False, True, False, False]
Scene graph at timestep 7557 is [False, True, False, True, False, False]
State prediction error at timestep 7557 is tensor(3.0253e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7558. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.09109041 -0.18471879 -0.10799092  0.926913  ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 7558 is [False, True, False, True, False, False]
Current timestep = 7559. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.07761151 -0.04482351 -0.20007655  0.09057784]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 7559 is [False, True, False, True, False, False]
Current timestep = 7560. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.14400613 -0.12733288  0.16433805  0.3893485 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 7560 is [False, True, False, True, False, False]
Current timestep = 7561. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.07336579 -0.17507553  0.01953956  0.5894482 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 7561 is [False, True, False, True, False, False]
Current timestep = 7562. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.05613887 -0.2473918   0.10051724  0.5318339 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 7562 is [False, True, False, True, False, False]
Current timestep = 7563. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.11998948 -0.11228675 -0.2318654   0.35112095]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 7563 is [False, True, False, True, False, False]
Current timestep = 7564. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.09588096 -0.24466617 -0.23807813  0.22388053]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 7564 is [False, True, False, True, False, False]
Current timestep = 7565. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.06293185 -0.23685837  0.23757291  0.5884254 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 7565 is [False, True, False, True, False, False]
Current timestep = 7566. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.13946803 -0.03718859 -0.12431788  0.77296996]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 7566 is [False, True, False, True, False, False]
Current timestep = 7567. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.11440413 -0.23342648 -0.22208792  0.06741714]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 7567 is [False, True, False, True, False, False]
Current timestep = 7568. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.11175856 -0.22526355  0.14467967 -0.7609345 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 7568 is [False, True, False, True, False, False]
Scene graph at timestep 7568 is [False, True, False, True, False, False]
State prediction error at timestep 7568 is tensor(1.3898e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7569. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.13938704 -0.24689408 -0.16788003  0.75361586]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 7569 is [False, True, False, True, False, False]
Current timestep = 7570. State = [[ 0.00256333 -0.31502885]]. Action = [[-0.14526147  0.08930951  0.01299858 -0.02629876]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 7570 is [False, True, False, True, False, False]
Scene graph at timestep 7570 is [False, True, False, True, False, False]
State prediction error at timestep 7570 is tensor(6.5643e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7571. State = [[-0.24584578 -0.01222469]]. Action = [[-0.14626183  0.20965138 -0.14551002  0.4535482 ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 7571 is [False, True, False, True, False, False]
Current timestep = 7572. State = [[-0.24391294 -0.01309781]]. Action = [[ 0.24315214 -0.07554449 -0.13489565 -0.24476457]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 7572 is [True, False, False, False, True, False]
Current timestep = 7573. State = [[-0.24150561 -0.01404121]]. Action = [[ 0.238549   -0.09684333 -0.146491   -0.15401018]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 7573 is [True, False, False, False, True, False]
Current timestep = 7574. State = [[-0.23700307 -0.0155751 ]]. Action = [[ 0.21857256 -0.10695466 -0.14241038 -0.20028794]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 7574 is [True, False, False, False, True, False]
Current timestep = 7575. State = [[-0.23106836 -0.01798342]]. Action = [[ 0.23390844 -0.07004769 -0.13967718 -0.18572462]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 7575 is [True, False, False, False, True, False]
Current timestep = 7576. State = [[-0.22507319 -0.02034106]]. Action = [[ 0.23374784 -0.06704745 -0.15207204 -0.25879228]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 7576 is [True, False, False, False, True, False]
Current timestep = 7577. State = [[-0.21821767 -0.0227568 ]]. Action = [[ 0.24284756 -0.10117704 -0.142247   -0.14208496]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 7577 is [True, False, False, False, True, False]
Current timestep = 7578. State = [[-0.20973074 -0.02611208]]. Action = [[ 0.20531315 -0.10035625 -0.12344462 -0.13921148]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 7578 is [True, False, False, False, True, False]
Current timestep = 7579. State = [[-0.2021207  -0.02939357]]. Action = [[ 0.2431562  -0.07241008 -0.12270021  0.00200915]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 7579 is [True, False, False, False, True, False]
Current timestep = 7580. State = [[-0.19411412 -0.03285516]]. Action = [[ 0.20896503 -0.08288822 -0.14059801 -0.12517345]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 7580 is [True, False, False, False, True, False]
Scene graph at timestep 7580 is [True, False, False, False, True, False]
State prediction error at timestep 7580 is tensor(1.9059e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7581. State = [[-0.18575743 -0.03598927]]. Action = [[ 0.21940398 -0.05453125 -0.13916138 -0.00217825]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 7581 is [True, False, False, False, True, False]
Current timestep = 7582. State = [[-0.17823172 -0.03823216]]. Action = [[ 0.18481106 -0.0698666  -0.13523832 -0.1395477 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 7582 is [True, False, False, False, True, False]
Current timestep = 7583. State = [[-0.17060849 -0.04084787]]. Action = [[ 0.24521616 -0.06999324 -0.14078814 -0.06590837]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 7583 is [True, False, False, False, True, False]
Scene graph at timestep 7583 is [True, False, False, False, True, False]
State prediction error at timestep 7583 is tensor(1.5400e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7583 of 1
Current timestep = 7584. State = [[-0.16168268 -0.04359328]]. Action = [[ 0.24223942 -0.06308103 -0.16264845 -0.05889773]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 7584 is [True, False, False, False, True, False]
Current timestep = 7585. State = [[-0.15378176 -0.04613826]]. Action = [[ 0.24331996 -0.09733194 -0.1427925  -0.15633416]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 7585 is [True, False, False, False, True, False]
Current timestep = 7586. State = [[-0.1452826  -0.04904315]]. Action = [[ 0.220543   -0.07471249 -0.15780358 -0.14053106]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 7586 is [True, False, False, False, True, False]
Human Feedback received at timestep 7586 of 1
Current timestep = 7587. State = [[-0.13639714 -0.05206044]]. Action = [[ 0.23459512 -0.1069838  -0.10081059 -0.16953295]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 7587 is [True, False, False, False, True, False]
Current timestep = 7588. State = [[-0.12843296 -0.05559026]]. Action = [[ 0.2108404  -0.14554448 -0.16028102  0.10501695]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 7588 is [True, False, False, False, True, False]
Scene graph at timestep 7588 is [True, False, False, False, True, False]
State prediction error at timestep 7588 is tensor(1.9683e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7589. State = [[-0.11907359 -0.05972787]]. Action = [[ 0.2053597  -0.06325951 -0.12456478 -0.10864365]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 7589 is [True, False, False, False, True, False]
Current timestep = 7590. State = [[-0.11135666 -0.06286671]]. Action = [[ 0.19164953 -0.09335366 -0.11541359  0.00035071]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 7590 is [True, False, False, False, True, False]
Scene graph at timestep 7590 is [True, False, False, False, True, False]
State prediction error at timestep 7590 is tensor(1.7401e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7591. State = [[-0.10300075 -0.06652542]]. Action = [[ 0.03522363 -0.09982657 -0.13125028 -0.24154693]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 7591 is [True, False, False, False, True, False]
Human Feedback received at timestep 7591 of 1
Current timestep = 7592. State = [[-0.09674736 -0.0699316 ]]. Action = [[ 0.20672897 -0.06722513 -0.1763888   0.12505996]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 7592 is [True, False, False, False, True, False]
Current timestep = 7593. State = [[-0.09112869 -0.07289025]]. Action = [[ 0.1329925  -0.10475469 -0.15084736  0.00431538]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 7593 is [True, False, False, False, True, False]
Current timestep = 7594. State = [[-0.08572178 -0.07600053]]. Action = [[ 0.19334635 -0.09525853 -0.07955354 -0.05331898]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 7594 is [True, False, False, False, True, False]
Human Feedback received at timestep 7594 of 1
Current timestep = 7595. State = [[-0.07920444 -0.07960574]]. Action = [[ 0.22801489 -0.02808523 -0.1601156  -0.1318019 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 7595 is [True, False, False, False, True, False]
Current timestep = 7596. State = [[-0.07326631 -0.08184644]]. Action = [[ 0.1716026  -0.10863593 -0.12665476 -0.00451189]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 7596 is [True, False, False, False, True, False]
Current timestep = 7597. State = [[-0.06672914 -0.08472279]]. Action = [[ 0.14949116 -0.09415439 -0.16005895 -0.07327211]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 7597 is [True, False, False, False, True, False]
Current timestep = 7598. State = [[-0.06042862 -0.08760851]]. Action = [[ 0.14479005 -0.05977722 -0.15072629  0.0914948 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 7598 is [True, False, False, False, True, False]
Human Feedback received at timestep 7598 of 1
Current timestep = 7599. State = [[-0.0538976  -0.09074344]]. Action = [[ 0.18229419 -0.14251862 -0.1346893  -0.36589545]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 7599 is [True, False, False, False, True, False]
Current timestep = 7600. State = [[-0.04649719 -0.09516004]]. Action = [[ 0.188909   -0.1662424  -0.16292928  0.05230081]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 7600 is [True, False, False, False, True, False]
Scene graph at timestep 7600 is [False, True, False, False, True, False]
State prediction error at timestep 7600 is tensor(2.8569e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7600 of 1
Current timestep = 7601. State = [[-0.03964039 -0.10039519]]. Action = [[ 0.14216304 -0.15999469 -0.17118302  0.09071147]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 7601 is [False, True, False, False, True, False]
Current timestep = 7602. State = [[-0.03404125 -0.1055849 ]]. Action = [[ 0.06623426 -0.07491866 -0.13857082  0.07423329]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 7602 is [False, True, False, False, True, False]
Current timestep = 7603. State = [[-0.02948193 -0.10959723]]. Action = [[ 0.09875491 -0.07528079 -0.13545762  0.27247512]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 7603 is [False, True, False, False, True, False]
Scene graph at timestep 7603 is [False, True, False, False, True, False]
State prediction error at timestep 7603 is tensor(1.4607e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7603 of 1
Current timestep = 7604. State = [[-0.02455438 -0.11328987]]. Action = [[ 0.06758183  0.06269285 -0.2105575   0.30472207]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 7604 is [False, True, False, False, True, False]
Current timestep = 7605. State = [[-0.02075223 -0.11434189]]. Action = [[-0.0284629  -0.10411304 -0.1349193   0.03134704]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 7605 is [False, True, False, False, True, False]
Current timestep = 7606. State = [[-0.01893101 -0.11611145]]. Action = [[0.09813312 0.02137244 0.05812642 0.20973933]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 7606 is [False, True, False, False, True, False]
Current timestep = 7607. State = [[-0.01713542 -0.11704586]]. Action = [[ 0.13924426 -0.10546112 -0.1474397   0.35888422]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 7607 is [False, True, False, False, True, False]
Human Feedback received at timestep 7607 of 1
Current timestep = 7608. State = [[-0.01355378 -0.11907966]]. Action = [[-0.11523619 -0.14926589 -0.06341588  0.39757895]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 7608 is [False, True, False, False, True, False]
Current timestep = 7609. State = [[-0.01118538 -0.12322839]]. Action = [[-0.01032574 -0.16414173 -0.0970718   0.14690638]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 7609 is [False, True, False, False, True, False]
Current timestep = 7610. State = [[-0.00998433 -0.12769504]]. Action = [[-0.00309452 -0.15447703 -0.17240511  0.3275789 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 7610 is [False, True, False, False, True, False]
Current timestep = 7611. State = [[-0.0094115  -0.13273884]]. Action = [[-0.07788824  0.07367894 -0.22087932  0.33519173]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 7611 is [False, True, False, True, False, False]
Scene graph at timestep 7611 is [False, True, False, True, False, False]
State prediction error at timestep 7611 is tensor(1.9922e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7612. State = [[-0.00954009 -0.13477361]]. Action = [[ 0.04409096 -0.11187445 -0.15758108  0.37743044]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 7612 is [False, True, False, True, False, False]
Current timestep = 7613. State = [[-0.00976312 -0.1377003 ]]. Action = [[ 0.16121215  0.00991449 -0.1146739  -0.02018124]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 7613 is [False, True, False, True, False, False]
Current timestep = 7614. State = [[-0.00878074 -0.13877277]]. Action = [[ 0.01504886 -0.09163602 -0.15356101  0.47637796]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 7614 is [False, True, False, True, False, False]
Scene graph at timestep 7614 is [False, True, False, True, False, False]
State prediction error at timestep 7614 is tensor(3.4332e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7615. State = [[-0.00781377 -0.14095497]]. Action = [[ 0.11427921 -0.08827519 -0.11737223  0.462929  ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 7615 is [False, True, False, True, False, False]
Scene graph at timestep 7615 is [False, True, False, True, False, False]
State prediction error at timestep 7615 is tensor(4.1704e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7616. State = [[-0.0063519  -0.14353172]]. Action = [[ 0.06860846 -0.1725971  -0.13968074 -0.21736443]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 7616 is [False, True, False, True, False, False]
Current timestep = 7617. State = [[-0.00459905 -0.14772682]]. Action = [[ 0.03121895 -0.06633565 -0.02373567 -0.2273103 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 7617 is [False, True, False, True, False, False]
Current timestep = 7618. State = [[-0.00308209 -0.15131037]]. Action = [[ 0.14485937 -0.10420915 -0.21275754  0.5945251 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 7618 is [False, True, False, True, False, False]
Human Feedback received at timestep 7618 of -1
Current timestep = 7619. State = [[ 1.10382374e-04 -1.55054271e-01]]. Action = [[ 0.00835255 -0.0890896  -0.13164797  0.17360044]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 7619 is [False, True, False, True, False, False]
Scene graph at timestep 7619 is [False, True, False, True, False, False]
State prediction error at timestep 7619 is tensor(8.1943e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7620. State = [[ 0.00355502 -0.15872683]]. Action = [[-0.02946314 -0.07741109 -0.21186367 -0.1520533 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 7620 is [False, True, False, True, False, False]
Scene graph at timestep 7620 is [False, True, False, True, False, False]
State prediction error at timestep 7620 is tensor(7.4935e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7620 of -1
Current timestep = 7621. State = [[ 0.0060224  -0.16274418]]. Action = [[ 0.07790875 -0.2025898  -0.20754798 -0.1213305 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 7621 is [False, True, False, True, False, False]
Current timestep = 7622. State = [[ 0.00887592 -0.16759117]]. Action = [[-0.08021715 -0.05257812 -0.18520077  0.35563064]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 7622 is [False, True, False, True, False, False]
Scene graph at timestep 7622 is [False, True, False, True, False, False]
State prediction error at timestep 7622 is tensor(1.5747e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7622 of -1
Current timestep = 7623. State = [[ 0.00989667 -0.17157759]]. Action = [[-0.00878331 -0.19261573 -0.15471466  0.35175335]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 7623 is [False, True, False, True, False, False]
Human Feedback received at timestep 7623 of -1
Current timestep = 7624. State = [[ 0.0095848 -0.1766166]]. Action = [[-0.10295098 -0.21447971 -0.01919191 -0.15186882]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 7624 is [False, True, False, True, False, False]
Human Feedback received at timestep 7624 of -1
Current timestep = 7625. State = [[ 0.00912895 -0.1831058 ]]. Action = [[-0.06242013 -0.13476348 -0.16999298 -0.25863934]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 7625 is [False, True, False, True, False, False]
Scene graph at timestep 7625 is [False, True, False, True, False, False]
State prediction error at timestep 7625 is tensor(1.7499e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7626. State = [[ 0.00865475 -0.18939675]]. Action = [[-0.00755922  0.10669112 -0.16327871  0.17625868]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 7626 is [False, True, False, True, False, False]
Scene graph at timestep 7626 is [False, True, False, True, False, False]
State prediction error at timestep 7626 is tensor(2.4328e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7627. State = [[ 0.00835951 -0.19120896]]. Action = [[-0.00073797 -0.2160297  -0.23630238  0.54016614]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 7627 is [False, True, False, True, False, False]
Human Feedback received at timestep 7627 of -1
Current timestep = 7628. State = [[ 0.00776541 -0.19528447]]. Action = [[-0.06257173  0.02242547 -0.21333444  0.11558342]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 7628 is [False, True, False, True, False, False]
Human Feedback received at timestep 7628 of -1
Current timestep = 7629. State = [[ 0.00722334 -0.19742544]]. Action = [[-0.15985633  0.00352639 -0.08601275  0.50629234]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 7629 is [False, True, False, True, False, False]
Scene graph at timestep 7629 is [False, True, False, True, False, False]
State prediction error at timestep 7629 is tensor(5.1347e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7630. State = [[ 0.00640278 -0.1999741 ]]. Action = [[-0.02844642 -0.23501718  0.01296774  0.04453397]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 7630 is [False, True, False, True, False, False]
Current timestep = 7631. State = [[ 0.00512432 -0.20461534]]. Action = [[-0.02300726 -0.14969711 -0.21998735 -0.6141426 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 7631 is [False, True, False, True, False, False]
Scene graph at timestep 7631 is [False, True, False, True, False, False]
State prediction error at timestep 7631 is tensor(1.1267e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7631 of -1
Current timestep = 7632. State = [[ 0.00373099 -0.20951779]]. Action = [[-0.08727011 -0.20671433 -0.13151649  0.52604866]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 7632 is [False, True, False, True, False, False]
Scene graph at timestep 7632 is [False, True, False, True, False, False]
State prediction error at timestep 7632 is tensor(1.3588e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7632 of -1
Current timestep = 7633. State = [[ 0.00263921 -0.2150359 ]]. Action = [[-0.11716554  0.18342417 -0.19457561 -0.21147609]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 7633 is [False, True, False, True, False, False]
Current timestep = 7634. State = [[ 0.00205666 -0.21639211]]. Action = [[-0.13372764  0.06705695  0.06558126  0.28697097]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 7634 is [False, True, False, True, False, False]
Scene graph at timestep 7634 is [False, True, False, True, False, False]
State prediction error at timestep 7634 is tensor(8.2404e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7634 of -1
Current timestep = 7635. State = [[ 0.00110706 -0.21717001]]. Action = [[-0.16682185 -0.18659772  0.19587547 -0.19042975]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 7635 is [False, True, False, True, False, False]
Current timestep = 7636. State = [[-0.00180503 -0.22065246]]. Action = [[-0.13304493 -0.13279162  0.2130579  -0.03475064]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 7636 is [False, True, False, True, False, False]
Human Feedback received at timestep 7636 of -1
Current timestep = 7637. State = [[-0.0046505  -0.22492525]]. Action = [[-0.12816802 -0.07226405 -0.2015059   0.09124005]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 7637 is [False, True, False, True, False, False]
Scene graph at timestep 7637 is [False, True, False, True, False, False]
State prediction error at timestep 7637 is tensor(1.6566e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7638. State = [[-0.00715875 -0.22908163]]. Action = [[-0.05678321 -0.24815428  0.08909869  0.3521031 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 7638 is [False, True, False, True, False, False]
Scene graph at timestep 7638 is [False, True, False, True, False, False]
State prediction error at timestep 7638 is tensor(1.1065e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7638 of -1
Current timestep = 7639. State = [[-0.00918454 -0.23462445]]. Action = [[-0.13654643  0.17168903 -0.01425895  0.91003203]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 7639 is [False, True, False, True, False, False]
Current timestep = 7640. State = [[-0.01133806 -0.23665527]]. Action = [[-0.06192294 -0.188469   -0.23651716  0.88229895]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 7640 is [False, True, False, True, False, False]
Human Feedback received at timestep 7640 of -1
Current timestep = 7641. State = [[-0.01325675 -0.239863  ]]. Action = [[-0.10105096  0.18178844 -0.24448963  0.03230798]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 7641 is [False, True, False, True, False, False]
Scene graph at timestep 7641 is [False, True, False, True, False, False]
State prediction error at timestep 7641 is tensor(4.1974e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7642. State = [[-0.01485073 -0.24032216]]. Action = [[-0.128562   -0.01055019 -0.22200659  0.5278101 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 7642 is [False, True, False, True, False, False]
Current timestep = 7643. State = [[-0.01662184 -0.24084955]]. Action = [[-0.11303136 -0.24642092  0.11277515  0.31109953]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 7643 is [False, True, False, True, False, False]
Current timestep = 7644. State = [[-0.02043289 -0.24407125]]. Action = [[-0.13123107 -0.18056063  0.03180891  0.6830294 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 7644 is [False, True, False, True, False, False]
Scene graph at timestep 7644 is [False, True, False, True, False, False]
State prediction error at timestep 7644 is tensor(6.3603e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7644 of -1
Current timestep = 7645. State = [[-0.02588918 -0.24882567]]. Action = [[-0.05560429 -0.23591682 -0.19295493 -0.4872393 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 7645 is [False, True, False, True, False, False]
Scene graph at timestep 7645 is [False, True, False, True, False, False]
State prediction error at timestep 7645 is tensor(3.8770e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7646. State = [[-0.03037391 -0.25518885]]. Action = [[-0.1337486   0.09131163  0.09561113  0.87216544]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 7646 is [False, True, False, True, False, False]
Human Feedback received at timestep 7646 of -1
Current timestep = 7647. State = [[-0.03401943 -0.25669804]]. Action = [[-0.12244767  0.17779696 -0.1717573   0.7210226 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 7647 is [False, True, False, True, False, False]
Current timestep = 7648. State = [[-0.03892035 -0.25565654]]. Action = [[-0.07546176 -0.18839484 -0.05112769 -0.29529727]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 7648 is [False, True, False, True, False, False]
Human Feedback received at timestep 7648 of -1
Current timestep = 7649. State = [[-0.0423914 -0.2573279]]. Action = [[-0.13714936 -0.22670221  0.1709319   0.15669572]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 7649 is [False, True, False, True, False, False]
Current timestep = 7650. State = [[-0.04605984 -0.2621548 ]]. Action = [[-0.13551328  0.10553277 -0.24809128 -0.10543597]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 7650 is [False, True, False, True, False, False]
Human Feedback received at timestep 7650 of -1
Current timestep = 7651. State = [[-0.04994904 -0.2644838 ]]. Action = [[-0.0743525   0.1309914   0.20292693  0.75613606]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 7651 is [False, True, False, True, False, False]
Human Feedback received at timestep 7651 of -1
Current timestep = 7652. State = [[-0.05330852 -0.26432618]]. Action = [[-0.01060455 -0.2488415  -0.19809043  0.11893845]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 7652 is [False, True, False, True, False, False]
Current timestep = 7653. State = [[-0.05634069 -0.26663598]]. Action = [[-0.03981289 -0.21820427 -0.23587178 -0.12302136]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 7653 is [True, False, False, True, False, False]
Human Feedback received at timestep 7653 of -1
Current timestep = 7654. State = [[-0.05971741 -0.2704696 ]]. Action = [[-0.10939899 -0.24445185 -0.02408254  0.71754205]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 7654 is [True, False, False, True, False, False]
Current timestep = 7655. State = [[-0.06379231 -0.27715895]]. Action = [[-0.05886152 -0.0053017  -0.06923273  0.14600801]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 7655 is [True, False, False, True, False, False]
Scene graph at timestep 7655 is [True, False, False, True, False, False]
State prediction error at timestep 7655 is tensor(3.1906e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7656. State = [[-0.06688692 -0.2817333 ]]. Action = [[-0.11734745 -0.0084874  -0.24623632  0.17373371]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 7656 is [True, False, False, True, False, False]
Scene graph at timestep 7656 is [True, False, False, True, False, False]
State prediction error at timestep 7656 is tensor(1.3733e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7656 of -1
Current timestep = 7657. State = [[-0.06954933 -0.28475738]]. Action = [[-0.12595077  0.07637215 -0.24255553 -0.20152605]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 7657 is [True, False, False, True, False, False]
Scene graph at timestep 7657 is [True, False, False, True, False, False]
State prediction error at timestep 7657 is tensor(3.7599e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7658. State = [[-0.07156877 -0.2861774 ]]. Action = [[-0.05693582 -0.21722044 -0.05547675  0.6212914 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 7658 is [True, False, False, True, False, False]
Human Feedback received at timestep 7658 of -1
Current timestep = 7659. State = [[-0.07396811 -0.28952807]]. Action = [[-0.05338928 -0.01366423 -0.21073513  0.22133255]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 7659 is [True, False, False, True, False, False]
Human Feedback received at timestep 7659 of -1
Current timestep = 7660. State = [[-0.07692642 -0.29257816]]. Action = [[-0.05099469 -0.18480515  0.03468832 -0.38098407]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 7660 is [True, False, False, True, False, False]
Current timestep = 7661. State = [[-0.08004982 -0.29603502]]. Action = [[-0.12000965 -0.22571222 -0.23486824 -0.13646144]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 7661 is [True, False, False, True, False, False]
Human Feedback received at timestep 7661 of -1
Current timestep = 7662. State = [[-0.08381341 -0.30150467]]. Action = [[-0.13585576 -0.22349785 -0.15280138  0.11511767]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 7662 is [True, False, False, True, False, False]
Current timestep = 7663. State = [[-0.08850511 -0.30839553]]. Action = [[ 0.06350338 -0.07409829 -0.24600972 -0.8700827 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 7663 is [True, False, False, True, False, False]
Current timestep = 7664. State = [[-0.09115039 -0.3126004 ]]. Action = [[-0.15162556  0.21768817 -0.16380286  0.71057653]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 7664 is [True, False, False, True, False, False]
Human Feedback received at timestep 7664 of -1
Current timestep = 7665. State = [[-0.09344523 -0.31327218]]. Action = [[-0.18517184 -0.23329666  0.15230697  0.4647057 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 7665 is [True, False, False, True, False, False]
Current timestep = 7666. State = [[-0.09498652 -0.31388327]]. Action = [[-0.15944995 -0.03302571 -0.21936207  0.37630057]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 7666 is [True, False, False, True, False, False]
Human Feedback received at timestep 7666 of -1
Current timestep = 7667. State = [[-0.09572363 -0.31473428]]. Action = [[-0.1967245  -0.12776718 -0.10808291  0.5508654 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 7667 is [True, False, False, True, False, False]
Human Feedback received at timestep 7667 of -1
Current timestep = 7668. State = [[-0.09623507 -0.3152697 ]]. Action = [[-0.17805511  0.00328872 -0.24272034  0.9271399 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 7668 is [True, False, False, True, False, False]
Current timestep = 7669. State = [[-0.09641539 -0.3155578 ]]. Action = [[-0.17349263 -0.12792331 -0.23431344  0.7169061 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 7669 is [True, False, False, True, False, False]
Human Feedback received at timestep 7669 of -1
Current timestep = 7670. State = [[-0.09660223 -0.3158504 ]]. Action = [[-0.16041255 -0.24806257  0.09602872  0.27904594]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 7670 is [True, False, False, True, False, False]
Current timestep = 7671. State = [[-0.09682518 -0.31596375]]. Action = [[-0.1375291  -0.07522318 -0.14099152 -0.38003308]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 7671 is [True, False, False, True, False, False]
Human Feedback received at timestep 7671 of -1
Current timestep = 7672. State = [[-0.09700065 -0.3162051 ]]. Action = [[-0.15081596  0.01219794 -0.16690892  0.6551533 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 7672 is [True, False, False, True, False, False]
Current timestep = 7673. State = [[-0.09714399 -0.31623876]]. Action = [[-0.0816921   0.2147567  -0.18017213  0.7407141 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 7673 is [True, False, False, True, False, False]
Human Feedback received at timestep 7673 of -1
Current timestep = 7674. State = [[-0.09753401 -0.31485102]]. Action = [[-0.11819494 -0.20566855 -0.13469969 -0.5095268 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 7674 is [True, False, False, True, False, False]
Scene graph at timestep 7674 is [True, False, False, True, False, False]
State prediction error at timestep 7674 is tensor(3.0535e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7675. State = [[-0.09804038 -0.31427592]]. Action = [[-0.15960327 -0.24141157 -0.18897887 -0.14504796]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 7675 is [True, False, False, True, False, False]
Scene graph at timestep 7675 is [True, False, False, True, False, False]
State prediction error at timestep 7675 is tensor(1.8108e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7676. State = [[-0.09821158 -0.3139552 ]]. Action = [[-0.06143501 -0.05105543 -0.17869653  0.1137929 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 7676 is [True, False, False, True, False, False]
Human Feedback received at timestep 7676 of -1
Current timestep = 7677. State = [[-0.09829462 -0.31376657]]. Action = [[-0.12725587 -0.10738534  0.01398253  0.7175484 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 7677 is [True, False, False, True, False, False]
Current timestep = 7678. State = [[-0.09856251 -0.3135414 ]]. Action = [[-0.14109541  0.15518397 -0.22064528  0.26549268]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 7678 is [True, False, False, True, False, False]
Human Feedback received at timestep 7678 of -1
Current timestep = 7679. State = [[-0.09934465 -0.31224245]]. Action = [[-0.18128127  0.15895283  0.04594353 -0.2729336 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 7679 is [True, False, False, True, False, False]
Current timestep = 7680. State = [[-0.10164608 -0.30981496]]. Action = [[-0.11424434 -0.04252931 -0.17744112  0.16917789]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 7680 is [True, False, False, True, False, False]
Scene graph at timestep 7680 is [True, False, False, True, False, False]
State prediction error at timestep 7680 is tensor(1.5120e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7680 of -1
Current timestep = 7681. State = [[-0.10643841 -0.30708227]]. Action = [[ 0.02026165  0.20662034 -0.07783169  0.42036486]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 7681 is [True, False, False, True, False, False]
Current timestep = 7682. State = [[-0.10890323 -0.30301762]]. Action = [[-0.11008729 -0.12159657  0.14909914  0.03505361]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 7682 is [True, False, False, True, False, False]
Current timestep = 7683. State = [[-0.11138879 -0.2995549 ]]. Action = [[-0.08551443 -0.24029113 -0.04610533 -0.16967833]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 7683 is [True, False, False, True, False, False]
Human Feedback received at timestep 7683 of -1
Current timestep = 7684. State = [[-0.11244208 -0.29783815]]. Action = [[-0.05232775 -0.01784298 -0.17200255  0.2781602 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 7684 is [True, False, False, True, False, False]
Scene graph at timestep 7684 is [True, False, False, True, False, False]
State prediction error at timestep 7684 is tensor(3.1228e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7685. State = [[-0.11333305 -0.2972009 ]]. Action = [[-0.1263706  -0.23362924  0.16263542 -0.1268642 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 7685 is [True, False, False, True, False, False]
Current timestep = 7686. State = [[-0.11425681 -0.29838055]]. Action = [[-0.09004894 -0.07609804 -0.19143732 -0.10455596]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 7686 is [True, False, False, True, False, False]
Human Feedback received at timestep 7686 of -1
Current timestep = 7687. State = [[-0.11597718 -0.30015948]]. Action = [[-0.03439708  0.05901158 -0.20481701  0.2800367 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 7687 is [True, False, False, True, False, False]
Current timestep = 7688. State = [[-0.11781059 -0.300452  ]]. Action = [[-0.13797915 -0.09746423  0.09687638  0.36080205]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 7688 is [True, False, False, True, False, False]
Human Feedback received at timestep 7688 of -1
Current timestep = 7689. State = [[-0.12007327 -0.30161202]]. Action = [[-0.15514742  0.22998607 -0.24535342  0.08898556]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 7689 is [True, False, False, True, False, False]
Current timestep = 7690. State = [[-0.12413219 -0.30015436]]. Action = [[-0.17498583 -0.04484974  0.07681063  0.5461203 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 7690 is [True, False, False, True, False, False]
Current timestep = 7691. State = [[-0.12962241 -0.29853788]]. Action = [[-0.08878931  0.05729729 -0.21692218 -0.13723063]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 7691 is [True, False, False, True, False, False]
Human Feedback received at timestep 7691 of -1
Current timestep = 7692. State = [[-0.13525674 -0.29676336]]. Action = [[-0.14278667  0.07018691 -0.19830458  0.6039208 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 7692 is [True, False, False, True, False, False]
Scene graph at timestep 7692 is [True, False, False, True, False, False]
State prediction error at timestep 7692 is tensor(2.6948e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7693. State = [[-0.14060225 -0.2954869 ]]. Action = [[-0.03308597 -0.2435985  -0.00893064  0.5628694 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 7693 is [True, False, False, True, False, False]
Current timestep = 7694. State = [[-0.14503844 -0.29618567]]. Action = [[-0.10244651 -0.15864266 -0.13947657  0.7584728 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 7694 is [True, False, False, True, False, False]
Human Feedback received at timestep 7694 of -1
Current timestep = 7695. State = [[-0.15063022 -0.29819456]]. Action = [[-0.12826045  0.12589133 -0.16665463  0.08404732]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 7695 is [True, False, False, True, False, False]
Current timestep = 7696. State = [[-0.1547021  -0.29864147]]. Action = [[-0.0998641  -0.03882411  0.09438276 -0.10811234]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 7696 is [True, False, False, True, False, False]
Current timestep = 7697. State = [[-0.15757033 -0.300263  ]]. Action = [[-0.05086488  0.16068554 -0.09036908 -0.56583834]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 7697 is [True, False, False, True, False, False]
Scene graph at timestep 7697 is [True, False, False, True, False, False]
State prediction error at timestep 7697 is tensor(5.8875e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7697 of -1
Current timestep = 7698. State = [[-0.15957363 -0.30091682]]. Action = [[-0.09746146 -0.17975055 -0.17971529  0.23975682]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 7698 is [True, False, False, True, False, False]
Scene graph at timestep 7698 is [True, False, False, True, False, False]
State prediction error at timestep 7698 is tensor(1.8949e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7699. State = [[-0.1619958  -0.30288753]]. Action = [[-0.22324443  0.02555275 -0.235602   -0.16277534]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 7699 is [True, False, False, True, False, False]
Current timestep = 7700. State = [[-0.16590409 -0.3043604 ]]. Action = [[-1.0274351e-04 -3.8921520e-02 -1.2388271e-01  6.3711190e-01]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 7700 is [True, False, False, True, False, False]
Human Feedback received at timestep 7700 of -1
Current timestep = 7701. State = [[-0.16978267 -0.30462292]]. Action = [[-0.12164406 -0.05420065 -0.14065713  0.37436843]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 7701 is [True, False, False, True, False, False]
Current timestep = 7702. State = [[-0.17439872 -0.3045943 ]]. Action = [[ 0.05587018 -0.08669636 -0.03319144  0.72111917]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 7702 is [True, False, False, True, False, False]
Human Feedback received at timestep 7702 of -1
Current timestep = 7703. State = [[-0.1787385  -0.30517986]]. Action = [[-0.08319013 -0.22732621 -0.17331538  0.68273413]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 7703 is [True, False, False, True, False, False]
Current timestep = 7704. State = [[-0.18103874 -0.3060778 ]]. Action = [[-0.10132487 -0.11378865 -0.23414625  0.48750937]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 7704 is [True, False, False, True, False, False]
Human Feedback received at timestep 7704 of -1
Current timestep = 7705. State = [[-0.18210033 -0.30706072]]. Action = [[-0.13418913 -0.18461263 -0.15961875 -0.33510375]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 7705 is [True, False, False, True, False, False]
Scene graph at timestep 7705 is [True, False, False, True, False, False]
State prediction error at timestep 7705 is tensor(7.1444e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7706. State = [[-0.18251772 -0.3077683 ]]. Action = [[-0.06275976 -0.23154055 -0.18340696  0.38931692]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 7706 is [True, False, False, True, False, False]
Human Feedback received at timestep 7706 of -1
Current timestep = 7707. State = [[-0.18281111 -0.30804887]]. Action = [[ 0.01013848 -0.10025716 -0.20204866  0.31978607]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 7707 is [True, False, False, True, False, False]
Current timestep = 7708. State = [[-0.18315503 -0.3083121 ]]. Action = [[-0.16882671 -0.12668006 -0.22346938 -0.13693166]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 7708 is [True, False, False, True, False, False]
Human Feedback received at timestep 7708 of -1
Current timestep = 7709. State = [[-0.18346263 -0.30852568]]. Action = [[-0.01681851 -0.19097774 -0.1335717   0.01276267]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 7709 is [True, False, False, True, False, False]
Current timestep = 7710. State = [[-0.18365139 -0.30857906]]. Action = [[-0.06879036 -0.23711188 -0.16742823  0.5676422 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 7710 is [True, False, False, True, False, False]
Current timestep = 7711. State = [[-0.18376288 -0.3085273 ]]. Action = [[-0.11815794 -0.0900189  -0.05236314  0.3043735 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 7711 is [True, False, False, True, False, False]
Current timestep = 7712. State = [[-0.18385983 -0.30855703]]. Action = [[-0.0011823  -0.04696037 -0.20158735  0.74551666]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 7712 is [True, False, False, True, False, False]
Scene graph at timestep 7712 is [True, False, False, True, False, False]
State prediction error at timestep 7712 is tensor(1.8094e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7712 of -1
Current timestep = 7713. State = [[-0.18390422 -0.30857018]]. Action = [[ 0.03935376 -0.09364624 -0.23608565  0.8097322 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 7713 is [True, False, False, True, False, False]
Scene graph at timestep 7713 is [True, False, False, True, False, False]
State prediction error at timestep 7713 is tensor(1.6787e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7714. State = [[-0.1839135 -0.308621 ]]. Action = [[-0.17180645 -0.02090117 -0.08431187  0.4135319 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 7714 is [True, False, False, True, False, False]
Current timestep = 7715. State = [[-0.18537058 -0.3089469 ]]. Action = [[-0.16349751  0.2254495   0.0471012  -0.36892045]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 7715 is [True, False, False, True, False, False]
Current timestep = 7716. State = [[-0.18687992 -0.30803445]]. Action = [[-0.1814666  -0.1982983   0.02050754 -0.07205915]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 7716 is [True, False, False, True, False, False]
Current timestep = 7717. State = [[-0.18773524 -0.30756587]]. Action = [[-0.02247849  0.08023351 -0.21488313  0.02810848]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 7717 is [True, False, False, True, False, False]
Human Feedback received at timestep 7717 of -1
Current timestep = 7718. State = [[-0.18855946 -0.30673668]]. Action = [[ 0.02195424 -0.06571531 -0.14797124  0.6548948 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 7718 is [True, False, False, True, False, False]
Scene graph at timestep 7718 is [True, False, False, True, False, False]
State prediction error at timestep 7718 is tensor(1.2647e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7719. State = [[-0.1896115  -0.30615208]]. Action = [[-0.04300509 -0.21292895  0.12543863  0.28178525]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 7719 is [True, False, False, True, False, False]
Current timestep = 7720. State = [[-0.19105348 -0.30543253]]. Action = [[-0.1408714  -0.20944272 -0.21634471 -0.19941437]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 7720 is [True, False, False, True, False, False]
Scene graph at timestep 7720 is [True, False, False, True, False, False]
State prediction error at timestep 7720 is tensor(4.8328e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7720 of -1
Current timestep = 7721. State = [[-0.19364797 -0.30419663]]. Action = [[-0.08617443 -0.24041158 -0.03972341  0.26295173]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 7721 is [True, False, False, True, False, False]
Current timestep = 7722. State = [[-0.19605087 -0.3031472 ]]. Action = [[-0.02069512 -0.19969904 -0.20043582 -0.44101894]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 7722 is [True, False, False, True, False, False]
Scene graph at timestep 7722 is [True, False, False, True, False, False]
State prediction error at timestep 7722 is tensor(5.8674e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7722 of -1
Current timestep = 7723. State = [[-0.19694044 -0.30265063]]. Action = [[ 0.06917053  0.17631805 -0.04349121  0.33998   ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 7723 is [True, False, False, True, False, False]
Current timestep = 7724. State = [[-0.19707303 -0.30061358]]. Action = [[ 0.0136919   0.00680906 -0.05860166  0.2979617 ]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 7724 is [True, False, False, True, False, False]
Human Feedback received at timestep 7724 of -1
Current timestep = 7725. State = [[-0.19701162 -0.29943505]]. Action = [[-0.11600168  0.18076319 -0.03531735 -0.44497603]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 7725 is [True, False, False, True, False, False]
Scene graph at timestep 7725 is [True, False, False, True, False, False]
State prediction error at timestep 7725 is tensor(1.1360e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7726. State = [[-0.19741236 -0.29668546]]. Action = [[-0.02459303 -0.15336627 -0.10066034  0.5784435 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 7726 is [True, False, False, True, False, False]
Scene graph at timestep 7726 is [True, False, False, True, False, False]
State prediction error at timestep 7726 is tensor(1.1441e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7726 of -1
Current timestep = 7727. State = [[-0.19827466 -0.29615852]]. Action = [[-0.10035217  0.07808685  0.12842262  0.7202076 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 7727 is [True, False, False, True, False, False]
Scene graph at timestep 7727 is [True, False, False, True, False, False]
State prediction error at timestep 7727 is tensor(2.6438e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7728. State = [[-0.19958553 -0.29569447]]. Action = [[-0.14833032 -0.12216365 -0.20758322  0.4258535 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 7728 is [True, False, False, True, False, False]
Human Feedback received at timestep 7728 of -1
Current timestep = 7729. State = [[-0.20161282 -0.29576114]]. Action = [[-0.05944735 -0.20742656 -0.18116726  0.8515612 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 7729 is [True, False, False, True, False, False]
Current timestep = 7730. State = [[-0.2035171  -0.29785305]]. Action = [[-0.10612512 -0.14957151 -0.18179418  0.18770063]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 7730 is [True, False, False, True, False, False]
Current timestep = 7731. State = [[-0.20605616 -0.30086514]]. Action = [[ 0.12759584 -0.06258479 -0.06343612 -0.46265274]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 7731 is [True, False, False, True, False, False]
Scene graph at timestep 7731 is [True, False, False, True, False, False]
State prediction error at timestep 7731 is tensor(3.8491e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7731 of -1
Current timestep = 7732. State = [[-0.20702317 -0.30282855]]. Action = [[ 0.03191012  0.05786356 -0.16008948 -0.6753478 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 7732 is [True, False, False, True, False, False]
Current timestep = 7733. State = [[-0.20713843 -0.30374274]]. Action = [[-0.08696613 -0.24441384  0.10152167  0.5428587 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 7733 is [True, False, False, True, False, False]
Current timestep = 7734. State = [[-0.207197   -0.30448246]]. Action = [[-0.05194595 -0.01530938 -0.21325894 -0.16175866]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 7734 is [True, False, False, True, False, False]
Scene graph at timestep 7734 is [True, False, False, True, False, False]
State prediction error at timestep 7734 is tensor(2.4550e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7734 of -1
Current timestep = 7735. State = [[-0.20816275 -0.3054568 ]]. Action = [[-0.02474949 -0.24427633 -0.1970361   0.05190527]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 7735 is [True, False, False, True, False, False]
Current timestep = 7736. State = [[-0.20864618 -0.30611566]]. Action = [[ 0.03478053 -0.14173816 -0.10727884  0.32560277]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 7736 is [True, False, False, True, False, False]
Scene graph at timestep 7736 is [True, False, False, True, False, False]
State prediction error at timestep 7736 is tensor(1.3626e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7737. State = [[-0.20905572 -0.3065411 ]]. Action = [[-0.02307187 -0.06982408 -0.23603341  0.5678303 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 7737 is [True, False, False, True, False, False]
Scene graph at timestep 7737 is [True, False, False, True, False, False]
State prediction error at timestep 7737 is tensor(1.6776e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7737 of -1
Current timestep = 7738. State = [[-0.2097274  -0.30752438]]. Action = [[-0.05550215 -0.19125909 -0.16023774 -0.25204372]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 7738 is [True, False, False, True, False, False]
Current timestep = 7739. State = [[-0.21006866 -0.30779797]]. Action = [[-0.00337489  0.20502532  0.13608915  0.9556689 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 7739 is [True, False, False, True, False, False]
Scene graph at timestep 7739 is [True, False, False, True, False, False]
State prediction error at timestep 7739 is tensor(2.9380e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7739 of -1
Current timestep = 7740. State = [[-0.21023084 -0.30734998]]. Action = [[-0.0936217  -0.05613257 -0.21975128 -0.16939884]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 7740 is [True, False, False, True, False, False]
Current timestep = 7741. State = [[-0.21036623 -0.30716318]]. Action = [[-0.08438718  0.11106247 -0.1647828   0.66435087]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 7741 is [True, False, False, True, False, False]
Human Feedback received at timestep 7741 of -1
Current timestep = 7742. State = [[-0.2108869 -0.3061315]]. Action = [[-0.14979    -0.1547809  -0.09100702  0.0819447 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 7742 is [True, False, False, True, False, False]
Current timestep = 7743. State = [[-0.21114258 -0.30570796]]. Action = [[-0.06531703 -0.0182831  -0.08593118  0.587621  ]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 7743 is [True, False, False, True, False, False]
Current timestep = 7744. State = [[-0.21188642 -0.3051999 ]]. Action = [[-0.02932695 -0.04535672 -0.15535592  0.71085024]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 7744 is [True, False, False, True, False, False]
Human Feedback received at timestep 7744 of -1
Current timestep = 7745. State = [[-0.21270612 -0.304902  ]]. Action = [[-0.18386807 -0.19992778 -0.22173005  0.59860325]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 7745 is [True, False, False, True, False, False]
Current timestep = 7746. State = [[-0.21363229 -0.30443493]]. Action = [[-0.06682777 -0.05586785 -0.15127212 -0.6379949 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 7746 is [True, False, False, True, False, False]
Human Feedback received at timestep 7746 of -1
Current timestep = 7747. State = [[-0.21539918 -0.30400133]]. Action = [[-0.0016332  -0.08053428 -0.05730453  0.66123927]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 7747 is [True, False, False, True, False, False]
Current timestep = 7748. State = [[-0.2183646  -0.30361986]]. Action = [[-0.11126164  0.09123948 -0.12256718  0.81304026]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 7748 is [True, False, False, True, False, False]
Scene graph at timestep 7748 is [True, False, False, True, False, False]
State prediction error at timestep 7748 is tensor(1.3592e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7748 of -1
Current timestep = 7749. State = [[-0.22184438 -0.30217814]]. Action = [[ 0.0817526  -0.13048148 -0.08619756 -0.34712613]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 7749 is [True, False, False, True, False, False]
Scene graph at timestep 7749 is [True, False, False, True, False, False]
State prediction error at timestep 7749 is tensor(1.4514e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7750. State = [[-0.22267556 -0.3028393 ]]. Action = [[-0.00840938 -0.15877086 -0.14043052  0.9053012 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 7750 is [True, False, False, True, False, False]
Current timestep = 7751. State = [[-0.2237649  -0.30494356]]. Action = [[-0.174792   -0.05096032 -0.13640146 -0.1015622 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 7751 is [True, False, False, True, False, False]
Current timestep = 7752. State = [[-0.22624865 -0.30739865]]. Action = [[ 0.00986782 -0.20853178 -0.04854535 -0.23235893]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 7752 is [True, False, False, True, False, False]
Human Feedback received at timestep 7752 of -1
Current timestep = 7753. State = [[-0.2282169  -0.30888015]]. Action = [[-0.08025469 -0.11543989 -0.10541439  0.15017545]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 7753 is [True, False, False, True, False, False]
Human Feedback received at timestep 7753 of -1
Current timestep = 7754. State = [[-0.22963579 -0.3101159 ]]. Action = [[-0.01860994  0.11663014 -0.22135223  0.45463586]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 7754 is [True, False, False, True, False, False]
Scene graph at timestep 7754 is [True, False, False, True, False, False]
State prediction error at timestep 7754 is tensor(9.4379e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7755. State = [[-0.2308532 -0.3102989]]. Action = [[-0.14892161 -0.13562904 -0.12882224  0.71288013]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 7755 is [True, False, False, True, False, False]
Scene graph at timestep 7755 is [True, False, False, True, False, False]
State prediction error at timestep 7755 is tensor(1.6472e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7755 of -1
Current timestep = 7756. State = [[-0.23135684 -0.3105135 ]]. Action = [[-0.09643674 -0.19753714 -0.0020344  -0.01022708]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 7756 is [True, False, False, True, False, False]
Current timestep = 7757. State = [[-0.23165081 -0.31049046]]. Action = [[-0.10146339 -0.22974338  0.01404962  0.542812  ]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 7757 is [True, False, False, True, False, False]
Current timestep = 7758. State = [[-0.23205589 -0.3103448 ]]. Action = [[-0.07026994  0.22021508  0.00830039 -0.05287755]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 7758 is [True, False, False, True, False, False]
Current timestep = 7759. State = [[-0.23263966 -0.3090341 ]]. Action = [[-0.08658622  0.11888605 -0.16130461  0.19538522]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 7759 is [True, False, False, True, False, False]
Scene graph at timestep 7759 is [True, False, False, True, False, False]
State prediction error at timestep 7759 is tensor(9.5006e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7759 of -1
Current timestep = 7760. State = [[-0.23332931 -0.30745926]]. Action = [[ 0.04923648  0.15072185 -0.06440155  0.54881716]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 7760 is [True, False, False, True, False, False]
Current timestep = 7761. State = [[-0.2331827  -0.30432993]]. Action = [[-0.08985251  0.23105186 -0.0290748  -0.08087409]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 7761 is [True, False, False, True, False, False]
Scene graph at timestep 7761 is [True, False, False, True, False, False]
State prediction error at timestep 7761 is tensor(1.0813e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7762. State = [[-0.23289113 -0.30013084]]. Action = [[ 0.00954598 -0.09188953 -0.20780566 -0.13666862]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 7762 is [True, False, False, True, False, False]
Current timestep = 7763. State = [[-0.23324628 -0.29846433]]. Action = [[-0.1256345  -0.22238244 -0.17695075 -0.40343153]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 7763 is [True, False, False, True, False, False]
Current timestep = 7764. State = [[-0.23479147 -0.29970402]]. Action = [[-0.13204435 -0.2466352  -0.2068299   0.4303249 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 7764 is [True, False, False, True, False, False]
Human Feedback received at timestep 7764 of -1
Current timestep = 7765. State = [[-0.23778431 -0.30283493]]. Action = [[-0.13620445 -0.21270254 -0.21380381  0.47039688]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 7765 is [True, False, False, True, False, False]
Scene graph at timestep 7765 is [True, False, False, True, False, False]
State prediction error at timestep 7765 is tensor(1.2127e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7766. State = [[-0.24225146 -0.30706787]]. Action = [[-0.09978704 -0.10319486 -0.17907074  0.00565577]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 7766 is [True, False, False, True, False, False]
Scene graph at timestep 7766 is [True, False, False, True, False, False]
State prediction error at timestep 7766 is tensor(2.4038e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7767. State = [[-0.24769937 -0.310347  ]]. Action = [[-0.1035924  -0.07158765 -0.14037289  0.9255433 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 7767 is [True, False, False, True, False, False]
Human Feedback received at timestep 7767 of -1
Current timestep = 7768. State = [[-0.2520042  -0.31227532]]. Action = [[-0.16093948 -0.1901818  -0.20368317 -0.5726868 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 7768 is [True, False, False, True, False, False]
Scene graph at timestep 7768 is [True, False, False, True, False, False]
State prediction error at timestep 7768 is tensor(7.8637e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7769. State = [[-0.2542887 -0.3130217]]. Action = [[-0.06831491  0.22508362  0.08578879 -0.05560476]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 7769 is [True, False, False, True, False, False]
Current timestep = 7770. State = [[-0.25650156 -0.31161156]]. Action = [[-0.03520146 -0.18422323 -0.02628133  0.3558135 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 7770 is [True, False, False, True, False, False]
Current timestep = 7771. State = [[-0.25801492 -0.31063086]]. Action = [[-0.1829954  -0.21368635 -0.15395385 -0.09400707]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 7771 is [True, False, False, True, False, False]
Scene graph at timestep 7771 is [True, False, False, True, False, False]
State prediction error at timestep 7771 is tensor(2.1070e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7771 of -1
Current timestep = 7772. State = [[-0.25892946 -0.31008056]]. Action = [[-0.14093494 -0.14514132 -0.24205725 -0.21418697]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 7772 is [True, False, False, True, False, False]
Scene graph at timestep 7772 is [True, False, False, True, False, False]
State prediction error at timestep 7772 is tensor(3.2178e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7773. State = [[-0.259137   -0.30997208]]. Action = [[-0.21473908 -0.1719034  -0.17482637 -0.44693935]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 7773 is [True, False, False, True, False, False]
Current timestep = 7774. State = [[-0.2591705  -0.30994377]]. Action = [[-0.04045489 -0.24698876 -0.05371961  0.106215  ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 7774 is [True, False, False, True, False, False]
Human Feedback received at timestep 7774 of -1
Current timestep = 7775. State = [[-0.25933662 -0.30984533]]. Action = [[-0.11536366 -0.05450469 -0.22991592  0.6969969 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 7775 is [True, False, False, True, False, False]
Scene graph at timestep 7775 is [True, False, False, True, False, False]
State prediction error at timestep 7775 is tensor(1.0589e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7776. State = [[-0.259308   -0.30985135]]. Action = [[-0.1167219  -0.06748191  0.06011316 -0.17744875]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 7776 is [True, False, False, True, False, False]
Scene graph at timestep 7776 is [True, False, False, True, False, False]
State prediction error at timestep 7776 is tensor(1.8561e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7777. State = [[-0.259308   -0.30985135]]. Action = [[ 0.09074306  0.0156548  -0.14072675  0.52544117]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 7777 is [True, False, False, True, False, False]
Human Feedback received at timestep 7777 of -1
Current timestep = 7778. State = [[-0.259308   -0.30985135]]. Action = [[-0.12594093 -0.0424991  -0.19945861 -0.44829607]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 7778 is [True, False, False, True, False, False]
Scene graph at timestep 7778 is [True, False, False, True, False, False]
State prediction error at timestep 7778 is tensor(3.4860e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7779. State = [[-0.259308   -0.30985135]]. Action = [[ 0.0394026  -0.1623413  -0.14171627  0.71561706]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 7779 is [True, False, False, True, False, False]
Human Feedback received at timestep 7779 of -1
Current timestep = 7780. State = [[-0.259308   -0.30985135]]. Action = [[-0.08213618 -0.1811167  -0.07787399  0.9139521 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 7780 is [True, False, False, True, False, False]
Current timestep = 7781. State = [[-0.259308   -0.30985135]]. Action = [[ 0.06908265 -0.01267879 -0.13651802  0.4738376 ]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 7781 is [True, False, False, True, False, False]
Current timestep = 7782. State = [[-0.259308   -0.30985135]]. Action = [[-0.00417435  0.03932333  0.11508924  0.01337373]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 7782 is [True, False, False, True, False, False]
Current timestep = 7783. State = [[-0.259308   -0.30985135]]. Action = [[ 0.08893937 -0.19336666 -0.13908415  0.37596667]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 7783 is [True, False, False, True, False, False]
Scene graph at timestep 7783 is [True, False, False, True, False, False]
State prediction error at timestep 7783 is tensor(2.4366e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7784. State = [[-0.259308   -0.30985135]]. Action = [[-0.12041754  0.05487135 -0.23528096  0.67859054]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 7784 is [True, False, False, True, False, False]
Current timestep = 7785. State = [[-0.25964367 -0.30960476]]. Action = [[-0.20591323 -0.23304726 -0.20045011 -0.07152581]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 7785 is [True, False, False, True, False, False]
Current timestep = 7786. State = [[-0.25968835 -0.30940515]]. Action = [[ 0.01733834 -0.00888979 -0.09300207 -0.27875715]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 7786 is [True, False, False, True, False, False]
Current timestep = 7787. State = [[-0.25961503 -0.30926788]]. Action = [[ 0.11633763 -0.23912553 -0.09124605 -0.45284462]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 7787 is [True, False, False, True, False, False]
Scene graph at timestep 7787 is [True, False, False, True, False, False]
State prediction error at timestep 7787 is tensor(7.1637e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7788. State = [[-0.25980088 -0.30916578]]. Action = [[-0.12490165 -0.12625043 -0.145014   -0.26368636]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 7788 is [True, False, False, True, False, False]
Scene graph at timestep 7788 is [True, False, False, True, False, False]
State prediction error at timestep 7788 is tensor(2.4962e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7789. State = [[-0.2597724  -0.30917177]]. Action = [[ 0.04273224 -0.0736827  -0.16452953 -0.03484493]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 7789 is [True, False, False, True, False, False]
Human Feedback received at timestep 7789 of -1
Current timestep = 7790. State = [[-0.2597724  -0.30917177]]. Action = [[-0.04245786 -0.02156091 -0.13683182  0.6762786 ]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 7790 is [True, False, False, True, False, False]
Current timestep = 7791. State = [[-0.2597694  -0.30897537]]. Action = [[-0.05821019  0.21783876 -0.04504487 -0.91316676]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 7791 is [True, False, False, True, False, False]
Current timestep = 7792. State = [[-0.2598809 -0.3071363]]. Action = [[-0.1331652  -0.21004663 -0.18797114  0.12967777]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 7792 is [True, False, False, True, False, False]
Scene graph at timestep 7792 is [True, False, False, True, False, False]
State prediction error at timestep 7792 is tensor(1.8733e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7792 of -1
Current timestep = 7793. State = [[-0.26008165 -0.3062148 ]]. Action = [[0.05141941 0.00596318 0.03041705 0.6200671 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 7793 is [True, False, False, True, False, False]
Current timestep = 7794. State = [[-0.2599476  -0.30564776]]. Action = [[-0.01823959 -0.21076976 -0.17260881 -0.6815485 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 7794 is [True, False, False, True, False, False]
Current timestep = 7795. State = [[-0.25986835 -0.3053492 ]]. Action = [[-0.01870087  0.00848281 -0.12902072 -0.38350022]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 7795 is [True, False, False, True, False, False]
Current timestep = 7796. State = [[-0.25978255 -0.30506796]]. Action = [[-0.05939582  0.17691034 -0.2046301   0.5970042 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 7796 is [True, False, False, True, False, False]
Scene graph at timestep 7796 is [True, False, False, True, False, False]
State prediction error at timestep 7796 is tensor(2.2297e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7797. State = [[-0.25990543 -0.30267286]]. Action = [[-0.08864278 -0.07750988 -0.1882953  -0.33871448]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 7797 is [True, False, False, True, False, False]
Human Feedback received at timestep 7797 of -1
Current timestep = 7798. State = [[-0.26075763 -0.3020655 ]]. Action = [[-0.16995817 -0.04251927 -0.14187774 -0.6921831 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 7798 is [True, False, False, True, False, False]
Current timestep = 7799. State = [[-0.26322046 -0.30078846]]. Action = [[ 0.06274837 -0.18155967 -0.15026733 -0.95743567]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 7799 is [True, False, False, True, False, False]
Current timestep = 7800. State = [[-0.26683575 -0.29885   ]]. Action = [[-0.05902083 -0.02933067  0.05663508  0.03884256]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 7800 is [True, False, False, True, False, False]
Current timestep = 7801. State = [[-0.2704301  -0.29707053]]. Action = [[-0.16184127  0.21130449  0.04598504 -0.13083643]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 7801 is [True, False, False, True, False, False]
Human Feedback received at timestep 7801 of -1
Current timestep = 7802. State = [[-0.27488616 -0.29449123]]. Action = [[-0.04904684  0.02634239 -0.02906039 -0.19011366]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 7802 is [True, False, False, True, False, False]
Current timestep = 7803. State = [[-0.27778313 -0.2929488 ]]. Action = [[-0.11757867  0.18993726 -0.17979825 -0.7257511 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 7803 is [True, False, False, True, False, False]
Current timestep = 7804. State = [[-0.27935368 -0.29206055]]. Action = [[-0.06660026  0.07110608 -0.00211418  0.16233015]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 7804 is [True, False, False, True, False, False]
Human Feedback received at timestep 7804 of -1
Current timestep = 7805. State = [[-0.28119394 -0.29110438]]. Action = [[ 0.00701889 -0.22469997 -0.06314018 -0.00902396]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 7805 is [True, False, False, True, False, False]
Current timestep = 7806. State = [[-0.28309253 -0.29011568]]. Action = [[-0.1841981  -0.1102701   0.07556206  0.5502279 ]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 7806 is [True, False, False, True, False, False]
Current timestep = 7807. State = [[-0.28461114 -0.2892955 ]]. Action = [[ 0.11371928  0.06688875 -0.07869309  0.5933727 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 7807 is [True, False, False, True, False, False]
Human Feedback received at timestep 7807 of -1
Current timestep = 7808. State = [[-0.28542784 -0.28894398]]. Action = [[-0.12854333 -0.13045911 -0.09561178  0.5420244 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 7808 is [True, False, False, True, False, False]
Current timestep = 7809. State = [[-0.28619406 -0.28845635]]. Action = [[ 0.06705588  0.11110622 -0.21908344  0.29450285]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 7809 is [True, False, False, True, False, False]
Human Feedback received at timestep 7809 of -1
Current timestep = 7810. State = [[-0.28659558 -0.28822544]]. Action = [[ 0.12239629 -0.02753431 -0.03430653 -0.32213306]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 7810 is [True, False, False, True, False, False]
Current timestep = 7811. State = [[-0.28676605 -0.28811824]]. Action = [[-0.09420773  0.01534322 -0.16950049  0.1731242 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 7811 is [True, False, False, True, False, False]
Current timestep = 7812. State = [[-0.28679398 -0.28809083]]. Action = [[-0.21788359 -0.15826893  0.02925488 -0.0073275 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 7812 is [True, False, False, True, False, False]
Human Feedback received at timestep 7812 of -1
Current timestep = 7813. State = [[-0.28721806 -0.2878687 ]]. Action = [[-0.03086588 -0.19243886 -0.21718447  0.6131928 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 7813 is [True, False, False, True, False, False]
Current timestep = 7814. State = [[-0.28721806 -0.2878687 ]]. Action = [[-0.00888747  0.04692233 -0.16260009  0.4453349 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 7814 is [True, False, False, True, False, False]
Current timestep = 7815. State = [[-0.28735733 -0.2877712 ]]. Action = [[ 0.03929168 -0.03236978 -0.1093792   0.22660267]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 7815 is [True, False, False, True, False, False]
Scene graph at timestep 7815 is [True, False, False, True, False, False]
State prediction error at timestep 7815 is tensor(2.5695e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7816. State = [[-0.28735733 -0.2877712 ]]. Action = [[ 0.03891283 -0.12878712 -0.15218624  0.5134016 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 7816 is [True, False, False, True, False, False]
Current timestep = 7817. State = [[-0.28741318 -0.28773603]]. Action = [[ 0.0623433  -0.23579761 -0.15713644  0.03659606]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 7817 is [True, False, False, True, False, False]
Current timestep = 7818. State = [[-0.28741318 -0.28773603]]. Action = [[-0.01668151  0.1998541  -0.14073613  0.15574574]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 7818 is [True, False, False, True, False, False]
Scene graph at timestep 7818 is [True, False, False, True, False, False]
State prediction error at timestep 7818 is tensor(2.5245e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7819. State = [[-0.28741318 -0.28773603]]. Action = [[-0.11547059 -0.14007059 -0.09025343  0.57192504]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 7819 is [True, False, False, True, False, False]
Current timestep = 7820. State = [[-0.28741318 -0.28773603]]. Action = [[-0.01646531  0.23113194 -0.03070392  0.11140597]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 7820 is [True, False, False, True, False, False]
Current timestep = 7821. State = [[-0.2874803 -0.2877152]]. Action = [[ 0.12956092 -0.20086488 -0.17086168  0.69640946]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 7821 is [True, False, False, True, False, False]
Current timestep = 7822. State = [[-0.2874803 -0.2877152]]. Action = [[ 0.08350006 -0.15375422 -0.06994528  0.765581  ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 7822 is [True, False, False, True, False, False]
Current timestep = 7823. State = [[-0.2874524  -0.28772292]]. Action = [[-0.1182137  -0.18280639 -0.06676927 -0.49197257]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 7823 is [True, False, False, True, False, False]
Current timestep = 7824. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.06060305 -0.12642674  0.06357262 -0.6458083 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 7824 is [True, False, False, True, False, False]
Current timestep = 7825. State = [[-0.2874524  -0.28772292]]. Action = [[-0.07297525 -0.08415788 -0.14155255 -0.08375937]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 7825 is [True, False, False, True, False, False]
Scene graph at timestep 7825 is [True, False, False, True, False, False]
State prediction error at timestep 7825 is tensor(1.0217e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7826. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.10910964 -0.19511206 -0.16067864  0.34707344]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 7826 is [True, False, False, True, False, False]
Current timestep = 7827. State = [[-0.2874524  -0.28772292]]. Action = [[-0.21586668 -0.17577054  0.00393131 -0.2253043 ]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 7827 is [True, False, False, True, False, False]
Current timestep = 7828. State = [[-0.2874524  -0.28772292]]. Action = [[-0.13168345 -0.06519625  0.1445001  -0.9035353 ]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 7828 is [True, False, False, True, False, False]
Scene graph at timestep 7828 is [True, False, False, True, False, False]
State prediction error at timestep 7828 is tensor(4.0337e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7829. State = [[-0.2874524  -0.28772292]]. Action = [[-0.16307278 -0.20121667 -0.17782566 -0.32954103]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 7829 is [True, False, False, True, False, False]
Scene graph at timestep 7829 is [True, False, False, True, False, False]
State prediction error at timestep 7829 is tensor(2.5125e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7830. State = [[-0.2874524  -0.28772292]]. Action = [[-0.12314531  0.05451754 -0.15004973 -0.22057766]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 7830 is [True, False, False, True, False, False]
Scene graph at timestep 7830 is [True, False, False, True, False, False]
State prediction error at timestep 7830 is tensor(1.5806e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7831. State = [[-0.2874524  -0.28772292]]. Action = [[-0.21022917 -0.09919946 -0.02732161  0.20974648]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 7831 is [True, False, False, True, False, False]
Current timestep = 7832. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.12206689  0.08344617 -0.05657227  0.0427444 ]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 7832 is [True, False, False, True, False, False]
Current timestep = 7833. State = [[-0.2874524  -0.28772292]]. Action = [[-0.08408383 -0.15863153 -0.1628715   0.45931435]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 7833 is [True, False, False, True, False, False]
Current timestep = 7834. State = [[-0.2874524  -0.28772292]]. Action = [[-0.0986269   0.06871581 -0.07404371 -0.533979  ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 7834 is [True, False, False, True, False, False]
Current timestep = 7835. State = [[-0.2874524  -0.28772292]]. Action = [[-0.10330927  0.11754549 -0.03973082  0.14448714]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 7835 is [True, False, False, True, False, False]
Human Feedback received at timestep 7835 of -1
Current timestep = 7836. State = [[-0.2874524  -0.28772292]]. Action = [[-0.16034013  0.16333699 -0.14816919  0.650157  ]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 7836 is [True, False, False, True, False, False]
Current timestep = 7837. State = [[-0.2874524  -0.28772292]]. Action = [[-0.01372541 -0.04894894 -0.21103162 -0.57666874]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 7837 is [True, False, False, True, False, False]
Current timestep = 7838. State = [[-0.2874524  -0.28772292]]. Action = [[-0.14478484  0.17107189 -0.04357907  0.06806779]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 7838 is [True, False, False, True, False, False]
Current timestep = 7839. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.10684913 -0.1433129  -0.15450972 -0.3645743 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 7839 is [True, False, False, True, False, False]
Current timestep = 7840. State = [[-0.2874524  -0.28772292]]. Action = [[-0.01354542 -0.1659296  -0.13831057 -0.46736073]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 7840 is [True, False, False, True, False, False]
Current timestep = 7841. State = [[-0.2874524  -0.28772292]]. Action = [[-0.04191363 -0.01085003 -0.06149153 -0.20038748]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 7841 is [True, False, False, True, False, False]
Current timestep = 7842. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.00821644 -0.03471175 -0.14958698  0.3642776 ]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 7842 is [True, False, False, True, False, False]
Current timestep = 7843. State = [[-0.2874524  -0.28772292]]. Action = [[-0.00813156 -0.18309093 -0.17842677  0.18151939]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 7843 is [True, False, False, True, False, False]
Scene graph at timestep 7843 is [True, False, False, True, False, False]
State prediction error at timestep 7843 is tensor(1.6979e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7844. State = [[-0.2874524  -0.28772292]]. Action = [[-0.06645733  0.08574504 -0.17262459 -0.10388595]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 7844 is [True, False, False, True, False, False]
Current timestep = 7845. State = [[-0.2874524  -0.28772292]]. Action = [[-0.11054739 -0.23663326 -0.17769669  0.42440784]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 7845 is [True, False, False, True, False, False]
Current timestep = 7846. State = [[-0.2874524  -0.28772292]]. Action = [[-0.04252176  0.11445802 -0.10661635  0.2904029 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 7846 is [True, False, False, True, False, False]
Scene graph at timestep 7846 is [True, False, False, True, False, False]
State prediction error at timestep 7846 is tensor(1.8823e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7847. State = [[-0.2874524  -0.28772292]]. Action = [[-0.07473052 -0.2269173  -0.03834549  0.04870963]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 7847 is [True, False, False, True, False, False]
Current timestep = 7848. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.11994493  0.11964524 -0.20859273 -0.58195543]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 7848 is [True, False, False, True, False, False]
Current timestep = 7849. State = [[-0.2874524  -0.28772292]]. Action = [[-0.11856627  0.18278986 -0.06712812  0.85151935]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 7849 is [True, False, False, True, False, False]
Current timestep = 7850. State = [[-0.2874524  -0.28772292]]. Action = [[-0.11622976 -0.02964202 -0.09631202 -0.55079746]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 7850 is [True, False, False, True, False, False]
Scene graph at timestep 7850 is [True, False, False, True, False, False]
State prediction error at timestep 7850 is tensor(1.1382e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7851. State = [[-0.2874524  -0.28772292]]. Action = [[-0.09466428  0.02908143 -0.10999474 -0.8346714 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 7851 is [True, False, False, True, False, False]
Current timestep = 7852. State = [[-0.2874524  -0.28772292]]. Action = [[-0.07544588  0.07364395  0.04475975  0.42015302]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 7852 is [True, False, False, True, False, False]
Current timestep = 7853. State = [[-0.2874524  -0.28772292]]. Action = [[-0.0917111   0.00247103  0.06643653 -0.71387434]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 7853 is [True, False, False, True, False, False]
Current timestep = 7854. State = [[-0.2874524  -0.28772292]]. Action = [[-0.11504021  0.17969525 -0.11514467  0.24522841]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 7854 is [True, False, False, True, False, False]
Current timestep = 7855. State = [[-0.2874524  -0.28772292]]. Action = [[-0.15098506 -0.19707562  0.02830693 -0.5299436 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 7855 is [True, False, False, True, False, False]
Current timestep = 7856. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.0191375  -0.15782091 -0.1753106  -0.84579325]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 7856 is [True, False, False, True, False, False]
Current timestep = 7857. State = [[-0.2874524  -0.28772292]]. Action = [[-0.04624966  0.20532474 -0.14684299 -0.6186145 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 7857 is [True, False, False, True, False, False]
Scene graph at timestep 7857 is [True, False, False, True, False, False]
State prediction error at timestep 7857 is tensor(3.3257e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7858. State = [[-0.2874524  -0.28772292]]. Action = [[-0.00980787 -0.13305974 -0.01908417  0.85210204]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 7858 is [True, False, False, True, False, False]
Current timestep = 7859. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.00163957  0.09178519 -0.19328626  0.52170205]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 7859 is [True, False, False, True, False, False]
Scene graph at timestep 7859 is [True, False, False, True, False, False]
State prediction error at timestep 7859 is tensor(9.9956e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7860. State = [[-0.2874524  -0.28772292]]. Action = [[-0.0227288  -0.00087166 -0.21439669 -0.85001063]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 7860 is [True, False, False, True, False, False]
Scene graph at timestep 7860 is [True, False, False, True, False, False]
State prediction error at timestep 7860 is tensor(1.2465e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7861. State = [[-0.2874524  -0.28772292]]. Action = [[-0.09753513 -0.11043015 -0.16589816 -0.30446213]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 7861 is [True, False, False, True, False, False]
Current timestep = 7862. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.14699736 -0.10796353 -0.18255769 -0.86638707]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 7862 is [True, False, False, True, False, False]
Scene graph at timestep 7862 is [True, False, False, True, False, False]
State prediction error at timestep 7862 is tensor(1.2474e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7863. State = [[-0.2874524  -0.28772292]]. Action = [[-0.10477014 -0.09129713 -0.05189422 -0.58495384]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 7863 is [True, False, False, True, False, False]
Scene graph at timestep 7863 is [True, False, False, True, False, False]
State prediction error at timestep 7863 is tensor(1.2205e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7864. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.03806061 -0.06950596 -0.19435292  0.48789835]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 7864 is [True, False, False, True, False, False]
Scene graph at timestep 7864 is [True, False, False, True, False, False]
State prediction error at timestep 7864 is tensor(1.8396e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7865. State = [[-0.2874524  -0.28772292]]. Action = [[-0.01988226  0.16114908  0.05740526 -0.3956591 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 7865 is [True, False, False, True, False, False]
Current timestep = 7866. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.03680456  0.17760578 -0.13425024 -0.20356256]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 7866 is [True, False, False, True, False, False]
Current timestep = 7867. State = [[-0.2874524  -0.28772292]]. Action = [[-0.18070236 -0.18494512  0.00723997  0.7672155 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 7867 is [True, False, False, True, False, False]
Current timestep = 7868. State = [[-0.2874524  -0.28772292]]. Action = [[-0.13247326 -0.0275366  -0.06210433 -0.5357836 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 7868 is [True, False, False, True, False, False]
Scene graph at timestep 7868 is [True, False, False, True, False, False]
State prediction error at timestep 7868 is tensor(2.5102e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7869. State = [[-0.2874524  -0.28772292]]. Action = [[-0.1743351  -0.10082565 -0.02532597 -0.5240345 ]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 7869 is [True, False, False, True, False, False]
Scene graph at timestep 7869 is [True, False, False, True, False, False]
State prediction error at timestep 7869 is tensor(4.4137e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7870. State = [[-0.2874524  -0.28772292]]. Action = [[-0.02894311  0.12734461 -0.16319852 -0.2683376 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 7870 is [True, False, False, True, False, False]
Current timestep = 7871. State = [[-0.2874524  -0.28772292]]. Action = [[-0.11214766  0.14053226 -0.216431    0.24505258]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 7871 is [True, False, False, True, False, False]
Current timestep = 7872. State = [[-0.2874524  -0.28772292]]. Action = [[ 0.00299284 -0.07178155 -0.1310603   0.716545  ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 7872 is [True, False, False, True, False, False]
Current timestep = 7873. State = [[-0.2874524  -0.28772292]]. Action = [[-0.08550426  0.05877805 -0.21397567  0.08718681]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 7873 is [True, False, False, True, False, False]
Scene graph at timestep 7873 is [True, False, False, True, False, False]
State prediction error at timestep 7873 is tensor(2.5491e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7874. State = [[-0.2874524  -0.28772292]]. Action = [[-0.03859869 -0.11338106 -0.01642895 -0.39043593]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 7874 is [True, False, False, True, False, False]
Scene graph at timestep 7874 is [True, False, False, True, False, False]
State prediction error at timestep 7874 is tensor(1.0080e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7875. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.10637179  0.15988296 -0.22496454  0.28867173]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 7875 is [True, False, False, True, False, False]
Scene graph at timestep 7875 is [True, False, False, True, False, False]
State prediction error at timestep 7875 is tensor(1.4809e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7876. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.0280195  -0.04132758 -0.23570156  0.49388885]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 7876 is [True, False, False, True, False, False]
Current timestep = 7877. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.07164425 -0.12281542 -0.14975184  0.3363161 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 7877 is [True, False, False, True, False, False]
Current timestep = 7878. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.18821928  0.18259996 -0.16498682  0.4524355 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 7878 is [True, False, False, True, False, False]
Current timestep = 7879. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.10220392 -0.18017118 -0.11309655 -0.4581287 ]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 7879 is [True, False, False, True, False, False]
Current timestep = 7880. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.1144633   0.13989523 -0.20886265  0.459525  ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 7880 is [True, False, False, True, False, False]
Current timestep = 7881. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.03059235  0.0071716  -0.05658446 -0.03945458]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 7881 is [True, False, False, True, False, False]
Scene graph at timestep 7881 is [True, False, False, True, False, False]
State prediction error at timestep 7881 is tensor(1.1335e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7882. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.10184824  0.22959232 -0.19469532 -0.12928176]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 7882 is [True, False, False, True, False, False]
Current timestep = 7883. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.00798985  0.09781265 -0.17254019  0.44999337]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 7883 is [True, False, False, True, False, False]
Current timestep = 7884. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.03112361  0.05542833 -0.13135587 -0.39138794]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 7884 is [True, False, False, True, False, False]
Scene graph at timestep 7884 is [True, False, False, True, False, False]
State prediction error at timestep 7884 is tensor(2.2646e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7885. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.05563076  0.10602871 -0.10794632 -0.5264603 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 7885 is [True, False, False, True, False, False]
Current timestep = 7886. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.01040426 -0.1484804  -0.1812333  -0.7198729 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 7886 is [True, False, False, True, False, False]
Current timestep = 7887. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.19436544  0.00068709 -0.08821058 -0.15741134]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 7887 is [True, False, False, True, False, False]
Current timestep = 7888. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.19020356 -0.08352876 -0.09910291 -0.231098  ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 7888 is [True, False, False, True, False, False]
Current timestep = 7889. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.1009205  -0.0568286   0.01550841 -0.25251675]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 7889 is [True, False, False, True, False, False]
Current timestep = 7890. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.00287783  0.02976015 -0.22228847 -0.7410273 ]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 7890 is [True, False, False, True, False, False]
Current timestep = 7891. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.13512762 -0.18282895  0.11355823  0.487548  ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 7891 is [True, False, False, True, False, False]
Scene graph at timestep 7891 is [True, False, False, True, False, False]
State prediction error at timestep 7891 is tensor(1.4627e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7892. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.18251856  0.16412097  0.15212834 -0.20050359]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 7892 is [True, False, False, True, False, False]
Current timestep = 7893. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.14624666 -0.11727995 -0.0756515   0.67037296]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 7893 is [True, False, False, True, False, False]
Current timestep = 7894. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.01414889 -0.14425686 -0.16736557 -0.9502632 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 7894 is [True, False, False, True, False, False]
Current timestep = 7895. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.16068016  0.07561558 -0.10843624  0.34704733]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 7895 is [True, False, False, True, False, False]
Current timestep = 7896. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.0439201   0.21212596 -0.22865877  0.6012924 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 7896 is [True, False, False, True, False, False]
Current timestep = 7897. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.13529424  0.20961124 -0.18522133 -0.43453026]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 7897 is [True, False, False, True, False, False]
Scene graph at timestep 7897 is [True, False, False, True, False, False]
State prediction error at timestep 7897 is tensor(2.3654e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7898. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.15579496 -0.04750153 -0.19232415 -0.7876072 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 7898 is [True, False, False, True, False, False]
Current timestep = 7899. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.14590538  0.06559703 -0.02580486 -0.7493587 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 7899 is [True, False, False, True, False, False]
Current timestep = 7900. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.05992395  0.02683395  0.06816405  0.6796098 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 7900 is [True, False, False, True, False, False]
Current timestep = 7901. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.16467275  0.23120707 -0.16576912 -0.56637174]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 7901 is [True, False, False, True, False, False]
Current timestep = 7902. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.07813999  0.17374963 -0.10485753 -0.9567395 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 7902 is [True, False, False, True, False, False]
Current timestep = 7903. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.12806244 -0.14845787 -0.20229475  0.23265338]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 7903 is [True, False, False, True, False, False]
Current timestep = 7904. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.16189909 -0.22260946 -0.1894918  -0.3281647 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 7904 is [True, False, False, True, False, False]
Scene graph at timestep 7904 is [True, False, False, True, False, False]
State prediction error at timestep 7904 is tensor(2.5893e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7905. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.01883718  0.15154922 -0.20257996 -0.6214268 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 7905 is [True, False, False, True, False, False]
Scene graph at timestep 7905 is [True, False, False, True, False, False]
State prediction error at timestep 7905 is tensor(2.5905e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7906. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.10339487  0.17275685 -0.15705748 -0.88674563]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 7906 is [True, False, False, True, False, False]
Current timestep = 7907. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.03530398  0.11117771 -0.08146399 -0.46721756]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 7907 is [True, False, False, True, False, False]
Scene graph at timestep 7907 is [True, False, False, True, False, False]
State prediction error at timestep 7907 is tensor(8.5897e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7908. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.17911126  0.00846511 -0.21553078 -0.7985736 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 7908 is [True, False, False, True, False, False]
Scene graph at timestep 7908 is [True, False, False, True, False, False]
State prediction error at timestep 7908 is tensor(1.5179e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7909. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.06499082  0.13618377 -0.08266282  0.5620389 ]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 7909 is [True, False, False, True, False, False]
Scene graph at timestep 7909 is [True, False, False, True, False, False]
State prediction error at timestep 7909 is tensor(1.5643e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7910. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.21506295 -0.22647694  0.10359237  0.5217309 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 7910 is [True, False, False, True, False, False]
Current timestep = 7911. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.20978865  0.15624207 -0.12373367 -0.81038463]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 7911 is [True, False, False, True, False, False]
Current timestep = 7912. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.05509543  0.09349456  0.15223074 -0.23325628]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 7912 is [True, False, False, True, False, False]
Current timestep = 7913. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.18511617  0.1405043  -0.16627108 -0.19811988]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 7913 is [True, False, False, True, False, False]
Current timestep = 7914. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.19104302  0.22703564 -0.09536943 -0.5324158 ]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 7914 is [True, False, False, True, False, False]
Current timestep = 7915. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.21153904  0.15710655  0.04558092  0.02181017]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 7915 is [True, False, False, True, False, False]
Current timestep = 7916. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.20934051  0.16744965 -0.06374621  0.06866062]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 7916 is [True, False, False, True, False, False]
Current timestep = 7917. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.16853248 -0.04978094 -0.0801864  -0.7900026 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 7917 is [True, False, False, True, False, False]
Current timestep = 7918. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.0868935  -0.01250096 -0.00772525  0.94195366]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 7918 is [True, False, False, True, False, False]
Current timestep = 7919. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.16110784  0.151122   -0.2005428  -0.97960013]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 7919 is [True, False, False, True, False, False]
Current timestep = 7920. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.12591958 -0.08741155 -0.06531441 -0.87119097]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 7920 is [True, False, False, True, False, False]
Current timestep = 7921. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.22942959 -0.19348364  0.12239549 -0.9206454 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 7921 is [True, False, False, True, False, False]
Scene graph at timestep 7921 is [True, False, False, True, False, False]
State prediction error at timestep 7921 is tensor(1.2660e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7922. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.11686763 -0.03186628 -0.05070172  0.5828326 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 7922 is [True, False, False, True, False, False]
Current timestep = 7923. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.07708958  0.10622835 -0.1430871   0.59003353]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 7923 is [True, False, False, True, False, False]
Current timestep = 7924. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.14288405 -0.12253302 -0.00303563  0.09545183]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 7924 is [True, False, False, True, False, False]
Current timestep = 7925. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.01561809 -0.18490598 -0.02772854 -0.75202966]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 7925 is [True, False, False, True, False, False]
Scene graph at timestep 7925 is [True, False, False, True, False, False]
State prediction error at timestep 7925 is tensor(4.7832e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7926. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.20333168 -0.2099566  -0.01941392 -0.98553836]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 7926 is [True, False, False, True, False, False]
Current timestep = 7927. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.00166944 -0.05043902 -0.13832805 -0.9098743 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 7927 is [True, False, False, True, False, False]
Current timestep = 7928. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.02974576  0.06088346 -0.2196825  -0.6209106 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 7928 is [True, False, False, True, False, False]
Current timestep = 7929. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.05216146  0.1311247  -0.1854845  -0.32836086]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 7929 is [True, False, False, True, False, False]
Current timestep = 7930. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.04911488  0.04082724  0.16135797 -0.3129024 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 7930 is [True, False, False, True, False, False]
Current timestep = 7931. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.1153172  -0.02099185  0.12778813 -0.8876497 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 7931 is [True, False, False, True, False, False]
Current timestep = 7932. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.2172162  -0.00618729 -0.22520946  0.26557314]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 7932 is [True, False, False, True, False, False]
Current timestep = 7933. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.1662627  -0.14976467 -0.02698129 -0.0335533 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 7933 is [True, False, False, True, False, False]
Current timestep = 7934. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.2313632   0.16198301 -0.13881701 -0.7996215 ]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 7934 is [True, False, False, True, False, False]
Current timestep = 7935. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.18093045  0.03483349 -0.04983383 -0.18336523]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 7935 is [True, False, False, True, False, False]
Current timestep = 7936. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.18052779 -0.11554283 -0.21198235  0.22742176]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 7936 is [True, False, False, True, False, False]
Current timestep = 7937. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.03856355 -0.23343608 -0.07178026 -0.5294789 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 7937 is [True, False, False, True, False, False]
Scene graph at timestep 7937 is [True, False, False, True, False, False]
State prediction error at timestep 7937 is tensor(4.3728e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7938. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.1518767   0.20985243 -0.06454051 -0.9070031 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 7938 is [True, False, False, True, False, False]
Scene graph at timestep 7938 is [True, False, False, True, False, False]
State prediction error at timestep 7938 is tensor(3.6050e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7939. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.17067507 -0.07904175  0.08217311 -0.3475163 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 7939 is [True, False, False, True, False, False]
Current timestep = 7940. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.13379566  0.10451251  0.08186799 -0.26597357]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 7940 is [True, False, False, True, False, False]
Current timestep = 7941. State = [[-0.28751165 -0.2876856 ]]. Action = [[ 0.06557015  0.10691273 -0.21335325 -0.4908402 ]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 7941 is [True, False, False, True, False, False]
Current timestep = 7942. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.17333671  0.18888    -0.20823139 -0.72757524]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 7942 is [True, False, False, True, False, False]
Current timestep = 7943. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.22868243 -0.20652281 -0.00079791  0.25265217]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 7943 is [True, False, False, True, False, False]
Current timestep = 7944. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.15030621 -0.20535208 -0.21194626 -0.9969458 ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 7944 is [True, False, False, True, False, False]
Current timestep = 7945. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.06142575 -0.18758371 -0.18993697  0.8312113 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 7945 is [True, False, False, True, False, False]
Current timestep = 7946. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.02831028 -0.19588223 -0.11594847 -0.04081738]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 7946 is [True, False, False, True, False, False]
Current timestep = 7947. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.22068669  0.07435024 -0.1929483  -0.9842781 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 7947 is [True, False, False, True, False, False]
Current timestep = 7948. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.24700212 -0.22221524 -0.18821153 -0.8134215 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 7948 is [True, False, False, True, False, False]
Current timestep = 7949. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.03896512 -0.23034063 -0.16232431 -0.91631234]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 7949 is [True, False, False, True, False, False]
Current timestep = 7950. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.2249735  -0.21360013 -0.08886677 -0.44138026]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 7950 is [True, False, False, True, False, False]
Current timestep = 7951. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.20353246 -0.10747227 -0.22784199 -0.9705586 ]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 7951 is [True, False, False, True, False, False]
Scene graph at timestep 7951 is [True, False, False, True, False, False]
State prediction error at timestep 7951 is tensor(6.8716e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7952. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.01405582  0.19849253 -0.10104632 -0.3154742 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 7952 is [True, False, False, True, False, False]
Current timestep = 7953. State = [[-0.28753954 -0.28767788]]. Action = [[-0.2245899  -0.0862543  -0.21877775  0.9623194 ]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 7953 is [True, False, False, True, False, False]
Current timestep = 7954. State = [[-0.28753954 -0.28767788]]. Action = [[-0.16055074  0.21650839  0.03184468 -0.27026308]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 7954 is [True, False, False, True, False, False]
Current timestep = 7955. State = [[-0.28751165 -0.2876856 ]]. Action = [[-0.00673479  0.24570635 -0.1855873  -0.2620858 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 7955 is [True, False, False, True, False, False]
Current timestep = 7956. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22927195  0.1541028  -0.20926951 -0.6597716 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 7956 is [True, False, False, True, False, False]
Current timestep = 7957. State = [[-0.28753954 -0.28767788]]. Action = [[-0.18005149  0.10650444 -0.18104482 -0.59927404]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 7957 is [True, False, False, True, False, False]
Current timestep = 7958. State = [[-0.28753954 -0.28767788]]. Action = [[ 0.08770168  0.23722947 -0.18203753 -0.75400674]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 7958 is [True, False, False, True, False, False]
Current timestep = 7959. State = [[-0.28753954 -0.28767788]]. Action = [[-0.04150772  0.06110859 -0.23184977 -0.9358627 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 7959 is [True, False, False, True, False, False]
Scene graph at timestep 7959 is [True, False, False, True, False, False]
State prediction error at timestep 7959 is tensor(6.7289e-08, grad_fn=<MseLossBackward0>)
Current timestep = 7960. State = [[-0.28753954 -0.28767788]]. Action = [[-0.11298013  0.17744413 -0.18679951  0.13224614]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 7960 is [True, False, False, True, False, False]
Current timestep = 7961. State = [[-0.28753954 -0.28767788]]. Action = [[-0.04074168 -0.07452646 -0.10374936 -0.88208073]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 7961 is [True, False, False, True, False, False]
Current timestep = 7962. State = [[-0.28753954 -0.28767788]]. Action = [[-0.14001387 -0.16374259 -0.07597533  0.102036  ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 7962 is [True, False, False, True, False, False]
Current timestep = 7963. State = [[-0.28753954 -0.28767788]]. Action = [[ 0.08235872 -0.01587094 -0.21084222  0.4606129 ]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 7963 is [True, False, False, True, False, False]
Current timestep = 7964. State = [[-0.28753954 -0.28767788]]. Action = [[-0.01809889 -0.08508301  0.12634084 -0.3372885 ]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 7964 is [True, False, False, True, False, False]
Current timestep = 7965. State = [[-0.28753954 -0.28767788]]. Action = [[-0.21324079 -0.21081518  0.1806823   0.5144383 ]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 7965 is [True, False, False, True, False, False]
Current timestep = 7966. State = [[-0.28753954 -0.28767788]]. Action = [[0.07666525 0.03494483 0.00500435 0.24019265]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 7966 is [True, False, False, True, False, False]
Current timestep = 7967. State = [[-0.28753954 -0.28767788]]. Action = [[-0.09692222 -0.05948433 -0.19211645 -0.44265813]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 7967 is [True, False, False, True, False, False]
Current timestep = 7968. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17561013 -0.07158621 -0.07404046 -0.42978835]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 7968 is [True, False, False, True, False, False]
Current timestep = 7969. State = [[-0.28753954 -0.28767788]]. Action = [[-0.16284321  0.22813663 -0.11653861 -0.7639582 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 7969 is [True, False, False, True, False, False]
Current timestep = 7970. State = [[-0.28753954 -0.28767788]]. Action = [[-0.11914746  0.12994975 -0.22218414  0.5644374 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 7970 is [True, False, False, True, False, False]
Current timestep = 7971. State = [[-0.28753954 -0.28767788]]. Action = [[-0.23399341  0.05152518  0.02870914  0.51360273]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 7971 is [True, False, False, True, False, False]
Scene graph at timestep 7971 is [True, False, False, True, False, False]
State prediction error at timestep 7971 is tensor(1.5820e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7972. State = [[-0.28753954 -0.28767788]]. Action = [[-0.21492164  0.0526579  -0.11781196 -0.47408777]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 7972 is [True, False, False, True, False, False]
Current timestep = 7973. State = [[-0.28753954 -0.28767788]]. Action = [[-0.1726527   0.01359534 -0.04564005 -0.96568435]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 7973 is [True, False, False, True, False, False]
Current timestep = 7974. State = [[-0.28753954 -0.28767788]]. Action = [[-0.15323873  0.12986368 -0.10426396  0.29977155]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 7974 is [True, False, False, True, False, False]
Current timestep = 7975. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17166325  0.08937979 -0.18136404 -0.09741384]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 7975 is [True, False, False, True, False, False]
Scene graph at timestep 7975 is [True, False, False, True, False, False]
State prediction error at timestep 7975 is tensor(1.2063e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7976. State = [[-0.28753954 -0.28767788]]. Action = [[-0.19749022  0.07818872  0.14080602 -0.93543196]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 7976 is [True, False, False, True, False, False]
Current timestep = 7977. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17512214  0.2360931  -0.24628015 -0.01090074]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 7977 is [True, False, False, True, False, False]
Current timestep = 7978. State = [[-0.28753954 -0.28767788]]. Action = [[-0.18034887  0.0273765   0.12967724 -0.57086277]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 7978 is [True, False, False, True, False, False]
Current timestep = 7979. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17901036  0.07170391 -0.18171348 -0.7313672 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 7979 is [True, False, False, True, False, False]
Current timestep = 7980. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22204243  0.05640557 -0.00375874  0.3334818 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 7980 is [True, False, False, True, False, False]
Current timestep = 7981. State = [[-0.28753954 -0.28767788]]. Action = [[-0.21655369  0.18868348 -0.20127872 -0.467893  ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 7981 is [True, False, False, True, False, False]
Current timestep = 7982. State = [[-0.28753954 -0.28767788]]. Action = [[-0.15022504  0.24143618 -0.20514658 -0.89024436]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 7982 is [True, False, False, True, False, False]
Scene graph at timestep 7982 is [True, False, False, True, False, False]
State prediction error at timestep 7982 is tensor(7.9477e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7983. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20669766  0.1678968  -0.13452522 -0.6779758 ]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 7983 is [True, False, False, True, False, False]
Scene graph at timestep 7983 is [True, False, False, True, False, False]
State prediction error at timestep 7983 is tensor(8.3408e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7984. State = [[-0.28753954 -0.28767788]]. Action = [[-0.23774473  0.16978145 -0.07140291 -0.8369011 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 7984 is [True, False, False, True, False, False]
Current timestep = 7985. State = [[-0.28753954 -0.28767788]]. Action = [[-0.08427069 -0.02230084  0.0818857  -0.96037716]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 7985 is [True, False, False, True, False, False]
Current timestep = 7986. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20622307  0.0753144  -0.11152424 -0.87451637]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 7986 is [True, False, False, True, False, False]
Current timestep = 7987. State = [[-0.28753954 -0.28767788]]. Action = [[-0.11719787 -0.10230684  0.08900654 -0.09469378]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 7987 is [True, False, False, True, False, False]
Current timestep = 7988. State = [[-0.28753954 -0.28767788]]. Action = [[-0.10472514  0.23337078 -0.14484495 -0.8715566 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 7988 is [True, False, False, True, False, False]
Scene graph at timestep 7988 is [True, False, False, True, False, False]
State prediction error at timestep 7988 is tensor(4.5797e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7989. State = [[-0.28753954 -0.28767788]]. Action = [[-0.02597493  0.14773262 -0.18429577 -0.98659426]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 7989 is [True, False, False, True, False, False]
Current timestep = 7990. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22059649 -0.00799042 -0.1878431  -0.9729668 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 7990 is [True, False, False, True, False, False]
Current timestep = 7991. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20925535 -0.12212119 -0.16088966 -0.3077153 ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 7991 is [True, False, False, True, False, False]
Scene graph at timestep 7991 is [True, False, False, True, False, False]
State prediction error at timestep 7991 is tensor(1.5215e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7992. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22236173 -0.21737671 -0.23280202  0.9400346 ]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 7992 is [True, False, False, True, False, False]
Scene graph at timestep 7992 is [True, False, False, True, False, False]
State prediction error at timestep 7992 is tensor(1.5833e-05, grad_fn=<MseLossBackward0>)
Current timestep = 7993. State = [[-0.28753954 -0.28767788]]. Action = [[-0.11271432  0.23908794 -0.02177836 -0.59003896]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 7993 is [True, False, False, True, False, False]
Current timestep = 7994. State = [[-0.28753954 -0.28767788]]. Action = [[-0.01800208  0.08150411 -0.23106076 -0.07718021]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 7994 is [True, False, False, True, False, False]
Current timestep = 7995. State = [[-0.28753954 -0.28767788]]. Action = [[-0.19417122  0.03939995  0.13007289 -0.74109215]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 7995 is [True, False, False, True, False, False]
Current timestep = 7996. State = [[-0.28753954 -0.28767788]]. Action = [[-0.19766966 -0.19111699 -0.23162325 -0.50987685]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 7996 is [True, False, False, True, False, False]
Scene graph at timestep 7996 is [True, False, False, True, False, False]
State prediction error at timestep 7996 is tensor(1.8421e-06, grad_fn=<MseLossBackward0>)
Current timestep = 7997. State = [[-0.28753954 -0.28767788]]. Action = [[-0.06569421  0.20261705 -0.22731932 -0.40567303]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 7997 is [True, False, False, True, False, False]
Scene graph at timestep 7997 is [True, False, False, True, False, False]
State prediction error at timestep 7997 is tensor(4.0928e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7998. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22711566  0.24927193 -0.17558128 -0.9535515 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 7998 is [True, False, False, True, False, False]
Scene graph at timestep 7998 is [True, False, False, True, False, False]
State prediction error at timestep 7998 is tensor(1.1788e-07, grad_fn=<MseLossBackward0>)
Current timestep = 7999. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20163187  0.10116446 -0.14354493 -0.59294593]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 7999 is [True, False, False, True, False, False]
Current timestep = 8000. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17784256 -0.01726654  0.11413947 -0.6079028 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 8000 is [True, False, False, True, False, False]
Current timestep = 8001. State = [[-0.28753954 -0.28767788]]. Action = [[-0.23439357  0.21643674 -0.03750639 -0.8235357 ]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 8001 is [True, False, False, True, False, False]
Current timestep = 8002. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22602335  0.21953803 -0.05695453 -0.9873213 ]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 8002 is [True, False, False, True, False, False]
Current timestep = 8003. State = [[-0.28753954 -0.28767788]]. Action = [[ 0.04861373 -0.17827362  0.03460646 -0.07373053]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 8003 is [True, False, False, True, False, False]
Current timestep = 8004. State = [[-0.28753954 -0.28767788]]. Action = [[-0.18951695  0.2208153  -0.01019216 -0.96708816]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 8004 is [True, False, False, True, False, False]
Current timestep = 8005. State = [[-0.28753954 -0.28767788]]. Action = [[-0.19005346  0.02099138  0.07764864 -0.5246737 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 8005 is [True, False, False, True, False, False]
Current timestep = 8006. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20019232 -0.03426504 -0.23151307 -0.6024195 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 8006 is [True, False, False, True, False, False]
Scene graph at timestep 8006 is [True, False, False, True, False, False]
State prediction error at timestep 8006 is tensor(1.8612e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8007. State = [[-0.28753954 -0.28767788]]. Action = [[-0.04316269  0.22191283 -0.16974734  0.58867264]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 8007 is [True, False, False, True, False, False]
Current timestep = 8008. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22820006 -0.18209288  0.03155354 -0.9706835 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 8008 is [True, False, False, True, False, False]
Current timestep = 8009. State = [[-0.28753954 -0.28767788]]. Action = [[-0.0173008   0.20869973 -0.08467859 -0.27540028]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 8009 is [True, False, False, True, False, False]
Current timestep = 8010. State = [[-0.28753954 -0.28767788]]. Action = [[ 0.12694454 -0.10925663 -0.23117515  0.43875957]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 8010 is [True, False, False, True, False, False]
Current timestep = 8011. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20914763  0.24342602 -0.15057014 -0.1769433 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 8011 is [True, False, False, True, False, False]
Current timestep = 8012. State = [[-0.28753954 -0.28767788]]. Action = [[-0.12876296 -0.16910367  0.21482807 -0.38895905]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 8012 is [True, False, False, True, False, False]
Current timestep = 8013. State = [[-0.28753954 -0.28767788]]. Action = [[-0.14509504  0.16910058 -0.19287331 -0.87102735]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 8013 is [True, False, False, True, False, False]
Current timestep = 8014. State = [[-0.28753954 -0.28767788]]. Action = [[-0.116238    0.17591503  0.03199583 -0.66866785]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 8014 is [True, False, False, True, False, False]
Scene graph at timestep 8014 is [True, False, False, True, False, False]
State prediction error at timestep 8014 is tensor(9.2092e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8015. State = [[-0.28753954 -0.28767788]]. Action = [[ 0.04971063  0.19862044 -0.04822667  0.64738333]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 8015 is [True, False, False, True, False, False]
Scene graph at timestep 8015 is [True, False, False, True, False, False]
State prediction error at timestep 8015 is tensor(6.7325e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8016. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17543514  0.19948545  0.03242263  0.21395528]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 8016 is [True, False, False, True, False, False]
Current timestep = 8017. State = [[-0.28753954 -0.28767788]]. Action = [[-0.00941949 -0.09751958 -0.1118142   0.06109941]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 8017 is [True, False, False, True, False, False]
Scene graph at timestep 8017 is [True, False, False, True, False, False]
State prediction error at timestep 8017 is tensor(6.8221e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8018. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20809299 -0.23931392 -0.23266938 -0.40521312]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 8018 is [True, False, False, True, False, False]
Current timestep = 8019. State = [[-0.28753954 -0.28767788]]. Action = [[-0.05103725  0.22094595 -0.05998722 -0.8789687 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 8019 is [True, False, False, True, False, False]
Current timestep = 8020. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17338794 -0.01029906 -0.10790682  0.7077849 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 8020 is [True, False, False, True, False, False]
Scene graph at timestep 8020 is [True, False, False, True, False, False]
State prediction error at timestep 8020 is tensor(1.8502e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8021. State = [[-0.28753954 -0.28767788]]. Action = [[-0.19012715  0.1686126   0.21102095 -0.6195035 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 8021 is [True, False, False, True, False, False]
Current timestep = 8022. State = [[-0.28753954 -0.28767788]]. Action = [[-0.1102282   0.01534313 -0.22431533 -0.09799367]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 8022 is [True, False, False, True, False, False]
Scene graph at timestep 8022 is [True, False, False, True, False, False]
State prediction error at timestep 8022 is tensor(4.0137e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8023. State = [[-0.28753954 -0.28767788]]. Action = [[-0.21301821  0.06011522 -0.0576728  -0.99069625]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 8023 is [True, False, False, True, False, False]
Current timestep = 8024. State = [[-0.28753954 -0.28767788]]. Action = [[-0.06629865  0.1522946  -0.12755202 -0.86859035]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 8024 is [True, False, False, True, False, False]
Current timestep = 8025. State = [[-0.28753954 -0.28767788]]. Action = [[-0.1479854   0.09009039 -0.22674185 -0.88276756]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 8025 is [True, False, False, True, False, False]
Current timestep = 8026. State = [[-0.28753954 -0.28767788]]. Action = [[-0.12514661  0.17803389 -0.24075589 -0.9276235 ]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 8026 is [True, False, False, True, False, False]
Current timestep = 8027. State = [[-0.28753954 -0.28767788]]. Action = [[-0.14648682  0.11221099 -0.2131745  -0.79178596]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 8027 is [True, False, False, True, False, False]
Current timestep = 8028. State = [[-0.28753954 -0.28767788]]. Action = [[-0.1789635  -0.17777088 -0.05792539 -0.7204124 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 8028 is [True, False, False, True, False, False]
Current timestep = 8029. State = [[-0.28753954 -0.28767788]]. Action = [[-0.23843715  0.0957239  -0.23861638 -0.81038374]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 8029 is [True, False, False, True, False, False]
Current timestep = 8030. State = [[-0.28753954 -0.28767788]]. Action = [[-0.09425652  0.20270503  0.17443955 -0.9436275 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 8030 is [True, False, False, True, False, False]
Current timestep = 8031. State = [[-0.28753954 -0.28767788]]. Action = [[-0.19001438  0.09107426 -0.16480869 -0.97581357]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 8031 is [True, False, False, True, False, False]
Current timestep = 8032. State = [[-0.28753954 -0.28767788]]. Action = [[-0.10761301 -0.1781606  -0.23681419 -0.85507005]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 8032 is [True, False, False, True, False, False]
Current timestep = 8033. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17052838  0.2116341  -0.23294646 -0.91936636]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 8033 is [True, False, False, True, False, False]
Current timestep = 8034. State = [[-0.28753954 -0.28767788]]. Action = [[-0.07744263  0.17107585 -0.16509245 -0.6750046 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 8034 is [True, False, False, True, False, False]
Current timestep = 8035. State = [[-0.28753954 -0.28767788]]. Action = [[-0.06697956 -0.02532887 -0.23189163 -0.7489282 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 8035 is [True, False, False, True, False, False]
Current timestep = 8036. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22272457  0.20211619 -0.18584009 -0.9310475 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 8036 is [True, False, False, True, False, False]
Current timestep = 8037. State = [[-0.28753954 -0.28767788]]. Action = [[ 0.01069504  0.11678407  0.05483741 -0.24342263]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 8037 is [True, False, False, True, False, False]
Scene graph at timestep 8037 is [True, False, False, True, False, False]
State prediction error at timestep 8037 is tensor(2.2367e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8038. State = [[-0.28753954 -0.28767788]]. Action = [[-0.22304982  0.10546213  0.09282514 -0.2985655 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 8038 is [True, False, False, True, False, False]
Current timestep = 8039. State = [[-0.28753954 -0.28767788]]. Action = [[-0.23354878  0.18118608 -0.1368416  -0.1612835 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 8039 is [True, False, False, True, False, False]
Current timestep = 8040. State = [[-0.28753954 -0.28767788]]. Action = [[-0.10328335  0.16994119  0.06979102 -0.84416884]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 8040 is [True, False, False, True, False, False]
Scene graph at timestep 8040 is [True, False, False, True, False, False]
State prediction error at timestep 8040 is tensor(2.0259e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8041. State = [[-0.28753954 -0.28767788]]. Action = [[-0.16340229  0.20637769 -0.23135436 -0.9276622 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 8041 is [True, False, False, True, False, False]
Current timestep = 8042. State = [[-0.28753954 -0.28767788]]. Action = [[-0.11291027  0.21937457 -0.15183444 -0.17930746]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 8042 is [True, False, False, True, False, False]
Current timestep = 8043. State = [[-0.28753954 -0.28767788]]. Action = [[-0.15593188  0.23488343  0.17697191 -0.8375247 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 8043 is [True, False, False, True, False, False]
Current timestep = 8044. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20131396  0.12638962 -0.08372872 -0.95029914]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 8044 is [True, False, False, True, False, False]
Current timestep = 8045. State = [[-0.28753954 -0.28767788]]. Action = [[-0.10590339 -0.08017334  0.11833549 -0.9288886 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 8045 is [True, False, False, True, False, False]
Scene graph at timestep 8045 is [True, False, False, True, False, False]
State prediction error at timestep 8045 is tensor(1.8225e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8046. State = [[-0.28753954 -0.28767788]]. Action = [[-0.10284701  0.17307603  0.1194098  -0.529173  ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 8046 is [True, False, False, True, False, False]
Current timestep = 8047. State = [[-0.28753954 -0.28767788]]. Action = [[-0.14272016  0.04153362 -0.22455022 -0.8969487 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 8047 is [True, False, False, True, False, False]
Current timestep = 8048. State = [[-0.28753954 -0.28767788]]. Action = [[-0.10684451  0.17278507 -0.08473729 -0.86216515]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 8048 is [True, False, False, True, False, False]
Current timestep = 8049. State = [[-0.28753954 -0.28767788]]. Action = [[-0.16984548 -0.16493757  0.02617022 -0.798641  ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 8049 is [True, False, False, True, False, False]
Current timestep = 8050. State = [[-0.28753954 -0.28767788]]. Action = [[-0.21868546  0.21644941  0.10139489 -0.89398575]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 8050 is [True, False, False, True, False, False]
Current timestep = 8051. State = [[-0.28753954 -0.28767788]]. Action = [[-0.21492541  0.215729   -0.21346241 -0.7187719 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 8051 is [True, False, False, True, False, False]
Current timestep = 8052. State = [[-0.28753954 -0.28767788]]. Action = [[-0.07600458  0.22538269  0.17561266 -0.944472  ]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 8052 is [True, False, False, True, False, False]
Current timestep = 8053. State = [[-0.28753954 -0.28767788]]. Action = [[-0.07701284  0.1381517  -0.20032746 -0.40950263]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 8053 is [True, False, False, True, False, False]
Current timestep = 8054. State = [[-0.28753954 -0.28767788]]. Action = [[-0.12678373  0.20192009 -0.17317383 -0.6592564 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 8054 is [True, False, False, True, False, False]
Current timestep = 8055. State = [[-0.28753954 -0.28767788]]. Action = [[-0.08647124  0.00417593 -0.24463356 -0.9921458 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 8055 is [True, False, False, True, False, False]
Current timestep = 8056. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17700188  0.22473413 -0.14266701 -0.7823623 ]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 8056 is [True, False, False, True, False, False]
Scene graph at timestep 8056 is [True, False, False, True, False, False]
State prediction error at timestep 8056 is tensor(3.3048e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8057. State = [[-0.28753954 -0.28767788]]. Action = [[ 0.00195295  0.19317883  0.21628141 -0.81856775]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 8057 is [True, False, False, True, False, False]
Current timestep = 8058. State = [[-0.28753954 -0.28767788]]. Action = [[-0.14265683  0.19506097 -0.15614614 -0.7901091 ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 8058 is [True, False, False, True, False, False]
Current timestep = 8059. State = [[-0.28753954 -0.28767788]]. Action = [[-0.20021889  0.06092703 -0.12116298 -0.9489793 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 8059 is [True, False, False, True, False, False]
Current timestep = 8060. State = [[-0.28753954 -0.28767788]]. Action = [[-0.19399765  0.21230704 -0.24379449 -0.7136236 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 8060 is [True, False, False, True, False, False]
Scene graph at timestep 8060 is [True, False, False, True, False, False]
State prediction error at timestep 8060 is tensor(6.1982e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8061. State = [[-0.28753954 -0.28767788]]. Action = [[-0.16018009  0.22175577 -0.18647075 -0.8628994 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 8061 is [True, False, False, True, False, False]
Current timestep = 8062. State = [[-0.28753954 -0.28767788]]. Action = [[-0.10501483  0.1801933  -0.24324982  0.1610955 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 8062 is [True, False, False, True, False, False]
Scene graph at timestep 8062 is [True, False, False, True, False, False]
State prediction error at timestep 8062 is tensor(9.3980e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8063. State = [[-0.28753954 -0.28767788]]. Action = [[-0.12204751  0.13192838  0.17487133 -0.4475962 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 8063 is [True, False, False, True, False, False]
Current timestep = 8064. State = [[-0.28753954 -0.28767788]]. Action = [[-0.13628735 -0.15228833 -0.24704999 -0.7913469 ]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 8064 is [True, False, False, True, False, False]
Current timestep = 8065. State = [[-0.28753954 -0.28767788]]. Action = [[-0.0045715   0.08731374  0.19618621 -0.8271699 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 8065 is [True, False, False, True, False, False]
Current timestep = 8066. State = [[-0.28753954 -0.28767788]]. Action = [[-0.17371115  0.18212658  0.13389647 -0.63092095]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 8066 is [True, False, False, True, False, False]
Current timestep = 8067. State = [[-0.28753954 -0.28767788]]. Action = [[-0.04929654  0.22461396 -0.20246565  0.0011847 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 8067 is [True, False, False, True, False, False]
Current timestep = 8068. State = [[-0.28753954 -0.28767788]]. Action = [[-0.07125369  0.03729206 -0.24059927 -0.581615  ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 8068 is [True, False, False, True, False, False]
Current timestep = 8069. State = [[-0.28753954 -0.28767788]]. Action = [[-0.16229194  0.09021699 -0.18823743 -0.8196737 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 8069 is [True, False, False, True, False, False]
Scene graph at timestep 8069 is [True, False, False, True, False, False]
State prediction error at timestep 8069 is tensor(7.7681e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8070. State = [[-0.28753954 -0.28767788]]. Action = [[-0.18761598 -0.00805472  0.17485777 -0.07845956]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 8070 is [True, False, False, True, False, False]
Current timestep = 8071. State = [[-0.28753954 -0.28767788]]. Action = [[-0.04670997  0.05225891 -0.20477058 -0.7560846 ]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 8071 is [True, False, False, True, False, False]
Current timestep = 8072. State = [[-0.28753954 -0.28767788]]. Action = [[-0.1623076  -0.06720096 -0.04074875 -0.4499879 ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 8072 is [True, False, False, True, False, False]
Current timestep = 8073. State = [[-0.2348598   0.05879381]]. Action = [[-0.10999492  0.11435589 -0.22603998 -0.9603313 ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 8073 is [True, False, False, True, False, False]
Current timestep = 8074. State = [[-0.22909139  0.06387983]]. Action = [[ 0.23760271 -0.0969772  -0.1477169  -0.2466492 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 8074 is [True, False, False, False, True, False]
Current timestep = 8075. State = [[-0.22724788  0.06394553]]. Action = [[ 0.21564227 -0.09699985 -0.1485233  -0.16776562]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 8075 is [True, False, False, False, True, False]
Current timestep = 8076. State = [[-0.22362196  0.06307933]]. Action = [[ 0.24888325 -0.10825285 -0.1310408  -0.17578495]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 8076 is [True, False, False, False, True, False]
Current timestep = 8077. State = [[-0.21908139  0.06127945]]. Action = [[ 0.24547568 -0.09557784 -0.13441484 -0.32035518]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 8077 is [True, False, False, False, True, False]
Current timestep = 8078. State = [[-0.21220547  0.05929346]]. Action = [[ 0.22392684 -0.10687676 -0.14019305 -0.21617877]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 8078 is [True, False, False, False, True, False]
Scene graph at timestep 8078 is [True, False, False, False, True, False]
State prediction error at timestep 8078 is tensor(1.6026e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8079. State = [[-0.20411699  0.05714509]]. Action = [[ 0.24034753 -0.06199619 -0.15042047 -0.2775576 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 8079 is [True, False, False, False, True, False]
Scene graph at timestep 8079 is [True, False, False, False, True, False]
State prediction error at timestep 8079 is tensor(1.4897e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8079 of 1
Current timestep = 8080. State = [[-0.19475098  0.05512146]]. Action = [[ 0.24306428 -0.09798567 -0.13785644 -0.2622342 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 8080 is [True, False, False, False, True, False]
Current timestep = 8081. State = [[-0.18674043  0.05253121]]. Action = [[ 0.24400699 -0.07611255 -0.15364382 -0.14992082]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 8081 is [True, False, False, False, True, False]
Current timestep = 8082. State = [[-0.17834958  0.04984052]]. Action = [[ 0.23379445 -0.08766232 -0.14999083 -0.22306907]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8082 is [True, False, False, False, True, False]
Current timestep = 8083. State = [[-0.17026821  0.04678021]]. Action = [[ 0.24074939 -0.11223188 -0.15609126 -0.19038975]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 8083 is [True, False, False, False, True, False]
Human Feedback received at timestep 8083 of 1
Current timestep = 8084. State = [[-0.16216528  0.04320685]]. Action = [[ 0.22938138 -0.09271133 -0.14454581 -0.17088646]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 8084 is [True, False, False, False, True, False]
Scene graph at timestep 8084 is [True, False, False, False, True, False]
State prediction error at timestep 8084 is tensor(2.5133e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8085. State = [[-0.15231496  0.03986129]]. Action = [[ 0.24316978 -0.1002931  -0.14694361 -0.09851259]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 8085 is [True, False, False, False, True, False]
Human Feedback received at timestep 8085 of 1
Current timestep = 8086. State = [[-0.14413553  0.03664192]]. Action = [[ 0.2244249  -0.10245588 -0.16525511 -0.12052095]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 8086 is [True, False, False, False, True, False]
Current timestep = 8087. State = [[-0.13467735  0.03289294]]. Action = [[ 0.24299347 -0.09482291 -0.14822662 -0.16626257]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 8087 is [True, False, False, False, True, False]
Human Feedback received at timestep 8087 of 1
Current timestep = 8088. State = [[-0.12677579  0.02922677]]. Action = [[ 0.22882283 -0.07613418 -0.15836002 -0.15179074]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 8088 is [True, False, False, False, True, False]
Current timestep = 8089. State = [[-0.11867962  0.02608088]]. Action = [[ 0.21688437 -0.08422223 -0.13174136 -0.14609396]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 8089 is [True, False, False, False, True, False]
Current timestep = 8090. State = [[-0.11006086  0.02289327]]. Action = [[ 0.22378021 -0.10747732 -0.1662995  -0.20980245]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 8090 is [True, False, False, False, True, False]
Scene graph at timestep 8090 is [True, False, False, False, True, False]
State prediction error at timestep 8090 is tensor(2.7549e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8090 of 1
Current timestep = 8091. State = [[-0.10062818  0.01950008]]. Action = [[ 0.19103378 -0.08230248 -0.14378445 -0.09975982]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 8091 is [True, False, False, False, True, False]
Current timestep = 8092. State = [[-0.09233114  0.01634434]]. Action = [[ 0.1595945  -0.08603635 -0.1465736  -0.11345869]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 8092 is [True, False, False, False, True, False]
Scene graph at timestep 8092 is [True, False, False, False, True, False]
State prediction error at timestep 8092 is tensor(1.6602e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8092 of 1
Current timestep = 8093. State = [[-0.08505277  0.0131329 ]]. Action = [[ 0.23517376 -0.05999827 -0.14058375 -0.16319788]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 8093 is [True, False, False, False, True, False]
Human Feedback received at timestep 8093 of 1
Current timestep = 8094. State = [[-0.07679666  0.01002129]]. Action = [[ 0.24592185 -0.13081345 -0.12127772 -0.2170242 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 8094 is [True, False, False, False, True, False]
Scene graph at timestep 8094 is [True, False, False, False, True, False]
State prediction error at timestep 8094 is tensor(2.0101e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8095. State = [[-0.06875439  0.00638676]]. Action = [[ 0.22324848 -0.11550105 -0.12025774  0.01378572]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 8095 is [True, False, False, False, True, False]
Scene graph at timestep 8095 is [True, False, False, False, True, False]
State prediction error at timestep 8095 is tensor(1.1768e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8096. State = [[-0.06139011  0.00242016]]. Action = [[ 0.21347362 -0.0987758  -0.15493537 -0.00997913]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 8096 is [True, False, False, False, True, False]
Human Feedback received at timestep 8096 of 1
Current timestep = 8097. State = [[-0.05311118 -0.00167537]]. Action = [[ 0.24323109 -0.11419998 -0.14831525 -0.20696986]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 8097 is [True, False, False, False, True, False]
Current timestep = 8098. State = [[-0.0455379  -0.00524627]]. Action = [[ 0.2298443  -0.15705374 -0.16807413 -0.380136  ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 8098 is [True, False, False, False, True, False]
Scene graph at timestep 8098 is [False, True, False, False, True, False]
State prediction error at timestep 8098 is tensor(2.8356e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8099. State = [[-0.03701971 -0.00975022]]. Action = [[ 0.23534915 -0.0848842  -0.18235826 -0.06287003]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 8099 is [False, True, False, False, True, False]
Human Feedback received at timestep 8099 of 1
Current timestep = 8100. State = [[-0.21095201 -0.06106682]]. Action = [[ 0.20044798 -0.02550489 -0.13154401 -0.22941446]]. Reward = [100.]
Curr episode timestep = 26
Scene graph at timestep 8100 is [False, True, False, False, True, False]
Current timestep = 8101. State = [[-0.20851897 -0.06526727]]. Action = [[ 0.24510288 -0.09564146 -0.1222094  -0.18057513]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 8101 is [True, False, False, False, True, False]
Scene graph at timestep 8101 is [True, False, False, False, True, False]
State prediction error at timestep 8101 is tensor(1.3291e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8102. State = [[-0.20607804 -0.06654856]]. Action = [[ 0.17684901 -0.13063209 -0.16282116 -0.06906295]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 8102 is [True, False, False, False, True, False]
Current timestep = 8103. State = [[-0.20195945 -0.06943888]]. Action = [[ 0.19480935 -0.10878299 -0.14158995 -0.14637876]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 8103 is [True, False, False, False, True, False]
Current timestep = 8104. State = [[-0.19702308 -0.0730362 ]]. Action = [[ 0.2398768  -0.14937189 -0.1772513   0.10938096]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 8104 is [True, False, False, False, True, False]
Current timestep = 8105. State = [[-0.18975368 -0.07778469]]. Action = [[ 0.2404919  -0.11062038 -0.16018794  0.11804259]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 8105 is [True, False, False, False, True, False]
Scene graph at timestep 8105 is [True, False, False, False, True, False]
State prediction error at timestep 8105 is tensor(2.2275e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8106. State = [[-0.18175702 -0.08228583]]. Action = [[ 0.14856857 -0.04355471 -0.16856213 -0.46910018]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 8106 is [True, False, False, False, True, False]
Current timestep = 8107. State = [[-0.17508225 -0.08571649]]. Action = [[ 0.22315523 -0.07352909 -0.18132755 -0.2335347 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 8107 is [True, False, False, False, True, False]
Current timestep = 8108. State = [[-0.16846694 -0.08862299]]. Action = [[ 0.2419509  -0.02131884 -0.14445902 -0.13489437]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 8108 is [True, False, False, False, True, False]
Current timestep = 8109. State = [[-0.16044098 -0.09112307]]. Action = [[-0.00320168 -0.1429335  -0.17178023 -0.10345888]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8109 is [True, False, False, False, True, False]
Scene graph at timestep 8109 is [True, False, False, False, True, False]
State prediction error at timestep 8109 is tensor(1.7903e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8109 of 1
Current timestep = 8110. State = [[-0.15341605 -0.09438606]]. Action = [[ 0.2106714   0.02171284 -0.15847284 -0.11548936]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 8110 is [True, False, False, False, True, False]
Scene graph at timestep 8110 is [True, False, False, False, True, False]
State prediction error at timestep 8110 is tensor(1.1740e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8111. State = [[-0.14754441 -0.09606691]]. Action = [[ 0.11474711 -0.13355237 -0.15500166 -0.08834416]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 8111 is [True, False, False, False, True, False]
Current timestep = 8112. State = [[-0.14138658 -0.09890894]]. Action = [[ 0.17335135 -0.04626423 -0.19263813 -0.42730272]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 8112 is [True, False, False, False, True, False]
Current timestep = 8113. State = [[-0.13582954 -0.1010211 ]]. Action = [[ 0.16233093 -0.02884877 -0.14995731 -0.19089544]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 8113 is [True, False, False, False, True, False]
Scene graph at timestep 8113 is [True, False, False, False, True, False]
State prediction error at timestep 8113 is tensor(4.7953e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8114. State = [[-0.13082878 -0.10246742]]. Action = [[ 0.17492673 -0.1016003  -0.153021   -0.47067654]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 8114 is [True, False, False, False, True, False]
Current timestep = 8115. State = [[-0.12488865 -0.10483843]]. Action = [[ 0.20325369 -0.13035367 -0.06487608 -0.6305395 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 8115 is [True, False, False, False, True, False]
Current timestep = 8116. State = [[-0.11849503 -0.10837077]]. Action = [[-0.07219924  0.00572547 -0.15427086 -0.3599103 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 8116 is [True, False, False, False, True, False]
Human Feedback received at timestep 8116 of -1
Current timestep = 8117. State = [[-0.11457862 -0.11046982]]. Action = [[ 0.03389245  0.04170099 -0.1176455  -0.45862877]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 8117 is [True, False, False, False, True, False]
Current timestep = 8118. State = [[-0.11198352 -0.11119407]]. Action = [[ 0.09456491 -0.09019202 -0.12897307 -0.39785862]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 8118 is [True, False, False, False, True, False]
Current timestep = 8119. State = [[-0.10931549 -0.11285017]]. Action = [[ 0.01046708  0.01142758 -0.17785662 -0.07294285]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 8119 is [True, False, False, False, True, False]
Current timestep = 8120. State = [[-0.10764953 -0.1137143 ]]. Action = [[ 0.01647028  0.16009682 -0.18338703  0.1405356 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 8120 is [True, False, False, False, True, False]
Current timestep = 8121. State = [[-0.1065707  -0.11357266]]. Action = [[ 0.09568208 -0.15476912 -0.22675967  0.00818014]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 8121 is [True, False, False, False, True, False]
Current timestep = 8122. State = [[-0.10468462 -0.11436596]]. Action = [[ 0.06344044  0.06739777 -0.14115791 -0.3243568 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 8122 is [True, False, False, False, True, False]
Current timestep = 8123. State = [[-0.10194439 -0.11473283]]. Action = [[ 0.0415293  -0.03449307 -0.22783324 -0.68003535]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 8123 is [True, False, False, False, True, False]
Current timestep = 8124. State = [[-0.09860223 -0.11515345]]. Action = [[ 0.04933929  0.06700677 -0.03388985 -0.4307778 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 8124 is [True, False, False, False, True, False]
Current timestep = 8125. State = [[-0.09679079 -0.11515997]]. Action = [[ 0.12436885  0.1467194  -0.10227552 -0.4190991 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 8125 is [True, False, False, False, True, False]
Human Feedback received at timestep 8125 of -1
Current timestep = 8126. State = [[-0.09543078 -0.11354813]]. Action = [[ 0.15201369  0.13139158 -0.03618672 -0.81947976]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 8126 is [True, False, False, False, True, False]
Current timestep = 8127. State = [[-0.09335942 -0.11084801]]. Action = [[ 0.14229646  0.00181389 -0.23534426 -0.5278024 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 8127 is [True, False, False, False, True, False]
Current timestep = 8128. State = [[-0.09004736 -0.10922527]]. Action = [[ 0.06872192  0.13310862 -0.03785826 -0.73651046]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 8128 is [True, False, False, False, True, False]
Scene graph at timestep 8128 is [True, False, False, False, True, False]
State prediction error at timestep 8128 is tensor(2.7885e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8129. State = [[-0.08605317 -0.1076711 ]]. Action = [[ 0.04700157 -0.04413149  0.03175783 -0.57605404]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 8129 is [True, False, False, False, True, False]
Scene graph at timestep 8129 is [True, False, False, False, True, False]
State prediction error at timestep 8129 is tensor(5.5972e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8130. State = [[-0.08151392 -0.10744523]]. Action = [[-0.01114997 -0.04721157 -0.04539379 -0.6219847 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 8130 is [True, False, False, False, True, False]
Scene graph at timestep 8130 is [True, False, False, False, True, False]
State prediction error at timestep 8130 is tensor(4.9271e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8131. State = [[-0.07783884 -0.10738274]]. Action = [[ 0.05023751  0.14376315 -0.22135328 -0.48142332]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 8131 is [True, False, False, False, True, False]
Scene graph at timestep 8131 is [True, False, False, False, True, False]
State prediction error at timestep 8131 is tensor(2.0142e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8132. State = [[-0.07609669 -0.10568839]]. Action = [[ 0.13698903  0.19986278 -0.11850268 -0.8553049 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 8132 is [True, False, False, False, True, False]
Current timestep = 8133. State = [[-0.0744832 -0.1020871]]. Action = [[ 0.00389376 -0.07144725 -0.1033597  -0.59514207]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 8133 is [True, False, False, False, True, False]
Current timestep = 8134. State = [[-0.07236119 -0.10131448]]. Action = [[ 0.07742506 -0.13209236 -0.21268795 -0.63710076]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 8134 is [True, False, False, False, True, False]
Current timestep = 8135. State = [[-0.06923147 -0.10150211]]. Action = [[ 0.16423684  0.21506763 -0.19547474 -0.2511729 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 8135 is [True, False, False, False, True, False]
Scene graph at timestep 8135 is [True, False, False, False, True, False]
State prediction error at timestep 8135 is tensor(1.4846e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8136. State = [[-0.06577146 -0.09927879]]. Action = [[-0.05538449  0.06187737 -0.07884413 -0.6249169 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 8136 is [True, False, False, False, True, False]
Human Feedback received at timestep 8136 of -1
Current timestep = 8137. State = [[-0.06355761 -0.09671423]]. Action = [[ 0.15593222  0.11554602 -0.2032348  -0.5254267 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 8137 is [True, False, False, False, True, False]
Current timestep = 8138. State = [[-0.06102455 -0.09364764]]. Action = [[-0.02960378  0.16644502 -0.15444346 -0.53602904]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 8138 is [True, False, False, False, True, False]
Current timestep = 8139. State = [[-0.05909875 -0.08946807]]. Action = [[-0.04255842 -0.00997603 -0.12555681 -0.4315096 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 8139 is [True, False, False, False, True, False]
Current timestep = 8140. State = [[-0.0586108  -0.08734748]]. Action = [[ 0.10479921 -0.1375858  -0.08862332 -0.44745874]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 8140 is [True, False, False, False, True, False]
Scene graph at timestep 8140 is [True, False, False, False, True, False]
State prediction error at timestep 8140 is tensor(5.8972e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8141. State = [[-0.05795206 -0.08709106]]. Action = [[ 0.08036059  0.10545206 -0.16442522 -0.83380044]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 8141 is [True, False, False, False, True, False]
Scene graph at timestep 8141 is [True, False, False, False, True, False]
State prediction error at timestep 8141 is tensor(1.9949e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8142. State = [[-0.05673531 -0.08554931]]. Action = [[ 0.07981014  0.2347306  -0.12305495 -0.5119924 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 8142 is [True, False, False, False, True, False]
Current timestep = 8143. State = [[-0.05532569 -0.08198071]]. Action = [[-0.02052455 -0.00807925 -0.14267279 -0.15714467]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 8143 is [True, False, False, False, True, False]
Current timestep = 8144. State = [[-0.05329379 -0.0795949 ]]. Action = [[ 0.15748018  0.17707855 -0.13874365 -0.28553092]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 8144 is [True, False, False, False, True, False]
Current timestep = 8145. State = [[-0.05033   -0.0762324]]. Action = [[ 0.1438734   0.02872172 -0.14335185 -0.5956483 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 8145 is [True, False, False, False, True, False]
Current timestep = 8146. State = [[-0.04647978 -0.07366496]]. Action = [[ 0.0934132  -0.04444517 -0.18324956 -0.35623062]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 8146 is [True, False, False, False, True, False]
Current timestep = 8147. State = [[-0.04277674 -0.0727855 ]]. Action = [[-0.10215959  0.10474375 -0.14306623 -0.66345507]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 8147 is [False, True, False, False, True, False]
Human Feedback received at timestep 8147 of 1
Current timestep = 8148. State = [[-0.03948271 -0.07087602]]. Action = [[ 0.06661588  0.0335165  -0.18773477 -0.28889036]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 8148 is [False, True, False, False, True, False]
Current timestep = 8149. State = [[-0.03810962 -0.06927259]]. Action = [[ 0.15793148 -0.01968986 -0.16278581 -0.77754736]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 8149 is [False, True, False, False, True, False]
Current timestep = 8150. State = [[-0.03655012 -0.06791836]]. Action = [[ 0.10805929 -0.02471556 -0.17758839 -0.33040464]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 8150 is [False, True, False, False, True, False]
Current timestep = 8151. State = [[-0.26218423 -0.04320748]]. Action = [[ 0.03465676  0.00383812 -0.05954823 -0.6722782 ]]. Reward = [100.]
Curr episode timestep = 50
Scene graph at timestep 8151 is [False, True, False, False, True, False]
Scene graph at timestep 8151 is [True, False, False, False, True, False]
State prediction error at timestep 8151 is tensor(0.0260, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8151 of -1
Current timestep = 8152. State = [[-0.26206893 -0.04640323]]. Action = [[ 0.13492128 -0.09572068 -0.1132649  -0.05363083]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 8152 is [True, False, False, False, True, False]
Current timestep = 8153. State = [[-0.26073018 -0.04746449]]. Action = [[ 0.23659712 -0.10932821 -0.15250525  0.07804441]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 8153 is [True, False, False, False, True, False]
Current timestep = 8154. State = [[-0.25707155 -0.04954845]]. Action = [[ 0.21932226 -0.05021621 -0.1746833   0.09477699]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 8154 is [True, False, False, False, True, False]
Current timestep = 8155. State = [[-0.25155196 -0.05173557]]. Action = [[ 0.18530962 -0.18100387 -0.14929348 -0.04704887]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 8155 is [True, False, False, False, True, False]
Scene graph at timestep 8155 is [True, False, False, False, True, False]
State prediction error at timestep 8155 is tensor(1.4328e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8156. State = [[-0.24503846 -0.05559735]]. Action = [[ 0.19196588 -0.08439845 -0.15873875 -0.23765981]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 8156 is [True, False, False, False, True, False]
Current timestep = 8157. State = [[-0.23889235 -0.05941262]]. Action = [[ 0.21232343 -0.11540216 -0.1380704   0.09038723]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 8157 is [True, False, False, False, True, False]
Current timestep = 8158. State = [[-0.23217803 -0.06376317]]. Action = [[ 0.20403892 -0.1119068  -0.14643447 -0.0822503 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 8158 is [True, False, False, False, True, False]
Current timestep = 8159. State = [[-0.22577177 -0.06795439]]. Action = [[ 0.2048676  -0.12807693 -0.16243103  0.12839818]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 8159 is [True, False, False, False, True, False]
Current timestep = 8160. State = [[-0.21884704 -0.07184867]]. Action = [[ 0.09541225 -0.13675551 -0.15071392 -0.16087592]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8160 is [True, False, False, False, True, False]
Current timestep = 8161. State = [[-0.21267591 -0.07618442]]. Action = [[ 0.02617982 -0.04774755 -0.10675135  0.17078698]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 8161 is [True, False, False, False, True, False]
Scene graph at timestep 8161 is [True, False, False, False, True, False]
State prediction error at timestep 8161 is tensor(1.4442e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8162. State = [[-0.20764932 -0.08050314]]. Action = [[ 0.17680463 -0.15222342 -0.09799585 -0.12800825]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 8162 is [True, False, False, False, True, False]
Current timestep = 8163. State = [[-0.20335348 -0.08456292]]. Action = [[ 0.16222996 -0.16029088 -0.145871    0.08833826]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 8163 is [True, False, False, False, True, False]
Current timestep = 8164. State = [[-0.19902009 -0.08919846]]. Action = [[ 0.20276117 -0.14238508 -0.14773491  0.20574641]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 8164 is [True, False, False, False, True, False]
Current timestep = 8165. State = [[-0.19322962 -0.09460765]]. Action = [[ 0.1376434  -0.14150909 -0.17801055 -0.04204214]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 8165 is [True, False, False, False, True, False]
Current timestep = 8166. State = [[-0.1876963  -0.09971566]]. Action = [[ 0.11890715 -0.15165761 -0.16539401  0.10924327]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 8166 is [True, False, False, False, True, False]
Current timestep = 8167. State = [[-0.18195435 -0.10504155]]. Action = [[ 0.11549231 -0.01526825 -0.1215671  -0.13667667]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 8167 is [True, False, False, False, True, False]
Current timestep = 8168. State = [[-0.1776448  -0.10828733]]. Action = [[ 0.13825372  0.05772221 -0.12000978  0.02724874]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 8168 is [True, False, False, False, True, False]
Scene graph at timestep 8168 is [True, False, False, False, True, False]
State prediction error at timestep 8168 is tensor(1.0067e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8169. State = [[-0.17281142 -0.10965639]]. Action = [[ 0.07734394  0.04888502 -0.21149863  0.09508538]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 8169 is [True, False, False, False, True, False]
Current timestep = 8170. State = [[-0.16828983 -0.11044171]]. Action = [[ 0.15393215  0.06979871 -0.09568751  0.03123462]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 8170 is [True, False, False, False, True, False]
Current timestep = 8171. State = [[-0.1636505  -0.11071628]]. Action = [[ 0.15536463 -0.08446795 -0.13164587  0.0636971 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 8171 is [True, False, False, False, True, False]
Current timestep = 8172. State = [[-0.15855691 -0.11162957]]. Action = [[ 0.11177281 -0.06071022 -0.19765721 -0.3273127 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 8172 is [True, False, False, False, True, False]
Current timestep = 8173. State = [[-0.15410289 -0.11253666]]. Action = [[ 0.12553924 -0.1221945  -0.172813   -0.5763431 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 8173 is [True, False, False, False, True, False]
Current timestep = 8174. State = [[-0.14979234 -0.11406515]]. Action = [[ 0.11562967 -0.10876982 -0.16869037 -0.37014246]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 8174 is [True, False, False, False, True, False]
Current timestep = 8175. State = [[-0.14525448 -0.11636753]]. Action = [[ 0.0896517   0.02292359 -0.14753531  0.2404933 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 8175 is [True, False, False, False, True, False]
Current timestep = 8176. State = [[-0.1403541  -0.11773685]]. Action = [[ 0.07249355  0.06579289 -0.1293214  -0.24725282]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 8176 is [True, False, False, False, True, False]
Current timestep = 8177. State = [[-0.13651426 -0.11827448]]. Action = [[ 0.12541172  0.05141324 -0.20651798 -0.6015429 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 8177 is [True, False, False, False, True, False]
Current timestep = 8178. State = [[-0.1331907  -0.11860295]]. Action = [[ 0.08064592 -0.07611451 -0.20275475 -0.057136  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 8178 is [True, False, False, False, True, False]
Current timestep = 8179. State = [[-0.1287619  -0.11934456]]. Action = [[ 0.03157237  0.09106961 -0.16885078 -0.37846088]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 8179 is [True, False, False, False, True, False]
Scene graph at timestep 8179 is [True, False, False, False, True, False]
State prediction error at timestep 8179 is tensor(9.3790e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8180. State = [[-0.1258858  -0.11946187]]. Action = [[-0.01440856 -0.01614696  0.01533425  0.00196898]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 8180 is [True, False, False, False, True, False]
Current timestep = 8181. State = [[-0.1238604  -0.11961845]]. Action = [[ 0.00090963  0.03376001 -0.1163255  -0.07090247]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 8181 is [True, False, False, False, True, False]
Current timestep = 8182. State = [[-0.12247109 -0.11954669]]. Action = [[ 0.00776669 -0.0183403  -0.13935372 -0.7133454 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 8182 is [True, False, False, False, True, False]
Current timestep = 8183. State = [[-0.1211464  -0.11974752]]. Action = [[-0.08344409 -0.00280325 -0.21016549 -0.8204805 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 8183 is [True, False, False, False, True, False]
Scene graph at timestep 8183 is [True, False, False, False, True, False]
State prediction error at timestep 8183 is tensor(4.2965e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8184. State = [[-0.12111972 -0.1197867 ]]. Action = [[-0.11145461 -0.13609841 -0.15170987 -0.49639845]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 8184 is [True, False, False, False, True, False]
Current timestep = 8185. State = [[-0.12139279 -0.12059724]]. Action = [[ 0.01548114 -0.07587029 -0.19582696  0.5197332 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 8185 is [True, False, False, False, True, False]
Scene graph at timestep 8185 is [True, False, False, False, True, False]
State prediction error at timestep 8185 is tensor(4.0559e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8186. State = [[-0.12158871 -0.1215418 ]]. Action = [[ 0.07558316  0.07346064 -0.15542606 -0.38377947]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 8186 is [True, False, False, False, True, False]
Current timestep = 8187. State = [[-0.12156088 -0.12158977]]. Action = [[ 0.03411132 -0.05608228 -0.19031216 -0.8285067 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 8187 is [True, False, False, False, True, False]
Human Feedback received at timestep 8187 of 1
Current timestep = 8188. State = [[-0.12132093 -0.12158059]]. Action = [[ 0.12369496  0.18600017 -0.13504483 -0.66200083]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 8188 is [True, False, False, False, True, False]
Current timestep = 8189. State = [[-0.12073638 -0.12089291]]. Action = [[ 0.00444847  0.16384816 -0.0439291  -0.4853987 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 8189 is [True, False, False, False, True, False]
Current timestep = 8190. State = [[-0.12004096 -0.11857773]]. Action = [[ 0.07047328  0.17908973 -0.16132441  0.4605441 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 8190 is [True, False, False, False, True, False]
Current timestep = 8191. State = [[-0.11900972 -0.11554163]]. Action = [[-0.02897561 -0.01885395 -0.09489937 -0.2473883 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 8191 is [True, False, False, False, True, False]
Human Feedback received at timestep 8191 of 1
Current timestep = 8192. State = [[-0.11848805 -0.11364997]]. Action = [[-0.05338685  0.07633409 -0.18986392 -0.37703383]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 8192 is [True, False, False, False, True, False]
Current timestep = 8193. State = [[-0.11806142 -0.11195298]]. Action = [[ 0.1067062  -0.181671   -0.12386388 -0.6787254 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 8193 is [True, False, False, False, True, False]
Current timestep = 8194. State = [[-0.11760135 -0.11212344]]. Action = [[ 0.04828593  0.05709845 -0.13527398 -0.00715822]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 8194 is [True, False, False, False, True, False]
Human Feedback received at timestep 8194 of 1
Current timestep = 8195. State = [[-0.11728749 -0.11190155]]. Action = [[-0.09810591 -0.01010895 -0.16172548 -0.58732045]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 8195 is [True, False, False, False, True, False]
Current timestep = 8196. State = [[-0.11718926 -0.11182193]]. Action = [[ 0.0659464  -0.1607212  -0.05669977 -0.10576892]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 8196 is [True, False, False, False, True, False]
Human Feedback received at timestep 8196 of 1
Current timestep = 8197. State = [[-0.11688509 -0.11266057]]. Action = [[ 0.10134378 -0.04025914 -0.08452334 -0.26039773]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 8197 is [True, False, False, False, True, False]
Current timestep = 8198. State = [[-0.11603131 -0.11355006]]. Action = [[-0.04283324 -0.17644894 -0.13620035 -0.47435594]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 8198 is [True, False, False, False, True, False]
Current timestep = 8199. State = [[-0.11569326 -0.11611934]]. Action = [[ 0.03794515  0.11911532 -0.22100636 -0.5345943 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 8199 is [True, False, False, False, True, False]
Current timestep = 8200. State = [[-0.11512835 -0.11637555]]. Action = [[ 0.13636583  0.08094567 -0.16860183 -0.15745413]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 8200 is [True, False, False, False, True, False]
Human Feedback received at timestep 8200 of 1
Current timestep = 8201. State = [[-0.11339989 -0.1160773 ]]. Action = [[-0.01870316  0.07535779 -0.19567262 -0.62442327]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 8201 is [True, False, False, False, True, False]
Scene graph at timestep 8201 is [True, False, False, False, True, False]
State prediction error at timestep 8201 is tensor(3.6439e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8202. State = [[-0.11135285 -0.11560912]]. Action = [[ 0.1334033   0.04148966 -0.14218557 -0.00933897]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 8202 is [True, False, False, False, True, False]
Current timestep = 8203. State = [[-0.10865362 -0.1148169 ]]. Action = [[ 0.07183525  0.09546512 -0.19691527 -0.6143026 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 8203 is [True, False, False, False, True, False]
Human Feedback received at timestep 8203 of 1
Current timestep = 8204. State = [[-0.10565241 -0.11395922]]. Action = [[ 0.08728987 -0.03966966 -0.11541234 -0.80428106]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 8204 is [True, False, False, False, True, False]
Scene graph at timestep 8204 is [True, False, False, False, True, False]
State prediction error at timestep 8204 is tensor(3.6317e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8205. State = [[-0.10178831 -0.11383574]]. Action = [[-0.10478456 -0.10565044 -0.15184264 -0.71801955]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 8205 is [True, False, False, False, True, False]
Current timestep = 8206. State = [[-0.09911041 -0.11440964]]. Action = [[-0.00325695 -0.16134337 -0.17630412 -0.76610583]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 8206 is [True, False, False, False, True, False]
Human Feedback received at timestep 8206 of 1
Current timestep = 8207. State = [[-0.09791461 -0.11575855]]. Action = [[ 0.11965367  0.07119441 -0.12360181  0.43003178]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 8207 is [True, False, False, False, True, False]
Current timestep = 8208. State = [[-0.0972704  -0.11597929]]. Action = [[ 0.05815428  0.1579324  -0.18699755 -0.55711776]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 8208 is [True, False, False, False, True, False]
Human Feedback received at timestep 8208 of 1
Current timestep = 8209. State = [[-0.09714738 -0.1144589 ]]. Action = [[ 0.08623856  0.09001869 -0.17838942 -0.46750504]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 8209 is [True, False, False, False, True, False]
Current timestep = 8210. State = [[-0.09640785 -0.11259945]]. Action = [[ 4.0188432e-04 -6.1756551e-02 -1.3148439e-01 -4.4777107e-01]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 8210 is [True, False, False, False, True, False]
Current timestep = 8211. State = [[-0.09588768 -0.11243819]]. Action = [[ 0.01417047 -0.09748971 -0.09967136 -0.19107473]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 8211 is [True, False, False, False, True, False]
Scene graph at timestep 8211 is [True, False, False, False, True, False]
State prediction error at timestep 8211 is tensor(5.3934e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8211 of 1
Current timestep = 8212. State = [[-0.0949566  -0.11281259]]. Action = [[ 0.02924836 -0.05904086 -0.12288135 -0.90552026]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 8212 is [True, False, False, False, True, False]
Scene graph at timestep 8212 is [True, False, False, False, True, False]
State prediction error at timestep 8212 is tensor(2.8290e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8213. State = [[-0.09437896 -0.11327922]]. Action = [[ 0.02063975  0.03645009 -0.19152033  0.22481501]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 8213 is [True, False, False, False, True, False]
Current timestep = 8214. State = [[-0.09392197 -0.11331457]]. Action = [[-0.04059118 -0.00640374 -0.17032865  0.02585685]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 8214 is [True, False, False, False, True, False]
Human Feedback received at timestep 8214 of 1
Current timestep = 8215. State = [[-0.09403623 -0.11337137]]. Action = [[-0.02170581 -0.14778386 -0.10709292 -0.5261256 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 8215 is [True, False, False, False, True, False]
Scene graph at timestep 8215 is [True, False, False, False, True, False]
State prediction error at timestep 8215 is tensor(4.6899e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8216. State = [[-0.09392183 -0.11476553]]. Action = [[-0.01201813  0.00552469 -0.18391354 -0.45494092]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 8216 is [True, False, False, False, True, False]
Scene graph at timestep 8216 is [True, False, False, False, True, False]
State prediction error at timestep 8216 is tensor(2.4769e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8217. State = [[-0.0939232  -0.11515204]]. Action = [[-0.0023586   0.1833683  -0.16711497 -0.3507902 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 8217 is [True, False, False, False, True, False]
Current timestep = 8218. State = [[-0.09395551 -0.11453728]]. Action = [[-0.00192244 -0.04872206 -0.22180258 -0.3154359 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 8218 is [True, False, False, False, True, False]
Human Feedback received at timestep 8218 of 1
Current timestep = 8219. State = [[-0.09397862 -0.11459325]]. Action = [[ 0.01516286 -0.04977697 -0.09217954 -0.47571945]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 8219 is [True, False, False, False, True, False]
Scene graph at timestep 8219 is [True, False, False, False, True, False]
State prediction error at timestep 8219 is tensor(2.7222e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8220. State = [[-0.09397501 -0.11465617]]. Action = [[ 0.01334044  0.08944088 -0.22841205 -0.6186015 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 8220 is [True, False, False, False, True, False]
Current timestep = 8221. State = [[-0.09391098 -0.1142623 ]]. Action = [[ 0.07764611  0.13874748 -0.17861074 -0.90055674]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 8221 is [True, False, False, False, True, False]
Current timestep = 8222. State = [[-0.09368703 -0.11256269]]. Action = [[-0.02468075  0.00910908 -0.14920703  0.02632356]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 8222 is [True, False, False, False, True, False]
Human Feedback received at timestep 8222 of 1
Current timestep = 8223. State = [[-0.09343781 -0.11161114]]. Action = [[ 0.03445932 -0.1249032  -0.0508339  -0.02180457]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 8223 is [True, False, False, False, True, False]
Current timestep = 8224. State = [[-0.09266647 -0.11217567]]. Action = [[-0.00447476  0.00050887 -0.16227752  0.14303064]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 8224 is [True, False, False, False, True, False]
Scene graph at timestep 8224 is [True, False, False, False, True, False]
State prediction error at timestep 8224 is tensor(2.2984e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8225. State = [[-0.09242655 -0.11232578]]. Action = [[-0.02512883  0.09197217 -0.12596071 -0.6258163 ]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 8225 is [True, False, False, False, True, False]
Current timestep = 8226. State = [[-0.09195234 -0.11171853]]. Action = [[ 0.08922225 -0.17242727 -0.11570582  0.00589001]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 8226 is [True, False, False, False, True, False]
Human Feedback received at timestep 8226 of 1
Current timestep = 8227. State = [[-0.09084477 -0.11269584]]. Action = [[-0.02687182 -0.06487353 -0.17016217  0.15281343]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 8227 is [True, False, False, False, True, False]
Current timestep = 8228. State = [[-0.08968314 -0.11350657]]. Action = [[-0.00263572  0.10771781 -0.14832747 -0.27995056]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 8228 is [True, False, False, False, True, False]
Current timestep = 8229. State = [[-0.08913759 -0.11356103]]. Action = [[-0.06622688 -0.08011582 -0.15235977 -0.47858906]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 8229 is [True, False, False, False, True, False]
Scene graph at timestep 8229 is [True, False, False, False, True, False]
State prediction error at timestep 8229 is tensor(1.6736e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8229 of 1
Current timestep = 8230. State = [[-0.08859382 -0.11402338]]. Action = [[-0.01042728  0.13721818 -0.118994   -0.3737644 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 8230 is [True, False, False, False, True, False]
Current timestep = 8231. State = [[-0.08816623 -0.11391349]]. Action = [[-0.15799935 -0.09730625 -0.04580763 -0.7002965 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 8231 is [True, False, False, False, True, False]
Scene graph at timestep 8231 is [True, False, False, False, True, False]
State prediction error at timestep 8231 is tensor(7.0690e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8232. State = [[-0.08856194 -0.11413539]]. Action = [[ 0.00464234  0.05794597 -0.04359227 -0.03334022]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 8232 is [True, False, False, False, True, False]
Scene graph at timestep 8232 is [True, False, False, False, True, False]
State prediction error at timestep 8232 is tensor(6.0900e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8232 of 1
Current timestep = 8233. State = [[-0.08883354 -0.11392802]]. Action = [[-0.01915231  0.14575022 -0.14947796 -0.71041965]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 8233 is [True, False, False, False, True, False]
Current timestep = 8234. State = [[-0.08883751 -0.11297123]]. Action = [[ 0.1120429  -0.0778071  -0.07487541  0.1744299 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 8234 is [True, False, False, False, True, False]
Current timestep = 8235. State = [[-0.08886158 -0.11298468]]. Action = [[ 0.08694148  0.03867608 -0.05602184 -0.6895748 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 8235 is [True, False, False, False, True, False]
Human Feedback received at timestep 8235 of 1
Current timestep = 8236. State = [[-0.08849698 -0.11246567]]. Action = [[ 0.00117484 -0.09408368 -0.17496336  0.0618453 ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 8236 is [True, False, False, False, True, False]
Current timestep = 8237. State = [[-0.08813674 -0.11263607]]. Action = [[-0.06416225 -0.01317155 -0.16518328 -0.4236288 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 8237 is [True, False, False, False, True, False]
Human Feedback received at timestep 8237 of 1
Current timestep = 8238. State = [[-0.08807112 -0.11290722]]. Action = [[-0.13422751  0.08520323 -0.00516599  0.06027472]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 8238 is [True, False, False, False, True, False]
Current timestep = 8239. State = [[-0.0880345  -0.11285077]]. Action = [[-0.09373578 -0.02275831 -0.20216177 -0.62854195]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 8239 is [True, False, False, False, True, False]
Scene graph at timestep 8239 is [True, False, False, False, True, False]
State prediction error at timestep 8239 is tensor(6.9704e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8240. State = [[-0.08829993 -0.11279145]]. Action = [[ 0.12363794  0.04635552 -0.12149881 -0.14924133]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 8240 is [True, False, False, False, True, False]
Scene graph at timestep 8240 is [True, False, False, False, True, False]
State prediction error at timestep 8240 is tensor(9.6286e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8240 of 1
Current timestep = 8241. State = [[-0.08837278 -0.11253629]]. Action = [[-0.0026969  -0.01333097 -0.15884033 -0.09916043]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 8241 is [True, False, False, False, True, False]
Current timestep = 8242. State = [[-0.08856444 -0.11242501]]. Action = [[0.03923497 0.0747751  0.01709437 0.11752629]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 8242 is [True, False, False, False, True, False]
Current timestep = 8243. State = [[-0.08848874 -0.11189727]]. Action = [[ 0.08023521 -0.04443228 -0.19933309 -0.50495267]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 8243 is [True, False, False, False, True, False]
Current timestep = 8244. State = [[-0.08841918 -0.11162831]]. Action = [[ 0.09106934  0.08061311 -0.02795145 -0.5733703 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 8244 is [True, False, False, False, True, False]
Current timestep = 8245. State = [[-0.08770107 -0.11087433]]. Action = [[ 0.07424223 -0.03881517 -0.20196928 -0.5671247 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 8245 is [True, False, False, False, True, False]
Current timestep = 8246. State = [[-0.08658092 -0.11070046]]. Action = [[ 0.04349044 -0.09994859 -0.18385637  0.06746101]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 8246 is [True, False, False, False, True, False]
Current timestep = 8247. State = [[-0.08477452 -0.11084662]]. Action = [[ 0.07030571  0.19050446 -0.16867189 -0.2865162 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 8247 is [True, False, False, False, True, False]
Current timestep = 8248. State = [[-0.08292779 -0.10971956]]. Action = [[ 0.10862347  0.14140862 -0.18896832 -0.6772662 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 8248 is [True, False, False, False, True, False]
Current timestep = 8249. State = [[-0.08035307 -0.10761464]]. Action = [[ 0.12863332 -0.03552905 -0.17218235 -0.13337922]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 8249 is [True, False, False, False, True, False]
Human Feedback received at timestep 8249 of 1
Current timestep = 8250. State = [[-0.07647228 -0.10669985]]. Action = [[-0.09557465  0.21016997 -0.1734473  -0.6473899 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 8250 is [True, False, False, False, True, False]
Current timestep = 8251. State = [[-0.07502636 -0.10326254]]. Action = [[ 0.02498055  0.16430855 -0.12474971 -0.36298954]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 8251 is [True, False, False, False, True, False]
Human Feedback received at timestep 8251 of 1
Current timestep = 8252. State = [[-0.07453211 -0.09867516]]. Action = [[ 0.03426665  0.04588443 -0.15817554 -0.26692772]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 8252 is [True, False, False, False, True, False]
Current timestep = 8253. State = [[-0.07405447 -0.0951646 ]]. Action = [[ 0.05164915  0.02194938 -0.1611893  -0.45356488]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 8253 is [True, False, False, False, True, False]
Human Feedback received at timestep 8253 of 1
Current timestep = 8254. State = [[-0.07322083 -0.09279642]]. Action = [[ 0.07956302 -0.06432512 -0.175013   -0.45151496]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 8254 is [True, False, False, False, True, False]
Human Feedback received at timestep 8254 of 1
Current timestep = 8255. State = [[-0.07147547 -0.09233073]]. Action = [[ 0.12319797  0.07565483 -0.08639339 -0.26142418]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 8255 is [True, False, False, False, True, False]
Scene graph at timestep 8255 is [True, False, False, False, True, False]
State prediction error at timestep 8255 is tensor(1.3320e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8256. State = [[-0.06789795 -0.09102964]]. Action = [[ 0.1032798   0.03120473 -0.10915893 -0.1311642 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 8256 is [True, False, False, False, True, False]
Scene graph at timestep 8256 is [True, False, False, False, True, False]
State prediction error at timestep 8256 is tensor(3.3532e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8256 of 1
Current timestep = 8257. State = [[-0.06424081 -0.08973152]]. Action = [[ 0.13464344  0.12903333 -0.11290869 -0.27678752]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 8257 is [True, False, False, False, True, False]
Current timestep = 8258. State = [[-0.0605029 -0.0873699]]. Action = [[ 0.05993435 -0.05608866 -0.16635168 -0.42554116]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 8258 is [True, False, False, False, True, False]
Human Feedback received at timestep 8258 of 1
Current timestep = 8259. State = [[-0.05669512 -0.08613256]]. Action = [[ 0.09526861  0.07469532 -0.13022411 -0.20040607]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 8259 is [True, False, False, False, True, False]
Current timestep = 8260. State = [[-0.0519974  -0.08370288]]. Action = [[ 0.09064308 -0.03872249 -0.10305513  0.11508191]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 8260 is [True, False, False, False, True, False]
Scene graph at timestep 8260 is [True, False, False, False, True, False]
State prediction error at timestep 8260 is tensor(8.5986e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8261. State = [[-0.04891109 -0.08295621]]. Action = [[ 0.1013141   0.09612525 -0.1128979  -0.41239452]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 8261 is [True, False, False, False, True, False]
Current timestep = 8262. State = [[-0.04628593 -0.08182632]]. Action = [[ 0.03335413 -0.17593722 -0.16405877 -0.5945067 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 8262 is [False, True, False, False, True, False]
Human Feedback received at timestep 8262 of 1
Current timestep = 8263. State = [[-0.04309599 -0.08243381]]. Action = [[-0.03895381  0.10124677 -0.13591765 -0.4224831 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 8263 is [False, True, False, False, True, False]
Current timestep = 8264. State = [[-0.04055788 -0.08204993]]. Action = [[ 0.05904609  0.00784487 -0.12991977 -0.51493824]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 8264 is [False, True, False, False, True, False]
Current timestep = 8265. State = [[-0.03952039 -0.08144554]]. Action = [[ 0.05413905  0.14850643 -0.15561625 -0.36237788]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 8265 is [False, True, False, False, True, False]
Human Feedback received at timestep 8265 of 1
Current timestep = 8266. State = [[-0.03940036 -0.07943811]]. Action = [[ 0.08977887  0.13807493 -0.12044334 -0.5400369 ]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 8266 is [False, True, False, False, True, False]
Scene graph at timestep 8266 is [False, True, False, False, True, False]
State prediction error at timestep 8266 is tensor(1.3392e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8267. State = [[-0.03868607 -0.07593429]]. Action = [[ 0.11151072  0.00949064 -0.13840745 -0.69318396]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 8267 is [False, True, False, False, True, False]
Current timestep = 8268. State = [[-0.03743201 -0.0739511 ]]. Action = [[ 0.03945869 -0.06800696 -0.09848849 -0.8100359 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 8268 is [False, True, False, False, True, False]
Human Feedback received at timestep 8268 of 1
Current timestep = 8269. State = [[-0.24382327 -0.08871547]]. Action = [[ 0.10583836 -0.12561299 -0.02958161 -0.3184603 ]]. Reward = [100.]
Curr episode timestep = 117
Scene graph at timestep 8269 is [False, True, False, False, True, False]
Current timestep = 8270. State = [[-0.24262041 -0.09424786]]. Action = [[ 0.07579926 -0.17296174 -0.13857846  0.19429553]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 8270 is [True, False, False, False, True, False]
Current timestep = 8271. State = [[-0.24156836 -0.09633206]]. Action = [[ 0.15944025  0.01312059 -0.14509681  0.08844733]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 8271 is [True, False, False, False, True, False]
Scene graph at timestep 8271 is [True, False, False, False, True, False]
State prediction error at timestep 8271 is tensor(4.8452e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8272. State = [[-0.23870033 -0.09781983]]. Action = [[ 0.08780727 -0.10767896 -0.13900931 -0.28518957]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 8272 is [True, False, False, False, True, False]
Scene graph at timestep 8272 is [True, False, False, False, True, False]
State prediction error at timestep 8272 is tensor(4.1632e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8273. State = [[-0.23520398 -0.10007454]]. Action = [[ 0.16913074 -0.12655544 -0.19304237  0.18475914]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 8273 is [True, False, False, False, True, False]
Current timestep = 8274. State = [[-0.23049432 -0.10332748]]. Action = [[ 0.13852227  0.03070486 -0.1473703  -0.04659462]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 8274 is [True, False, False, False, True, False]
Current timestep = 8275. State = [[-0.22700919 -0.10477941]]. Action = [[ 0.12755191 -0.0999604  -0.15204875 -0.28021634]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 8275 is [True, False, False, False, True, False]
Scene graph at timestep 8275 is [True, False, False, False, True, False]
State prediction error at timestep 8275 is tensor(1.0669e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8276. State = [[-0.22254121 -0.10701687]]. Action = [[ 0.07680836 -0.14339085 -0.16739467 -0.3584776 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 8276 is [True, False, False, False, True, False]
Current timestep = 8277. State = [[-0.21814093 -0.11046013]]. Action = [[ 0.08138624 -0.08247139 -0.13366723  0.4279995 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 8277 is [True, False, False, False, True, False]
Scene graph at timestep 8277 is [True, False, False, False, True, False]
State prediction error at timestep 8277 is tensor(1.4072e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8278. State = [[-0.21462981 -0.11394117]]. Action = [[ 0.1421234  -0.15599172 -0.14785422  0.09556937]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8278 is [True, False, False, False, True, False]
Current timestep = 8279. State = [[-0.21055   -0.1185314]]. Action = [[ 0.18167698 -0.14733237 -0.22039303 -0.19074422]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 8279 is [True, False, False, False, True, False]
Current timestep = 8280. State = [[-0.20580159 -0.12354484]]. Action = [[ 0.03560907 -0.04989465 -0.16676639 -0.0917033 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 8280 is [True, False, False, False, True, False]
Current timestep = 8281. State = [[-0.20219417 -0.12728557]]. Action = [[ 0.11060265 -0.04404931 -0.19158183 -0.16542065]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 8281 is [True, False, False, False, True, False]
Human Feedback received at timestep 8281 of -1
Current timestep = 8282. State = [[-0.19888484 -0.13003278]]. Action = [[ 0.12625986 -0.11084101 -0.14770077  0.03450465]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 8282 is [True, False, False, True, False, False]
Current timestep = 8283. State = [[-0.19474842 -0.13285077]]. Action = [[ 0.14138567  0.01120108 -0.13490862  0.07185757]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 8283 is [True, False, False, True, False, False]
Current timestep = 8284. State = [[-0.18993789 -0.13470091]]. Action = [[ 0.07476124 -0.0963845  -0.14875877 -0.17602754]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 8284 is [True, False, False, True, False, False]
Current timestep = 8285. State = [[-0.18604288 -0.13698024]]. Action = [[ 0.17297843 -0.13544661 -0.21569453  0.580879  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 8285 is [True, False, False, True, False, False]
Human Feedback received at timestep 8285 of -1
Current timestep = 8286. State = [[-0.18136235 -0.14066614]]. Action = [[ 0.07663614 -0.14746115 -0.17292674 -0.21744406]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 8286 is [True, False, False, True, False, False]
Scene graph at timestep 8286 is [True, False, False, True, False, False]
State prediction error at timestep 8286 is tensor(1.9307e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8287. State = [[-0.17709276 -0.1453681 ]]. Action = [[-0.06893969 -0.1146287  -0.13174218 -0.6037027 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 8287 is [True, False, False, True, False, False]
Current timestep = 8288. State = [[-0.17521948 -0.1494704 ]]. Action = [[ 0.05764887 -0.1142717  -0.13865665 -0.72799265]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 8288 is [True, False, False, True, False, False]
Current timestep = 8289. State = [[-0.17419422 -0.1535917 ]]. Action = [[ 0.0017111  -0.09510681 -0.15479435 -0.70465565]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 8289 is [True, False, False, True, False, False]
Human Feedback received at timestep 8289 of -1
Current timestep = 8290. State = [[-0.17347068 -0.15758379]]. Action = [[ 0.00680155 -0.06424364 -0.01982458  0.15661597]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 8290 is [True, False, False, True, False, False]
Scene graph at timestep 8290 is [True, False, False, True, False, False]
State prediction error at timestep 8290 is tensor(8.5751e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8291. State = [[-0.17216454 -0.16125485]]. Action = [[ 0.00109905 -0.1647275  -0.10378228  0.87006044]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 8291 is [True, False, False, True, False, False]
Current timestep = 8292. State = [[-0.17115466 -0.16518204]]. Action = [[ 0.03790432  0.2065028  -0.05940601 -0.01295817]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 8292 is [True, False, False, True, False, False]
Scene graph at timestep 8292 is [True, False, False, True, False, False]
State prediction error at timestep 8292 is tensor(1.6488e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8293. State = [[-0.16988808 -0.16589881]]. Action = [[ 0.18683201 -0.15301926 -0.10155943  0.05069768]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 8293 is [True, False, False, True, False, False]
Human Feedback received at timestep 8293 of -1
Current timestep = 8294. State = [[-0.16704702 -0.16727746]]. Action = [[ 0.0892508   0.09425524 -0.18548654  0.01545298]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 8294 is [True, False, False, True, False, False]
Current timestep = 8295. State = [[-0.16360524 -0.16802117]]. Action = [[-0.03173244  0.1873573  -0.15706947  0.25213242]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 8295 is [True, False, False, True, False, False]
Scene graph at timestep 8295 is [True, False, False, True, False, False]
State prediction error at timestep 8295 is tensor(2.7275e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8296. State = [[-0.16179122 -0.1669485 ]]. Action = [[ 0.06768832  0.14628786 -0.21753459 -0.7458347 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 8296 is [True, False, False, True, False, False]
Scene graph at timestep 8296 is [True, False, False, True, False, False]
State prediction error at timestep 8296 is tensor(1.2053e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8297. State = [[-0.16049983 -0.16479005]]. Action = [[ 0.07098719 -0.18113057 -0.16787766 -0.79098785]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 8297 is [True, False, False, True, False, False]
Scene graph at timestep 8297 is [True, False, False, True, False, False]
State prediction error at timestep 8297 is tensor(5.6712e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8298. State = [[-0.15935944 -0.16489826]]. Action = [[-0.06256267  0.1116381  -0.13891314 -0.00941449]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 8298 is [True, False, False, True, False, False]
Scene graph at timestep 8298 is [True, False, False, True, False, False]
State prediction error at timestep 8298 is tensor(1.1059e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8298 of -1
Current timestep = 8299. State = [[-0.15920347 -0.16437504]]. Action = [[-0.14319761 -0.13904667 -0.15494597 -0.59530246]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 8299 is [True, False, False, True, False, False]
Current timestep = 8300. State = [[-0.15943156 -0.16460504]]. Action = [[ 0.0263873   0.01819518 -0.14463425  0.01983154]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 8300 is [True, False, False, True, False, False]
Current timestep = 8301. State = [[-0.15943156 -0.16460504]]. Action = [[ 0.07664129  0.03374824 -0.02494909 -0.25263178]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 8301 is [True, False, False, True, False, False]
Scene graph at timestep 8301 is [True, False, False, True, False, False]
State prediction error at timestep 8301 is tensor(2.4612e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8302. State = [[-0.15943463 -0.16471799]]. Action = [[ 0.00963375 -0.16867541 -0.07737826 -0.5902234 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 8302 is [True, False, False, True, False, False]
Current timestep = 8303. State = [[-0.15946157 -0.16582534]]. Action = [[ 0.01715904 -0.18279155 -0.09755456 -0.39183044]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 8303 is [True, False, False, True, False, False]
Current timestep = 8304. State = [[-0.15947098 -0.16880581]]. Action = [[-0.02676003 -0.08251137 -0.0869218  -0.9280091 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 8304 is [True, False, False, True, False, False]
Current timestep = 8305. State = [[-0.15968081 -0.17128311]]. Action = [[-0.05266364  0.05412951 -0.09060659  0.02416754]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 8305 is [True, False, False, True, False, False]
Current timestep = 8306. State = [[-0.15979837 -0.1722138 ]]. Action = [[ 0.13774258 -0.10920236 -0.12798803  0.05004036]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 8306 is [True, False, False, True, False, False]
Current timestep = 8307. State = [[-0.15915385 -0.17370676]]. Action = [[ 0.14211196  0.178361   -0.16304524 -0.63070744]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 8307 is [True, False, False, True, False, False]
Current timestep = 8308. State = [[-0.15779649 -0.17344825]]. Action = [[ 0.13523561  0.10446718 -0.15962836 -0.15101123]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 8308 is [True, False, False, True, False, False]
Scene graph at timestep 8308 is [True, False, False, True, False, False]
State prediction error at timestep 8308 is tensor(1.6704e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8309. State = [[-0.15579246 -0.17256309]]. Action = [[ 0.10370541  0.02344218 -0.00371079 -0.41455567]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 8309 is [True, False, False, True, False, False]
Current timestep = 8310. State = [[-0.1533907  -0.17212422]]. Action = [[-0.0056559   0.06242388 -0.15140453 -0.76294726]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 8310 is [True, False, False, True, False, False]
Current timestep = 8311. State = [[-0.1505334  -0.17167418]]. Action = [[ 0.04943815  0.06259632 -0.17672195  0.51731277]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 8311 is [True, False, False, True, False, False]
Human Feedback received at timestep 8311 of -1
Current timestep = 8312. State = [[-0.14729452 -0.17139332]]. Action = [[ 0.14064756 -0.19235623 -0.09577699  0.73929274]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 8312 is [True, False, False, True, False, False]
Current timestep = 8313. State = [[-0.14308049 -0.17279516]]. Action = [[-0.10845257 -0.07083669 -0.1581882  -0.26177073]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 8313 is [True, False, False, True, False, False]
Current timestep = 8314. State = [[-0.14048384 -0.17383915]]. Action = [[-0.0151462   0.05791509 -0.19904204 -0.58273894]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 8314 is [True, False, False, True, False, False]
Current timestep = 8315. State = [[-0.13866945 -0.1743365 ]]. Action = [[ 0.06148052 -0.10478178 -0.05356357 -0.40719724]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 8315 is [True, False, False, True, False, False]
Current timestep = 8316. State = [[-0.13733627 -0.17525977]]. Action = [[-0.02447751 -0.23455967 -0.1431404  -0.11686516]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 8316 is [True, False, False, True, False, False]
Current timestep = 8317. State = [[-0.13765383 -0.17819563]]. Action = [[-0.05745122  0.18087792 -0.1287435  -0.7339466 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 8317 is [True, False, False, True, False, False]
Current timestep = 8318. State = [[-0.13760595 -0.17841832]]. Action = [[ 0.0149453  -0.17489874 -0.14956795 -0.09349328]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 8318 is [True, False, False, True, False, False]
Current timestep = 8319. State = [[-0.13798444 -0.18012655]]. Action = [[-0.05631331  0.10124642 -0.1172069   0.15569592]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 8319 is [True, False, False, True, False, False]
Current timestep = 8320. State = [[-0.13805567 -0.18037273]]. Action = [[ 0.10540134 -0.09174022 -0.11639078  0.23918223]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 8320 is [True, False, False, True, False, False]
Current timestep = 8321. State = [[-0.13814506 -0.180846  ]]. Action = [[-0.02635427 -0.05303155 -0.14058404 -0.5688691 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 8321 is [True, False, False, True, False, False]
Scene graph at timestep 8321 is [True, False, False, True, False, False]
State prediction error at timestep 8321 is tensor(4.9664e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8322. State = [[-0.13840736 -0.18174991]]. Action = [[-0.13407676  0.00373948 -0.19340856 -0.5615292 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 8322 is [True, False, False, True, False, False]
Scene graph at timestep 8322 is [True, False, False, True, False, False]
State prediction error at timestep 8322 is tensor(4.5608e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8323. State = [[-0.13891138 -0.18302642]]. Action = [[-0.04367134 -0.19153702 -0.16808821 -0.56343   ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 8323 is [True, False, False, True, False, False]
Current timestep = 8324. State = [[-0.13946961 -0.18577322]]. Action = [[ 0.0036113   0.16872019 -0.09681058  0.33946967]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 8324 is [True, False, False, True, False, False]
Scene graph at timestep 8324 is [True, False, False, True, False, False]
State prediction error at timestep 8324 is tensor(3.5438e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8325. State = [[-0.13969402 -0.18600763]]. Action = [[-0.06139532 -0.00424056 -0.09189938 -0.61195207]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 8325 is [True, False, False, True, False, False]
Scene graph at timestep 8325 is [True, False, False, True, False, False]
State prediction error at timestep 8325 is tensor(3.7672e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8326. State = [[-0.13977979 -0.18623552]]. Action = [[ 0.05289984  0.01770535 -0.11479431 -0.7287925 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 8326 is [True, False, False, True, False, False]
Current timestep = 8327. State = [[-0.1397666  -0.18644379]]. Action = [[-0.00298527 -0.22010711  0.04195461 -0.28015703]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 8327 is [True, False, False, True, False, False]
Current timestep = 8328. State = [[-0.14014366 -0.18840687]]. Action = [[-0.02008907  0.23182023 -0.17268509 -0.84851277]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 8328 is [True, False, False, True, False, False]
Current timestep = 8329. State = [[-0.14029838 -0.18821184]]. Action = [[ 0.05229881 -0.01707272 -0.14897674 -0.68423146]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 8329 is [True, False, False, True, False, False]
Current timestep = 8330. State = [[-0.14027795 -0.18824825]]. Action = [[-0.15585029 -0.18328303 -0.15036821  0.2009579 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 8330 is [True, False, False, True, False, False]
Scene graph at timestep 8330 is [True, False, False, True, False, False]
State prediction error at timestep 8330 is tensor(1.5790e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8331. State = [[-0.14080566 -0.18978146]]. Action = [[-0.02923903  0.07255524 -0.12924786 -0.78996927]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 8331 is [True, False, False, True, False, False]
Current timestep = 8332. State = [[-0.14099842 -0.19032137]]. Action = [[-0.09425803 -0.03140692 -0.20980553 -0.8968571 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 8332 is [True, False, False, True, False, False]
Scene graph at timestep 8332 is [True, False, False, True, False, False]
State prediction error at timestep 8332 is tensor(1.7106e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8332 of -1
Current timestep = 8333. State = [[-0.14143035 -0.19124395]]. Action = [[ 0.03590438 -0.05726412 -0.13927108 -0.8624967 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 8333 is [True, False, False, True, False, False]
Current timestep = 8334. State = [[-0.14192054 -0.19234572]]. Action = [[-0.09880461  0.02994528 -0.15798216 -0.03542471]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 8334 is [True, False, False, True, False, False]
Scene graph at timestep 8334 is [True, False, False, True, False, False]
State prediction error at timestep 8334 is tensor(5.1569e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8335. State = [[-0.14249793 -0.19327231]]. Action = [[-0.13578284  0.10168678 -0.1358716  -0.28221583]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 8335 is [True, False, False, True, False, False]
Current timestep = 8336. State = [[-0.14360037 -0.1934148 ]]. Action = [[-0.0936662   0.06493053 -0.10238266 -0.38390934]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 8336 is [True, False, False, True, False, False]
Current timestep = 8337. State = [[-0.14503138 -0.19295484]]. Action = [[-0.18606839  0.1055392  -0.21142589 -0.9120715 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 8337 is [True, False, False, True, False, False]
Current timestep = 8338. State = [[-0.14722897 -0.19209209]]. Action = [[-1.21455476e-01  7.33196735e-04 -1.09160870e-01 -8.67788196e-01]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 8338 is [True, False, False, True, False, False]
Current timestep = 8339. State = [[-0.14978914 -0.19178408]]. Action = [[-0.06692362 -0.07185015 -0.13632008 -0.29200542]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 8339 is [True, False, False, True, False, False]
Scene graph at timestep 8339 is [True, False, False, True, False, False]
State prediction error at timestep 8339 is tensor(9.5034e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8340. State = [[-0.15183835 -0.19295214]]. Action = [[-0.0288271  -0.13764918 -0.00991869 -0.6100642 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 8340 is [True, False, False, True, False, False]
Current timestep = 8341. State = [[-0.15374213 -0.19524388]]. Action = [[-0.08766921 -0.18660764 -0.13465865  0.29477334]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 8341 is [True, False, False, True, False, False]
Current timestep = 8342. State = [[-0.15591396 -0.19848579]]. Action = [[-0.12880965  0.03239268 -0.19890225  0.02792549]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 8342 is [True, False, False, True, False, False]
Current timestep = 8343. State = [[-0.15868649 -0.20039767]]. Action = [[-0.1225861   0.23450124 -0.16381258 -0.38730907]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 8343 is [True, False, False, True, False, False]
Human Feedback received at timestep 8343 of -1
Current timestep = 8344. State = [[-0.16220836 -0.19884792]]. Action = [[-0.05897918  0.14635533 -0.13676096  0.04225147]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 8344 is [True, False, False, True, False, False]
Scene graph at timestep 8344 is [True, False, False, True, False, False]
State prediction error at timestep 8344 is tensor(1.2663e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8345. State = [[-0.16652745 -0.19663359]]. Action = [[-0.08468288 -0.23103578 -0.10675606 -0.7982082 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 8345 is [True, False, False, True, False, False]
Human Feedback received at timestep 8345 of -1
Current timestep = 8346. State = [[-0.17021887 -0.19755375]]. Action = [[-0.07125244 -0.16237867 -0.15710016 -0.87391496]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 8346 is [True, False, False, True, False, False]
Human Feedback received at timestep 8346 of -1
Current timestep = 8347. State = [[-0.1734503  -0.19988258]]. Action = [[-0.11746189  0.2237925  -0.16776474  0.19707656]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 8347 is [True, False, False, True, False, False]
Current timestep = 8348. State = [[-0.17650878 -0.19918035]]. Action = [[-0.00424255  0.0956766  -0.21698351 -0.5756057 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 8348 is [True, False, False, True, False, False]
Scene graph at timestep 8348 is [True, False, False, True, False, False]
State prediction error at timestep 8348 is tensor(1.8885e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8349. State = [[-0.17882428 -0.19782697]]. Action = [[-0.08711147  0.07459545 -0.21300414  0.43530297]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 8349 is [True, False, False, True, False, False]
Human Feedback received at timestep 8349 of -1
Current timestep = 8350. State = [[-0.18084995 -0.1963601 ]]. Action = [[ 0.03746846  0.22514564 -0.07126372 -0.2459321 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 8350 is [True, False, False, True, False, False]
Human Feedback received at timestep 8350 of -1
Current timestep = 8351. State = [[-0.18180941 -0.1925138 ]]. Action = [[-0.11334243  0.05737856 -0.14264114 -0.8462048 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 8351 is [True, False, False, True, False, False]
Human Feedback received at timestep 8351 of -1
Current timestep = 8352. State = [[-0.18345775 -0.18912251]]. Action = [[-0.11560464 -0.22177772 -0.2174594  -0.55199486]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 8352 is [True, False, False, True, False, False]
Current timestep = 8353. State = [[-0.18644498 -0.18897925]]. Action = [[-0.17337903  0.05449823 -0.08896983  0.13914037]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 8353 is [True, False, False, True, False, False]
Human Feedback received at timestep 8353 of -1
Current timestep = 8354. State = [[-0.1907764  -0.18818896]]. Action = [[ 0.0034948  -0.02822898  0.04496104 -0.74791694]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 8354 is [True, False, False, True, False, False]
Scene graph at timestep 8354 is [True, False, False, True, False, False]
State prediction error at timestep 8354 is tensor(4.1094e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8354 of -1
Current timestep = 8355. State = [[-0.19323123 -0.18768051]]. Action = [[-0.04029647  0.08303261 -0.02957886 -0.7032627 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 8355 is [True, False, False, True, False, False]
Current timestep = 8356. State = [[-0.19579066 -0.18700504]]. Action = [[ 0.02975592 -0.2427884  -0.07556081 -0.8146081 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 8356 is [True, False, False, True, False, False]
Current timestep = 8357. State = [[-0.1971352  -0.18816079]]. Action = [[-0.02089548  0.18657553 -0.16061518 -0.8258054 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 8357 is [True, False, False, True, False, False]
Scene graph at timestep 8357 is [True, False, False, True, False, False]
State prediction error at timestep 8357 is tensor(1.7524e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8357 of -1
Current timestep = 8358. State = [[-0.19905095 -0.18747497]]. Action = [[ 0.02287212 -0.17751595 -0.05351405 -0.78458697]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 8358 is [True, False, False, True, False, False]
Human Feedback received at timestep 8358 of -1
Current timestep = 8359. State = [[-0.20026521 -0.18812232]]. Action = [[ 0.10523272 -0.05496588 -0.18431826  0.49833083]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 8359 is [True, False, False, True, False, False]
Scene graph at timestep 8359 is [True, False, False, True, False, False]
State prediction error at timestep 8359 is tensor(2.3097e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8360. State = [[-0.20045587 -0.18881893]]. Action = [[-0.03909495 -0.1328372  -0.16599452 -0.7376405 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 8360 is [True, False, False, True, False, False]
Scene graph at timestep 8360 is [True, False, False, True, False, False]
State prediction error at timestep 8360 is tensor(2.9656e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8360 of -1
Current timestep = 8361. State = [[-0.20093459 -0.19081944]]. Action = [[ 0.07711858  0.21484321 -0.11767632 -0.00937057]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 8361 is [True, False, False, True, False, False]
Scene graph at timestep 8361 is [True, False, False, True, False, False]
State prediction error at timestep 8361 is tensor(4.8744e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8362. State = [[-0.20088546 -0.19050317]]. Action = [[-0.00803857  0.21296003  0.02813056 -0.5490643 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 8362 is [True, False, False, True, False, False]
Human Feedback received at timestep 8362 of -1
Current timestep = 8363. State = [[-0.20062055 -0.18830454]]. Action = [[-0.13464814  0.04391789 -0.17977248  0.9124595 ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 8363 is [True, False, False, True, False, False]
Scene graph at timestep 8363 is [True, False, False, True, False, False]
State prediction error at timestep 8363 is tensor(3.0828e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8364. State = [[-0.20131382 -0.18693876]]. Action = [[-0.0739557  -0.09562278 -0.05272716 -0.6437376 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 8364 is [True, False, False, True, False, False]
Current timestep = 8365. State = [[-0.20201269 -0.18677957]]. Action = [[ 0.0773029  -0.10714898 -0.05930074 -0.11154884]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 8365 is [True, False, False, True, False, False]
Human Feedback received at timestep 8365 of -1
Current timestep = 8366. State = [[-0.20201571 -0.18684763]]. Action = [[-0.07602674  0.02719665 -0.17586297  0.41727686]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 8366 is [True, False, False, True, False, False]
Current timestep = 8367. State = [[-0.20236184 -0.18703058]]. Action = [[ 0.06167454 -0.09523281 -0.11040972 -0.8877177 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 8367 is [True, False, False, True, False, False]
Human Feedback received at timestep 8367 of -1
Current timestep = 8368. State = [[-0.20258376 -0.18809555]]. Action = [[ 0.03576064 -0.13444525 -0.15887205  0.28651488]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 8368 is [True, False, False, True, False, False]
Human Feedback received at timestep 8368 of -1
Current timestep = 8369. State = [[-0.20305802 -0.18990582]]. Action = [[ 0.05257773 -0.1723692  -0.1906766   0.0758754 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 8369 is [True, False, False, True, False, False]
Scene graph at timestep 8369 is [True, False, False, True, False, False]
State prediction error at timestep 8369 is tensor(4.1594e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8370. State = [[-0.2039129  -0.19273603]]. Action = [[ 0.02401999  0.08239254 -0.04264878 -0.36711484]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 8370 is [True, False, False, True, False, False]
Scene graph at timestep 8370 is [True, False, False, True, False, False]
State prediction error at timestep 8370 is tensor(4.0000e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8371. State = [[-0.20431457 -0.19350937]]. Action = [[ 0.13025796  0.11084506 -0.10615556 -0.6350387 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 8371 is [True, False, False, True, False, False]
Human Feedback received at timestep 8371 of -1
Current timestep = 8372. State = [[-0.20418368 -0.19331516]]. Action = [[ 0.09595802 -0.03911547 -0.14504623 -0.3469165 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 8372 is [True, False, False, True, False, False]
Current timestep = 8373. State = [[-0.20377848 -0.19340858]]. Action = [[ 0.08926171  0.00737515 -0.10842064 -0.15066886]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 8373 is [True, False, False, True, False, False]
Human Feedback received at timestep 8373 of -1
Current timestep = 8374. State = [[-0.2029636  -0.19357194]]. Action = [[-0.06654593  0.08430615 -0.13857745  0.666705  ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 8374 is [True, False, False, True, False, False]
Scene graph at timestep 8374 is [True, False, False, True, False, False]
State prediction error at timestep 8374 is tensor(2.6048e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8375. State = [[-0.2025129  -0.19308743]]. Action = [[-0.04161459 -0.04847452 -0.1536239   0.5896958 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 8375 is [True, False, False, True, False, False]
Human Feedback received at timestep 8375 of -1
Current timestep = 8376. State = [[-0.20254107 -0.19309331]]. Action = [[ 0.07476601  0.10436293 -0.12693366 -0.31365538]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 8376 is [True, False, False, True, False, False]
Scene graph at timestep 8376 is [True, False, False, True, False, False]
State prediction error at timestep 8376 is tensor(7.5255e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8377. State = [[-0.20225088 -0.19253577]]. Action = [[ 0.00482994 -0.22414224 -0.06681979 -0.4609666 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 8377 is [True, False, False, True, False, False]
Current timestep = 8378. State = [[-0.20189904 -0.19295114]]. Action = [[-0.09947807 -0.07267535 -0.10956383 -0.5384601 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 8378 is [True, False, False, True, False, False]
Current timestep = 8379. State = [[-0.20189266 -0.19416061]]. Action = [[-0.08110315 -0.23411512 -0.16186611 -0.57040536]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 8379 is [True, False, False, True, False, False]
Current timestep = 8380. State = [[-0.2028384  -0.19836465]]. Action = [[-0.04881279 -0.16123393 -0.17982125 -0.5303219 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 8380 is [True, False, False, True, False, False]
Human Feedback received at timestep 8380 of -1
Current timestep = 8381. State = [[-0.20392175 -0.2028679 ]]. Action = [[ 0.05354774  0.10878646  0.0508759  -0.89414674]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 8381 is [True, False, False, True, False, False]
Current timestep = 8382. State = [[-0.20426305 -0.20438468]]. Action = [[-0.10109587 -0.16788551 -0.0222846  -0.76332885]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 8382 is [True, False, False, True, False, False]
Current timestep = 8383. State = [[-0.20493613 -0.20767103]]. Action = [[ 0.00849316  0.00068694 -0.18481092 -0.12373567]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 8383 is [True, False, False, True, False, False]
Human Feedback received at timestep 8383 of -1
Current timestep = 8384. State = [[-0.20516361 -0.20942311]]. Action = [[-0.1296285   0.17525935 -0.18161896 -0.43939483]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 8384 is [True, False, False, True, False, False]
Current timestep = 8385. State = [[-0.20528372 -0.20950837]]. Action = [[-0.0262574   0.10638517 -0.21037784  0.60591364]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 8385 is [True, False, False, True, False, False]
Human Feedback received at timestep 8385 of -1
Current timestep = 8386. State = [[-0.20539401 -0.20918879]]. Action = [[-0.06822085  0.22395962 -0.1566146   0.87439394]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 8386 is [True, False, False, True, False, False]
Human Feedback received at timestep 8386 of -1
Current timestep = 8387. State = [[-0.20632255 -0.20695102]]. Action = [[-0.05375481 -0.01332122 -0.06312695 -0.22122717]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 8387 is [True, False, False, True, False, False]
Scene graph at timestep 8387 is [True, False, False, True, False, False]
State prediction error at timestep 8387 is tensor(4.9146e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8388. State = [[-0.20719227 -0.20615353]]. Action = [[ 0.16954792  0.14272457 -0.1849402   0.21345448]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 8388 is [True, False, False, True, False, False]
Current timestep = 8389. State = [[-0.20655051 -0.20389117]]. Action = [[ 0.01617727 -0.22774233 -0.20338327  0.16546917]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 8389 is [True, False, False, True, False, False]
Current timestep = 8390. State = [[-0.20661154 -0.2041497 ]]. Action = [[-0.0550022  -0.15581751 -0.18065125 -0.41155082]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 8390 is [True, False, False, True, False, False]
Current timestep = 8391. State = [[-0.20677486 -0.20513482]]. Action = [[ 0.09241894  0.24682134 -0.14661188  0.04398322]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 8391 is [True, False, False, True, False, False]
Current timestep = 8392. State = [[-0.20651388 -0.20410565]]. Action = [[-0.16827258  0.16688722 -0.11738878  0.126603  ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 8392 is [True, False, False, True, False, False]
Current timestep = 8393. State = [[-0.20654558 -0.20232472]]. Action = [[-0.03905329  0.1308156  -0.10587347  0.05998087]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 8393 is [True, False, False, True, False, False]
Human Feedback received at timestep 8393 of -1
Current timestep = 8394. State = [[-0.2066508  -0.19995259]]. Action = [[-0.00980312 -0.13776377 -0.12229581 -0.7417676 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 8394 is [True, False, False, True, False, False]
Current timestep = 8395. State = [[-0.20668127 -0.19979763]]. Action = [[ 0.05966428  0.05112374 -0.07292293 -0.7614261 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 8395 is [True, False, False, True, False, False]
Current timestep = 8396. State = [[-0.20662138 -0.19912724]]. Action = [[ 0.1081993   0.01177293 -0.11644745 -0.32298344]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 8396 is [True, False, False, True, False, False]
Current timestep = 8397. State = [[-0.20624621 -0.19841565]]. Action = [[ 0.0412454   0.15286979 -0.1323662  -0.636815  ]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 8397 is [True, False, False, True, False, False]
Scene graph at timestep 8397 is [True, False, False, True, False, False]
State prediction error at timestep 8397 is tensor(3.2971e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8397 of -1
Current timestep = 8398. State = [[-0.20572278 -0.19594422]]. Action = [[ 0.11653161 -0.21280609 -0.181658    0.46751583]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 8398 is [True, False, False, True, False, False]
Scene graph at timestep 8398 is [True, False, False, True, False, False]
State prediction error at timestep 8398 is tensor(5.9858e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8399. State = [[-0.2055201  -0.19586875]]. Action = [[ 0.10245335 -0.1840314  -0.19014625  0.6370435 ]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 8399 is [True, False, False, True, False, False]
Current timestep = 8400. State = [[-0.20523901 -0.19687746]]. Action = [[ 0.00739253 -0.22999667 -0.15997708 -0.20637667]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 8400 is [True, False, False, True, False, False]
Scene graph at timestep 8400 is [True, False, False, True, False, False]
State prediction error at timestep 8400 is tensor(6.2770e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8400 of -1
Current timestep = 8401. State = [[-0.2045678  -0.20010814]]. Action = [[ 0.04546347 -0.22827561 -0.15350279 -0.55829746]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 8401 is [True, False, False, True, False, False]
Scene graph at timestep 8401 is [True, False, False, True, False, False]
State prediction error at timestep 8401 is tensor(2.0171e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8401 of -1
Current timestep = 8402. State = [[-0.2039636  -0.20522371]]. Action = [[ 0.0329676  -0.13903357 -0.09835523  0.12294805]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 8402 is [True, False, False, True, False, False]
Current timestep = 8403. State = [[-0.20364635 -0.20929766]]. Action = [[-0.00453585  0.10951748 -0.15773034 -0.5438037 ]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 8403 is [True, False, False, True, False, False]
Human Feedback received at timestep 8403 of -1
Current timestep = 8404. State = [[-0.20367564 -0.2106114 ]]. Action = [[-0.04617736  0.01712811 -0.16597521 -0.67277485]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 8404 is [True, False, False, True, False, False]
Scene graph at timestep 8404 is [True, False, False, True, False, False]
State prediction error at timestep 8404 is tensor(1.3546e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8405. State = [[-0.2035002  -0.21078983]]. Action = [[ 0.01867777  0.15836793 -0.07237823 -0.24765563]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 8405 is [True, False, False, True, False, False]
Human Feedback received at timestep 8405 of -1
Current timestep = 8406. State = [[-0.20339476 -0.2104727 ]]. Action = [[-0.03698546  0.10194975 -0.08340031 -0.730347  ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 8406 is [True, False, False, True, False, False]
Human Feedback received at timestep 8406 of -1
Current timestep = 8407. State = [[-0.20328094 -0.20989875]]. Action = [[ 0.03332004 -0.19194378 -0.04651305  0.74179614]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 8407 is [True, False, False, True, False, False]
Human Feedback received at timestep 8407 of -1
Current timestep = 8408. State = [[-0.20329994 -0.21035863]]. Action = [[-0.12990597 -0.18025233 -0.16859376 -0.834689  ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 8408 is [True, False, False, True, False, False]
Scene graph at timestep 8408 is [True, False, False, True, False, False]
State prediction error at timestep 8408 is tensor(6.1001e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8409. State = [[-0.20385978 -0.21303397]]. Action = [[-0.12229314  0.01677504  0.07658887 -0.3196807 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 8409 is [True, False, False, True, False, False]
Human Feedback received at timestep 8409 of -1
Current timestep = 8410. State = [[-0.20452043 -0.21467032]]. Action = [[ 0.05618912 -0.12783566 -0.09975207 -0.79844725]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 8410 is [True, False, False, True, False, False]
Current timestep = 8411. State = [[-0.20509537 -0.2165103 ]]. Action = [[ 0.08837938 -0.13940956 -0.16889222 -0.2545675 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 8411 is [True, False, False, True, False, False]
Scene graph at timestep 8411 is [True, False, False, True, False, False]
State prediction error at timestep 8411 is tensor(2.5327e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8412. State = [[-0.2051751  -0.21950907]]. Action = [[ 0.10948151  0.06269911 -0.1156674  -0.805318  ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 8412 is [True, False, False, True, False, False]
Current timestep = 8413. State = [[-0.20463833 -0.22030945]]. Action = [[ 0.08220011  0.06520197 -0.03201419 -0.43836027]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 8413 is [True, False, False, True, False, False]
Human Feedback received at timestep 8413 of -1
Current timestep = 8414. State = [[-0.20407255 -0.22036289]]. Action = [[ 0.04060322 -0.24003045 -0.1536285  -0.7220773 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 8414 is [True, False, False, True, False, False]
Current timestep = 8415. State = [[-0.20291214 -0.22321573]]. Action = [[-0.04488522  0.15592414 -0.16081753  0.3060844 ]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 8415 is [True, False, False, True, False, False]
Human Feedback received at timestep 8415 of -1
Current timestep = 8416. State = [[-0.20283353 -0.22331056]]. Action = [[-0.04576555  0.12606886 -0.15385415 -0.97496283]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 8416 is [True, False, False, True, False, False]
Current timestep = 8417. State = [[-0.2028128  -0.22326341]]. Action = [[-0.07510298  0.01745966 -0.15589644 -0.23016697]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 8417 is [True, False, False, True, False, False]
Current timestep = 8418. State = [[-0.20295452 -0.22304103]]. Action = [[-0.08055194  0.20329309 -0.07337299 -0.32972527]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 8418 is [True, False, False, True, False, False]
Human Feedback received at timestep 8418 of -1
Current timestep = 8419. State = [[-0.20274213 -0.22176267]]. Action = [[-0.09109937  0.15349436 -0.04448405 -0.9978287 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 8419 is [True, False, False, True, False, False]
Current timestep = 8420. State = [[-0.20259139 -0.21979305]]. Action = [[-0.1346075   0.09930521 -0.13449287  0.67432666]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 8420 is [True, False, False, True, False, False]
Human Feedback received at timestep 8420 of -1
Current timestep = 8421. State = [[-0.20362547 -0.21738863]]. Action = [[-0.09756176  0.17173141 -0.08198719 -0.58396065]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 8421 is [True, False, False, True, False, False]
Current timestep = 8422. State = [[-0.20458761 -0.21386564]]. Action = [[-0.1334887   0.16728812 -0.13111591 -0.95826125]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 8422 is [True, False, False, True, False, False]
Current timestep = 8423. State = [[-0.20601542 -0.20954733]]. Action = [[ 0.0364238  -0.16583164 -0.11802724  0.7174537 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 8423 is [True, False, False, True, False, False]
Current timestep = 8424. State = [[-0.20692605 -0.20882843]]. Action = [[-0.0591597  -0.09817192 -0.09222746 -0.6926711 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 8424 is [True, False, False, True, False, False]
Current timestep = 8425. State = [[-0.20756334 -0.20893854]]. Action = [[-0.08366215  0.16746965 -0.17957063 -0.59928733]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 8425 is [True, False, False, True, False, False]
Scene graph at timestep 8425 is [True, False, False, True, False, False]
State prediction error at timestep 8425 is tensor(7.4992e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8426. State = [[-0.20877998 -0.20781247]]. Action = [[ 0.01119983 -0.04698637 -0.18795924 -0.8805257 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 8426 is [True, False, False, True, False, False]
Scene graph at timestep 8426 is [True, False, False, True, False, False]
State prediction error at timestep 8426 is tensor(6.8531e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8426 of -1
Current timestep = 8427. State = [[-0.20935294 -0.20771062]]. Action = [[-0.1333978  -0.14601713 -0.14163883  0.57411146]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 8427 is [True, False, False, True, False, False]
Current timestep = 8428. State = [[-0.21101144 -0.20878692]]. Action = [[-0.10512123 -0.04470313 -0.1534434  -0.22857249]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 8428 is [True, False, False, True, False, False]
Current timestep = 8429. State = [[-0.21341044 -0.21044928]]. Action = [[-0.09165603 -0.0931574  -0.01192319 -0.87159306]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 8429 is [True, False, False, True, False, False]
Scene graph at timestep 8429 is [True, False, False, True, False, False]
State prediction error at timestep 8429 is tensor(8.4294e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8430. State = [[-0.21624288 -0.21290223]]. Action = [[ 0.06243804 -0.13935335 -0.03929216 -0.6667576 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 8430 is [True, False, False, True, False, False]
Scene graph at timestep 8430 is [True, False, False, True, False, False]
State prediction error at timestep 8430 is tensor(7.4807e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8430 of -1
Current timestep = 8431. State = [[-0.21856701 -0.21513584]]. Action = [[ 0.06409067  0.24768591 -0.09649111 -0.9757776 ]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 8431 is [True, False, False, True, False, False]
Human Feedback received at timestep 8431 of -1
Current timestep = 8432. State = [[-0.21908335 -0.21437432]]. Action = [[-0.10277827 -0.03877273 -0.0928928   0.33484387]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 8432 is [True, False, False, True, False, False]
Current timestep = 8433. State = [[-0.22014648 -0.21387383]]. Action = [[ 0.09603682 -0.08909394 -0.21914034 -0.89144677]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 8433 is [True, False, False, True, False, False]
Human Feedback received at timestep 8433 of -1
Current timestep = 8434. State = [[-0.22035794 -0.21397057]]. Action = [[-0.05070521 -0.04787542 -0.18457966 -0.6569477 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 8434 is [True, False, False, True, False, False]
Current timestep = 8435. State = [[-0.22076507 -0.21441555]]. Action = [[ 0.11384785  0.10725543 -0.11938664  0.6286762 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 8435 is [True, False, False, True, False, False]
Human Feedback received at timestep 8435 of -1
Current timestep = 8436. State = [[-0.22074515 -0.21426532]]. Action = [[-0.02341118 -0.08145279 -0.06399639 -0.9048144 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 8436 is [True, False, False, True, False, False]
Current timestep = 8437. State = [[-0.22083713 -0.21423022]]. Action = [[-0.11845224  0.1932267  -0.0944858  -0.7506574 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 8437 is [True, False, False, True, False, False]
Current timestep = 8438. State = [[-0.22149573 -0.21379708]]. Action = [[-0.02023277 -0.05853696 -0.13939224 -0.74244386]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 8438 is [True, False, False, True, False, False]
Current timestep = 8439. State = [[-0.22195809 -0.21328862]]. Action = [[ 0.03859612  0.23990881 -0.14251234 -0.43173385]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 8439 is [True, False, False, True, False, False]
Human Feedback received at timestep 8439 of -1
Current timestep = 8440. State = [[-0.22215256 -0.21068583]]. Action = [[-0.07770318  0.10964954 -0.17329983 -0.7734222 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 8440 is [True, False, False, True, False, False]
Current timestep = 8441. State = [[-0.2228278  -0.20752862]]. Action = [[-0.12811686  0.02596393 -0.1353945  -0.16773206]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 8441 is [True, False, False, True, False, False]
Current timestep = 8442. State = [[-0.22415209 -0.20539157]]. Action = [[ 0.03953102 -0.10578135 -0.16182932 -0.23553818]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 8442 is [True, False, False, True, False, False]
Current timestep = 8443. State = [[-0.22518077 -0.20480844]]. Action = [[ 0.02814829  0.09628388 -0.1810444   0.2620964 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 8443 is [True, False, False, True, False, False]
Current timestep = 8444. State = [[-0.22588858 -0.20398057]]. Action = [[ 0.02356881 -0.09841163 -0.20871386 -0.0711025 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 8444 is [True, False, False, True, False, False]
Current timestep = 8445. State = [[-0.22651015 -0.20359205]]. Action = [[ 0.06640545  0.13350838 -0.15055384 -0.18212539]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 8445 is [True, False, False, True, False, False]
Current timestep = 8446. State = [[-0.22657332 -0.20248157]]. Action = [[ 0.01652563 -0.08984964 -0.15181942  0.6507406 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 8446 is [True, False, False, True, False, False]
Human Feedback received at timestep 8446 of -1
Current timestep = 8447. State = [[-0.22677822 -0.2024115 ]]. Action = [[ 0.04221007 -0.21064627 -0.16706976  0.3116634 ]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 8447 is [True, False, False, True, False, False]
Current timestep = 8448. State = [[-0.22680026 -0.2029475 ]]. Action = [[ 0.04583919  0.11291546 -0.07669565  0.0883795 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 8448 is [True, False, False, True, False, False]
Current timestep = 8449. State = [[-0.22653984 -0.20312802]]. Action = [[ 0.06669098 -0.2111838  -0.21449557  0.8565904 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 8449 is [True, False, False, True, False, False]
Current timestep = 8450. State = [[-0.22638097 -0.20402293]]. Action = [[ 0.0896638  -0.18109742 -0.12229413 -0.48584652]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 8450 is [True, False, False, True, False, False]
Current timestep = 8451. State = [[-0.22612564 -0.20601173]]. Action = [[-0.03814107  0.16702157 -0.23614539  0.17303288]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 8451 is [True, False, False, True, False, False]
Scene graph at timestep 8451 is [True, False, False, True, False, False]
State prediction error at timestep 8451 is tensor(6.0206e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8452. State = [[-0.2258171  -0.20620394]]. Action = [[ 0.10854703 -0.23981853 -0.00267906  0.02234447]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 8452 is [True, False, False, True, False, False]
Current timestep = 8453. State = [[-0.2254476  -0.20773558]]. Action = [[ 0.06960166  0.22059748 -0.18728337  0.80056477]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 8453 is [True, False, False, True, False, False]
Current timestep = 8454. State = [[-0.22518314 -0.20733014]]. Action = [[ 0.08523595  0.23680115 -0.20110676 -0.78814787]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 8454 is [True, False, False, True, False, False]
Current timestep = 8455. State = [[-0.22435533 -0.20517592]]. Action = [[-0.05164845 -0.17584802 -0.12587562  0.47890663]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 8455 is [True, False, False, True, False, False]
Current timestep = 8456. State = [[-0.22406629 -0.20538199]]. Action = [[-0.08112966 -0.08464816 -0.02076994 -0.54734784]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 8456 is [True, False, False, True, False, False]
Current timestep = 8457. State = [[-0.22409113 -0.20570119]]. Action = [[ 0.14260298 -0.17136717 -0.15650596  0.10157669]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 8457 is [True, False, False, True, False, False]
Current timestep = 8458. State = [[-0.22365071 -0.2075044 ]]. Action = [[ 0.01401439 -0.17915277 -0.16053009 -0.9220211 ]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 8458 is [True, False, False, True, False, False]
Current timestep = 8459. State = [[-0.22335932 -0.21114276]]. Action = [[-0.06819499 -0.24477942 -0.15480307  0.05258954]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 8459 is [True, False, False, True, False, False]
Current timestep = 8460. State = [[-0.22375107 -0.21611197]]. Action = [[ 0.01397625  0.16777003 -0.11091338  0.60115445]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 8460 is [True, False, False, True, False, False]
Current timestep = 8461. State = [[-0.22380292 -0.21737571]]. Action = [[-0.00626485  0.19495255 -0.09056363 -0.7058566 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 8461 is [True, False, False, True, False, False]
Human Feedback received at timestep 8461 of -1
Current timestep = 8462. State = [[-0.2237553 -0.21701  ]]. Action = [[-0.06228022  0.16767025 -0.16257368 -0.97109115]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 8462 is [True, False, False, True, False, False]
Current timestep = 8463. State = [[-0.22343355 -0.2157296 ]]. Action = [[-0.07598367  0.21739244 -0.12205099  0.44123065]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 8463 is [True, False, False, True, False, False]
Scene graph at timestep 8463 is [True, False, False, True, False, False]
State prediction error at timestep 8463 is tensor(4.2617e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8463 of -1
Current timestep = 8464. State = [[-0.22294389 -0.21276712]]. Action = [[0.02886736 0.05973521 0.01030478 0.36689484]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 8464 is [True, False, False, True, False, False]
Scene graph at timestep 8464 is [True, False, False, True, False, False]
State prediction error at timestep 8464 is tensor(4.9561e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8465. State = [[-0.22244819 -0.21038097]]. Action = [[-0.08916929 -0.19181086 -0.2004731  -0.9150048 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 8465 is [True, False, False, True, False, False]
Human Feedback received at timestep 8465 of -1
Current timestep = 8466. State = [[-0.22250496 -0.21065076]]. Action = [[-0.00057155 -0.01083076 -0.21734846  0.33789396]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 8466 is [True, False, False, True, False, False]
Current timestep = 8467. State = [[-0.22252937 -0.21065307]]. Action = [[-0.06791666  0.23037785 -0.08833683  0.9306437 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 8467 is [True, False, False, True, False, False]
Current timestep = 8468. State = [[-0.22302447 -0.20935121]]. Action = [[-0.09996344  0.01721659 -0.11799814 -0.05707294]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 8468 is [True, False, False, True, False, False]
Current timestep = 8469. State = [[-0.22396435 -0.20834613]]. Action = [[-0.02461156  0.16402483  0.0191941   0.8594847 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 8469 is [True, False, False, True, False, False]
Current timestep = 8470. State = [[-0.22476545 -0.20593956]]. Action = [[-0.01113632 -0.03211311 -0.05816615  0.47813714]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 8470 is [True, False, False, True, False, False]
Current timestep = 8471. State = [[-0.22530334 -0.20488548]]. Action = [[ 0.01526684 -0.09653303 -0.1809087   0.04315054]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 8471 is [True, False, False, True, False, False]
Scene graph at timestep 8471 is [True, False, False, True, False, False]
State prediction error at timestep 8471 is tensor(1.9966e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8472. State = [[-0.22537804 -0.20465158]]. Action = [[ 0.02348703  0.16734001 -0.1728072  -0.683466  ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 8472 is [True, False, False, True, False, False]
Current timestep = 8473. State = [[-0.22521494 -0.20349097]]. Action = [[ 0.04337573 -0.17681693 -0.1584064   0.6518234 ]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 8473 is [True, False, False, True, False, False]
Scene graph at timestep 8473 is [True, False, False, True, False, False]
State prediction error at timestep 8473 is tensor(2.3344e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8474. State = [[-0.22523125 -0.20330065]]. Action = [[-0.09383921  0.22054738 -0.19343516  0.82110035]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 8474 is [True, False, False, True, False, False]
Current timestep = 8475. State = [[-0.22522861 -0.20153636]]. Action = [[ 0.09998119  0.16786212 -0.05358845  0.5091324 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 8475 is [True, False, False, True, False, False]
Current timestep = 8476. State = [[-0.224823   -0.19756275]]. Action = [[ 0.0322839   0.1916281  -0.03736746 -0.59734136]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 8476 is [True, False, False, True, False, False]
Scene graph at timestep 8476 is [True, False, False, True, False, False]
State prediction error at timestep 8476 is tensor(1.6077e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8477. State = [[-0.22430912 -0.19229144]]. Action = [[ 0.0924575  -0.07757151 -0.14150241 -0.611318  ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 8477 is [True, False, False, True, False, False]
Current timestep = 8478. State = [[-0.22410212 -0.18976694]]. Action = [[-0.12615064  0.23040238 -0.11819011 -0.7540482 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 8478 is [True, False, False, True, False, False]
Current timestep = 8479. State = [[-0.22382122 -0.18582566]]. Action = [[ 0.01236966 -0.2186959  -0.20956089 -0.26686132]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 8479 is [True, False, False, True, False, False]
Scene graph at timestep 8479 is [True, False, False, True, False, False]
State prediction error at timestep 8479 is tensor(1.2819e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8479 of -1
Current timestep = 8480. State = [[-0.22389884 -0.18555905]]. Action = [[ 0.01682931 -0.08836523 -0.17341661 -0.4660859 ]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 8480 is [True, False, False, True, False, False]
Current timestep = 8481. State = [[-0.22393177 -0.18571278]]. Action = [[-0.14727433 -0.11702827 -0.00952668 -0.20297402]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 8481 is [True, False, False, True, False, False]
Current timestep = 8482. State = [[-0.22478099 -0.18730225]]. Action = [[ 0.044918   -0.07683936 -0.17277494  0.02052104]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 8482 is [True, False, False, True, False, False]
Scene graph at timestep 8482 is [True, False, False, True, False, False]
State prediction error at timestep 8482 is tensor(2.3311e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8483. State = [[-0.22535506 -0.18847217]]. Action = [[ 0.04406238  0.12387976 -0.13148461  0.5122974 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 8483 is [True, False, False, True, False, False]
Current timestep = 8484. State = [[-0.2253175  -0.18842034]]. Action = [[ 0.06247297 -0.11152788 -0.2225308  -0.46440524]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 8484 is [True, False, False, True, False, False]
Current timestep = 8485. State = [[-0.22527899 -0.18864086]]. Action = [[ 0.20471507 -0.04329498  0.02401599 -0.53057975]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 8485 is [True, False, False, True, False, False]
Current timestep = 8486. State = [[-0.22523418 -0.18878353]]. Action = [[-0.08508393 -0.07119586 -0.05297062 -0.7920857 ]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 8486 is [True, False, False, True, False, False]
Current timestep = 8487. State = [[-0.2251701  -0.18953203]]. Action = [[ 0.07391679 -0.18262123 -0.21346535  0.45826113]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 8487 is [True, False, False, True, False, False]
Scene graph at timestep 8487 is [True, False, False, True, False, False]
State prediction error at timestep 8487 is tensor(2.2673e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8488. State = [[-0.22503333 -0.19223896]]. Action = [[ 0.01854652 -0.08592135 -0.17054103 -0.53183585]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 8488 is [True, False, False, True, False, False]
Current timestep = 8489. State = [[-0.22495489 -0.19491144]]. Action = [[ 0.03685862 -0.18509412 -0.07909459  0.22355878]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 8489 is [True, False, False, True, False, False]
Current timestep = 8490. State = [[-0.22459808 -0.19890974]]. Action = [[ 0.08382553 -0.19118243 -0.08353075 -0.10585195]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 8490 is [True, False, False, True, False, False]
Current timestep = 8491. State = [[-0.22434613 -0.20451042]]. Action = [[ 0.0610013  -0.24526387 -0.16931772 -0.5399451 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 8491 is [True, False, False, True, False, False]
Current timestep = 8492. State = [[-0.22365567 -0.21058206]]. Action = [[ 0.04946607 -0.1576597  -0.20010781 -0.8209513 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 8492 is [True, False, False, True, False, False]
Current timestep = 8493. State = [[-0.22304119 -0.21649332]]. Action = [[-0.03137745  0.1334002  -0.21818446 -0.52662164]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 8493 is [True, False, False, True, False, False]
Current timestep = 8494. State = [[-0.22273745 -0.2186371 ]]. Action = [[-0.01457623  0.10119998 -0.19380715 -0.4741202 ]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 8494 is [True, False, False, True, False, False]
Current timestep = 8495. State = [[-0.22244133 -0.21885751]]. Action = [[-0.02554004  0.00459862 -0.23934214 -0.4892618 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 8495 is [True, False, False, True, False, False]
Scene graph at timestep 8495 is [True, False, False, True, False, False]
State prediction error at timestep 8495 is tensor(2.1969e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8496. State = [[-0.22243993 -0.21895301]]. Action = [[ 0.00352246  0.05267707 -0.15674756 -0.5420386 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 8496 is [True, False, False, True, False, False]
Human Feedback received at timestep 8496 of -1
Current timestep = 8497. State = [[-0.2222553  -0.21892284]]. Action = [[ 0.05559495 -0.15274367 -0.1946319  -0.23327875]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 8497 is [True, False, False, True, False, False]
Scene graph at timestep 8497 is [True, False, False, True, False, False]
State prediction error at timestep 8497 is tensor(1.0816e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8498. State = [[-0.22185138 -0.21955015]]. Action = [[ 0.11740303  0.23319316 -0.05463608  0.78279805]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 8498 is [True, False, False, True, False, False]
Current timestep = 8499. State = [[-0.22071813 -0.21825282]]. Action = [[-0.02598867  0.08860454 -0.06734934 -0.93674845]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 8499 is [True, False, False, True, False, False]
Scene graph at timestep 8499 is [True, False, False, True, False, False]
State prediction error at timestep 8499 is tensor(4.3286e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8499 of -1
Current timestep = 8500. State = [[-0.21993852 -0.21688664]]. Action = [[ 0.10614389 -0.01710534 -0.16498217 -0.92080784]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 8500 is [True, False, False, True, False, False]
Human Feedback received at timestep 8500 of -1
Current timestep = 8501. State = [[-0.21888964 -0.21617001]]. Action = [[ 0.07060987  0.2050299  -0.18448044 -0.6371152 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 8501 is [True, False, False, True, False, False]
Scene graph at timestep 8501 is [True, False, False, True, False, False]
State prediction error at timestep 8501 is tensor(2.5352e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8502. State = [[-0.21728995 -0.21377206]]. Action = [[-0.12295306  0.01457384 -0.13722518  0.52736926]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 8502 is [True, False, False, True, False, False]
Current timestep = 8503. State = [[-0.21719226 -0.21308158]]. Action = [[-0.07646388 -0.18580458 -0.04946919  0.32279587]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 8503 is [True, False, False, True, False, False]
Scene graph at timestep 8503 is [True, False, False, True, False, False]
State prediction error at timestep 8503 is tensor(2.4429e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8504. State = [[-0.21720532 -0.21318263]]. Action = [[-0.04780458 -0.00773959  0.0402194  -0.93525714]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 8504 is [True, False, False, True, False, False]
Scene graph at timestep 8504 is [True, False, False, True, False, False]
State prediction error at timestep 8504 is tensor(2.5564e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8505. State = [[-0.21724696 -0.21338886]]. Action = [[-0.17096135  0.12551999 -0.1436466  -0.7500925 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 8505 is [True, False, False, True, False, False]
Current timestep = 8506. State = [[-0.21726865 -0.21333997]]. Action = [[-0.045958    0.05522108  0.05893692 -0.3780275 ]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 8506 is [True, False, False, True, False, False]
Scene graph at timestep 8506 is [True, False, False, True, False, False]
State prediction error at timestep 8506 is tensor(1.8472e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8507. State = [[-0.2172924  -0.21322942]]. Action = [[-0.09593648 -0.00250629 -0.12677623 -0.6271303 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 8507 is [True, False, False, True, False, False]
Current timestep = 8508. State = [[-0.2179825  -0.21280485]]. Action = [[-0.01661029  0.1894919  -0.00315326 -0.33504385]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 8508 is [True, False, False, True, False, False]
Scene graph at timestep 8508 is [True, False, False, True, False, False]
State prediction error at timestep 8508 is tensor(5.7764e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8508 of 1
Current timestep = 8509. State = [[-0.21866369 -0.21089213]]. Action = [[-0.16431184  0.02093828 -0.20845117 -0.6472528 ]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 8509 is [True, False, False, True, False, False]
Current timestep = 8510. State = [[-0.22010526 -0.20984076]]. Action = [[-0.08712757 -0.11043993 -0.2107558   0.01763582]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 8510 is [True, False, False, True, False, False]
Current timestep = 8511. State = [[-0.22253324 -0.20954108]]. Action = [[-0.19030872 -0.15386677 -0.20521703  0.8528378 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 8511 is [True, False, False, True, False, False]
Current timestep = 8512. State = [[-0.22531165 -0.21135154]]. Action = [[-0.0608874   0.1981794  -0.06964511 -0.09089357]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 8512 is [True, False, False, True, False, False]
Scene graph at timestep 8512 is [True, False, False, True, False, False]
State prediction error at timestep 8512 is tensor(1.3098e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8513. State = [[-0.22705685 -0.21058102]]. Action = [[-0.12454084 -0.02877502 -0.23156025 -0.45888698]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 8513 is [True, False, False, True, False, False]
Current timestep = 8514. State = [[-0.22880954 -0.2106179 ]]. Action = [[ 0.02191696  0.01435104 -0.09334838  0.8079939 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 8514 is [True, False, False, True, False, False]
Scene graph at timestep 8514 is [True, False, False, True, False, False]
State prediction error at timestep 8514 is tensor(4.6688e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8515. State = [[-0.22996216 -0.2103406 ]]. Action = [[ 0.04166165  0.09759906 -0.13971788 -0.9602977 ]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 8515 is [True, False, False, True, False, False]
Human Feedback received at timestep 8515 of -1
Current timestep = 8516. State = [[-0.23066908 -0.20964536]]. Action = [[-0.0681684  -0.09745422 -0.13457921 -0.19763315]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 8516 is [True, False, False, True, False, False]
Current timestep = 8517. State = [[-0.23138297 -0.20967315]]. Action = [[-0.03818315  0.1066387  -0.03233609 -0.70402074]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 8517 is [True, False, False, True, False, False]
Scene graph at timestep 8517 is [True, False, False, True, False, False]
State prediction error at timestep 8517 is tensor(3.0091e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8517 of -1
Current timestep = 8518. State = [[-0.2321981  -0.20897631]]. Action = [[-0.13835506  0.12948006 -0.18225852 -0.8117891 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 8518 is [True, False, False, True, False, False]
Current timestep = 8519. State = [[-0.23405461 -0.2077555 ]]. Action = [[-0.05822952 -0.07962441 -0.06533718  0.3015343 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 8519 is [True, False, False, True, False, False]
Current timestep = 8520. State = [[-0.23608206 -0.2074852 ]]. Action = [[-0.12725447  0.24078298  0.0257307   0.9610131 ]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 8520 is [True, False, False, True, False, False]
Current timestep = 8521. State = [[-0.23866433 -0.2049021 ]]. Action = [[ 0.01312432 -0.02411701 -0.08734283 -0.86425334]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 8521 is [True, False, False, True, False, False]
Current timestep = 8522. State = [[-0.24119627 -0.20275953]]. Action = [[-0.09981315  0.11855808 -0.09189092  0.5040555 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 8522 is [True, False, False, True, False, False]
Scene graph at timestep 8522 is [True, False, False, True, False, False]
State prediction error at timestep 8522 is tensor(2.9346e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8523. State = [[-0.24561664 -0.19981395]]. Action = [[-0.0332323  -0.21009661  0.0143646   0.03595066]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 8523 is [True, False, False, True, False, False]
Scene graph at timestep 8523 is [True, False, False, True, False, False]
State prediction error at timestep 8523 is tensor(1.0089e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8523 of -1
Current timestep = 8524. State = [[-0.2495711  -0.19873412]]. Action = [[ 0.05385873  0.16218892 -0.03227295 -0.48590386]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 8524 is [True, False, False, True, False, False]
Scene graph at timestep 8524 is [True, False, False, True, False, False]
State prediction error at timestep 8524 is tensor(1.2319e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8525. State = [[-0.25139427 -0.19762976]]. Action = [[ 0.01714149 -0.1962505  -0.11233026 -0.41867626]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 8525 is [True, False, False, True, False, False]
Human Feedback received at timestep 8525 of -1
Current timestep = 8526. State = [[-0.2513995  -0.19796298]]. Action = [[ 0.16217938 -0.12395597 -0.1495165   0.57300544]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 8526 is [True, False, False, True, False, False]
Current timestep = 8527. State = [[-0.25145826 -0.19854574]]. Action = [[ 0.02868003 -0.2313903  -0.18330543 -0.5511673 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 8527 is [True, False, False, True, False, False]
Current timestep = 8528. State = [[-0.2517638  -0.20116538]]. Action = [[ 0.09702259  0.20618886 -0.12658848  0.7590028 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 8528 is [True, False, False, True, False, False]
Scene graph at timestep 8528 is [True, False, False, True, False, False]
State prediction error at timestep 8528 is tensor(3.7215e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8528 of -1
Current timestep = 8529. State = [[-0.25168565 -0.20094503]]. Action = [[-0.03212777  0.20282936 -0.13645984 -0.9637935 ]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 8529 is [True, False, False, True, False, False]
Scene graph at timestep 8529 is [True, False, False, True, False, False]
State prediction error at timestep 8529 is tensor(4.8586e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8530. State = [[-0.25142992 -0.19974738]]. Action = [[ 0.00178605  0.09412676 -0.09275323  0.19959831]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 8530 is [True, False, False, True, False, False]
Human Feedback received at timestep 8530 of -1
Current timestep = 8531. State = [[-0.25104353 -0.19820152]]. Action = [[ 8.8837385e-02  4.7190905e-02 -5.1976740e-04 -6.6101372e-01]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 8531 is [True, False, False, True, False, False]
Current timestep = 8532. State = [[-0.2504161  -0.19681852]]. Action = [[ 0.08358735 -0.202464   -0.1856337   0.45285785]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 8532 is [True, False, False, True, False, False]
Human Feedback received at timestep 8532 of -1
Current timestep = 8533. State = [[-0.25000688 -0.19694978]]. Action = [[ 0.17469472  0.07068163 -0.21957213 -0.28219962]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 8533 is [True, False, False, True, False, False]
Current timestep = 8534. State = [[-0.24847066 -0.19638616]]. Action = [[ 0.03699219 -0.16776963 -0.10189644  0.34312522]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 8534 is [True, False, False, True, False, False]
Current timestep = 8535. State = [[-0.24738045 -0.1966388 ]]. Action = [[ 0.11934757  0.1989283  -0.00322193 -0.2913472 ]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 8535 is [True, False, False, True, False, False]
Current timestep = 8536. State = [[-0.24577935 -0.19554898]]. Action = [[-0.03279345 -0.15278558 -0.167573   -0.6092641 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 8536 is [True, False, False, True, False, False]
Current timestep = 8537. State = [[-0.24487911 -0.19588974]]. Action = [[-0.05048114  0.09469897 -0.00692204  0.29882097]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 8537 is [True, False, False, True, False, False]
Human Feedback received at timestep 8537 of 1
Current timestep = 8538. State = [[-0.24432908 -0.19582266]]. Action = [[ 0.10652614 -0.23803838 -0.2071228   0.25392044]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 8538 is [True, False, False, True, False, False]
Current timestep = 8539. State = [[-0.24299707 -0.19684494]]. Action = [[ 0.04146668  0.04979819 -0.19049346 -0.13458472]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 8539 is [True, False, False, True, False, False]
Human Feedback received at timestep 8539 of 1
Current timestep = 8540. State = [[-0.24137816 -0.19748798]]. Action = [[ 0.04863244  0.1074588  -0.0097606  -0.51132494]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 8540 is [True, False, False, True, False, False]
Current timestep = 8541. State = [[-0.24037763 -0.19744746]]. Action = [[-0.09125867  0.06976718 -0.1215046  -0.2991693 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 8541 is [True, False, False, True, False, False]
Human Feedback received at timestep 8541 of 1
Current timestep = 8542. State = [[-0.24016747 -0.19688222]]. Action = [[ 0.00169325  0.09189278 -0.03711188 -0.24599123]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 8542 is [True, False, False, True, False, False]
Current timestep = 8543. State = [[-0.23989266 -0.19616635]]. Action = [[-0.09324545  0.11059368  0.00694314 -0.71676815]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 8543 is [True, False, False, True, False, False]
Human Feedback received at timestep 8543 of 1
Current timestep = 8544. State = [[-0.23974314 -0.19485904]]. Action = [[ 0.01270923  0.0293034  -0.17947038  0.22027469]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 8544 is [True, False, False, True, False, False]
Scene graph at timestep 8544 is [True, False, False, True, False, False]
State prediction error at timestep 8544 is tensor(1.9518e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8545. State = [[-0.23947126 -0.19317323]]. Action = [[ 0.09126431  0.1905474  -0.22027928 -0.27176166]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 8545 is [True, False, False, True, False, False]
Scene graph at timestep 8545 is [True, False, False, True, False, False]
State prediction error at timestep 8545 is tensor(2.7127e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8545 of 1
Current timestep = 8546. State = [[-0.23880984 -0.18966767]]. Action = [[ 0.11436254 -0.14056168 -0.11274482  0.18937743]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 8546 is [True, False, False, True, False, False]
Scene graph at timestep 8546 is [True, False, False, True, False, False]
State prediction error at timestep 8546 is tensor(5.3622e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8547. State = [[-0.23836319 -0.18847847]]. Action = [[-0.02318203  0.1912525  -0.17077538  0.21992218]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 8547 is [True, False, False, True, False, False]
Scene graph at timestep 8547 is [True, False, False, True, False, False]
State prediction error at timestep 8547 is tensor(7.5809e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8547 of 1
Current timestep = 8548. State = [[-0.23780644 -0.18566372]]. Action = [[-0.04172391 -0.1842194  -0.00929515  0.95062697]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 8548 is [True, False, False, True, False, False]
Current timestep = 8549. State = [[-0.23776661 -0.18561096]]. Action = [[ 0.09892988  0.1490038  -0.13213684 -0.49091935]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 8549 is [True, False, False, True, False, False]
Human Feedback received at timestep 8549 of 1
Current timestep = 8550. State = [[-0.23751546 -0.1845231 ]]. Action = [[-0.03599544 -0.1472201  -0.17996581  0.25379753]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 8550 is [True, False, False, True, False, False]
Scene graph at timestep 8550 is [True, False, False, True, False, False]
State prediction error at timestep 8550 is tensor(1.5919e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8551. State = [[-0.23739824 -0.18447554]]. Action = [[ 0.10442847 -0.0696445  -0.07761325  0.49516463]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 8551 is [True, False, False, True, False, False]
Human Feedback received at timestep 8551 of 1
Current timestep = 8552. State = [[-0.23626283 -0.18495022]]. Action = [[ 0.05657828 -0.14531085 -0.10354492 -0.6038083 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 8552 is [True, False, False, True, False, False]
Current timestep = 8553. State = [[-0.23473358 -0.18641244]]. Action = [[-0.14956601  0.09294468 -0.21978946 -0.3199414 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 8553 is [True, False, False, True, False, False]
Current timestep = 8554. State = [[-0.23467979 -0.18663314]]. Action = [[-0.05601215 -0.12525035 -0.1944791  -0.30489135]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 8554 is [True, False, False, True, False, False]
Human Feedback received at timestep 8554 of 1
Current timestep = 8555. State = [[-0.2348178  -0.18790406]]. Action = [[ 0.0375188 -0.1795581 -0.170579  -0.8004929]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 8555 is [True, False, False, True, False, False]
Current timestep = 8556. State = [[-0.23507878 -0.19025038]]. Action = [[0.05199176 0.07745439 0.0397841  0.27310753]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 8556 is [True, False, False, True, False, False]
Human Feedback received at timestep 8556 of 1
Current timestep = 8557. State = [[-0.23492564 -0.19065791]]. Action = [[ 0.05333525 -0.08373511 -0.09457585 -0.7321255 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 8557 is [True, False, False, True, False, False]
Current timestep = 8558. State = [[-0.23434953 -0.19180408]]. Action = [[-0.00847454 -0.2419696  -0.203911   -0.03466594]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 8558 is [True, False, False, True, False, False]
Current timestep = 8559. State = [[-0.23410058 -0.19569704]]. Action = [[-0.16364715 -0.00135292 -0.19662766 -0.22555137]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 8559 is [True, False, False, True, False, False]
Scene graph at timestep 8559 is [True, False, False, True, False, False]
State prediction error at timestep 8559 is tensor(6.3165e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8560. State = [[-0.23455773 -0.19910021]]. Action = [[ 0.00709799 -0.10081315 -0.19214599 -0.8438452 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 8560 is [True, False, False, True, False, False]
Scene graph at timestep 8560 is [True, False, False, True, False, False]
State prediction error at timestep 8560 is tensor(4.8002e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8561. State = [[-0.23499192 -0.201976  ]]. Action = [[-0.16373435 -0.19027151 -0.090462    0.02920938]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 8561 is [True, False, False, True, False, False]
Current timestep = 8562. State = [[-0.23611164 -0.20687436]]. Action = [[-0.16298938 -0.11829162 -0.14271967  0.39364076]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 8562 is [True, False, False, True, False, False]
Scene graph at timestep 8562 is [True, False, False, True, False, False]
State prediction error at timestep 8562 is tensor(1.5584e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8562 of -1
Current timestep = 8563. State = [[-0.23752278 -0.21204019]]. Action = [[-0.1551397   0.1928336  -0.20158187  0.8335614 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 8563 is [True, False, False, True, False, False]
Current timestep = 8564. State = [[-0.23980874 -0.21313371]]. Action = [[ 0.04290041  0.02678707 -0.12131509 -0.8394244 ]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 8564 is [True, False, False, True, False, False]
Current timestep = 8565. State = [[-0.24069825 -0.21405503]]. Action = [[-0.10242185 -0.21475177 -0.06385306 -0.05822641]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 8565 is [True, False, False, True, False, False]
Human Feedback received at timestep 8565 of -1
Current timestep = 8566. State = [[-0.24270114 -0.21684833]]. Action = [[ 0.02719879  0.24089992 -0.03111947  0.7677777 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 8566 is [True, False, False, True, False, False]
Scene graph at timestep 8566 is [True, False, False, True, False, False]
State prediction error at timestep 8566 is tensor(5.9745e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8566 of -1
Current timestep = 8567. State = [[-0.24340785 -0.21643679]]. Action = [[ 0.08587039  0.17273855 -0.08639953  0.44211006]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 8567 is [True, False, False, True, False, False]
Human Feedback received at timestep 8567 of -1
Current timestep = 8568. State = [[-0.24318622 -0.21504924]]. Action = [[ 0.05509576  0.07196939 -0.22631207 -0.7751351 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 8568 is [True, False, False, True, False, False]
Scene graph at timestep 8568 is [True, False, False, True, False, False]
State prediction error at timestep 8568 is tensor(1.9771e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8569. State = [[-0.24277404 -0.21340841]]. Action = [[-0.09819092  0.07998037 -0.03900385 -0.20497805]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 8569 is [True, False, False, True, False, False]
Current timestep = 8570. State = [[-0.24306218 -0.21153568]]. Action = [[-0.12008595  0.24526757 -0.15805148 -0.49637997]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 8570 is [True, False, False, True, False, False]
Current timestep = 8571. State = [[-0.2446599  -0.20811546]]. Action = [[-0.19284913 -0.08293256  0.05581245  0.6205983 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 8571 is [True, False, False, True, False, False]
Human Feedback received at timestep 8571 of -1
Current timestep = 8572. State = [[-0.24709177 -0.20663089]]. Action = [[ 0.01660427  0.01164982 -0.13294013 -0.55030864]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 8572 is [True, False, False, True, False, False]
Current timestep = 8573. State = [[-0.2485099  -0.20593925]]. Action = [[-0.00714302 -0.24689788 -0.21159214  0.2759881 ]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 8573 is [True, False, False, True, False, False]
Scene graph at timestep 8573 is [True, False, False, True, False, False]
State prediction error at timestep 8573 is tensor(1.0690e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8573 of -1
Current timestep = 8574. State = [[-0.24981563 -0.20687376]]. Action = [[-0.1511911  -0.15492113 -0.13497669  0.0793848 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 8574 is [True, False, False, True, False, False]
Current timestep = 8575. State = [[-0.25206468 -0.2094208 ]]. Action = [[ 0.14249817 -0.1456691  -0.1284287  -0.7251013 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 8575 is [True, False, False, True, False, False]
Current timestep = 8576. State = [[-0.25298846 -0.21146257]]. Action = [[ 0.08204117  0.01326418 -0.12904619  0.6093776 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 8576 is [True, False, False, True, False, False]
Current timestep = 8577. State = [[-0.25332022 -0.21195999]]. Action = [[-0.05116273  0.07242316  0.06352475  0.36240506]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 8577 is [True, False, False, True, False, False]
Scene graph at timestep 8577 is [True, False, False, True, False, False]
State prediction error at timestep 8577 is tensor(9.9946e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8577 of 1
Current timestep = 8578. State = [[-0.25331494 -0.21180485]]. Action = [[-0.12133595  0.21894509 -0.12169307 -0.6793274 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 8578 is [True, False, False, True, False, False]
Scene graph at timestep 8578 is [True, False, False, True, False, False]
State prediction error at timestep 8578 is tensor(7.1226e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8579. State = [[-0.25421786 -0.21122237]]. Action = [[ 0.14359426 -0.19242245 -0.1048041  -0.6523459 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 8579 is [True, False, False, True, False, False]
Scene graph at timestep 8579 is [True, False, False, True, False, False]
State prediction error at timestep 8579 is tensor(4.5214e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8580. State = [[-0.25420797 -0.21133175]]. Action = [[ 0.09793425 -0.05691619 -0.21833839 -0.12042975]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 8580 is [True, False, False, True, False, False]
Human Feedback received at timestep 8580 of -1
Current timestep = 8581. State = [[-0.25421867 -0.211703  ]]. Action = [[-0.05403376 -0.21030886 -0.06611535  0.55409396]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 8581 is [True, False, False, True, False, False]
Current timestep = 8582. State = [[-0.2544239  -0.21386983]]. Action = [[-0.00580262  0.07185599 -0.02502792  0.46633852]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 8582 is [True, False, False, True, False, False]
Current timestep = 8583. State = [[-0.25447428 -0.21429129]]. Action = [[ 0.01388696  0.00919667 -0.1671818   0.47874534]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 8583 is [True, False, False, True, False, False]
Current timestep = 8584. State = [[-0.2545827  -0.21466853]]. Action = [[-0.1280445  -0.06193632 -0.15942933 -0.8522396 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 8584 is [True, False, False, True, False, False]
Scene graph at timestep 8584 is [True, False, False, True, False, False]
State prediction error at timestep 8584 is tensor(1.3140e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8585. State = [[-0.25535512 -0.21628337]]. Action = [[-0.11885917 -0.21682489 -0.01539908 -0.55018866]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 8585 is [True, False, False, True, False, False]
Scene graph at timestep 8585 is [True, False, False, True, False, False]
State prediction error at timestep 8585 is tensor(5.1626e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8586. State = [[-0.25750792 -0.22098574]]. Action = [[-0.03390919 -0.24797246  0.11558676  0.29221177]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 8586 is [True, False, False, True, False, False]
Current timestep = 8587. State = [[-0.2600667  -0.22661847]]. Action = [[-0.08127189  0.00354028  0.04595026 -0.8661147 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 8587 is [True, False, False, True, False, False]
Current timestep = 8588. State = [[-0.26174128 -0.22990176]]. Action = [[-0.20953861  0.05274919 -0.12836967  0.77199113]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 8588 is [True, False, False, True, False, False]
Current timestep = 8589. State = [[-0.26469415 -0.23256926]]. Action = [[-0.19597526 -0.24454083 -0.10875233 -0.10725111]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 8589 is [True, False, False, True, False, False]
Scene graph at timestep 8589 is [True, False, False, True, False, False]
State prediction error at timestep 8589 is tensor(9.2430e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8589 of -1
Current timestep = 8590. State = [[-0.26952812 -0.23835613]]. Action = [[-0.02296256 -0.24316835  0.15207916  0.8095684 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 8590 is [True, False, False, True, False, False]
Human Feedback received at timestep 8590 of -1
Current timestep = 8591. State = [[-0.27301395 -0.24424736]]. Action = [[ 0.11262062 -0.12507665  0.07739902  0.09383738]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 8591 is [True, False, False, True, False, False]
Current timestep = 8592. State = [[-0.27539968 -0.2495372 ]]. Action = [[-2.2051005e-01  6.1661005e-04 -1.5268430e-02 -9.4307047e-01]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 8592 is [True, False, False, True, False, False]
Current timestep = 8593. State = [[-0.27668375 -0.2522969 ]]. Action = [[ 0.07209411  0.08293408 -0.22477104 -0.79954225]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 8593 is [True, False, False, True, False, False]
Human Feedback received at timestep 8593 of -1
Current timestep = 8594. State = [[-0.27643675 -0.25281325]]. Action = [[-0.20109597 -0.21467376 -0.1715887   0.8313652 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 8594 is [True, False, False, True, False, False]
Human Feedback received at timestep 8594 of -1
Current timestep = 8595. State = [[-0.27631325 -0.25305703]]. Action = [[-0.16431093  0.23780262 -0.07933471 -0.6796373 ]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 8595 is [True, False, False, True, False, False]
Human Feedback received at timestep 8595 of -1
Current timestep = 8596. State = [[-0.27615505 -0.25311434]]. Action = [[ 0.06966114  0.20766014 -0.02418259 -0.9053826 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 8596 is [True, False, False, True, False, False]
Current timestep = 8597. State = [[-0.27593282 -0.2527057 ]]. Action = [[-0.08451885  0.13963291 -0.15010281 -0.7594913 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 8597 is [True, False, False, True, False, False]
Current timestep = 8598. State = [[-0.2756694  -0.25234562]]. Action = [[-0.06430702  0.02054563 -0.15812987 -0.9233073 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 8598 is [True, False, False, True, False, False]
Current timestep = 8599. State = [[-0.2755613  -0.25221056]]. Action = [[-0.21113388 -0.01567654  0.11707461  0.856079  ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 8599 is [True, False, False, True, False, False]
Current timestep = 8600. State = [[-0.2754891 -0.2521203]]. Action = [[-0.08777352 -0.05774985 -0.16828169 -0.74441683]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 8600 is [True, False, False, True, False, False]
Scene graph at timestep 8600 is [True, False, False, True, False, False]
State prediction error at timestep 8600 is tensor(4.5529e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8601. State = [[-0.275453   -0.25207514]]. Action = [[-0.0542509   0.23755497  0.06347242  0.95674086]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 8601 is [True, False, False, True, False, False]
Scene graph at timestep 8601 is [True, False, False, True, False, False]
State prediction error at timestep 8601 is tensor(2.4993e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8602. State = [[-0.275453   -0.25207514]]. Action = [[-0.07632646  0.15691313 -0.13091601  0.17856872]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 8602 is [True, False, False, True, False, False]
Scene graph at timestep 8602 is [True, False, False, True, False, False]
State prediction error at timestep 8602 is tensor(2.1076e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8603. State = [[-0.2754425  -0.25203082]]. Action = [[ 0.08249629  0.11534119 -0.02611683 -0.9879973 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 8603 is [True, False, False, True, False, False]
Current timestep = 8604. State = [[-0.27492777 -0.2510832 ]]. Action = [[-0.15029253 -0.0311394  -0.1718388  -0.6519079 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 8604 is [True, False, False, True, False, False]
Scene graph at timestep 8604 is [True, False, False, True, False, False]
State prediction error at timestep 8604 is tensor(1.7999e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8605. State = [[-0.27458125 -0.2506277 ]]. Action = [[-0.08403635 -0.15397924  0.04257423 -0.6633183 ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 8605 is [True, False, False, True, False, False]
Current timestep = 8606. State = [[-0.27428427 -0.25022125]]. Action = [[-0.16846101  0.11948657 -0.19170155 -0.6973977 ]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 8606 is [True, False, False, True, False, False]
Current timestep = 8607. State = [[-0.27410498 -0.24999443]]. Action = [[-0.01003508 -0.05168346 -0.20399535 -0.09313375]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 8607 is [True, False, False, True, False, False]
Scene graph at timestep 8607 is [True, False, False, True, False, False]
State prediction error at timestep 8607 is tensor(3.1764e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8608. State = [[-0.27410498 -0.24999443]]. Action = [[-0.08815068 -0.15584485  0.04834661 -0.9265241 ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 8608 is [True, False, False, True, False, False]
Scene graph at timestep 8608 is [True, False, False, True, False, False]
State prediction error at timestep 8608 is tensor(1.4357e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8609. State = [[-0.27410498 -0.24999443]]. Action = [[-0.1295155  -0.19584718 -0.16673848  0.83350587]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 8609 is [True, False, False, True, False, False]
Human Feedback received at timestep 8609 of -1
Current timestep = 8610. State = [[-0.2740694  -0.24994935]]. Action = [[-0.17501938 -0.10663904  0.07690352  0.598374  ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 8610 is [True, False, False, True, False, False]
Current timestep = 8611. State = [[-0.27403355 -0.24990396]]. Action = [[-0.15405434 -0.03356141  0.08263075 -0.49411798]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 8611 is [True, False, False, True, False, False]
Current timestep = 8612. State = [[-0.27399772 -0.24985857]]. Action = [[-0.08108106 -0.21773708 -0.21998157  0.69665647]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 8612 is [True, False, False, True, False, False]
Scene graph at timestep 8612 is [True, False, False, True, False, False]
State prediction error at timestep 8612 is tensor(2.5284e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8613. State = [[-0.27399772 -0.24985857]]. Action = [[-0.20487003 -0.06179005 -0.22814749 -0.73624486]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 8613 is [True, False, False, True, False, False]
Scene graph at timestep 8613 is [True, False, False, True, False, False]
State prediction error at timestep 8613 is tensor(5.9203e-09, grad_fn=<MseLossBackward0>)
Current timestep = 8614. State = [[-0.27399772 -0.24985857]]. Action = [[-0.10737827  0.19554406 -0.03650759 -0.54496896]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 8614 is [True, False, False, True, False, False]
Current timestep = 8615. State = [[-0.2739794  -0.24990553]]. Action = [[ 0.14771509 -0.15181729 -0.22307356  0.0301702 ]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 8615 is [True, False, False, True, False, False]
Current timestep = 8616. State = [[-0.2738078 -0.2500379]]. Action = [[-0.13152389  0.06141362  0.1452434   0.19924843]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 8616 is [True, False, False, True, False, False]
Current timestep = 8617. State = [[-0.2736164  -0.25023934]]. Action = [[-0.05632621 -0.15546876 -0.02591376 -0.9323869 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 8617 is [True, False, False, True, False, False]
Current timestep = 8618. State = [[-0.2732905 -0.2513838]]. Action = [[-0.09229155  0.24567011 -0.19335122  0.36753726]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 8618 is [True, False, False, True, False, False]
Current timestep = 8619. State = [[-0.2729897  -0.25263584]]. Action = [[-0.02970529 -0.23608921 -0.22987705  0.25277233]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 8619 is [True, False, False, True, False, False]
Current timestep = 8620. State = [[-0.27314174 -0.2561432 ]]. Action = [[-0.11470935 -0.13441634  0.22004333 -0.7081097 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 8620 is [True, False, False, True, False, False]
Current timestep = 8621. State = [[-0.2729769 -0.2591666]]. Action = [[-0.02209857 -0.24023686 -0.14389193 -0.38774467]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 8621 is [True, False, False, True, False, False]
Current timestep = 8622. State = [[-0.2737287  -0.26373005]]. Action = [[ 0.01274985  0.22121099 -0.20895799  0.7201282 ]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 8622 is [True, False, False, True, False, False]
Current timestep = 8623. State = [[-0.27366737 -0.26431075]]. Action = [[-0.02307276  0.24911195 -0.02116299  0.59544265]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 8623 is [True, False, False, True, False, False]
Scene graph at timestep 8623 is [True, False, False, True, False, False]
State prediction error at timestep 8623 is tensor(2.6563e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8624. State = [[-0.27340576 -0.26336426]]. Action = [[ 0.03505859  0.23973992 -0.08241186  0.91828346]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 8624 is [True, False, False, True, False, False]
Human Feedback received at timestep 8624 of -1
Current timestep = 8625. State = [[-0.2724651  -0.26083362]]. Action = [[-0.16915238 -0.24914603 -0.01291917  0.5235379 ]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 8625 is [True, False, False, True, False, False]
Current timestep = 8626. State = [[-0.2719799  -0.25971657]]. Action = [[-0.12705572 -0.04811618 -0.1377201  -0.52206486]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 8626 is [True, False, False, True, False, False]
Scene graph at timestep 8626 is [True, False, False, True, False, False]
State prediction error at timestep 8626 is tensor(3.6600e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8626 of -1
Current timestep = 8627. State = [[-0.27161255 -0.25918102]]. Action = [[-0.13455963  0.18618414 -0.22032289  0.54034805]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 8627 is [True, False, False, True, False, False]
Current timestep = 8628. State = [[-0.27131912 -0.25882018]]. Action = [[-0.1486655  -0.21176203  0.00723162 -0.8906775 ]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 8628 is [True, False, False, True, False, False]
Scene graph at timestep 8628 is [True, False, False, True, False, False]
State prediction error at timestep 8628 is tensor(1.7661e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8629. State = [[-0.2710971  -0.25854662]]. Action = [[-0.20197798 -0.07425579 -0.00906675 -0.40423352]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 8629 is [True, False, False, True, False, False]
Current timestep = 8630. State = [[-0.27090594 -0.2582717 ]]. Action = [[-0.14900807 -0.13795356 -0.0262028  -0.3229701 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 8630 is [True, False, False, True, False, False]
Current timestep = 8631. State = [[-0.27069473 -0.2580497 ]]. Action = [[-0.19290581 -0.23523693 -0.23346737  0.50121   ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 8631 is [True, False, False, True, False, False]
Current timestep = 8632. State = [[-0.2704978 -0.2578684]]. Action = [[-0.0752435  -0.21809418 -0.20927998  0.68005276]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 8632 is [True, False, False, True, False, False]
Scene graph at timestep 8632 is [True, False, False, True, False, False]
State prediction error at timestep 8632 is tensor(8.9230e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8633. State = [[-0.2707585 -0.2584089]]. Action = [[-0.18662812  0.02526742 -0.08734176 -0.4243729 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 8633 is [True, False, False, True, False, False]
Current timestep = 8634. State = [[-0.27084336 -0.2584855 ]]. Action = [[-0.0599954   0.13826501 -0.2118929  -0.8640704 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 8634 is [True, False, False, True, False, False]
Current timestep = 8635. State = [[-0.2711493  -0.25829786]]. Action = [[ 0.01854175 -0.13858838 -0.23667337 -0.89285284]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 8635 is [True, False, False, True, False, False]
Scene graph at timestep 8635 is [True, False, False, True, False, False]
State prediction error at timestep 8635 is tensor(7.2490e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8636. State = [[-0.2714247  -0.25885278]]. Action = [[ 0.03588474 -0.17789344 -0.21015573 -0.9951122 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 8636 is [True, False, False, True, False, False]
Current timestep = 8637. State = [[-0.2721279 -0.260362 ]]. Action = [[-0.13486518  0.04482973 -0.2324894  -0.66215336]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 8637 is [True, False, False, True, False, False]
Current timestep = 8638. State = [[-0.2727892  -0.26131955]]. Action = [[-0.18752404  0.2414149  -0.1095608   0.01709747]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 8638 is [True, False, False, True, False, False]
Current timestep = 8639. State = [[-0.27323022 -0.26194632]]. Action = [[-0.18161973 -0.2116669  -0.13048117  0.94676316]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 8639 is [True, False, False, True, False, False]
Current timestep = 8640. State = [[-0.27344573 -0.2621685 ]]. Action = [[-0.14601976 -0.24681929 -0.22477625 -0.6369385 ]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 8640 is [True, False, False, True, False, False]
Scene graph at timestep 8640 is [True, False, False, True, False, False]
State prediction error at timestep 8640 is tensor(9.6113e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8641. State = [[-0.2735425  -0.26235425]]. Action = [[-0.18740036 -0.00078888  0.14760554 -0.22001982]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 8641 is [True, False, False, True, False, False]
Current timestep = 8642. State = [[-0.27367702 -0.26257792]]. Action = [[-0.19522516 -0.2091043  -0.2084675  -0.59216535]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 8642 is [True, False, False, True, False, False]
Scene graph at timestep 8642 is [True, False, False, True, False, False]
State prediction error at timestep 8642 is tensor(2.1368e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8643. State = [[-0.2737421 -0.2626181]]. Action = [[-0.06544989  0.08454573 -0.12616646 -0.9893805 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 8643 is [True, False, False, True, False, False]
Scene graph at timestep 8643 is [True, False, False, True, False, False]
State prediction error at timestep 8643 is tensor(2.3201e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8643 of -1
Current timestep = 8644. State = [[-0.27373055 -0.26257363]]. Action = [[-0.1856187   0.16523558 -0.08907822 -0.92689335]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 8644 is [True, False, False, True, False, False]
Current timestep = 8645. State = [[-0.27376765 -0.26261833]]. Action = [[-0.08863968 -0.194897    0.11842978 -0.43794286]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 8645 is [True, False, False, True, False, False]
Current timestep = 8646. State = [[-0.27383685 -0.262801  ]]. Action = [[-0.02039668 -0.1552881  -0.2177066  -0.8855562 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 8646 is [True, False, False, True, False, False]
Current timestep = 8647. State = [[-0.27432922 -0.26351503]]. Action = [[-0.20733094  0.14636293 -0.01914026 -0.7565703 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 8647 is [True, False, False, True, False, False]
Current timestep = 8648. State = [[-0.27472478 -0.2640496 ]]. Action = [[-0.19920589  0.16067624  0.0192464  -0.6875661 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 8648 is [True, False, False, True, False, False]
Current timestep = 8649. State = [[-0.27491355 -0.26427504]]. Action = [[ 0.01908913  0.00251064 -0.23778953 -0.67417544]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 8649 is [True, False, False, True, False, False]
Current timestep = 8650. State = [[-0.27502528 -0.26440832]]. Action = [[-0.14652978  0.09051102 -0.23208159  0.6900879 ]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 8650 is [True, False, False, True, False, False]
Current timestep = 8651. State = [[-0.27504164 -0.2643592 ]]. Action = [[-0.00602628  0.19950056 -0.1764869  -0.47947645]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 8651 is [True, False, False, True, False, False]
Current timestep = 8652. State = [[-0.27499256 -0.2642703 ]]. Action = [[-0.14242138  0.06720638 -0.10603854 -0.3560946 ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 8652 is [True, False, False, True, False, False]
Current timestep = 8653. State = [[-0.2749555  -0.26422608]]. Action = [[ 0.07785293 -0.09459558 -0.1484339   0.6396253 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 8653 is [True, False, False, True, False, False]
Current timestep = 8654. State = [[-0.2749555  -0.26422608]]. Action = [[-0.15648244  0.06175348  0.0854333  -0.9672908 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 8654 is [True, False, False, True, False, False]
Scene graph at timestep 8654 is [True, False, False, True, False, False]
State prediction error at timestep 8654 is tensor(2.3931e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8655. State = [[-0.2749555  -0.26422608]]. Action = [[ 0.07986778 -0.04711217 -0.05390729 -0.9558402 ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 8655 is [True, False, False, True, False, False]
Current timestep = 8656. State = [[-0.2749555  -0.26422608]]. Action = [[-0.17178766  0.20747605 -0.01382893 -0.7842156 ]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 8656 is [True, False, False, True, False, False]
Current timestep = 8657. State = [[-0.2749555  -0.26422608]]. Action = [[-0.14963995  0.23306674  0.16319102 -0.20200694]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 8657 is [True, False, False, True, False, False]
Current timestep = 8658. State = [[-0.2749555  -0.26422608]]. Action = [[-0.18414065 -0.21031299 -0.18115436 -0.53474766]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 8658 is [True, False, False, True, False, False]
Scene graph at timestep 8658 is [True, False, False, True, False, False]
State prediction error at timestep 8658 is tensor(1.8171e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8659. State = [[-0.2749182  -0.26418155]]. Action = [[-0.16777311  0.11632854  0.02523383 -0.91796774]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 8659 is [True, False, False, True, False, False]
Current timestep = 8660. State = [[-0.2749182  -0.26418155]]. Action = [[-0.14618807  0.08669966 -0.10253844  0.9152262 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 8660 is [True, False, False, True, False, False]
Current timestep = 8661. State = [[-0.2749182  -0.26418155]]. Action = [[-0.19952682 -0.19732825 -0.11507979  0.15710723]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 8661 is [True, False, False, True, False, False]
Scene graph at timestep 8661 is [True, False, False, True, False, False]
State prediction error at timestep 8661 is tensor(4.1128e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8662. State = [[-0.2749182  -0.26418155]]. Action = [[-0.15054277  0.1243166  -0.2298358  -0.21529639]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 8662 is [True, False, False, True, False, False]
Scene graph at timestep 8662 is [True, False, False, True, False, False]
State prediction error at timestep 8662 is tensor(5.7351e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8663. State = [[-0.2748413  -0.26408976]]. Action = [[-0.19459696  0.11670756  0.00981018 -0.39449084]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 8663 is [True, False, False, True, False, False]
Current timestep = 8664. State = [[-0.2748413  -0.26408976]]. Action = [[-0.04062083  0.0608235   0.01224601 -0.77444553]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 8664 is [True, False, False, True, False, False]
Current timestep = 8665. State = [[-0.27465516 -0.26386723]]. Action = [[-0.08850378 -0.24488129 -0.15520062 -0.13898754]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 8665 is [True, False, False, True, False, False]
Current timestep = 8666. State = [[-0.27458066 -0.26377806]]. Action = [[ 0.01979277  0.00155738 -0.10628198 -0.13626266]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 8666 is [True, False, False, True, False, False]
Current timestep = 8667. State = [[-0.2745434  -0.26373348]]. Action = [[-0.13956206 -0.24363567 -0.03219475 -0.8605578 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 8667 is [True, False, False, True, False, False]
Current timestep = 8668. State = [[-0.27448958 -0.2637376 ]]. Action = [[-0.01670918 -0.18515366 -0.19718009 -0.7050147 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 8668 is [True, False, False, True, False, False]
Current timestep = 8669. State = [[-0.2745972 -0.2643108]]. Action = [[-0.14268847 -0.23789904 -0.15090717 -0.4319623 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 8669 is [True, False, False, True, False, False]
Current timestep = 8670. State = [[-0.27477548 -0.26480514]]. Action = [[-0.21227926 -0.23537968 -0.13704346 -0.5898336 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 8670 is [True, False, False, True, False, False]
Current timestep = 8671. State = [[-0.2748481 -0.2649829]]. Action = [[-0.20092961 -0.17153151  0.17717117 -0.56871   ]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 8671 is [True, False, False, True, False, False]
Scene graph at timestep 8671 is [True, False, False, True, False, False]
State prediction error at timestep 8671 is tensor(1.0601e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8672. State = [[-0.2748837  -0.26511642]]. Action = [[-0.16068497  0.00373709 -0.22930954 -0.26864254]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 8672 is [True, False, False, True, False, False]
Current timestep = 8673. State = [[-0.2748837  -0.26511642]]. Action = [[-0.15872347  0.20374435 -0.17074065 -0.8882716 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 8673 is [True, False, False, True, False, False]
Current timestep = 8674. State = [[-0.2748837  -0.26511642]]. Action = [[-0.18215448 -0.24430493 -0.2129807  -0.7850335 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 8674 is [True, False, False, True, False, False]
Current timestep = 8675. State = [[-0.2748837  -0.26511642]]. Action = [[-0.13961186  0.1863668   0.09693727  0.17240655]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 8675 is [True, False, False, True, False, False]
Current timestep = 8676. State = [[-0.2748837  -0.26511642]]. Action = [[-0.1853385   0.23034072 -0.01643041 -0.9914347 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 8676 is [True, False, False, True, False, False]
Current timestep = 8677. State = [[-0.2748837  -0.26511642]]. Action = [[-0.21495299  0.24693316 -0.1337136  -0.14468062]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 8677 is [True, False, False, True, False, False]
Scene graph at timestep 8677 is [True, False, False, True, False, False]
State prediction error at timestep 8677 is tensor(1.8110e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8678. State = [[-0.27483472 -0.26502788]]. Action = [[-0.0129877   0.1781984   0.09213424 -0.9752894 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 8678 is [True, False, False, True, False, False]
Scene graph at timestep 8678 is [True, False, False, True, False, False]
State prediction error at timestep 8678 is tensor(4.1720e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8679. State = [[-0.27471343 -0.26484546]]. Action = [[-0.06202845 -0.08051486 -0.12881799 -0.9773892 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 8679 is [True, False, False, True, False, False]
Scene graph at timestep 8679 is [True, False, False, True, False, False]
State prediction error at timestep 8679 is tensor(1.9186e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8680. State = [[-0.2747044  -0.26479656]]. Action = [[-0.17187749  0.16045311 -0.2374473  -0.56222093]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 8680 is [True, False, False, True, False, False]
Current timestep = 8681. State = [[-0.2747044  -0.26479656]]. Action = [[-0.10202369  0.22853273 -0.11711322 -0.55742556]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 8681 is [True, False, False, True, False, False]
Current timestep = 8682. State = [[-0.27462995 -0.2647078 ]]. Action = [[-0.08389734  0.2372495  -0.06603083 -0.8533486 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 8682 is [True, False, False, True, False, False]
Current timestep = 8683. State = [[-0.27462995 -0.2647078 ]]. Action = [[-0.14234465  0.1499593  -0.07962498 -0.9456576 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 8683 is [True, False, False, True, False, False]
Current timestep = 8684. State = [[-0.27462995 -0.2647078 ]]. Action = [[-0.18510365 -0.17909847 -0.09993482 -0.89235234]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 8684 is [True, False, False, True, False, False]
Current timestep = 8685. State = [[-0.2745926  -0.26466328]]. Action = [[-0.22690031  0.14349511 -0.10629661 -0.9734604 ]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 8685 is [True, False, False, True, False, False]
Current timestep = 8686. State = [[-0.2745926  -0.26466328]]. Action = [[-0.1474001   0.2421909  -0.04176435  0.18413281]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 8686 is [True, False, False, True, False, False]
Current timestep = 8687. State = [[-0.2745926  -0.26466328]]. Action = [[-0.14034016 -0.23298705 -0.13708977 -0.95473343]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 8687 is [True, False, False, True, False, False]
Scene graph at timestep 8687 is [True, False, False, True, False, False]
State prediction error at timestep 8687 is tensor(1.1425e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8688. State = [[-0.2745926  -0.26466328]]. Action = [[-0.14139844  0.231711    0.09237248 -0.57580477]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 8688 is [True, False, False, True, False, False]
Current timestep = 8689. State = [[-0.2746322  -0.26467803]]. Action = [[ 0.01735038 -0.21668237 -0.16462351  0.24899244]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 8689 is [True, False, False, True, False, False]
Current timestep = 8690. State = [[-0.27486557 -0.26543978]]. Action = [[-0.05693972 -0.16191742 -0.22725378 -0.65616596]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 8690 is [True, False, False, True, False, False]
Scene graph at timestep 8690 is [True, False, False, True, False, False]
State prediction error at timestep 8690 is tensor(3.4189e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8691. State = [[-0.2753806  -0.26783627]]. Action = [[-0.16957572  0.17071706 -0.23951106 -0.97006655]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 8691 is [True, False, False, True, False, False]
Scene graph at timestep 8691 is [True, False, False, True, False, False]
State prediction error at timestep 8691 is tensor(5.2164e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8692. State = [[-0.27590558 -0.26993704]]. Action = [[-0.09052405  0.23314363 -0.19038153  0.6631113 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 8692 is [True, False, False, True, False, False]
Scene graph at timestep 8692 is [True, False, False, True, False, False]
State prediction error at timestep 8692 is tensor(1.8617e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8693. State = [[-0.27615654 -0.27102783]]. Action = [[-0.15440993 -0.24289028 -0.14084195 -0.8730356 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 8693 is [True, False, False, True, False, False]
Scene graph at timestep 8693 is [True, False, False, True, False, False]
State prediction error at timestep 8693 is tensor(2.9359e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8694. State = [[-0.27623847 -0.27155092]]. Action = [[-0.11434376  0.23469025 -0.03776728 -0.66146576]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 8694 is [True, False, False, True, False, False]
Scene graph at timestep 8694 is [True, False, False, True, False, False]
State prediction error at timestep 8694 is tensor(7.6078e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8695. State = [[-0.2762167  -0.27178782]]. Action = [[-0.16384965  0.00820827 -0.13340908  0.5096452 ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 8695 is [True, False, False, True, False, False]
Current timestep = 8696. State = [[-0.2762167  -0.27178782]]. Action = [[-0.13986202  0.22562718 -0.1672573  -0.6788985 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 8696 is [True, False, False, True, False, False]
Current timestep = 8697. State = [[-0.2762795  -0.27200982]]. Action = [[-0.05412346  0.15480345  0.07119384 -0.72125506]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 8697 is [True, False, False, True, False, False]
Current timestep = 8698. State = [[-0.27629206 -0.27205405]]. Action = [[-0.11775765 -0.23411036 -0.17427166 -0.9387048 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 8698 is [True, False, False, True, False, False]
Current timestep = 8699. State = [[-0.27630472 -0.2720987 ]]. Action = [[-0.16241333  0.2415986   0.00942236  0.44076622]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 8699 is [True, False, False, True, False, False]
Current timestep = 8700. State = [[-0.27630472 -0.2720987 ]]. Action = [[-0.19717783  0.04526103 -0.13712677 -0.02194357]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 8700 is [True, False, False, True, False, False]
Scene graph at timestep 8700 is [True, False, False, True, False, False]
State prediction error at timestep 8700 is tensor(4.7821e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8701. State = [[-0.27630472 -0.2720987 ]]. Action = [[-0.12162021 -0.23017523 -0.07663362  0.05701017]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 8701 is [True, False, False, True, False, False]
Current timestep = 8702. State = [[-0.27630472 -0.2720987 ]]. Action = [[-0.15690468  0.21982527 -0.1505028  -0.90737605]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 8702 is [True, False, False, True, False, False]
Current timestep = 8703. State = [[-0.27630472 -0.2720987 ]]. Action = [[-0.21137096  0.24014997 -0.16678603 -0.9974509 ]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 8703 is [True, False, False, True, False, False]
Scene graph at timestep 8703 is [True, False, False, True, False, False]
State prediction error at timestep 8703 is tensor(1.3077e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8704. State = [[-0.27630472 -0.2720987 ]]. Action = [[-0.07899484  0.19416243 -0.07195605 -0.9195153 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 8704 is [True, False, False, True, False, False]
Current timestep = 8705. State = [[-0.27630472 -0.2720987 ]]. Action = [[-0.17409053  0.07503566 -0.21666656 -0.69270915]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 8705 is [True, False, False, True, False, False]
Current timestep = 8706. State = [[-0.27634284 -0.27214262]]. Action = [[-0.03850147 -0.06763436  0.01052558 -0.98979753]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 8706 is [True, False, False, True, False, False]
Current timestep = 8707. State = [[-0.27634284 -0.27214262]]. Action = [[-0.19841567  0.16302902  0.03338659  0.15678513]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 8707 is [True, False, False, True, False, False]
Scene graph at timestep 8707 is [True, False, False, True, False, False]
State prediction error at timestep 8707 is tensor(6.2540e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8708. State = [[-0.27634284 -0.27214262]]. Action = [[ 0.03053179  0.02383274  0.0231832  -0.97867453]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 8708 is [True, False, False, True, False, False]
Current timestep = 8709. State = [[-0.27634284 -0.27214262]]. Action = [[-0.20115925  0.23094311 -0.2072549  -0.9778903 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 8709 is [True, False, False, True, False, False]
Current timestep = 8710. State = [[-0.27634284 -0.27214262]]. Action = [[-0.20429285 -0.05610727 -0.07831866  0.22971225]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 8710 is [True, False, False, True, False, False]
Current timestep = 8711. State = [[-0.27634284 -0.27214262]]. Action = [[-0.17490989  0.03689983  0.07697305 -0.7814848 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 8711 is [True, False, False, True, False, False]
Current timestep = 8712. State = [[-0.27634284 -0.27214262]]. Action = [[-0.09629515  0.10451016 -0.16542344 -0.01711178]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 8712 is [True, False, False, True, False, False]
Current timestep = 8713. State = [[-0.27634284 -0.27214262]]. Action = [[-0.20412526  0.07556316 -0.09542319 -0.99774814]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 8713 is [True, False, False, True, False, False]
Current timestep = 8714. State = [[-0.27634284 -0.27214262]]. Action = [[-0.06121522  0.20644063 -0.02948581 -0.99463016]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 8714 is [True, False, False, True, False, False]
Scene graph at timestep 8714 is [True, False, False, True, False, False]
State prediction error at timestep 8714 is tensor(2.5873e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8715. State = [[-0.27634284 -0.27214262]]. Action = [[-0.17770445  0.20681202 -0.11863132 -0.92410487]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 8715 is [True, False, False, True, False, False]
Current timestep = 8716. State = [[-0.27633065 -0.27222133]]. Action = [[-0.00739154 -0.11570066 -0.20321922 -0.8934216 ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 8716 is [True, False, False, True, False, False]
Current timestep = 8717. State = [[-0.27626356 -0.2729103 ]]. Action = [[-0.1360221   0.09778032 -0.21638525  0.7484653 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 8717 is [True, False, False, True, False, False]
Scene graph at timestep 8717 is [True, False, False, True, False, False]
State prediction error at timestep 8717 is tensor(5.0616e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8718. State = [[-0.27635708 -0.27345774]]. Action = [[-0.1817168  -0.20977493 -0.15800008 -0.5082882 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 8718 is [True, False, False, True, False, False]
Human Feedback received at timestep 8718 of -1
Current timestep = 8719. State = [[-0.2762905  -0.27384394]]. Action = [[-0.04035102  0.20623013 -0.20996279  0.7316185 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 8719 is [True, False, False, True, False, False]
Scene graph at timestep 8719 is [True, False, False, True, False, False]
State prediction error at timestep 8719 is tensor(1.7810e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8720. State = [[-0.27618518 -0.2741913 ]]. Action = [[-0.23180602  0.1131151  -0.14884159 -0.68047297]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 8720 is [True, False, False, True, False, False]
Human Feedback received at timestep 8720 of -1
Current timestep = 8721. State = [[-0.2762108  -0.27428016]]. Action = [[-0.2212476  -0.0477881  -0.00588688 -0.816024  ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 8721 is [True, False, False, True, False, False]
Current timestep = 8722. State = [[-0.27624926 -0.2744133 ]]. Action = [[-0.13547416 -0.0388875  -0.14325172 -0.8431423 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 8722 is [True, False, False, True, False, False]
Current timestep = 8723. State = [[-0.27627495 -0.27450216]]. Action = [[-0.11514471  0.14796174 -0.17959751 -0.9879176 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 8723 is [True, False, False, True, False, False]
Human Feedback received at timestep 8723 of -1
Current timestep = 8724. State = [[-0.27620363 -0.27456233]]. Action = [[-0.14380424 -0.06127182 -0.08564644 -0.99607056]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 8724 is [True, False, False, True, False, False]
Current timestep = 8725. State = [[-0.27617553 -0.27456766]]. Action = [[-0.11782593 -0.09833674 -0.07383931  0.16578841]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 8725 is [True, False, False, True, False, False]
Scene graph at timestep 8725 is [True, False, False, True, False, False]
State prediction error at timestep 8725 is tensor(1.2571e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8725 of -1
Current timestep = 8726. State = [[-0.27617553 -0.27456766]]. Action = [[-0.17144941  0.1763463   0.19371033 -0.36100852]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 8726 is [True, False, False, True, False, False]
Human Feedback received at timestep 8726 of -1
Current timestep = 8727. State = [[-0.27617553 -0.27456766]]. Action = [[-0.05218379 -0.11354965 -0.13789502 -0.98015136]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 8727 is [True, False, False, True, False, False]
Current timestep = 8728. State = [[-0.27617553 -0.27456766]]. Action = [[-0.18824284 -0.07753983  0.23726481 -0.83439577]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 8728 is [True, False, False, True, False, False]
Human Feedback received at timestep 8728 of -1
Current timestep = 8729. State = [[-0.27617553 -0.27456766]]. Action = [[-0.07316321 -0.24326998 -0.2005936  -0.9485997 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 8729 is [True, False, False, True, False, False]
Current timestep = 8730. State = [[-0.27617553 -0.27456766]]. Action = [[-0.12831892  0.16444427 -0.20322293 -0.04474223]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 8730 is [True, False, False, True, False, False]
Current timestep = 8731. State = [[-0.27617553 -0.27456766]]. Action = [[-0.180732    0.24341816 -0.24203032 -0.4860068 ]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 8731 is [True, False, False, True, False, False]
Current timestep = 8732. State = [[-0.27617553 -0.27456766]]. Action = [[-0.1368426   0.15152073 -0.24394695 -0.89501315]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 8732 is [True, False, False, True, False, False]
Current timestep = 8733. State = [[-0.27617553 -0.27456766]]. Action = [[-0.18330508  0.19330794  0.12053299  0.430004  ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 8733 is [True, False, False, True, False, False]
Scene graph at timestep 8733 is [True, False, False, True, False, False]
State prediction error at timestep 8733 is tensor(9.5544e-08, grad_fn=<MseLossBackward0>)
Current timestep = 8734. State = [[-0.27617553 -0.27456766]]. Action = [[-0.15176195  0.01555958 -0.1674046   0.9248333 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 8734 is [True, False, False, True, False, False]
Current timestep = 8735. State = [[-0.27617553 -0.27456766]]. Action = [[-0.11788183  0.23546505 -0.08391012  0.91053796]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 8735 is [True, False, False, True, False, False]
Human Feedback received at timestep 8735 of -1
Current timestep = 8736. State = [[-0.27617553 -0.27456766]]. Action = [[-0.21502872  0.15870601  0.15393096 -0.8244504 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 8736 is [True, False, False, True, False, False]
Current timestep = 8737. State = [[-0.27617553 -0.27456766]]. Action = [[-0.20463037  0.16727108  0.2138952  -0.9781399 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 8737 is [True, False, False, True, False, False]
Current timestep = 8738. State = [[-0.27617553 -0.27456766]]. Action = [[-0.1500294   0.11200994 -0.0743787  -0.9361773 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 8738 is [True, False, False, True, False, False]
Scene graph at timestep 8738 is [True, False, False, True, False, False]
State prediction error at timestep 8738 is tensor(2.1972e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8739. State = [[-0.27617553 -0.27456766]]. Action = [[-0.05109128  0.17857814 -0.10260339 -0.90635675]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 8739 is [True, False, False, True, False, False]
Scene graph at timestep 8739 is [True, False, False, True, False, False]
State prediction error at timestep 8739 is tensor(7.3233e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8739 of -1
Current timestep = 8740. State = [[-0.27617553 -0.27456766]]. Action = [[-0.15087505 -0.19559313 -0.19580977  0.41310954]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 8740 is [True, False, False, True, False, False]
Current timestep = 8741. State = [[-0.27619085 -0.27451807]]. Action = [[0.08443141 0.22692323 0.17913717 0.5789213 ]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 8741 is [True, False, False, True, False, False]
Current timestep = 8742. State = [[-0.27571595 -0.2738555 ]]. Action = [[-0.17243814  0.2454476   0.019164    0.42274463]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 8742 is [True, False, False, True, False, False]
Current timestep = 8743. State = [[-0.27537212 -0.27346087]]. Action = [[-0.19602095 -0.24172133 -0.23759173 -0.8174299 ]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 8743 is [True, False, False, True, False, False]
Scene graph at timestep 8743 is [True, False, False, True, False, False]
State prediction error at timestep 8743 is tensor(3.5146e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8743 of -1
Current timestep = 8744. State = [[-0.27514306 -0.2731974 ]]. Action = [[-0.11084229  0.07934347 -0.14095597  0.5170884 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 8744 is [True, False, False, True, False, False]
Scene graph at timestep 8744 is [True, False, False, True, False, False]
State prediction error at timestep 8744 is tensor(4.4404e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8745. State = [[-0.27491418 -0.27293378]]. Action = [[-0.04528472 -0.07767898 -0.23722821  0.6692145 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 8745 is [True, False, False, True, False, False]
Scene graph at timestep 8745 is [True, False, False, True, False, False]
State prediction error at timestep 8745 is tensor(9.5066e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8746. State = [[-0.27491418 -0.27293378]]. Action = [[-0.00093542  0.09517527 -0.17559804 -0.59590566]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 8746 is [True, False, False, True, False, False]
Current timestep = 8747. State = [[-0.27491418 -0.27293378]]. Action = [[-0.20364669 -0.08043766 -0.18335505 -0.39222622]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 8747 is [True, False, False, True, False, False]
Scene graph at timestep 8747 is [True, False, False, True, False, False]
State prediction error at timestep 8747 is tensor(7.8783e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8748. State = [[-0.27483785 -0.27284575]]. Action = [[-0.1507459   0.18235278  0.17988065 -0.7615687 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 8748 is [True, False, False, True, False, False]
Current timestep = 8749. State = [[-0.2747997  -0.27280173]]. Action = [[-0.12030828  0.2363497  -0.18785389  0.3920908 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 8749 is [True, False, False, True, False, False]
Current timestep = 8750. State = [[-0.2747997  -0.27280173]]. Action = [[-0.06239432 -0.05641407 -0.08335085  0.96799994]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 8750 is [True, False, False, True, False, False]
Current timestep = 8751. State = [[-0.27476156 -0.27275768]]. Action = [[-0.13124202 -0.14546546 -0.2172274  -0.6110015 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 8751 is [True, False, False, True, False, False]
Scene graph at timestep 8751 is [True, False, False, True, False, False]
State prediction error at timestep 8751 is tensor(9.3278e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8751 of -1
Current timestep = 8752. State = [[-0.27476156 -0.27275768]]. Action = [[-0.12905906 -0.00876415 -0.20622869  0.02077472]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 8752 is [True, False, False, True, False, False]
Scene graph at timestep 8752 is [True, False, False, True, False, False]
State prediction error at timestep 8752 is tensor(5.6616e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8753. State = [[-0.27473623 -0.2726685 ]]. Action = [[-0.02825636  0.16648567 -0.16280967 -0.20661306]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 8753 is [True, False, False, True, False, False]
Scene graph at timestep 8753 is [True, False, False, True, False, False]
State prediction error at timestep 8753 is tensor(2.0024e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8754. State = [[-0.27433133 -0.27186257]]. Action = [[-0.2371692  -0.2286574   0.04216567 -0.88961893]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 8754 is [True, False, False, True, False, False]
Human Feedback received at timestep 8754 of -1
Current timestep = 8755. State = [[-0.2740635  -0.27146158]]. Action = [[-0.04377872  0.0139015  -0.18931577 -0.57413846]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 8755 is [True, False, False, True, False, False]
Current timestep = 8756. State = [[-0.2738869  -0.27119622]]. Action = [[-0.09850305 -0.13709524  0.16820747 -0.85976493]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 8756 is [True, False, False, True, False, False]
Scene graph at timestep 8756 is [True, False, False, True, False, False]
State prediction error at timestep 8756 is tensor(3.2162e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8757. State = [[-0.27383634 -0.2711071 ]]. Action = [[-0.20096877  0.0322839  -0.19642814 -0.7026333 ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 8757 is [True, False, False, True, False, False]
Scene graph at timestep 8757 is [True, False, False, True, False, False]
State prediction error at timestep 8757 is tensor(7.6765e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8758. State = [[-0.27379838 -0.2710629 ]]. Action = [[-0.21470131  0.06952596 -0.09249723 -0.88066953]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 8758 is [True, False, False, True, False, False]
Scene graph at timestep 8758 is [True, False, False, True, False, False]
State prediction error at timestep 8758 is tensor(3.0287e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8758 of -1
Current timestep = 8759. State = [[-0.27368453 -0.27093023]]. Action = [[-0.14209823 -0.08795476  0.19176075 -0.6980451 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 8759 is [True, False, False, True, False, False]
Current timestep = 8760. State = [[-0.27360892 -0.27084208]]. Action = [[-0.21822587 -0.08155274 -0.23914282 -0.9275271 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 8760 is [True, False, False, True, False, False]
Current timestep = 8761. State = [[-0.27358016 -0.2706988 ]]. Action = [[-0.07809256  0.24607724 -0.20746318 -0.1426549 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 8761 is [True, False, False, True, False, False]
Scene graph at timestep 8761 is [True, False, False, True, False, False]
State prediction error at timestep 8761 is tensor(2.4210e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8762. State = [[-0.2737497 -0.2681226]]. Action = [[-0.19104351  0.125826   -0.14681397  0.71493864]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 8762 is [True, False, False, True, False, False]
Current timestep = 8763. State = [[-0.27389184 -0.26696876]]. Action = [[-0.03611428 -0.1852228   0.10911366 -0.7256211 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 8763 is [True, False, False, True, False, False]
Current timestep = 8764. State = [[-0.27410194 -0.26722586]]. Action = [[-0.02969898 -0.19778895 -0.15217723  0.63393426]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 8764 is [True, False, False, True, False, False]
Current timestep = 8765. State = [[-0.2749026  -0.26884565]]. Action = [[-0.00647847  0.15787238 -0.02297288 -0.3045752 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 8765 is [True, False, False, True, False, False]
Current timestep = 8766. State = [[-0.27515337 -0.2687021 ]]. Action = [[-0.1334173   0.23194072 -0.23428018 -0.2954005 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 8766 is [True, False, False, True, False, False]
Human Feedback received at timestep 8766 of 1
Current timestep = 8767. State = [[-0.27524328 -0.2688057 ]]. Action = [[ 0.01103452 -0.20843375  0.09490442  0.85491896]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 8767 is [True, False, False, True, False, False]
Scene graph at timestep 8767 is [True, False, False, True, False, False]
State prediction error at timestep 8767 is tensor(3.2439e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8768. State = [[-0.27604085 -0.26999933]]. Action = [[-0.15144932  0.11933318 -0.2099648  -0.6150753 ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 8768 is [True, False, False, True, False, False]
Current timestep = 8769. State = [[-0.276343  -0.2704402]]. Action = [[-0.22510809  0.1651226  -0.01273677 -0.36662185]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 8769 is [True, False, False, True, False, False]
Current timestep = 8770. State = [[-0.27664623 -0.27079153]]. Action = [[-0.20799528  0.04164869 -0.12281615 -0.7509531 ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 8770 is [True, False, False, True, False, False]
Current timestep = 8771. State = [[-0.23820485 -0.05711851]]. Action = [[-0.07143839  0.00385663 -0.17308855 -0.95553267]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 8771 is [True, False, False, True, False, False]
Scene graph at timestep 8771 is [True, False, False, False, True, False]
State prediction error at timestep 8771 is tensor(0.0238, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8771 of 1
Current timestep = 8772. State = [[-0.2358639  -0.06070342]]. Action = [[ 0.22927189 -0.09574953 -0.20170271 -0.07854474]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 8772 is [True, False, False, False, True, False]
Scene graph at timestep 8772 is [True, False, False, False, True, False]
State prediction error at timestep 8772 is tensor(9.4313e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8773. State = [[-0.23356399 -0.06176052]]. Action = [[ 0.18597728 -0.04756986 -0.15903276  0.08116138]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 8773 is [True, False, False, False, True, False]
Scene graph at timestep 8773 is [True, False, False, False, True, False]
State prediction error at timestep 8773 is tensor(2.9653e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8774. State = [[-0.22995077 -0.0630824 ]]. Action = [[ 0.18240023 -0.08854164 -0.1515183  -0.19382405]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 8774 is [True, False, False, False, True, False]
Scene graph at timestep 8774 is [True, False, False, False, True, False]
State prediction error at timestep 8774 is tensor(5.1110e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8775. State = [[-0.22515075 -0.06549607]]. Action = [[ 0.23094732 -0.10092399 -0.18337294 -0.09992331]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 8775 is [True, False, False, False, True, False]
Current timestep = 8776. State = [[-0.21887816 -0.06877694]]. Action = [[ 0.20939043 -0.06866574 -0.14948729 -0.0299499 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 8776 is [True, False, False, False, True, False]
Scene graph at timestep 8776 is [True, False, False, False, True, False]
State prediction error at timestep 8776 is tensor(2.0838e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8777. State = [[-0.21092539 -0.07222779]]. Action = [[ 0.19524908 -0.05645376 -0.19465813 -0.04227483]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 8777 is [True, False, False, False, True, False]
Current timestep = 8778. State = [[-0.20491806 -0.07458427]]. Action = [[ 0.22614163 -0.03082404 -0.16594791 -0.04407936]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 8778 is [True, False, False, False, True, False]
Human Feedback received at timestep 8778 of 1
Current timestep = 8779. State = [[-0.19843213 -0.07657012]]. Action = [[ 0.1860978  -0.02931033 -0.17029695 -0.24727118]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 8779 is [True, False, False, False, True, False]
Scene graph at timestep 8779 is [True, False, False, False, True, False]
State prediction error at timestep 8779 is tensor(9.5559e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8780. State = [[-0.19087139 -0.0781125 ]]. Action = [[ 0.2072506  -0.08196652 -0.17054039 -0.12502164]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 8780 is [True, False, False, False, True, False]
Human Feedback received at timestep 8780 of 1
Current timestep = 8781. State = [[-0.18258388 -0.07973307]]. Action = [[ 0.21356836 -0.0928195  -0.16082962 -0.18679047]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 8781 is [True, False, False, False, True, False]
Current timestep = 8782. State = [[-0.17581731 -0.08216882]]. Action = [[ 0.18830115 -0.06378058 -0.13478181 -0.19385302]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 8782 is [True, False, False, False, True, False]
Current timestep = 8783. State = [[-0.16918586 -0.08477117]]. Action = [[ 0.12082112 -0.109824   -0.17869516  0.23798966]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 8783 is [True, False, False, False, True, False]
Scene graph at timestep 8783 is [True, False, False, False, True, False]
State prediction error at timestep 8783 is tensor(1.0806e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8783 of 1
Current timestep = 8784. State = [[-0.16205704 -0.08851548]]. Action = [[ 0.18677086 -0.10397907 -0.13874647 -0.00111955]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 8784 is [True, False, False, False, True, False]
Current timestep = 8785. State = [[-0.15556265 -0.09195136]]. Action = [[ 0.0999679  -0.05661862 -0.14653695 -0.07935542]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 8785 is [True, False, False, False, True, False]
Current timestep = 8786. State = [[-0.1507894  -0.09488644]]. Action = [[ 0.16557914 -0.09944761 -0.04843    -0.01055795]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 8786 is [True, False, False, False, True, False]
Current timestep = 8787. State = [[-0.14532039 -0.09810923]]. Action = [[ 0.12672162 -0.13947272 -0.18297476  0.28155994]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 8787 is [True, False, False, False, True, False]
Scene graph at timestep 8787 is [True, False, False, False, True, False]
State prediction error at timestep 8787 is tensor(1.3972e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8788. State = [[-0.13880934 -0.10219634]]. Action = [[ 0.18809181 -0.07803732 -0.14058957  0.07557917]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 8788 is [True, False, False, False, True, False]
Current timestep = 8789. State = [[-0.13386081 -0.10536855]]. Action = [[ 0.0042358   0.01235536 -0.13226661  0.05285025]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 8789 is [True, False, False, False, True, False]
Scene graph at timestep 8789 is [True, False, False, False, True, False]
State prediction error at timestep 8789 is tensor(1.1764e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8790. State = [[-0.12914218 -0.10743731]]. Action = [[ 0.09345755 -0.03610891 -0.14168155 -0.40961516]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 8790 is [True, False, False, False, True, False]
Scene graph at timestep 8790 is [True, False, False, False, True, False]
State prediction error at timestep 8790 is tensor(4.3877e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8791. State = [[-0.12427772 -0.10922758]]. Action = [[ 0.16454333 -0.14859658 -0.05173671 -0.36585015]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 8791 is [True, False, False, False, True, False]
Current timestep = 8792. State = [[-0.12032045 -0.11207344]]. Action = [[ 0.08772114 -0.16434667 -0.15913047 -0.21424699]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 8792 is [True, False, False, False, True, False]
Current timestep = 8793. State = [[-0.11598555 -0.11670066]]. Action = [[-0.098456   -0.08635724 -0.19146423 -0.02952611]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 8793 is [True, False, False, False, True, False]
Scene graph at timestep 8793 is [True, False, False, False, True, False]
State prediction error at timestep 8793 is tensor(8.2790e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8794. State = [[-0.11439271 -0.12067704]]. Action = [[ 0.01048398  0.04105163 -0.13273074 -0.5985967 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 8794 is [True, False, False, False, True, False]
Current timestep = 8795. State = [[-0.11413056 -0.12234592]]. Action = [[ 0.01224279  0.0437685  -0.17863981 -0.03465194]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 8795 is [True, False, False, False, True, False]
Scene graph at timestep 8795 is [True, False, False, False, True, False]
State prediction error at timestep 8795 is tensor(2.0217e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8796. State = [[-0.11401521 -0.12288407]]. Action = [[ 0.11475772 -0.16621226  0.03942302 -0.37419403]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 8796 is [True, False, False, False, True, False]
Current timestep = 8797. State = [[-0.11301842 -0.12509374]]. Action = [[-0.00102895 -0.03198373 -0.16862153 -0.36618936]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 8797 is [True, False, False, False, True, False]
Human Feedback received at timestep 8797 of 1
Current timestep = 8798. State = [[-0.11170442 -0.12726411]]. Action = [[ 0.12747926 -0.14239393 -0.18774614  0.3490119 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 8798 is [True, False, False, True, False, False]
Current timestep = 8799. State = [[-0.10941106 -0.13021412]]. Action = [[-0.05856068 -0.07012329 -0.05824581 -0.402389  ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 8799 is [True, False, False, True, False, False]
Scene graph at timestep 8799 is [True, False, False, True, False, False]
State prediction error at timestep 8799 is tensor(6.6396e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8799 of -1
Current timestep = 8800. State = [[-0.10706705 -0.13464434]]. Action = [[ 0.06843784 -0.14427799 -0.1273483  -0.52381724]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 8800 is [True, False, False, True, False, False]
Current timestep = 8801. State = [[-0.10573028 -0.13843898]]. Action = [[ 0.05342874  0.00167206  0.0623025  -0.496799  ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 8801 is [True, False, False, True, False, False]
Scene graph at timestep 8801 is [True, False, False, True, False, False]
State prediction error at timestep 8801 is tensor(7.4411e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8801 of -1
Current timestep = 8802. State = [[-0.10418303 -0.14063579]]. Action = [[-0.03523126 -0.04866692 -0.07337928  0.07917488]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 8802 is [True, False, False, True, False, False]
Current timestep = 8803. State = [[-0.10281032 -0.14271325]]. Action = [[-0.11337008  0.0220148  -0.12200853 -0.67569566]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 8803 is [True, False, False, True, False, False]
Scene graph at timestep 8803 is [True, False, False, True, False, False]
State prediction error at timestep 8803 is tensor(1.6675e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8803 of -1
Current timestep = 8804. State = [[-0.10234291 -0.14365761]]. Action = [[ 0.04406995  0.1134434  -0.10094929 -0.713956  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 8804 is [True, False, False, True, False, False]
Current timestep = 8805. State = [[-0.10233869 -0.14359525]]. Action = [[ 0.01795974 -0.0607425  -0.21684754 -0.33733988]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 8805 is [True, False, False, True, False, False]
Scene graph at timestep 8805 is [True, False, False, True, False, False]
State prediction error at timestep 8805 is tensor(3.1580e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8805 of -1
Current timestep = 8806. State = [[-0.10215501 -0.1437571 ]]. Action = [[ 0.03125766 -0.10914381 -0.13058373 -0.6630764 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 8806 is [True, False, False, True, False, False]
Current timestep = 8807. State = [[-0.10149761 -0.14522836]]. Action = [[-0.01235114 -0.16528702 -0.14211886 -0.24203491]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 8807 is [True, False, False, True, False, False]
Current timestep = 8808. State = [[-0.10174362 -0.14779857]]. Action = [[-0.00711504  0.04930684 -0.05778676 -0.51378846]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 8808 is [True, False, False, True, False, False]
Current timestep = 8809. State = [[-0.10194381 -0.14886735]]. Action = [[-0.1432971  -0.0597523  -0.10143226 -0.17868292]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 8809 is [True, False, False, True, False, False]
Human Feedback received at timestep 8809 of -1
Current timestep = 8810. State = [[-0.10246814 -0.15099089]]. Action = [[-0.04696986 -0.00676699 -0.19764641  0.15165424]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 8810 is [True, False, False, True, False, False]
Scene graph at timestep 8810 is [True, False, False, True, False, False]
State prediction error at timestep 8810 is tensor(7.4110e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8811. State = [[-0.10300087 -0.15249546]]. Action = [[-0.06257179  0.10975558  0.02003536 -0.3914641 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 8811 is [True, False, False, True, False, False]
Scene graph at timestep 8811 is [True, False, False, True, False, False]
State prediction error at timestep 8811 is tensor(1.1173e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8811 of -1
Current timestep = 8812. State = [[-0.10323127 -0.15264243]]. Action = [[ 0.03994223 -0.09129399  0.00236619 -0.2979225 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 8812 is [True, False, False, True, False, False]
Scene graph at timestep 8812 is [True, False, False, True, False, False]
State prediction error at timestep 8812 is tensor(2.3467e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8813. State = [[-0.10358433 -0.15331122]]. Action = [[ 0.03354049 -0.11801419 -0.21601847 -0.6654261 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 8813 is [True, False, False, True, False, False]
Scene graph at timestep 8813 is [True, False, False, True, False, False]
State prediction error at timestep 8813 is tensor(7.7246e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8814. State = [[-0.10383517 -0.15483505]]. Action = [[-0.06720454  0.11752731 -0.096158   -0.38343763]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 8814 is [True, False, False, True, False, False]
Current timestep = 8815. State = [[-0.10394004 -0.15479948]]. Action = [[-0.14451252  0.10760593 -0.16159868 -0.52321136]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 8815 is [True, False, False, True, False, False]
Current timestep = 8816. State = [[-0.10417256 -0.15461601]]. Action = [[-0.00524066  0.04116529  0.03063971 -0.57687813]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 8816 is [True, False, False, True, False, False]
Current timestep = 8817. State = [[-0.1043499 -0.1544619]]. Action = [[ 0.00277764  0.12425461 -0.02790052 -0.5498858 ]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 8817 is [True, False, False, True, False, False]
Scene graph at timestep 8817 is [True, False, False, True, False, False]
State prediction error at timestep 8817 is tensor(6.2547e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8818. State = [[-0.10437086 -0.15295117]]. Action = [[ 0.00450099  0.10727054 -0.00253865 -0.644253  ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 8818 is [True, False, False, True, False, False]
Current timestep = 8819. State = [[-0.10425883 -0.15081409]]. Action = [[-0.04982534 -0.06776598  0.0026294  -0.93182665]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 8819 is [True, False, False, True, False, False]
Current timestep = 8820. State = [[-0.10423351 -0.15073998]]. Action = [[-1.8627942e-04 -1.3552858e-01 -2.0097989e-01 -6.1172783e-01]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 8820 is [True, False, False, True, False, False]
Current timestep = 8821. State = [[-0.10430035 -0.15143172]]. Action = [[ 0.03905994 -0.1117765  -0.17354807 -0.35384417]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 8821 is [True, False, False, True, False, False]
Current timestep = 8822. State = [[-0.10441297 -0.15244028]]. Action = [[-0.0582512   0.12198138 -0.04810606  0.2682544 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 8822 is [True, False, False, True, False, False]
Scene graph at timestep 8822 is [True, False, False, True, False, False]
State prediction error at timestep 8822 is tensor(4.1265e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8823. State = [[-0.10441168 -0.15212986]]. Action = [[-0.03511931  0.00971499 -0.22040436 -0.73111004]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 8823 is [True, False, False, True, False, False]
Current timestep = 8824. State = [[-0.10436438 -0.15205353]]. Action = [[-0.08630918 -0.1334416  -0.08663353 -0.70647025]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 8824 is [True, False, False, True, False, False]
Current timestep = 8825. State = [[-0.10461426 -0.1531533 ]]. Action = [[-0.0331599   0.15984821 -0.1644236  -0.84987795]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 8825 is [True, False, False, True, False, False]
Current timestep = 8826. State = [[-0.10477386 -0.15291049]]. Action = [[-0.13646314  0.0015761  -0.10182026 -0.5594351 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 8826 is [True, False, False, True, False, False]
Scene graph at timestep 8826 is [True, False, False, True, False, False]
State prediction error at timestep 8826 is tensor(2.2895e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8827. State = [[-0.10547967 -0.15251021]]. Action = [[ 0.04412347  0.09199926 -0.12459603 -0.8687188 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 8827 is [True, False, False, True, False, False]
Current timestep = 8828. State = [[-0.10545634 -0.15192902]]. Action = [[-0.06763904 -0.09496778 -0.1406633  -0.44323123]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 8828 is [True, False, False, True, False, False]
Current timestep = 8829. State = [[-0.10572875 -0.15224613]]. Action = [[-0.10774839  0.02336574 -0.15645348 -0.35334063]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 8829 is [True, False, False, True, False, False]
Current timestep = 8830. State = [[-0.10668792 -0.1522785 ]]. Action = [[ 0.10921621  0.10452649 -0.09902972 -0.09425932]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 8830 is [True, False, False, True, False, False]
Current timestep = 8831. State = [[-0.10672293 -0.15158258]]. Action = [[-0.02841535 -0.1203365  -0.15082331 -0.53372175]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 8831 is [True, False, False, True, False, False]
Scene graph at timestep 8831 is [True, False, False, True, False, False]
State prediction error at timestep 8831 is tensor(3.3347e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8831 of -1
Current timestep = 8832. State = [[-0.10678898 -0.15177703]]. Action = [[ 0.00784969  0.0672484  -0.0713823  -0.6803059 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 8832 is [True, False, False, True, False, False]
Current timestep = 8833. State = [[-0.10678822 -0.15152943]]. Action = [[ 0.0861502   0.11046988 -0.17424    -0.04180038]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 8833 is [True, False, False, True, False, False]
Current timestep = 8834. State = [[-0.10676426 -0.15035185]]. Action = [[ 0.06774163  0.05380297 -0.11298454 -0.40230274]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 8834 is [True, False, False, True, False, False]
Human Feedback received at timestep 8834 of -1
Current timestep = 8835. State = [[-0.10672629 -0.14871465]]. Action = [[-0.03425375  0.2192947   0.09999084 -0.82501644]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 8835 is [True, False, False, True, False, False]
Current timestep = 8836. State = [[-0.10670204 -0.14475775]]. Action = [[-0.08444303 -0.20278288 -0.0277683  -0.08315682]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 8836 is [True, False, False, True, False, False]
Current timestep = 8837. State = [[-0.10676574 -0.14447083]]. Action = [[ 0.03649953  0.1745176  -0.14199542 -0.7516032 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 8837 is [True, False, False, True, False, False]
Scene graph at timestep 8837 is [True, False, False, True, False, False]
State prediction error at timestep 8837 is tensor(1.3953e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8838. State = [[-0.1068845  -0.14271781]]. Action = [[-0.09530392  0.05815351  0.01881528 -0.6558373 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 8838 is [True, False, False, True, False, False]
Scene graph at timestep 8838 is [True, False, False, True, False, False]
State prediction error at timestep 8838 is tensor(1.0896e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8839. State = [[-0.10700833 -0.14071208]]. Action = [[ 0.04013455  0.03211439 -0.21251634 -0.37122828]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 8839 is [True, False, False, True, False, False]
Current timestep = 8840. State = [[-0.10703151 -0.139291  ]]. Action = [[ 0.08042336 -0.06519395 -0.09261191 -0.67161465]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 8840 is [True, False, False, True, False, False]
Scene graph at timestep 8840 is [True, False, False, True, False, False]
State prediction error at timestep 8840 is tensor(5.0445e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8841. State = [[-0.10706349 -0.13897365]]. Action = [[ 0.08351028 -0.15242517 -0.03954786 -0.85826266]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 8841 is [True, False, False, True, False, False]
Current timestep = 8842. State = [[-0.10707177 -0.1394829 ]]. Action = [[ 0.08513591 -0.08914632 -0.08531798  0.28786063]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 8842 is [True, False, False, True, False, False]
Current timestep = 8843. State = [[-0.10714801 -0.14024267]]. Action = [[-0.11074445  0.06746668 -0.20125185 -0.21568769]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 8843 is [True, False, False, True, False, False]
Current timestep = 8844. State = [[-0.10717081 -0.1401843 ]]. Action = [[ 0.01345676 -0.04409267 -0.09477299  0.6256561 ]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 8844 is [True, False, False, True, False, False]
Current timestep = 8845. State = [[-0.10719163 -0.14024964]]. Action = [[ 0.04246598  0.02090499 -0.11187446 -0.56029695]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 8845 is [True, False, False, True, False, False]
Current timestep = 8846. State = [[-0.10719163 -0.14024964]]. Action = [[ 0.03963745 -0.00341594  0.11866313 -0.30192643]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 8846 is [True, False, False, True, False, False]
Current timestep = 8847. State = [[-0.10721247 -0.140315  ]]. Action = [[-0.01853783  0.11117703  0.11903384  0.26782823]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 8847 is [True, False, False, True, False, False]
Scene graph at timestep 8847 is [True, False, False, True, False, False]
State prediction error at timestep 8847 is tensor(3.8211e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8848. State = [[-0.10718197 -0.14009216]]. Action = [[-0.07102445  0.11899373 -0.16874969 -0.30692303]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 8848 is [True, False, False, True, False, False]
Current timestep = 8849. State = [[-0.10721461 -0.13884942]]. Action = [[ 0.05437833  0.14125475 -0.09970972 -0.8160946 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 8849 is [True, False, False, True, False, False]
Current timestep = 8850. State = [[-0.10712472 -0.13620302]]. Action = [[-0.09037462  0.07172677  0.1346119  -0.35909003]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 8850 is [True, False, False, True, False, False]
Current timestep = 8851. State = [[-0.10709666 -0.13367483]]. Action = [[-0.01572271 -0.01479542 -0.21061976 -0.5200812 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 8851 is [True, False, False, True, False, False]
Scene graph at timestep 8851 is [True, False, False, True, False, False]
State prediction error at timestep 8851 is tensor(7.8417e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8852. State = [[-0.10714757 -0.13228786]]. Action = [[ 0.00233012 -0.17683092 -0.13818198  0.3187914 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 8852 is [True, False, False, True, False, False]
Current timestep = 8853. State = [[-0.1071927  -0.13257353]]. Action = [[ 0.05241758  0.13724342 -0.11653396 -0.19737363]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 8853 is [True, False, False, True, False, False]
Scene graph at timestep 8853 is [True, False, False, True, False, False]
State prediction error at timestep 8853 is tensor(2.8195e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8854. State = [[-0.10731273 -0.13219139]]. Action = [[-0.09051022 -0.2110903   0.01736242  0.49509382]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 8854 is [True, False, False, True, False, False]
Current timestep = 8855. State = [[-0.10732536 -0.13334523]]. Action = [[-0.04716538 -0.06887095 -0.05789289 -0.76522994]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 8855 is [True, False, False, True, False, False]
Current timestep = 8856. State = [[-0.10777511 -0.13500486]]. Action = [[-0.10299087 -0.15594342 -0.2344429  -0.06837696]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 8856 is [True, False, False, True, False, False]
Human Feedback received at timestep 8856 of 1
Current timestep = 8857. State = [[-0.10866422 -0.13830903]]. Action = [[-0.17037758 -0.02129546 -0.13067853  0.35997474]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 8857 is [True, False, False, True, False, False]
Current timestep = 8858. State = [[-0.11079676 -0.1414199 ]]. Action = [[-0.07736173  0.0302622   0.03385136 -0.07558817]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 8858 is [True, False, False, True, False, False]
Current timestep = 8859. State = [[-0.11275369 -0.14310993]]. Action = [[-0.01017796  0.08744988 -0.09250921 -0.3946134 ]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 8859 is [True, False, False, True, False, False]
Current timestep = 8860. State = [[-0.11430694 -0.14307794]]. Action = [[-0.09914806 -0.18758614 -0.02718918  0.24373579]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 8860 is [True, False, False, True, False, False]
Human Feedback received at timestep 8860 of -1
Current timestep = 8861. State = [[-0.11740605 -0.14506403]]. Action = [[-0.0058926  -0.216249   -0.18515639 -0.631178  ]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 8861 is [True, False, False, True, False, False]
Current timestep = 8862. State = [[-0.12018734 -0.1489413 ]]. Action = [[ 0.0169313  -0.16189712  0.00850248 -0.21733719]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 8862 is [True, False, False, True, False, False]
Human Feedback received at timestep 8862 of -1
Current timestep = 8863. State = [[-0.12212721 -0.15276323]]. Action = [[-0.1012134   0.15904433 -0.1907064  -0.9306168 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 8863 is [True, False, False, True, False, False]
Scene graph at timestep 8863 is [True, False, False, True, False, False]
State prediction error at timestep 8863 is tensor(2.0757e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8864. State = [[-0.12421801 -0.1538862 ]]. Action = [[-0.0524572   0.17964065 -0.01876916 -0.1971947 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 8864 is [True, False, False, True, False, False]
Human Feedback received at timestep 8864 of -1
Current timestep = 8865. State = [[-0.12568279 -0.15277247]]. Action = [[ 0.1158185  -0.07006404 -0.00322385  0.40692067]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 8865 is [True, False, False, True, False, False]
Human Feedback received at timestep 8865 of -1
Current timestep = 8866. State = [[-0.12597045 -0.15264402]]. Action = [[ 0.01672277  0.04376078 -0.12684162 -0.5229087 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 8866 is [True, False, False, True, False, False]
Human Feedback received at timestep 8866 of -1
Current timestep = 8867. State = [[-0.12613918 -0.15258756]]. Action = [[-0.12221384 -0.09099972 -0.16271602 -0.70562124]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 8867 is [True, False, False, True, False, False]
Current timestep = 8868. State = [[-0.12742428 -0.15319212]]. Action = [[-0.00235771  0.00369075 -0.16271128  0.78914404]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 8868 is [True, False, False, True, False, False]
Human Feedback received at timestep 8868 of -1
Current timestep = 8869. State = [[-0.12868096 -0.15319455]]. Action = [[ 0.06272286 -0.07693651 -0.03178807  0.7279655 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 8869 is [True, False, False, True, False, False]
Current timestep = 8870. State = [[-0.12957324 -0.15392812]]. Action = [[ 0.02740356 -0.22603521 -0.17643683 -0.22872728]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 8870 is [True, False, False, True, False, False]
Current timestep = 8871. State = [[-0.13017413 -0.15694761]]. Action = [[ 0.09959713  0.0118627  -0.06561072 -0.7636186 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 8871 is [True, False, False, True, False, False]
Current timestep = 8872. State = [[-0.13050541 -0.15846461]]. Action = [[ 0.12098879 -0.16958445 -0.13511309 -0.59382474]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 8872 is [True, False, False, True, False, False]
Human Feedback received at timestep 8872 of -1
Current timestep = 8873. State = [[-0.130497  -0.1609908]]. Action = [[-0.12222159 -0.09986672 -0.09473406 -0.43796158]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 8873 is [True, False, False, True, False, False]
Current timestep = 8874. State = [[-0.13088848 -0.16403632]]. Action = [[-0.10984698  0.20085245 -0.13944201 -0.63094383]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 8874 is [True, False, False, True, False, False]
Human Feedback received at timestep 8874 of -1
Current timestep = 8875. State = [[-0.13107441 -0.16432145]]. Action = [[-0.06973898  0.1427908  -0.15610173 -0.8223452 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 8875 is [True, False, False, True, False, False]
Current timestep = 8876. State = [[-0.13181712 -0.1634295 ]]. Action = [[-0.01983784 -0.2150575   0.06027603  0.63176525]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 8876 is [True, False, False, True, False, False]
Scene graph at timestep 8876 is [True, False, False, True, False, False]
State prediction error at timestep 8876 is tensor(2.8072e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8876 of -1
Current timestep = 8877. State = [[-0.13240862 -0.16498818]]. Action = [[-0.04271421 -0.21439874 -0.21216449 -0.26292038]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 8877 is [True, False, False, True, False, False]
Scene graph at timestep 8877 is [True, False, False, True, False, False]
State prediction error at timestep 8877 is tensor(1.1868e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8878. State = [[-0.13378263 -0.16896664]]. Action = [[-0.13281588  0.05067801 -0.17216842 -0.08224547]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 8878 is [True, False, False, True, False, False]
Current timestep = 8879. State = [[-0.13554737 -0.17095356]]. Action = [[-0.08870924  0.15250242 -0.24571237 -0.49104106]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 8879 is [True, False, False, True, False, False]
Scene graph at timestep 8879 is [True, False, False, True, False, False]
State prediction error at timestep 8879 is tensor(4.8422e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8880. State = [[-0.13737893 -0.17091118]]. Action = [[-0.01320219  0.11720097 -0.11591554  0.62565875]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 8880 is [True, False, False, True, False, False]
Human Feedback received at timestep 8880 of -1
Current timestep = 8881. State = [[-0.13841167 -0.1698293 ]]. Action = [[-0.05226383  0.05137762 -0.17375186 -0.4869764 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 8881 is [True, False, False, True, False, False]
Current timestep = 8882. State = [[-0.13928914 -0.16892335]]. Action = [[-0.03352755  0.02792898 -0.08458558 -0.04408556]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 8882 is [True, False, False, True, False, False]
Current timestep = 8883. State = [[-0.13999641 -0.16828033]]. Action = [[-0.09790534  0.11562747 -0.19979721 -0.69801676]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 8883 is [True, False, False, True, False, False]
Scene graph at timestep 8883 is [True, False, False, True, False, False]
State prediction error at timestep 8883 is tensor(3.7108e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8884. State = [[-0.1411622  -0.16636854]]. Action = [[-0.03886755  0.21228379  0.12500441 -0.56811535]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 8884 is [True, False, False, True, False, False]
Current timestep = 8885. State = [[-0.14203721 -0.16223325]]. Action = [[ 0.04726171  0.00135681  0.04543653 -0.3614459 ]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 8885 is [True, False, False, True, False, False]
Current timestep = 8886. State = [[-0.14204755 -0.1594389 ]]. Action = [[ 0.10508114  0.14669037 -0.07065533 -0.73546845]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 8886 is [True, False, False, True, False, False]
Current timestep = 8887. State = [[-0.14189136 -0.15587972]]. Action = [[ 0.07445306 -0.07093927 -0.10429019 -0.3537445 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 8887 is [True, False, False, True, False, False]
Current timestep = 8888. State = [[-0.1419291  -0.15430975]]. Action = [[ 0.00365931 -0.03152341  0.1257183  -0.6384515 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 8888 is [True, False, False, True, False, False]
Current timestep = 8889. State = [[-0.14197177 -0.15399003]]. Action = [[-0.02107416  0.07759663 -0.12645605  0.0132798 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 8889 is [True, False, False, True, False, False]
Scene graph at timestep 8889 is [True, False, False, True, False, False]
State prediction error at timestep 8889 is tensor(1.6813e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8890. State = [[-0.14198677 -0.1528302 ]]. Action = [[ 0.0028927   0.1046809  -0.05655043  0.68739486]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 8890 is [True, False, False, True, False, False]
Current timestep = 8891. State = [[-0.14198308 -0.15077516]]. Action = [[0.11713508 0.07359949 0.06301787 0.3847592 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 8891 is [True, False, False, True, False, False]
Human Feedback received at timestep 8891 of 1
Current timestep = 8892. State = [[-0.1419963  -0.14824349]]. Action = [[ 0.15133107 -0.08075507 -0.21649167 -0.07612318]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 8892 is [True, False, False, True, False, False]
Current timestep = 8893. State = [[-0.14191554 -0.14755309]]. Action = [[-0.05567613 -0.17778398 -0.2007803  -0.7360878 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 8893 is [True, False, False, True, False, False]
Scene graph at timestep 8893 is [True, False, False, True, False, False]
State prediction error at timestep 8893 is tensor(4.3121e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8893 of 1
Current timestep = 8894. State = [[-0.14180131 -0.14772537]]. Action = [[ 0.02349633 -0.0201731  -0.09740335 -0.76899225]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 8894 is [True, False, False, True, False, False]
Current timestep = 8895. State = [[-0.14169228 -0.14809316]]. Action = [[ 0.17304263 -0.10638359 -0.0916039   0.07871556]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 8895 is [True, False, False, True, False, False]
Scene graph at timestep 8895 is [True, False, False, True, False, False]
State prediction error at timestep 8895 is tensor(5.2132e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8895 of 1
Current timestep = 8896. State = [[-0.1406234  -0.14876421]]. Action = [[ 0.04464447  0.03647047 -0.12223476  0.16806757]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 8896 is [True, False, False, True, False, False]
Scene graph at timestep 8896 is [True, False, False, True, False, False]
State prediction error at timestep 8896 is tensor(1.2107e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8897. State = [[-0.13995674 -0.14867297]]. Action = [[ 0.02032363 -0.11779235 -0.10053366 -0.75427866]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 8897 is [True, False, False, True, False, False]
Current timestep = 8898. State = [[-0.13941477 -0.1500489 ]]. Action = [[ 0.06051639 -0.09911606 -0.10251631 -0.4355054 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 8898 is [True, False, False, True, False, False]
Human Feedback received at timestep 8898 of 1
Current timestep = 8899. State = [[-0.13843149 -0.15173063]]. Action = [[ 0.05920669  0.15599298 -0.14172022 -0.25831246]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 8899 is [True, False, False, True, False, False]
Current timestep = 8900. State = [[-0.13732684 -0.15176696]]. Action = [[ 0.0359911  -0.06179497  0.14015824 -0.10962844]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 8900 is [True, False, False, True, False, False]
Scene graph at timestep 8900 is [True, False, False, True, False, False]
State prediction error at timestep 8900 is tensor(2.6456e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8900 of 1
Current timestep = 8901. State = [[-0.1366748  -0.15222295]]. Action = [[-0.00926383 -0.16052559 -0.19618097 -0.49188554]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 8901 is [True, False, False, True, False, False]
Current timestep = 8902. State = [[-0.13636167 -0.15389065]]. Action = [[ 0.00064754 -0.0561395  -0.09299126 -0.5273734 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 8902 is [True, False, False, True, False, False]
Current timestep = 8903. State = [[-0.13603051 -0.15589325]]. Action = [[ 0.01350349  0.03683016 -0.20620652 -0.35357457]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 8903 is [True, False, False, True, False, False]
Current timestep = 8904. State = [[-0.13597341 -0.15649548]]. Action = [[-0.02461866  0.00370255  0.05713531 -0.7354424 ]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 8904 is [True, False, False, True, False, False]
Current timestep = 8905. State = [[-0.13606265 -0.15710539]]. Action = [[-0.10593504 -0.17007695 -0.18523858 -0.12087828]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 8905 is [True, False, False, True, False, False]
Scene graph at timestep 8905 is [True, False, False, True, False, False]
State prediction error at timestep 8905 is tensor(4.2837e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8906. State = [[-0.1362422  -0.16030121]]. Action = [[-0.19653161 -0.09388417 -0.13916123 -0.34264827]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 8906 is [True, False, False, True, False, False]
Current timestep = 8907. State = [[-0.13673602 -0.16352816]]. Action = [[-0.15239602  0.04285276 -0.10103029 -0.30886686]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 8907 is [True, False, False, True, False, False]
Current timestep = 8908. State = [[-0.13751072 -0.16551945]]. Action = [[-0.13702515  0.02194667  0.05402184 -0.7145897 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 8908 is [True, False, False, True, False, False]
Scene graph at timestep 8908 is [True, False, False, True, False, False]
State prediction error at timestep 8908 is tensor(1.2438e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8908 of 1
Current timestep = 8909. State = [[-0.1391684  -0.16675334]]. Action = [[-0.21081713 -0.13700056  0.01580858  0.51269484]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 8909 is [True, False, False, True, False, False]
Current timestep = 8910. State = [[-0.14202219 -0.17022265]]. Action = [[ 0.01974401 -0.15964568 -0.12839758  0.6391454 ]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 8910 is [True, False, False, True, False, False]
Scene graph at timestep 8910 is [True, False, False, True, False, False]
State prediction error at timestep 8910 is tensor(1.4929e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8911. State = [[-0.14386508 -0.17468008]]. Action = [[-0.08297716 -0.07102767  0.1623289   0.13032389]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 8911 is [True, False, False, True, False, False]
Scene graph at timestep 8911 is [True, False, False, True, False, False]
State prediction error at timestep 8911 is tensor(1.5777e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8911 of -1
Current timestep = 8912. State = [[-0.14632426 -0.17875685]]. Action = [[-0.08739717 -0.04622179  0.07220191 -0.65786314]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 8912 is [True, False, False, True, False, False]
Human Feedback received at timestep 8912 of -1
Current timestep = 8913. State = [[-0.14806657 -0.18131606]]. Action = [[-0.15382415  0.24264684 -0.16155666  0.20449483]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 8913 is [True, False, False, True, False, False]
Current timestep = 8914. State = [[-0.1505179  -0.18053152]]. Action = [[-0.09107009 -0.09897703  0.06802192  0.598675  ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 8914 is [True, False, False, True, False, False]
Scene graph at timestep 8914 is [True, False, False, True, False, False]
State prediction error at timestep 8914 is tensor(6.1039e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8914 of -1
Current timestep = 8915. State = [[-0.15441743 -0.18068844]]. Action = [[ 0.05843645  0.20928359  0.05227509 -0.80897814]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 8915 is [True, False, False, True, False, False]
Scene graph at timestep 8915 is [True, False, False, True, False, False]
State prediction error at timestep 8915 is tensor(2.3888e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8915 of -1
Current timestep = 8916. State = [[-0.1565708  -0.17923747]]. Action = [[-0.05480193 -0.02997507 -0.1455155  -0.253389  ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 8916 is [True, False, False, True, False, False]
Current timestep = 8917. State = [[-0.15800412 -0.17860408]]. Action = [[-0.0354977   0.08006433 -0.13331981 -0.35476965]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 8917 is [True, False, False, True, False, False]
Human Feedback received at timestep 8917 of -1
Current timestep = 8918. State = [[-0.15929651 -0.17772415]]. Action = [[-0.10998616 -0.00349583  0.03220451  0.92617416]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 8918 is [True, False, False, True, False, False]
Current timestep = 8919. State = [[-0.16117871 -0.1767104 ]]. Action = [[ 0.00892156  0.07540679 -0.24108429  0.68757343]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 8919 is [True, False, False, True, False, False]
Human Feedback received at timestep 8919 of -1
Current timestep = 8920. State = [[-0.16334984 -0.17570178]]. Action = [[ 0.03901085 -0.1339526   0.16043979 -0.81489414]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 8920 is [True, False, False, True, False, False]
Current timestep = 8921. State = [[-0.16463324 -0.175536  ]]. Action = [[ 0.08958554 -0.08258043 -0.1795831  -0.8139766 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 8921 is [True, False, False, True, False, False]
Current timestep = 8922. State = [[-0.16455185 -0.17581305]]. Action = [[ 0.03162122 -0.07545245 -0.23361528 -0.5557214 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 8922 is [True, False, False, True, False, False]
Current timestep = 8923. State = [[-0.16469744 -0.1769602 ]]. Action = [[-0.03431332 -0.20743753 -0.06447314 -0.16543174]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 8923 is [True, False, False, True, False, False]
Human Feedback received at timestep 8923 of -1
Current timestep = 8924. State = [[-0.16512276 -0.18006709]]. Action = [[-0.05689213  0.18286073 -0.06079632  0.44250786]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 8924 is [True, False, False, True, False, False]
Current timestep = 8925. State = [[-0.16528201 -0.1802839 ]]. Action = [[-0.0443202   0.04111177 -0.11393885 -0.75500107]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 8925 is [True, False, False, True, False, False]
Scene graph at timestep 8925 is [True, False, False, True, False, False]
State prediction error at timestep 8925 is tensor(1.8475e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8925 of -1
Current timestep = 8926. State = [[-0.16594937 -0.18035431]]. Action = [[-0.15846318 -0.11433339  0.12165195  0.11381817]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 8926 is [True, False, False, True, False, False]
Human Feedback received at timestep 8926 of -1
Current timestep = 8927. State = [[-0.16788836 -0.1819393 ]]. Action = [[-0.04091717  0.00282472 -0.07416457  0.26075077]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 8927 is [True, False, False, True, False, False]
Scene graph at timestep 8927 is [True, False, False, True, False, False]
State prediction error at timestep 8927 is tensor(3.6149e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8928. State = [[-0.16962911 -0.18316256]]. Action = [[-0.11508566  0.14103681  0.15615118 -0.35094517]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 8928 is [True, False, False, True, False, False]
Human Feedback received at timestep 8928 of -1
Current timestep = 8929. State = [[-0.17126748 -0.18245539]]. Action = [[-0.09679987  0.11134318 -0.23197427  0.4433849 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 8929 is [True, False, False, True, False, False]
Current timestep = 8930. State = [[-0.17288736 -0.18123223]]. Action = [[ 0.01194113 -0.09377962 -0.09141314 -0.48265088]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 8930 is [True, False, False, True, False, False]
Scene graph at timestep 8930 is [True, False, False, True, False, False]
State prediction error at timestep 8930 is tensor(3.5245e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8930 of -1
Current timestep = 8931. State = [[-0.17421706 -0.1813919 ]]. Action = [[-0.15512832 -0.08034053 -0.07897739  0.45082295]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 8931 is [True, False, False, True, False, False]
Current timestep = 8932. State = [[-0.1763939 -0.1825278]]. Action = [[-0.08276175 -0.11580406 -0.23410918 -0.4264806 ]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 8932 is [True, False, False, True, False, False]
Human Feedback received at timestep 8932 of -1
Current timestep = 8933. State = [[-0.17965873 -0.18368466]]. Action = [[ 0.12204605  0.14557213 -0.08570306 -0.03746635]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 8933 is [True, False, False, True, False, False]
Current timestep = 8934. State = [[-0.18199033 -0.18255791]]. Action = [[ 0.02107018  0.17510426  0.06880173 -0.527763  ]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 8934 is [True, False, False, True, False, False]
Human Feedback received at timestep 8934 of -1
Current timestep = 8935. State = [[-0.18368039 -0.18073201]]. Action = [[ 0.12023726 -0.10800552 -0.2111286  -0.8478523 ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 8935 is [True, False, False, True, False, False]
Human Feedback received at timestep 8935 of -1
Current timestep = 8936. State = [[-0.18398024 -0.1801804 ]]. Action = [[-0.11506221 -0.00130758 -0.17757641 -0.57686347]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 8936 is [True, False, False, True, False, False]
Current timestep = 8937. State = [[-0.18456487 -0.1802312 ]]. Action = [[-0.12994982 -0.13918616 -0.21242718  0.8710296 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 8937 is [True, False, False, True, False, False]
Human Feedback received at timestep 8937 of -1
Current timestep = 8938. State = [[-0.1868293  -0.18137302]]. Action = [[-0.21001936 -0.11300674  0.07081306 -0.3058722 ]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 8938 is [True, False, False, True, False, False]
Current timestep = 8939. State = [[-0.18994017 -0.18312298]]. Action = [[ 0.064695   -0.12396142  0.14634159 -0.28722346]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 8939 is [True, False, False, True, False, False]
Current timestep = 8940. State = [[-0.19171534 -0.18529542]]. Action = [[-0.03445674 -0.12663867  0.05255601  0.39695072]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 8940 is [True, False, False, True, False, False]
Scene graph at timestep 8940 is [True, False, False, True, False, False]
State prediction error at timestep 8940 is tensor(6.9110e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8940 of -1
Current timestep = 8941. State = [[-0.1933519  -0.18883403]]. Action = [[ 0.0226669   0.10302091 -0.13614936 -0.0979948 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 8941 is [True, False, False, True, False, False]
Current timestep = 8942. State = [[-0.1943411  -0.19045188]]. Action = [[-0.19372118 -0.13105269 -0.23433267  0.7942543 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 8942 is [True, False, False, True, False, False]
Human Feedback received at timestep 8942 of -1
Current timestep = 8943. State = [[-0.19700587 -0.1933813 ]]. Action = [[-0.11040184 -0.21142316  0.14296573 -0.34374905]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 8943 is [True, False, False, True, False, False]
Current timestep = 8944. State = [[-0.1994938  -0.19740205]]. Action = [[-5.6309104e-02 -8.6903572e-05 -2.4216898e-01  8.1156468e-01]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 8944 is [True, False, False, True, False, False]
Scene graph at timestep 8944 is [True, False, False, True, False, False]
State prediction error at timestep 8944 is tensor(1.0364e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8944 of -1
Current timestep = 8945. State = [[-0.20269158 -0.20067829]]. Action = [[-0.18384974  0.16581643 -0.11650196  0.5618279 ]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 8945 is [True, False, False, True, False, False]
Scene graph at timestep 8945 is [True, False, False, True, False, False]
State prediction error at timestep 8945 is tensor(1.4232e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8945 of -1
Current timestep = 8946. State = [[-0.20542873 -0.20209706]]. Action = [[-0.20700637 -0.24658427  0.05721822 -0.1224705 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 8946 is [True, False, False, True, False, False]
Current timestep = 8947. State = [[-0.20914273 -0.20549205]]. Action = [[-0.10368703  0.1431377  -0.164926   -0.5707667 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 8947 is [True, False, False, True, False, False]
Scene graph at timestep 8947 is [True, False, False, True, False, False]
State prediction error at timestep 8947 is tensor(9.6178e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8948. State = [[-0.21359737 -0.20573072]]. Action = [[ 0.02303812 -0.06619087  0.2003147   0.68407583]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 8948 is [True, False, False, True, False, False]
Scene graph at timestep 8948 is [True, False, False, True, False, False]
State prediction error at timestep 8948 is tensor(1.0021e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8948 of -1
Current timestep = 8949. State = [[-0.21819825 -0.20541501]]. Action = [[-0.18551143 -0.1257959   0.040369    0.01626909]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 8949 is [True, False, False, True, False, False]
Human Feedback received at timestep 8949 of -1
Current timestep = 8950. State = [[-0.22330025 -0.20691514]]. Action = [[-1.5266815e-01 -1.5213054e-01 -7.9125166e-05 -6.6590422e-01]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 8950 is [True, False, False, True, False, False]
Current timestep = 8951. State = [[-0.22879797 -0.21044296]]. Action = [[-0.15267178  0.16761315 -0.19892114 -0.54019266]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 8951 is [True, False, False, True, False, False]
Current timestep = 8952. State = [[-0.23319063 -0.2114675 ]]. Action = [[-0.18637519  0.19705802 -0.14740239 -0.39344215]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 8952 is [True, False, False, True, False, False]
Human Feedback received at timestep 8952 of -1
Current timestep = 8953. State = [[-0.23755443 -0.2097303 ]]. Action = [[-0.1469393   0.2437334   0.04785556 -0.96369284]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 8953 is [True, False, False, True, False, False]
Human Feedback received at timestep 8953 of -1
Current timestep = 8954. State = [[-0.24417691 -0.20567514]]. Action = [[-0.17516111  0.09772727  0.06594914 -0.9821006 ]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 8954 is [True, False, False, True, False, False]
Current timestep = 8955. State = [[-0.2511316 -0.2022871]]. Action = [[-0.1478746  -0.22502805  0.07541978 -0.7026593 ]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 8955 is [True, False, False, True, False, False]
Human Feedback received at timestep 8955 of -1
Current timestep = 8956. State = [[-0.25623363 -0.20186523]]. Action = [[ 0.03507414 -0.23311853  0.02539438  0.87099195]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 8956 is [True, False, False, True, False, False]
Current timestep = 8957. State = [[-0.26017293 -0.20384258]]. Action = [[ 0.01622221  0.06787392 -0.01151656 -0.10113668]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 8957 is [True, False, False, True, False, False]
Scene graph at timestep 8957 is [True, False, False, True, False, False]
State prediction error at timestep 8957 is tensor(1.0125e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8958. State = [[-0.2633916  -0.20434484]]. Action = [[-0.12752444  0.02132171 -0.11380264  0.8309641 ]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 8958 is [True, False, False, True, False, False]
Human Feedback received at timestep 8958 of -1
Current timestep = 8959. State = [[-0.26689595 -0.20477526]]. Action = [[ 0.00141796 -0.08428574 -0.05899945  0.68513775]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 8959 is [True, False, False, True, False, False]
Current timestep = 8960. State = [[-0.26898214 -0.20570831]]. Action = [[ 0.05684754  0.22761571 -0.1466059  -0.02996862]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 8960 is [True, False, False, True, False, False]
Scene graph at timestep 8960 is [True, False, False, True, False, False]
State prediction error at timestep 8960 is tensor(3.1475e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8960 of -1
Current timestep = 8961. State = [[-0.2697445  -0.20462134]]. Action = [[-0.11028826  0.18057662 -0.08085573  0.61076975]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 8961 is [True, False, False, True, False, False]
Scene graph at timestep 8961 is [True, False, False, True, False, False]
State prediction error at timestep 8961 is tensor(6.0521e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8961 of -1
Current timestep = 8962. State = [[-0.27182508 -0.20196329]]. Action = [[-0.18767887 -0.03082009  0.04657981 -0.68159413]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 8962 is [True, False, False, True, False, False]
Scene graph at timestep 8962 is [True, False, False, True, False, False]
State prediction error at timestep 8962 is tensor(1.0794e-05, grad_fn=<MseLossBackward0>)
Current timestep = 8963. State = [[-0.27295464 -0.20087737]]. Action = [[-0.18148744  0.19900697 -0.07277352 -0.2136122 ]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 8963 is [True, False, False, True, False, False]
Human Feedback received at timestep 8963 of -1
Current timestep = 8964. State = [[-0.2737144  -0.20031504]]. Action = [[ 0.10233349 -0.17674841 -0.06065083 -0.7610395 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 8964 is [True, False, False, True, False, False]
Current timestep = 8965. State = [[-0.27370694 -0.20039555]]. Action = [[-0.03035341  0.15822709 -0.20802636  0.17886734]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 8965 is [True, False, False, True, False, False]
Scene graph at timestep 8965 is [True, False, False, True, False, False]
State prediction error at timestep 8965 is tensor(8.2258e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8965 of -1
Current timestep = 8966. State = [[-0.2737819  -0.19998509]]. Action = [[ 0.05810675 -0.01919731 -0.17685103 -0.8826593 ]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 8966 is [True, False, False, True, False, False]
Current timestep = 8967. State = [[-0.27383643 -0.19982329]]. Action = [[ 0.05487132  0.10853639 -0.21038072  0.3224826 ]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 8967 is [True, False, False, True, False, False]
Current timestep = 8968. State = [[-0.27344775 -0.19852303]]. Action = [[ 0.05311999 -0.17304794 -0.2171982  -0.9713811 ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 8968 is [True, False, False, True, False, False]
Scene graph at timestep 8968 is [True, False, False, True, False, False]
State prediction error at timestep 8968 is tensor(1.6595e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8968 of -1
Current timestep = 8969. State = [[-0.27333716 -0.1986731 ]]. Action = [[-3.8704753e-02 -6.3046813e-05 -9.8486990e-02  6.1802268e-01]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 8969 is [True, False, False, True, False, False]
Current timestep = 8970. State = [[-0.27333716 -0.1986731 ]]. Action = [[-0.19307937  0.07165405 -0.2097568  -0.87236166]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 8970 is [True, False, False, True, False, False]
Current timestep = 8971. State = [[-0.27334216 -0.19871762]]. Action = [[ 0.10014981 -0.02692574 -0.1365249   0.1720972 ]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 8971 is [True, False, False, True, False, False]
Human Feedback received at timestep 8971 of -1
Current timestep = 8972. State = [[-0.27306402 -0.198791  ]]. Action = [[-0.07073176 -0.14252925 -0.09210193  0.04002559]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 8972 is [True, False, False, True, False, False]
Current timestep = 8973. State = [[-0.27273652 -0.19946864]]. Action = [[-0.00337048  0.03105587 -0.20405035  0.3551618 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 8973 is [True, False, False, True, False, False]
Current timestep = 8974. State = [[-0.27256173 -0.19955266]]. Action = [[-0.05637713  0.12957639 -0.10519129  0.23371315]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 8974 is [True, False, False, True, False, False]
Scene graph at timestep 8974 is [True, False, False, True, False, False]
State prediction error at timestep 8974 is tensor(2.3920e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8974 of -1
Current timestep = 8975. State = [[-0.27256322 -0.19959971]]. Action = [[-0.07896078 -0.12482893  0.00455731 -0.68151695]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 8975 is [True, False, False, True, False, False]
Current timestep = 8976. State = [[-0.27280974 -0.20003684]]. Action = [[ 0.03291798 -0.01908913 -0.086299    0.50238645]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 8976 is [True, False, False, True, False, False]
Human Feedback received at timestep 8976 of -1
Current timestep = 8977. State = [[-0.27300528 -0.20049381]]. Action = [[-0.06282946 -0.1048235  -0.06321883 -0.9476834 ]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 8977 is [True, False, False, True, False, False]
Current timestep = 8978. State = [[-0.27345034 -0.20176731]]. Action = [[-0.01702167 -0.0966865  -0.0436772  -0.79156506]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 8978 is [True, False, False, True, False, False]
Human Feedback received at timestep 8978 of -1
Current timestep = 8979. State = [[-0.2741338  -0.20406997]]. Action = [[-0.05109695 -0.17923906 -0.1782325   0.6498766 ]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 8979 is [True, False, False, True, False, False]
Current timestep = 8980. State = [[-0.27486658 -0.20737164]]. Action = [[0.02052918 0.03921068 0.04231256 0.6143551 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 8980 is [True, False, False, True, False, False]
Scene graph at timestep 8980 is [True, False, False, True, False, False]
State prediction error at timestep 8980 is tensor(8.7730e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8981. State = [[-0.27524814 -0.20907514]]. Action = [[-0.11947785 -0.19880426 -0.10050336 -0.33684742]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 8981 is [True, False, False, True, False, False]
Human Feedback received at timestep 8981 of -1
Current timestep = 8982. State = [[-0.27569884 -0.20981325]]. Action = [[ 0.02977452  0.22786856 -0.21047713  0.84081817]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 8982 is [True, False, False, True, False, False]
Current timestep = 8983. State = [[-0.27574345 -0.20957406]]. Action = [[ 0.03232121 -0.1782968  -0.21972358 -0.9430772 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 8983 is [True, False, False, True, False, False]
Scene graph at timestep 8983 is [True, False, False, True, False, False]
State prediction error at timestep 8983 is tensor(1.6832e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8983 of -1
Current timestep = 8984. State = [[-0.27577794 -0.2096465 ]]. Action = [[-2.0494935e-01  1.6463280e-01 -1.9203126e-04  1.5920520e-02]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 8984 is [True, False, False, True, False, False]
Current timestep = 8985. State = [[-0.27583846 -0.2098431 ]]. Action = [[ 0.02787837 -0.20071843  0.08961335 -0.9416107 ]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 8985 is [True, False, False, True, False, False]
Human Feedback received at timestep 8985 of -1
Current timestep = 8986. State = [[-0.27584112 -0.21200767]]. Action = [[-0.17861925 -0.10862976 -0.11381762 -0.7339925 ]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 8986 is [True, False, False, True, False, False]
Current timestep = 8987. State = [[-0.27599725 -0.21372145]]. Action = [[-0.13316846 -0.14399557 -0.20131172  0.9410738 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 8987 is [True, False, False, True, False, False]
Scene graph at timestep 8987 is [True, False, False, True, False, False]
State prediction error at timestep 8987 is tensor(2.4695e-06, grad_fn=<MseLossBackward0>)
Current timestep = 8988. State = [[-0.2757593  -0.21498702]]. Action = [[-0.23524359 -0.05670366  0.13754827 -0.80631256]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 8988 is [True, False, False, True, False, False]
Current timestep = 8989. State = [[-0.27547273 -0.21572302]]. Action = [[-0.20589842 -0.2363786  -0.1303934  -0.61235404]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 8989 is [True, False, False, True, False, False]
Human Feedback received at timestep 8989 of -1
Current timestep = 8990. State = [[-0.2751707 -0.2163252]]. Action = [[-0.06981695  0.15662152 -0.22714943  0.07740664]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 8990 is [True, False, False, True, False, False]
Current timestep = 8991. State = [[-0.2751733  -0.21660973]]. Action = [[ 0.01105529  0.13388407 -0.0705452   0.6400435 ]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 8991 is [True, False, False, True, False, False]
Current timestep = 8992. State = [[-0.27510327 -0.21651542]]. Action = [[-0.0708141   0.09853074 -0.23615335  0.8559439 ]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 8992 is [True, False, False, True, False, False]
Scene graph at timestep 8992 is [True, False, False, True, False, False]
State prediction error at timestep 8992 is tensor(1.0194e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8992 of -1
Current timestep = 8993. State = [[-0.27510327 -0.21651542]]. Action = [[-0.1303924  -0.23211586 -0.09108934  0.61334765]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 8993 is [True, False, False, True, False, False]
Current timestep = 8994. State = [[-0.27510327 -0.21651542]]. Action = [[-0.13953054  0.04538971 -0.20194104  0.9937097 ]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 8994 is [True, False, False, True, False, False]
Current timestep = 8995. State = [[-0.27510327 -0.21651542]]. Action = [[-0.11452708 -0.20388167 -0.22848594  0.775342  ]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 8995 is [True, False, False, True, False, False]
Scene graph at timestep 8995 is [True, False, False, True, False, False]
State prediction error at timestep 8995 is tensor(7.6367e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8996. State = [[-0.27510327 -0.21651542]]. Action = [[-0.05716154 -0.17518046 -0.11078285  0.80507827]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 8996 is [True, False, False, True, False, False]
Current timestep = 8997. State = [[-0.27510327 -0.21651542]]. Action = [[-0.18375935  0.18523827  0.07923478 -0.12819749]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 8997 is [True, False, False, True, False, False]
Scene graph at timestep 8997 is [True, False, False, True, False, False]
State prediction error at timestep 8997 is tensor(4.7833e-07, grad_fn=<MseLossBackward0>)
Current timestep = 8998. State = [[-0.27510327 -0.21651542]]. Action = [[-0.08536911 -0.00742042 -0.09460467 -0.84810406]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 8998 is [True, False, False, True, False, False]
Current timestep = 8999. State = [[-0.27507395 -0.2165443 ]]. Action = [[ 0.07365125 -0.18873924 -0.23534627 -0.76045173]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 8999 is [True, False, False, True, False, False]
Current timestep = 9000. State = [[-0.27460298 -0.2171301 ]]. Action = [[-0.13951372  0.09971446  0.09083721  0.88717747]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 9000 is [True, False, False, True, False, False]
Current timestep = 9001. State = [[-0.274086   -0.21743038]]. Action = [[-0.07743749  0.19573322 -0.05908899 -0.8981367 ]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 9001 is [True, False, False, True, False, False]
Current timestep = 9002. State = [[-0.27371296 -0.21790831]]. Action = [[-0.16926226 -0.19467482 -0.22257265  0.12664568]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 9002 is [True, False, False, True, False, False]
Current timestep = 9003. State = [[-0.27375454 -0.21828471]]. Action = [[-0.18886735 -0.18318461 -0.16185208  0.2350148 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 9003 is [True, False, False, True, False, False]
Scene graph at timestep 9003 is [True, False, False, True, False, False]
State prediction error at timestep 9003 is tensor(1.0509e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9004. State = [[-0.27361843 -0.21838813]]. Action = [[-0.11926614 -0.08823057 -0.23632014 -0.42288095]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 9004 is [True, False, False, True, False, False]
Current timestep = 9005. State = [[-0.27361843 -0.21838813]]. Action = [[-0.03733495 -0.00441541 -0.24547671  0.15100026]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 9005 is [True, False, False, True, False, False]
Current timestep = 9006. State = [[-0.27371597 -0.21852997]]. Action = [[-0.11642915  0.05522704  0.21473908  0.77267766]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 9006 is [True, False, False, True, False, False]
Scene graph at timestep 9006 is [True, False, False, True, False, False]
State prediction error at timestep 9006 is tensor(1.7685e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9007. State = [[-0.27397543 -0.21906382]]. Action = [[ 0.01724985 -0.08503303 -0.11010715 -0.7882539 ]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 9007 is [True, False, False, True, False, False]
Current timestep = 9008. State = [[-0.27413973 -0.21986386]]. Action = [[-0.1612571   0.16552061 -0.18911584  0.45762622]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 9008 is [True, False, False, True, False, False]
Current timestep = 9009. State = [[-0.27442122 -0.22104146]]. Action = [[ 0.03933477 -0.24199598 -0.24874154 -0.61163366]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 9009 is [True, False, False, True, False, False]
Current timestep = 9010. State = [[-0.27474108 -0.22425367]]. Action = [[-0.1979038  -0.22831802 -0.24051806  0.83846986]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 9010 is [True, False, False, True, False, False]
Current timestep = 9011. State = [[-0.27498227 -0.22672315]]. Action = [[-0.19554968 -0.12343785 -0.14153858 -0.18775421]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 9011 is [True, False, False, True, False, False]
Human Feedback received at timestep 9011 of -1
Current timestep = 9012. State = [[-0.2752302  -0.22833316]]. Action = [[-0.08948819  0.1086576   0.06078061  0.4525969 ]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 9012 is [True, False, False, True, False, False]
Scene graph at timestep 9012 is [True, False, False, True, False, False]
State prediction error at timestep 9012 is tensor(4.9559e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9013. State = [[-0.27541324 -0.22917648]]. Action = [[-0.09816991  0.24604478 -0.24532966 -0.8529774 ]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 9013 is [True, False, False, True, False, False]
Human Feedback received at timestep 9013 of -1
Current timestep = 9014. State = [[-0.27544594 -0.22935414]]. Action = [[-0.12771577 -0.06876028 -0.12453869 -0.73895776]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 9014 is [True, False, False, True, False, False]
Current timestep = 9015. State = [[-0.27549216 -0.2296254 ]]. Action = [[-0.12940106  0.02848682  0.02987674  0.7372718 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 9015 is [True, False, False, True, False, False]
Current timestep = 9016. State = [[-0.27553418 -0.22971632]]. Action = [[-0.19789883  0.12533677 -0.24789995  0.6028073 ]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 9016 is [True, False, False, True, False, False]
Current timestep = 9017. State = [[-0.27553418 -0.22971632]]. Action = [[-0.17398988 -0.22444318 -0.22126019 -0.53342515]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 9017 is [True, False, False, True, False, False]
Current timestep = 9018. State = [[-0.27553418 -0.22971632]]. Action = [[-0.13759413  0.23191702 -0.20284042  0.3237381 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 9018 is [True, False, False, True, False, False]
Current timestep = 9019. State = [[-0.27553418 -0.22971632]]. Action = [[-0.19348067  0.17293504  0.16266781  0.9290421 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 9019 is [True, False, False, True, False, False]
Current timestep = 9020. State = [[-0.27553418 -0.22971632]]. Action = [[-0.04606643  0.14842159 -0.11870553  0.2071284 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 9020 is [True, False, False, True, False, False]
Current timestep = 9021. State = [[-0.27553418 -0.22971632]]. Action = [[-0.21258122  0.2313636   0.08552268  0.6795554 ]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 9021 is [True, False, False, True, False, False]
Current timestep = 9022. State = [[-0.27553418 -0.22971632]]. Action = [[-0.09337607  0.06337413 -0.17384066 -0.48651916]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 9022 is [True, False, False, True, False, False]
Current timestep = 9023. State = [[-0.27553418 -0.22971632]]. Action = [[-0.1361945   0.20654312 -0.1075014  -0.23972845]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 9023 is [True, False, False, True, False, False]
Current timestep = 9024. State = [[-0.27553418 -0.22971632]]. Action = [[-0.11804292 -0.152546    0.15189266  0.9036677 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 9024 is [True, False, False, True, False, False]
Current timestep = 9025. State = [[-0.27553418 -0.22971632]]. Action = [[-0.2154292  -0.24727018 -0.24440776 -0.6717282 ]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 9025 is [True, False, False, True, False, False]
Current timestep = 9026. State = [[-0.27553418 -0.22971632]]. Action = [[-0.16403702  0.16344076  0.0710707  -0.9732804 ]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 9026 is [True, False, False, True, False, False]
Current timestep = 9027. State = [[-0.27553418 -0.22971632]]. Action = [[-0.15184152 -0.1124649  -0.1482234   0.35106838]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 9027 is [True, False, False, True, False, False]
Scene graph at timestep 9027 is [True, False, False, True, False, False]
State prediction error at timestep 9027 is tensor(2.0431e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9028. State = [[-0.27553418 -0.22971632]]. Action = [[-0.19287162 -0.08922182  0.15634727  0.97471476]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 9028 is [True, False, False, True, False, False]
Current timestep = 9029. State = [[-0.27553418 -0.22971632]]. Action = [[-0.14781067 -0.2485786  -0.12449056  0.19529688]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 9029 is [True, False, False, True, False, False]
Scene graph at timestep 9029 is [True, False, False, True, False, False]
State prediction error at timestep 9029 is tensor(6.0514e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9030. State = [[-0.27553418 -0.22971632]]. Action = [[-0.18102793  0.06094876  0.00288364  0.3834586 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 9030 is [True, False, False, True, False, False]
Scene graph at timestep 9030 is [True, False, False, True, False, False]
State prediction error at timestep 9030 is tensor(3.1336e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9030 of 1
Current timestep = 9031. State = [[-0.27553418 -0.22971632]]. Action = [[-0.23541272  0.19150233  0.00512341  0.94388723]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 9031 is [True, False, False, True, False, False]
Current timestep = 9032. State = [[-0.27553418 -0.22971632]]. Action = [[-0.1352253  -0.13567741 -0.20976159  0.95397377]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 9032 is [True, False, False, True, False, False]
Human Feedback received at timestep 9032 of 1
Current timestep = 9033. State = [[-0.27553418 -0.22971632]]. Action = [[-0.05958813 -0.2249774  -0.10367188 -0.2610252 ]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 9033 is [True, False, False, True, False, False]
Human Feedback received at timestep 9033 of 1
Current timestep = 9034. State = [[-0.27553418 -0.22971632]]. Action = [[-0.11320516  0.2355833   0.24640411 -0.4549799 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 9034 is [True, False, False, True, False, False]
Scene graph at timestep 9034 is [True, False, False, True, False, False]
State prediction error at timestep 9034 is tensor(3.5218e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9035. State = [[-0.27553418 -0.22971632]]. Action = [[-0.0501201  -0.24547455 -0.20143847 -0.88726324]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 9035 is [True, False, False, True, False, False]
Current timestep = 9036. State = [[-0.27553418 -0.22971632]]. Action = [[-0.17885874  0.13646147  0.16400585 -0.75875217]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 9036 is [True, False, False, True, False, False]
Current timestep = 9037. State = [[-0.27553418 -0.22971632]]. Action = [[-9.6316487e-02  1.9330591e-01 -5.2307546e-04 -5.3337544e-01]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 9037 is [True, False, False, True, False, False]
Current timestep = 9038. State = [[-0.27553418 -0.22971632]]. Action = [[-0.16986421  0.01299345 -0.20333809 -0.9869045 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 9038 is [True, False, False, True, False, False]
Human Feedback received at timestep 9038 of 1
Current timestep = 9039. State = [[-0.27553418 -0.22971632]]. Action = [[-0.04614413  0.04744706 -0.24489489 -0.0040251 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 9039 is [True, False, False, True, False, False]
Scene graph at timestep 9039 is [True, False, False, True, False, False]
State prediction error at timestep 9039 is tensor(3.8779e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9040. State = [[-0.27559248 -0.2298959 ]]. Action = [[-0.03363293 -0.08300506  0.04232678 -0.9838591 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 9040 is [True, False, False, True, False, False]
Scene graph at timestep 9040 is [True, False, False, True, False, False]
State prediction error at timestep 9040 is tensor(4.7890e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9041. State = [[-0.27596503 -0.23080416]]. Action = [[ 0.01729366  0.13518935 -0.11658096  0.16620648]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 9041 is [True, False, False, True, False, False]
Human Feedback received at timestep 9041 of -1
Current timestep = 9042. State = [[-0.27597335 -0.23084842]]. Action = [[-0.207611    0.2398414  -0.06546426  0.12476194]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 9042 is [True, False, False, True, False, False]
Current timestep = 9043. State = [[-0.27597335 -0.23084842]]. Action = [[-0.2367275  -0.19837482  0.20632017 -0.7763959 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 9043 is [True, False, False, True, False, False]
Scene graph at timestep 9043 is [True, False, False, True, False, False]
State prediction error at timestep 9043 is tensor(2.0103e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9044. State = [[-0.27597335 -0.23084842]]. Action = [[-0.11999546  0.22180113 -0.2498004  -0.8344783 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 9044 is [True, False, False, True, False, False]
Current timestep = 9045. State = [[-0.27597335 -0.23084842]]. Action = [[-0.22917984 -0.05411816 -0.16767244 -0.7851022 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 9045 is [True, False, False, True, False, False]
Current timestep = 9046. State = [[-0.27597335 -0.23084842]]. Action = [[-0.043663   -0.14817801 -0.01121847 -0.9658505 ]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 9046 is [True, False, False, True, False, False]
Scene graph at timestep 9046 is [True, False, False, True, False, False]
State prediction error at timestep 9046 is tensor(3.2336e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9046 of 1
Current timestep = 9047. State = [[-0.27597335 -0.23084842]]. Action = [[-0.02534702 -0.10855913  0.1553354  -0.7166936 ]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 9047 is [True, False, False, True, False, False]
Scene graph at timestep 9047 is [True, False, False, True, False, False]
State prediction error at timestep 9047 is tensor(1.9226e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9048. State = [[-0.27609986 -0.23125239]]. Action = [[-0.08895051 -0.23624878 -0.08673292 -0.8982523 ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 9048 is [True, False, False, True, False, False]
Human Feedback received at timestep 9048 of 1
Current timestep = 9049. State = [[-0.2761422  -0.23134308]]. Action = [[-0.22226276 -0.167797   -0.04591276  0.95562315]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 9049 is [True, False, False, True, False, False]
Human Feedback received at timestep 9049 of 1
Current timestep = 9050. State = [[-0.27617586 -0.23152088]]. Action = [[-0.2138568  -0.17047817 -0.13330358 -0.2927025 ]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 9050 is [True, False, False, True, False, False]
Current timestep = 9051. State = [[-0.27623516 -0.23170042]]. Action = [[-0.19015458 -0.11703019 -0.06767997 -0.8270199 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 9051 is [True, False, False, True, False, False]
Human Feedback received at timestep 9051 of 1
Current timestep = 9052. State = [[-0.276218   -0.23174289]]. Action = [[-0.17748019 -0.2353797  -0.24826403  0.07637739]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 9052 is [True, False, False, True, False, False]
Current timestep = 9053. State = [[-0.2762517  -0.23192023]]. Action = [[-0.23384121  0.20337814 -0.11717036 -0.7731436 ]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 9053 is [True, False, False, True, False, False]
Human Feedback received at timestep 9053 of 1
Current timestep = 9054. State = [[-0.2763096  -0.23223865]]. Action = [[ 0.006127   -0.22744259 -0.11406457 -0.06009907]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 9054 is [True, False, False, True, False, False]
Scene graph at timestep 9054 is [True, False, False, True, False, False]
State prediction error at timestep 9054 is tensor(1.6017e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9055. State = [[-0.27670962 -0.23544976]]. Action = [[-0.15863721 -0.09008271 -0.22658248 -0.95059776]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 9055 is [True, False, False, True, False, False]
Scene graph at timestep 9055 is [True, False, False, True, False, False]
State prediction error at timestep 9055 is tensor(7.2895e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9055 of 1
Current timestep = 9056. State = [[-0.27727816 -0.23808447]]. Action = [[-0.17964934 -0.15275559 -0.14937721  0.6788385 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 9056 is [True, False, False, True, False, False]
Human Feedback received at timestep 9056 of 1
Current timestep = 9057. State = [[-0.27746332 -0.23945127]]. Action = [[-0.17779554 -0.12259954 -0.05851533 -0.9849702 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 9057 is [True, False, False, True, False, False]
Scene graph at timestep 9057 is [True, False, False, True, False, False]
State prediction error at timestep 9057 is tensor(3.1159e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9058. State = [[-0.2775316  -0.24021277]]. Action = [[-0.04865642 -0.04756433 -0.14601052  0.1394167 ]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 9058 is [True, False, False, True, False, False]
Current timestep = 9059. State = [[-0.27724794 -0.24056225]]. Action = [[-0.01363702 -0.17413397 -0.05231179 -0.9686727 ]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 9059 is [True, False, False, True, False, False]
Current timestep = 9060. State = [[-0.27707765 -0.24055703]]. Action = [[-0.21338102  0.040979   -0.21056017  0.35615194]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 9060 is [True, False, False, True, False, False]
Current timestep = 9061. State = [[-0.27687892 -0.24060434]]. Action = [[-0.21111135  0.15448236 -0.13643359 -0.91403866]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 9061 is [True, False, False, True, False, False]
Human Feedback received at timestep 9061 of 1
Current timestep = 9062. State = [[-0.2767365  -0.24062085]]. Action = [[-0.20631462  0.22158429 -0.06045896 -0.82927716]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 9062 is [True, False, False, True, False, False]
Scene graph at timestep 9062 is [True, False, False, True, False, False]
State prediction error at timestep 9062 is tensor(1.8414e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9063. State = [[-0.2765178  -0.24069181]]. Action = [[-0.05119392  0.03380257 -0.23031245 -0.9665682 ]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 9063 is [True, False, False, True, False, False]
Current timestep = 9064. State = [[-0.27640656 -0.24068086]]. Action = [[-0.17233494  0.00987485 -0.13229571 -0.84033555]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 9064 is [True, False, False, True, False, False]
Current timestep = 9065. State = [[-0.2762065  -0.24072902]]. Action = [[-0.03401054 -0.11448532 -0.21725965 -0.94397724]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 9065 is [True, False, False, True, False, False]
Human Feedback received at timestep 9065 of 1
Current timestep = 9066. State = [[-0.2762358  -0.24070139]]. Action = [[-0.16529484  0.03541666  0.1155014   0.24943233]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 9066 is [True, False, False, True, False, False]
Current timestep = 9067. State = [[-0.2762358  -0.24070139]]. Action = [[-0.21164848  0.10462818  0.21564525 -0.5203822 ]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 9067 is [True, False, False, True, False, False]
Current timestep = 9068. State = [[-0.27606416 -0.24074627]]. Action = [[-0.19564047 -0.12463962  0.10522097  0.9544122 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 9068 is [True, False, False, True, False, False]
Current timestep = 9069. State = [[-0.27606416 -0.24074627]]. Action = [[-0.15062834 -0.03097241 -0.21649511  0.01060247]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 9069 is [True, False, False, True, False, False]
Human Feedback received at timestep 9069 of 1
Current timestep = 9070. State = [[-0.27606416 -0.24074627]]. Action = [[-0.23481265  0.24693137 -0.08428024 -0.6152448 ]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 9070 is [True, False, False, True, False, False]
Current timestep = 9071. State = [[-0.27606416 -0.24074627]]. Action = [[-0.19919483  0.10466278 -0.06405078 -0.65115136]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 9071 is [True, False, False, True, False, False]
Current timestep = 9072. State = [[-0.27606416 -0.24074627]]. Action = [[-0.0953057  -0.14225468 -0.13292052 -0.9772854 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 9072 is [True, False, False, True, False, False]
Current timestep = 9073. State = [[-0.27606416 -0.24074627]]. Action = [[-0.22453168  0.13446575  0.18132919  0.9626411 ]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 9073 is [True, False, False, True, False, False]
Current timestep = 9074. State = [[-0.27606416 -0.24074627]]. Action = [[-0.16625428 -0.24356101  0.00172612  0.8790066 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 9074 is [True, False, False, True, False, False]
Current timestep = 9075. State = [[-0.27606416 -0.24074627]]. Action = [[-0.19636673 -0.1235294  -0.2301358   0.91817474]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 9075 is [True, False, False, True, False, False]
Scene graph at timestep 9075 is [True, False, False, True, False, False]
State prediction error at timestep 9075 is tensor(2.8656e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9076. State = [[-0.27606416 -0.24074627]]. Action = [[-0.16158086 -0.10036466  0.13307315 -0.45434213]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 9076 is [True, False, False, True, False, False]
Current timestep = 9077. State = [[-0.27606416 -0.24074627]]. Action = [[-0.09214514 -0.0638392  -0.22850345  0.17768109]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 9077 is [True, False, False, True, False, False]
Scene graph at timestep 9077 is [True, False, False, True, False, False]
State prediction error at timestep 9077 is tensor(1.7975e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9078. State = [[-0.27606416 -0.24074627]]. Action = [[-0.1529836  -0.23565446  0.1463649  -0.9859328 ]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 9078 is [True, False, False, True, False, False]
Current timestep = 9079. State = [[-0.27606416 -0.24074627]]. Action = [[-0.09637401  0.24805039  0.13611543 -0.37615287]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 9079 is [True, False, False, True, False, False]
Human Feedback received at timestep 9079 of 1
Current timestep = 9080. State = [[-0.27606416 -0.24074627]]. Action = [[-0.13922125  0.2346384  -0.20189978 -0.9819119 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 9080 is [True, False, False, True, False, False]
Scene graph at timestep 9080 is [True, False, False, True, False, False]
State prediction error at timestep 9080 is tensor(6.8671e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9081. State = [[-0.27606416 -0.24074627]]. Action = [[-0.18646143  0.23767665 -0.19196649  0.5023085 ]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 9081 is [True, False, False, True, False, False]
Current timestep = 9082. State = [[-0.27606508 -0.24072215]]. Action = [[ 0.12088367 -0.05266547 -0.0113752  -0.3790605 ]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 9082 is [True, False, False, True, False, False]
Scene graph at timestep 9082 is [True, False, False, True, False, False]
State prediction error at timestep 9082 is tensor(1.9134e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9083. State = [[-0.27542305 -0.24100874]]. Action = [[-0.10382608  0.19263461 -0.19034286 -0.99394256]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 9083 is [True, False, False, True, False, False]
Current timestep = 9084. State = [[-0.27497852 -0.24120276]]. Action = [[-0.23160547  0.18784767 -0.18962525 -0.91728634]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 9084 is [True, False, False, True, False, False]
Current timestep = 9085. State = [[-0.27457273 -0.24149051]]. Action = [[-0.06870845 -0.0593891  -0.15315099 -0.7078285 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 9085 is [True, False, False, True, False, False]
Scene graph at timestep 9085 is [True, False, False, True, False, False]
State prediction error at timestep 9085 is tensor(4.4310e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9086. State = [[-0.27426136 -0.24167112]]. Action = [[-0.19615828  0.23883739 -0.13493538  0.6843827 ]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 9086 is [True, False, False, True, False, False]
Current timestep = 9087. State = [[-0.27429333 -0.2416385 ]]. Action = [[-0.19784975 -0.0383738   0.21490586 -0.9063775 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 9087 is [True, False, False, True, False, False]
Current timestep = 9088. State = [[-0.27432156 -0.24163458]]. Action = [[-0.23909332 -0.18141733 -0.05039117 -0.9735887 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 9088 is [True, False, False, True, False, False]
Human Feedback received at timestep 9088 of 1
Current timestep = 9089. State = [[-0.27429333 -0.2416385 ]]. Action = [[-0.11001912 -0.04836024  0.08642957  0.8453281 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 9089 is [True, False, False, True, False, False]
Current timestep = 9090. State = [[-0.27417907 -0.24167734]]. Action = [[-0.19710544  0.14851844 -0.1839384  -0.39541435]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 9090 is [True, False, False, True, False, False]
Scene graph at timestep 9090 is [True, False, False, True, False, False]
State prediction error at timestep 9090 is tensor(2.0553e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9090 of 1
Current timestep = 9091. State = [[-0.27417907 -0.24167734]]. Action = [[-0.18340273 -0.22285894  0.06803408 -0.7647153 ]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 9091 is [True, False, False, True, False, False]
Human Feedback received at timestep 9091 of 1
Current timestep = 9092. State = [[-0.27417907 -0.24167734]]. Action = [[-0.04774638  0.04037428  0.15798485 -0.9095611 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 9092 is [True, False, False, True, False, False]
Current timestep = 9093. State = [[-0.27417907 -0.24167734]]. Action = [[-0.14809687 -0.10098806 -0.21052797  0.9290893 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 9093 is [True, False, False, True, False, False]
Current timestep = 9094. State = [[-0.27417907 -0.24167734]]. Action = [[-0.13525191 -0.14206763 -0.01209666 -0.9963287 ]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 9094 is [True, False, False, True, False, False]
Current timestep = 9095. State = [[-0.27417907 -0.24167734]]. Action = [[-0.14818636  0.22541049  0.18326548  0.28940117]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 9095 is [True, False, False, True, False, False]
Scene graph at timestep 9095 is [True, False, False, True, False, False]
State prediction error at timestep 9095 is tensor(7.9967e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9096. State = [[-0.27417907 -0.24167734]]. Action = [[-0.2163906   0.24910787  0.03104073 -0.9037658 ]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 9096 is [True, False, False, True, False, False]
Current timestep = 9097. State = [[-0.27417907 -0.24167734]]. Action = [[-0.13839157  0.20669895 -0.14231059 -0.93427736]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 9097 is [True, False, False, True, False, False]
Current timestep = 9098. State = [[-0.27417907 -0.24167734]]. Action = [[-0.22196187  0.1320445  -0.18935826 -0.6223054 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 9098 is [True, False, False, True, False, False]
Human Feedback received at timestep 9098 of 1
Current timestep = 9099. State = [[-0.27417907 -0.24167734]]. Action = [[-0.20918587  0.2415179  -0.03447583 -0.9749601 ]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 9099 is [True, False, False, True, False, False]
Human Feedback received at timestep 9099 of 1
Current timestep = 9100. State = [[-0.27416965 -0.24163291]]. Action = [[ 0.05685559  0.23260295 -0.03827403 -0.99279094]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 9100 is [True, False, False, True, False, False]
Human Feedback received at timestep 9100 of 1
Current timestep = 9101. State = [[-0.27399832 -0.24109873]]. Action = [[-0.12190297  0.10152748  0.12797916  0.81449854]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 9101 is [True, False, False, True, False, False]
Human Feedback received at timestep 9101 of 1
Current timestep = 9102. State = [[-0.27369046 -0.24059613]]. Action = [[-0.05891293  0.18129635 -0.02796516  0.80205774]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 9102 is [True, False, False, True, False, False]
Human Feedback received at timestep 9102 of 1
Current timestep = 9103. State = [[-0.27336064 -0.23903374]]. Action = [[-0.03416821  0.19320238  0.03401604 -0.9076307 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 9103 is [True, False, False, True, False, False]
Scene graph at timestep 9103 is [True, False, False, True, False, False]
State prediction error at timestep 9103 is tensor(1.6622e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9103 of 1
Current timestep = 9104. State = [[-0.27329314 -0.23581798]]. Action = [[-0.22731954  0.10337138 -0.23126148 -0.9855418 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 9104 is [True, False, False, True, False, False]
Scene graph at timestep 9104 is [True, False, False, True, False, False]
State prediction error at timestep 9104 is tensor(7.6237e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9104 of 1
Current timestep = 9105. State = [[-0.27294445 -0.23402293]]. Action = [[-0.1775774   0.18006259  0.12814301 -0.21446347]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 9105 is [True, False, False, True, False, False]
Human Feedback received at timestep 9105 of 1
Current timestep = 9106. State = [[-0.2728043  -0.23285882]]. Action = [[-0.20808865  0.1958262   0.22839373 -0.9955152 ]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 9106 is [True, False, False, True, False, False]
Human Feedback received at timestep 9106 of 1
Current timestep = 9107. State = [[-0.27268225 -0.23234044]]. Action = [[-0.2389049   0.2376228   0.01403025 -0.96081865]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 9107 is [True, False, False, True, False, False]
Human Feedback received at timestep 9107 of 1
Current timestep = 9108. State = [[-0.27253762 -0.2318896 ]]. Action = [[-0.19062306 -0.24290778 -0.21059209  0.20797396]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 9108 is [True, False, False, True, False, False]
Scene graph at timestep 9108 is [True, False, False, True, False, False]
State prediction error at timestep 9108 is tensor(1.1563e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9108 of 1
Current timestep = 9109. State = [[-0.27230003 -0.23156276]]. Action = [[-0.12919548  0.06490001 -0.11503761 -0.77142656]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 9109 is [True, False, False, True, False, False]
Human Feedback received at timestep 9109 of 1
Current timestep = 9110. State = [[-0.27216426 -0.2313757 ]]. Action = [[-0.2269271   0.14078492  0.03334275  0.696581  ]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 9110 is [True, False, False, True, False, False]
Human Feedback received at timestep 9110 of 1
Current timestep = 9111. State = [[-0.2719949  -0.23114206]]. Action = [[-0.1669033   0.21374047 -0.04154453  0.9919908 ]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 9111 is [True, False, False, True, False, False]
Human Feedback received at timestep 9111 of 1
Current timestep = 9112. State = [[-0.27186784 -0.23095362]]. Action = [[-0.19022588  0.21755004 -0.1769418  -0.82923585]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 9112 is [True, False, False, True, False, False]
Human Feedback received at timestep 9112 of 1
Current timestep = 9113. State = [[-0.2718257 -0.2309083]]. Action = [[-0.12554207 -0.23528445 -0.15135352 -0.08684206]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 9113 is [True, False, False, True, False, False]
Human Feedback received at timestep 9113 of 1
Current timestep = 9114. State = [[-0.27172434 -0.2307681 ]]. Action = [[-0.14907707  0.02195993 -0.140291   -0.99368995]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 9114 is [True, False, False, True, False, False]
Human Feedback received at timestep 9114 of 1
Current timestep = 9115. State = [[-0.2716145  -0.23058285]]. Action = [[-0.06588805  0.24082619  0.11916703  0.9775412 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 9115 is [True, False, False, True, False, False]
Scene graph at timestep 9115 is [True, False, False, True, False, False]
State prediction error at timestep 9115 is tensor(7.3327e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9115 of 1
Current timestep = 9116. State = [[-0.2716376  -0.22772536]]. Action = [[-0.1237466   0.24196371  0.22895896 -0.55138564]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 9116 is [True, False, False, True, False, False]
Current timestep = 9117. State = [[-0.2724053  -0.21993245]]. Action = [[-0.19411929  0.08108276 -0.07310396  0.54181075]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 9117 is [True, False, False, True, False, False]
Current timestep = 9118. State = [[-0.27252355 -0.21759848]]. Action = [[-0.15069619  0.18090522  0.206801    0.92354655]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 9118 is [True, False, False, True, False, False]
Human Feedback received at timestep 9118 of 1
Current timestep = 9119. State = [[-0.27247304 -0.21621718]]. Action = [[-0.03588857  0.00437483 -0.2416238   0.3489095 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 9119 is [True, False, False, True, False, False]
Human Feedback received at timestep 9119 of 1
Current timestep = 9120. State = [[-0.2727427  -0.21555804]]. Action = [[-0.17968903  0.05520758 -0.16239707  0.8870615 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 9120 is [True, False, False, True, False, False]
Current timestep = 9121. State = [[-0.27308762 -0.21511722]]. Action = [[-0.18421595  0.07806897  0.00719038 -0.7905355 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 9121 is [True, False, False, True, False, False]
Scene graph at timestep 9121 is [True, False, False, True, False, False]
State prediction error at timestep 9121 is tensor(1.5878e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9121 of 1
Current timestep = 9122. State = [[-0.27324307 -0.21480499]]. Action = [[-0.07493094  0.14834559 -0.11149307 -0.9459137 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 9122 is [True, False, False, True, False, False]
Scene graph at timestep 9122 is [True, False, False, True, False, False]
State prediction error at timestep 9122 is tensor(6.6116e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9123. State = [[-0.27409005 -0.21266735]]. Action = [[-0.11682689  0.04680708  0.22395128  0.7968545 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 9123 is [True, False, False, True, False, False]
Current timestep = 9124. State = [[-0.2749443  -0.21134773]]. Action = [[-0.07541808  0.00084928 -0.24673463 -0.59958065]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 9124 is [True, False, False, True, False, False]
Human Feedback received at timestep 9124 of 1
Current timestep = 9125. State = [[-0.275865   -0.21047436]]. Action = [[-0.17820482 -0.14214477 -0.11070707  0.7530694 ]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 9125 is [True, False, False, True, False, False]
Current timestep = 9126. State = [[-0.27672496 -0.21007918]]. Action = [[-0.14351131  0.04083312 -0.146018    0.6964562 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 9126 is [True, False, False, True, False, False]
Current timestep = 9127. State = [[-0.2774843  -0.20960347]]. Action = [[-0.18596248  0.08369824 -0.2159672   0.19525719]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 9127 is [True, False, False, True, False, False]
Human Feedback received at timestep 9127 of 1
Current timestep = 9128. State = [[-0.2783784 -0.2091775]]. Action = [[-0.2301158  -0.20684738  0.23206359  0.8036361 ]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 9128 is [True, False, False, True, False, False]
Current timestep = 9129. State = [[-0.27913612 -0.20890988]]. Action = [[ 0.01092085 -0.02482413 -0.04114708 -0.7296769 ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 9129 is [True, False, False, True, False, False]
Current timestep = 9130. State = [[-0.27996007 -0.20865972]]. Action = [[-0.2279007  -0.23794417 -0.0705601  -0.11158103]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 9130 is [True, False, False, True, False, False]
Current timestep = 9131. State = [[-0.28043038 -0.20858799]]. Action = [[-0.22738095 -0.15693076 -0.24209088 -0.87079006]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 9131 is [True, False, False, True, False, False]
Scene graph at timestep 9131 is [True, False, False, True, False, False]
State prediction error at timestep 9131 is tensor(1.1244e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9132. State = [[-0.28100652 -0.20844354]]. Action = [[-0.19965483 -0.17470421 -0.23771991  0.47269344]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 9132 is [True, False, False, True, False, False]
Current timestep = 9133. State = [[-0.2813136 -0.2082202]]. Action = [[-0.14216961 -0.23836233  0.02400064  0.14322042]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 9133 is [True, False, False, True, False, False]
Current timestep = 9134. State = [[-0.28174713 -0.20803258]]. Action = [[-0.08928907 -0.08265415 -0.13453089  0.98760986]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 9134 is [True, False, False, True, False, False]
Current timestep = 9135. State = [[-0.28206903 -0.20789796]]. Action = [[ 0.0510717   0.08081362 -0.24705935  0.7655324 ]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 9135 is [True, False, False, True, False, False]
Scene graph at timestep 9135 is [True, False, False, True, False, False]
State prediction error at timestep 9135 is tensor(1.5094e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9136. State = [[-0.28234327 -0.20777892]]. Action = [[-0.15912998 -0.18427213 -0.12261978 -0.6043293 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 9136 is [True, False, False, True, False, False]
Current timestep = 9137. State = [[-0.28233105 -0.20781583]]. Action = [[-0.16208386  0.05849746 -0.06214777  0.4340582 ]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 9137 is [True, False, False, True, False, False]
Current timestep = 9138. State = [[-0.28244945 -0.2077095 ]]. Action = [[-0.0587696  -0.09793706 -0.02657542 -0.8543477 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 9138 is [True, False, False, True, False, False]
Current timestep = 9139. State = [[-0.28250998 -0.20768093]]. Action = [[-0.07494016  0.14308321 -0.14556168 -0.3703397 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 9139 is [True, False, False, True, False, False]
Current timestep = 9140. State = [[-0.28278032 -0.20756362]]. Action = [[-0.06853434 -0.05947803  0.1388945   0.7863411 ]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 9140 is [True, False, False, True, False, False]
Scene graph at timestep 9140 is [True, False, False, True, False, False]
State prediction error at timestep 9140 is tensor(1.5763e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9141. State = [[-0.28278032 -0.20756362]]. Action = [[-0.13928266 -0.01390478 -0.16982928 -0.83767784]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 9141 is [True, False, False, True, False, False]
Current timestep = 9142. State = [[-0.28278032 -0.20756362]]. Action = [[-0.05195388  0.06504506 -0.1083988  -0.02567607]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 9142 is [True, False, False, True, False, False]
Human Feedback received at timestep 9142 of -1
Current timestep = 9143. State = [[-0.28278032 -0.20756362]]. Action = [[-0.17066742  0.19192484 -0.24731028  0.45237637]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 9143 is [True, False, False, True, False, False]
Current timestep = 9144. State = [[-0.2830262  -0.20744741]]. Action = [[-0.08163103  0.0839611  -0.14916506 -0.458596  ]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 9144 is [True, False, False, True, False, False]
Human Feedback received at timestep 9144 of -1
Current timestep = 9145. State = [[-0.28306592 -0.20743693]]. Action = [[-0.09496339  0.06219834 -0.1909755  -0.13819385]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 9145 is [True, False, False, True, False, False]
Scene graph at timestep 9145 is [True, False, False, True, False, False]
State prediction error at timestep 9145 is tensor(1.3759e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9146. State = [[-0.2831844  -0.20740585]]. Action = [[-0.07240115  0.06119794  0.22483552  0.40860248]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 9146 is [True, False, False, True, False, False]
Current timestep = 9147. State = [[-0.28321636 -0.20738035]]. Action = [[-0.16074114 -0.12585285  0.07299715  0.14524484]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 9147 is [True, False, False, True, False, False]
Human Feedback received at timestep 9147 of 1
Current timestep = 9148. State = [[-0.28329623 -0.2073499 ]]. Action = [[-0.07392928  0.11140773 -0.1435145  -0.8876032 ]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 9148 is [True, False, False, True, False, False]
Scene graph at timestep 9148 is [True, False, False, True, False, False]
State prediction error at timestep 9148 is tensor(4.5747e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9149. State = [[-0.28326845 -0.20735386]]. Action = [[-0.20958884  0.20180851 -0.05144338  0.7164848 ]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 9149 is [True, False, False, True, False, False]
Current timestep = 9150. State = [[-0.28321883 -0.20730992]]. Action = [[-0.02520092 -0.23146038  0.1493302   0.8727803 ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 9150 is [True, False, False, True, False, False]
Current timestep = 9151. State = [[-0.28321883 -0.20730992]]. Action = [[-0.02798302  0.24312764  0.1556648  -0.23262823]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 9151 is [True, False, False, True, False, False]
Current timestep = 9152. State = [[-0.28321883 -0.20730992]]. Action = [[ 0.04721281 -0.02574706 -0.1751878  -0.03907472]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 9152 is [True, False, False, True, False, False]
Current timestep = 9153. State = [[-0.28321883 -0.20730992]]. Action = [[-0.11019197 -0.16653672  0.04856056 -0.04521841]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 9153 is [True, False, False, True, False, False]
Human Feedback received at timestep 9153 of 1
Current timestep = 9154. State = [[-0.28321883 -0.20730992]]. Action = [[-0.13299683 -0.2077677  -0.00837481 -0.75030357]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 9154 is [True, False, False, True, False, False]
Current timestep = 9155. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1404613   0.0630596   0.10688925 -0.8126194 ]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 9155 is [True, False, False, True, False, False]
Current timestep = 9156. State = [[-0.28321883 -0.20730992]]. Action = [[-0.24173506 -0.0593885  -0.02311596  0.7406862 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 9156 is [True, False, False, True, False, False]
Current timestep = 9157. State = [[-0.28321883 -0.20730992]]. Action = [[-0.03686203 -0.15169585  0.006538    0.832685  ]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 9157 is [True, False, False, True, False, False]
Scene graph at timestep 9157 is [True, False, False, True, False, False]
State prediction error at timestep 9157 is tensor(1.6107e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9158. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1274764  -0.18817492 -0.18752725 -0.95671654]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 9158 is [True, False, False, True, False, False]
Current timestep = 9159. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1646887  -0.07932664 -0.06596628 -0.2352162 ]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 9159 is [True, False, False, True, False, False]
Scene graph at timestep 9159 is [True, False, False, True, False, False]
State prediction error at timestep 9159 is tensor(2.8554e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9160. State = [[-0.28321883 -0.20730992]]. Action = [[-0.05692708  0.16767353  0.00584626  0.94714713]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 9160 is [True, False, False, True, False, False]
Current timestep = 9161. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20545396 -0.24177994 -0.09131357  0.6198683 ]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 9161 is [True, False, False, True, False, False]
Scene graph at timestep 9161 is [True, False, False, True, False, False]
State prediction error at timestep 9161 is tensor(7.6948e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9162. State = [[-0.28321883 -0.20730992]]. Action = [[-0.07129827 -0.03724548 -0.24827698 -0.8991747 ]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 9162 is [True, False, False, True, False, False]
Current timestep = 9163. State = [[-0.28321883 -0.20730992]]. Action = [[-0.05794476 -0.18611716  0.07678396 -0.33242977]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 9163 is [True, False, False, True, False, False]
Current timestep = 9164. State = [[-0.28321883 -0.20730992]]. Action = [[-0.13468996  0.05055147  0.14810365 -0.82870597]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 9164 is [True, False, False, True, False, False]
Current timestep = 9165. State = [[-0.28321883 -0.20730992]]. Action = [[-0.12739457 -0.08876003  0.05047214  0.49578822]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 9165 is [True, False, False, True, False, False]
Scene graph at timestep 9165 is [True, False, False, True, False, False]
State prediction error at timestep 9165 is tensor(5.2700e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9165 of 1
Current timestep = 9166. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1056293  -0.12253705 -0.0349068  -0.6889339 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 9166 is [True, False, False, True, False, False]
Human Feedback received at timestep 9166 of 1
Current timestep = 9167. State = [[-0.28321883 -0.20730992]]. Action = [[ 0.04764202  0.14078403 -0.20839712 -0.85504556]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 9167 is [True, False, False, True, False, False]
Current timestep = 9168. State = [[-0.28321883 -0.20730992]]. Action = [[-0.07481083 -0.0591245  -0.14031233 -0.2751671 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 9168 is [True, False, False, True, False, False]
Scene graph at timestep 9168 is [True, False, False, True, False, False]
State prediction error at timestep 9168 is tensor(2.0050e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9168 of 1
Current timestep = 9169. State = [[-0.28321883 -0.20730992]]. Action = [[-0.06350794  0.18912876  0.20754409 -0.3458135 ]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 9169 is [True, False, False, True, False, False]
Current timestep = 9170. State = [[-0.28321883 -0.20730992]]. Action = [[-0.09375603  0.03879347 -0.16135107  0.19685733]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 9170 is [True, False, False, True, False, False]
Current timestep = 9171. State = [[-0.28321883 -0.20730992]]. Action = [[-0.00718582  0.12871742 -0.24824849 -0.0387364 ]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 9171 is [True, False, False, True, False, False]
Scene graph at timestep 9171 is [True, False, False, True, False, False]
State prediction error at timestep 9171 is tensor(8.3479e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9172. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1864941  -0.18575507 -0.21003467  0.8347845 ]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 9172 is [True, False, False, True, False, False]
Scene graph at timestep 9172 is [True, False, False, True, False, False]
State prediction error at timestep 9172 is tensor(3.1231e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9173. State = [[-0.28321883 -0.20730992]]. Action = [[-0.09704971  0.02843457 -0.24091338 -0.29545557]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 9173 is [True, False, False, True, False, False]
Current timestep = 9174. State = [[-0.28321883 -0.20730992]]. Action = [[-0.15961976  0.19208091 -0.21815579  0.9543047 ]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 9174 is [True, False, False, True, False, False]
Current timestep = 9175. State = [[-0.28321883 -0.20730992]]. Action = [[-0.11180258  0.16218919 -0.23537798 -0.9057963 ]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 9175 is [True, False, False, True, False, False]
Current timestep = 9176. State = [[-0.28321883 -0.20730992]]. Action = [[-0.22635101  0.16207892  0.16395545 -0.30394578]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 9176 is [True, False, False, True, False, False]
Scene graph at timestep 9176 is [True, False, False, True, False, False]
State prediction error at timestep 9176 is tensor(2.1709e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9177. State = [[-0.28321883 -0.20730992]]. Action = [[-0.17139749  0.09140831 -0.199497   -0.5098071 ]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 9177 is [True, False, False, True, False, False]
Human Feedback received at timestep 9177 of 1
Current timestep = 9178. State = [[-0.28321883 -0.20730992]]. Action = [[-0.0412019  -0.21427767 -0.22322503  0.01138484]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 9178 is [True, False, False, True, False, False]
Current timestep = 9179. State = [[-0.28321883 -0.20730992]]. Action = [[-0.19539158  0.2323364   0.23702902  0.9562843 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 9179 is [True, False, False, True, False, False]
Current timestep = 9180. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20377593  0.19311273  0.12520939  0.642738  ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 9180 is [True, False, False, True, False, False]
Current timestep = 9181. State = [[-0.28321883 -0.20730992]]. Action = [[-0.14124787 -0.21256186 -0.20955266  0.8279048 ]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 9181 is [True, False, False, True, False, False]
Current timestep = 9182. State = [[-0.28321883 -0.20730992]]. Action = [[-0.18091159  0.23112425 -0.16208594  0.28565395]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 9182 is [True, False, False, True, False, False]
Scene graph at timestep 9182 is [True, False, False, True, False, False]
State prediction error at timestep 9182 is tensor(1.2722e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9183. State = [[-0.28321883 -0.20730992]]. Action = [[-0.19893007 -0.16927259  0.16628057 -0.01954067]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 9183 is [True, False, False, True, False, False]
Current timestep = 9184. State = [[-0.28321883 -0.20730992]]. Action = [[-0.04922564 -0.21102083 -0.07222547  0.6904609 ]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 9184 is [True, False, False, True, False, False]
Scene graph at timestep 9184 is [True, False, False, True, False, False]
State prediction error at timestep 9184 is tensor(2.7131e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9184 of 1
Current timestep = 9185. State = [[-0.28321883 -0.20730992]]. Action = [[-0.09092349 -0.10994655 -0.00630836 -0.1576196 ]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 9185 is [True, False, False, True, False, False]
Current timestep = 9186. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20104818  0.14760524  0.09786734  0.719882  ]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 9186 is [True, False, False, True, False, False]
Current timestep = 9187. State = [[-0.28321883 -0.20730992]]. Action = [[-0.18313971 -0.2402712  -0.02476729 -0.11296397]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 9187 is [True, False, False, True, False, False]
Current timestep = 9188. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20867597  0.06541258 -0.15179752 -0.8270868 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 9188 is [True, False, False, True, False, False]
Current timestep = 9189. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20598735  0.23639381 -0.10245377 -0.13510388]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 9189 is [True, False, False, True, False, False]
Current timestep = 9190. State = [[-0.28321883 -0.20730992]]. Action = [[-0.07127106  0.2010124  -0.21846761  0.5732064 ]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 9190 is [True, False, False, True, False, False]
Current timestep = 9191. State = [[-0.28321883 -0.20730992]]. Action = [[-0.18948872 -0.12512068 -0.19877255 -0.721918  ]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 9191 is [True, False, False, True, False, False]
Current timestep = 9192. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20044634 -0.10177594 -0.13534382  0.26576817]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 9192 is [True, False, False, True, False, False]
Human Feedback received at timestep 9192 of 1
Current timestep = 9193. State = [[-0.28321883 -0.20730992]]. Action = [[-0.2322789  -0.24938326 -0.12914634  0.95534825]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 9193 is [True, False, False, True, False, False]
Current timestep = 9194. State = [[-0.28321883 -0.20730992]]. Action = [[-0.21433467 -0.10694444 -0.21004611  0.5183313 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 9194 is [True, False, False, True, False, False]
Current timestep = 9195. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20537962 -0.09841833  0.17622733 -0.8153347 ]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 9195 is [True, False, False, True, False, False]
Current timestep = 9196. State = [[-0.28321883 -0.20730992]]. Action = [[-0.18059507 -0.24570854  0.00148016 -0.24144495]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 9196 is [True, False, False, True, False, False]
Scene graph at timestep 9196 is [True, False, False, True, False, False]
State prediction error at timestep 9196 is tensor(3.1573e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9197. State = [[-0.28321883 -0.20730992]]. Action = [[ 0.03672716 -0.12612587  0.18837878 -0.81000197]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 9197 is [True, False, False, True, False, False]
Scene graph at timestep 9197 is [True, False, False, True, False, False]
State prediction error at timestep 9197 is tensor(4.0444e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9198. State = [[-0.28321883 -0.20730992]]. Action = [[-0.10585469  0.15139803 -0.24629602  0.27539957]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 9198 is [True, False, False, True, False, False]
Current timestep = 9199. State = [[-0.28321883 -0.20730992]]. Action = [[-0.04552625 -0.24142659 -0.23561957  0.98541486]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 9199 is [True, False, False, True, False, False]
Scene graph at timestep 9199 is [True, False, False, True, False, False]
State prediction error at timestep 9199 is tensor(3.7328e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9200. State = [[-0.28321883 -0.20730992]]. Action = [[-0.14897238  0.20254552 -0.08461845 -0.9721798 ]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 9200 is [True, False, False, True, False, False]
Current timestep = 9201. State = [[-0.28321883 -0.20730992]]. Action = [[-0.19725531  0.21141148 -0.14227514 -0.31064367]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 9201 is [True, False, False, True, False, False]
Current timestep = 9202. State = [[-0.28321883 -0.20730992]]. Action = [[-0.21430074 -0.0966441   0.04789597 -0.77588373]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 9202 is [True, False, False, True, False, False]
Current timestep = 9203. State = [[-0.28321883 -0.20730992]]. Action = [[-0.22660054  0.17796096  0.00872734 -0.5610157 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 9203 is [True, False, False, True, False, False]
Current timestep = 9204. State = [[-0.28321883 -0.20730992]]. Action = [[-0.21158192 -0.04884833 -0.23558614  0.7584282 ]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 9204 is [True, False, False, True, False, False]
Current timestep = 9205. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1459874  -0.16235429 -0.16386628 -0.13018966]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 9205 is [True, False, False, True, False, False]
Current timestep = 9206. State = [[-0.28321883 -0.20730992]]. Action = [[-0.07568225 -0.22168745  0.04584435  0.01421976]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 9206 is [True, False, False, True, False, False]
Current timestep = 9207. State = [[-0.28321883 -0.20730992]]. Action = [[-0.22201407  0.20076463  0.01143482 -0.537309  ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 9207 is [True, False, False, True, False, False]
Human Feedback received at timestep 9207 of -1
Current timestep = 9208. State = [[-0.28321883 -0.20730992]]. Action = [[-0.23375817  0.11805177 -0.00205992 -0.7476637 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 9208 is [True, False, False, True, False, False]
Current timestep = 9209. State = [[-0.28321883 -0.20730992]]. Action = [[-0.21542741 -0.24903929 -0.24792589 -0.20511311]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 9209 is [True, False, False, True, False, False]
Current timestep = 9210. State = [[-0.28321883 -0.20730992]]. Action = [[-0.16317315  0.24877626  0.04043663 -0.8732142 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 9210 is [True, False, False, True, False, False]
Current timestep = 9211. State = [[-0.28321883 -0.20730992]]. Action = [[-0.16069373 -0.19303545  0.20977002 -0.5060492 ]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 9211 is [True, False, False, True, False, False]
Current timestep = 9212. State = [[-0.28321883 -0.20730992]]. Action = [[-0.12835786 -0.21021844 -0.08127537 -0.27388382]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 9212 is [True, False, False, True, False, False]
Scene graph at timestep 9212 is [True, False, False, True, False, False]
State prediction error at timestep 9212 is tensor(2.2896e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9213. State = [[-0.28321883 -0.20730992]]. Action = [[-0.02879417 -0.2242506  -0.2204438   0.5056355 ]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 9213 is [True, False, False, True, False, False]
Current timestep = 9214. State = [[-0.28321883 -0.20730992]]. Action = [[ 0.04842082 -0.01462564 -0.23724996 -0.97727126]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 9214 is [True, False, False, True, False, False]
Human Feedback received at timestep 9214 of 1
Current timestep = 9215. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1753304   0.24557129 -0.23089373 -0.95172065]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 9215 is [True, False, False, True, False, False]
Current timestep = 9216. State = [[-0.28321883 -0.20730992]]. Action = [[ 0.00823104 -0.05617441 -0.11798912  0.39486432]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 9216 is [True, False, False, True, False, False]
Current timestep = 9217. State = [[-0.28321883 -0.20730992]]. Action = [[-0.16798471  0.23901138 -0.16659804  0.46580935]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 9217 is [True, False, False, True, False, False]
Current timestep = 9218. State = [[-0.28321883 -0.20730992]]. Action = [[-0.11791165 -0.22474465 -0.24906592 -0.856821  ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 9218 is [True, False, False, True, False, False]
Current timestep = 9219. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1871358  -0.09898555 -0.00321767  0.86973476]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 9219 is [True, False, False, True, False, False]
Scene graph at timestep 9219 is [True, False, False, True, False, False]
State prediction error at timestep 9219 is tensor(1.2587e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9220. State = [[-0.28321883 -0.20730992]]. Action = [[-0.21470438 -0.22093275 -0.07845482  0.9610059 ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 9220 is [True, False, False, True, False, False]
Current timestep = 9221. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1868393   0.21063864 -0.18360445 -0.9826327 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 9221 is [True, False, False, True, False, False]
Current timestep = 9222. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1732317   0.17546189 -0.22418906  0.37608767]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 9222 is [True, False, False, True, False, False]
Scene graph at timestep 9222 is [True, False, False, True, False, False]
State prediction error at timestep 9222 is tensor(5.7413e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9223. State = [[-0.28321883 -0.20730992]]. Action = [[-0.06658348  0.23853421 -0.23517798 -0.5379711 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 9223 is [True, False, False, True, False, False]
Current timestep = 9224. State = [[-0.28321883 -0.20730992]]. Action = [[-0.20897032  0.1347759   0.12342003 -0.99124336]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 9224 is [True, False, False, True, False, False]
Current timestep = 9225. State = [[-0.28321883 -0.20730992]]. Action = [[-0.09732383 -0.22840714  0.0778504   0.9932399 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 9225 is [True, False, False, True, False, False]
Scene graph at timestep 9225 is [True, False, False, True, False, False]
State prediction error at timestep 9225 is tensor(7.3128e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9226. State = [[-0.28321883 -0.20730992]]. Action = [[-0.18062529 -0.24158254 -0.04280087 -0.40619338]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 9226 is [True, False, False, True, False, False]
Current timestep = 9227. State = [[-0.28321883 -0.20730992]]. Action = [[-0.1947171   0.02386254 -0.2481244  -0.07688701]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 9227 is [True, False, False, True, False, False]
Current timestep = 9228. State = [[-0.2831875  -0.20726302]]. Action = [[-0.0805919   0.2046735  -0.03782851  0.6823324 ]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 9228 is [True, False, False, True, False, False]
Scene graph at timestep 9228 is [True, False, False, True, False, False]
State prediction error at timestep 9228 is tensor(2.9180e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9229. State = [[-0.2831875  -0.20726302]]. Action = [[-0.07016169 -0.00306234  0.2083228  -0.79432213]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 9229 is [True, False, False, True, False, False]
Current timestep = 9230. State = [[-0.2831875  -0.20726302]]. Action = [[-0.03605078  0.00943112  0.20389438  0.7313876 ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 9230 is [True, False, False, True, False, False]
Scene graph at timestep 9230 is [True, False, False, True, False, False]
State prediction error at timestep 9230 is tensor(5.3978e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9231. State = [[-0.2831875  -0.20726302]]. Action = [[-0.22214937 -0.22266622  0.09816116 -0.09695768]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 9231 is [True, False, False, True, False, False]
Current timestep = 9232. State = [[-0.2831875  -0.20726302]]. Action = [[-0.19460627  0.13432837 -0.21895187  0.9498601 ]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 9232 is [True, False, False, True, False, False]
Current timestep = 9233. State = [[-0.2831875  -0.20726302]]. Action = [[ 0.00681341 -0.06963941 -0.16293053 -0.85361546]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 9233 is [True, False, False, True, False, False]
Current timestep = 9234. State = [[-0.2831875  -0.20726302]]. Action = [[-0.22297087 -0.237201    0.14142942  0.9262645 ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 9234 is [True, False, False, True, False, False]
Current timestep = 9235. State = [[-0.2831875  -0.20726302]]. Action = [[-0.17065585  0.21003616 -0.03910679  0.6801183 ]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 9235 is [True, False, False, True, False, False]
Current timestep = 9236. State = [[-0.2831875  -0.20726302]]. Action = [[-0.18682645 -0.21582292  0.02685902 -0.86233   ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 9236 is [True, False, False, True, False, False]
Current timestep = 9237. State = [[-0.2831875  -0.20726302]]. Action = [[-0.15487076 -0.01109523  0.07979259 -0.4073602 ]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 9237 is [True, False, False, True, False, False]
Scene graph at timestep 9237 is [True, False, False, True, False, False]
State prediction error at timestep 9237 is tensor(1.1978e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9238. State = [[-0.2831875  -0.20726302]]. Action = [[ 0.11459193  0.15602899 -0.2459322  -0.9512679 ]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 9238 is [True, False, False, True, False, False]
Current timestep = 9239. State = [[-0.2831875  -0.20726302]]. Action = [[-0.1826767  -0.03909096 -0.23816778  0.03071678]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 9239 is [True, False, False, True, False, False]
Scene graph at timestep 9239 is [True, False, False, True, False, False]
State prediction error at timestep 9239 is tensor(5.2179e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9240. State = [[-0.2831875  -0.20726302]]. Action = [[-0.06959504  0.23794872  0.17955238  0.15857863]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 9240 is [True, False, False, True, False, False]
Current timestep = 9241. State = [[-0.2831875  -0.20726302]]. Action = [[-0.20388179  0.03845975 -0.03144263 -0.6865754 ]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 9241 is [True, False, False, True, False, False]
Current timestep = 9242. State = [[-0.2831875  -0.20726302]]. Action = [[-0.13717487 -0.24581006 -0.07998481  0.4565562 ]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 9242 is [True, False, False, True, False, False]
Current timestep = 9243. State = [[-0.2831875  -0.20726302]]. Action = [[-0.11092867  0.15085173  0.22026137  0.18794918]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 9243 is [True, False, False, True, False, False]
Current timestep = 9244. State = [[-0.2831875  -0.20726302]]. Action = [[-0.17349021  0.00993276 -0.13098878 -0.9844399 ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 9244 is [True, False, False, True, False, False]
Current timestep = 9245. State = [[-0.2831875  -0.20726302]]. Action = [[-0.16224043  0.1096631   0.07736114  0.84351254]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 9245 is [True, False, False, True, False, False]
Current timestep = 9246. State = [[-0.2831875  -0.20726302]]. Action = [[-0.17523077 -0.22472945 -0.01227926  0.6719823 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 9246 is [True, False, False, True, False, False]
Scene graph at timestep 9246 is [True, False, False, True, False, False]
State prediction error at timestep 9246 is tensor(2.7013e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9247. State = [[-0.2831875  -0.20726302]]. Action = [[-0.12758793  0.16034448 -0.01905218 -0.93012   ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 9247 is [True, False, False, True, False, False]
Current timestep = 9248. State = [[-0.2831875  -0.20726302]]. Action = [[-0.13624561 -0.21581294 -0.24748288 -0.8386522 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 9248 is [True, False, False, True, False, False]
Current timestep = 9249. State = [[-0.2831875  -0.20726302]]. Action = [[-0.1093961   0.0438199  -0.21384779 -0.634233  ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 9249 is [True, False, False, True, False, False]
Scene graph at timestep 9249 is [True, False, False, True, False, False]
State prediction error at timestep 9249 is tensor(5.3145e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9250. State = [[-0.2831875  -0.20726302]]. Action = [[-0.0739342   0.17002046  0.11311951 -0.7231238 ]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 9250 is [True, False, False, True, False, False]
Scene graph at timestep 9250 is [True, False, False, True, False, False]
State prediction error at timestep 9250 is tensor(4.8495e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9251. State = [[-0.2831875  -0.20726302]]. Action = [[-0.10352442 -0.22290884  0.02318132 -0.9439225 ]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 9251 is [True, False, False, True, False, False]
Current timestep = 9252. State = [[-0.2831875  -0.20726302]]. Action = [[-0.19463621  0.23551726 -0.19501689  0.71085954]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 9252 is [True, False, False, True, False, False]
Current timestep = 9253. State = [[-0.2831875  -0.20726302]]. Action = [[-0.18579805 -0.21549073 -0.23520233  0.25784898]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 9253 is [True, False, False, True, False, False]
Current timestep = 9254. State = [[-0.2831875  -0.20726302]]. Action = [[-0.11275011 -0.23217948 -0.21617274 -0.57191616]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 9254 is [True, False, False, True, False, False]
Current timestep = 9255. State = [[-0.2831875  -0.20726302]]. Action = [[-0.02363996 -0.19230273 -0.24756658 -0.8428126 ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 9255 is [True, False, False, True, False, False]
Current timestep = 9256. State = [[-0.2831875  -0.20726302]]. Action = [[-0.04979062 -0.01560448 -0.24440284  0.23651695]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 9256 is [True, False, False, True, False, False]
Current timestep = 9257. State = [[-0.2831875  -0.20726302]]. Action = [[-0.23950435 -0.05601224 -0.21015996  0.14109635]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 9257 is [True, False, False, True, False, False]
Current timestep = 9258. State = [[-0.2831875  -0.20726302]]. Action = [[-0.20739911  0.19104081 -0.11612767  0.948009  ]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 9258 is [True, False, False, True, False, False]
Scene graph at timestep 9258 is [True, False, False, True, False, False]
State prediction error at timestep 9258 is tensor(2.3392e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9259. State = [[-0.2831875  -0.20726302]]. Action = [[-0.21188046 -0.07532886 -0.04526164 -0.318285  ]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 9259 is [True, False, False, True, False, False]
Current timestep = 9260. State = [[-0.2831875  -0.20726302]]. Action = [[-0.15436159  0.03698665 -0.203216    0.5518955 ]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 9260 is [True, False, False, True, False, False]
Scene graph at timestep 9260 is [True, False, False, True, False, False]
State prediction error at timestep 9260 is tensor(2.3847e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9261. State = [[-0.2831875  -0.20726302]]. Action = [[-0.20918554 -0.01889679 -0.14652091 -0.69017833]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 9261 is [True, False, False, True, False, False]
Current timestep = 9262. State = [[-0.2831875  -0.20726302]]. Action = [[-0.20363413  0.00254095 -0.22813396 -0.48373383]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 9262 is [True, False, False, True, False, False]
Current timestep = 9263. State = [[-0.2831875  -0.20726302]]. Action = [[-0.14387392 -0.21626346  0.24781007  0.5145644 ]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 9263 is [True, False, False, True, False, False]
Current timestep = 9264. State = [[-0.2831875  -0.20726302]]. Action = [[-0.23984696  0.24705857 -0.15708987 -0.02916902]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 9264 is [True, False, False, True, False, False]
Current timestep = 9265. State = [[-0.2831875  -0.20726302]]. Action = [[-0.19751774  0.1525285  -0.22857058 -0.5857387 ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 9265 is [True, False, False, True, False, False]
Scene graph at timestep 9265 is [True, False, False, True, False, False]
State prediction error at timestep 9265 is tensor(2.4744e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9266. State = [[-0.2831875  -0.20726302]]. Action = [[-0.2037729   0.04799721 -0.12936908 -0.535315  ]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 9266 is [True, False, False, True, False, False]
Current timestep = 9267. State = [[-0.2831875  -0.20726302]]. Action = [[-0.23055969  0.18112195 -0.17420194 -0.5692243 ]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 9267 is [True, False, False, True, False, False]
Current timestep = 9268. State = [[-0.2831875  -0.20726302]]. Action = [[-0.1668888   0.17667097  0.08721781  0.9204459 ]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 9268 is [True, False, False, True, False, False]
Scene graph at timestep 9268 is [True, False, False, True, False, False]
State prediction error at timestep 9268 is tensor(3.0468e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9269. State = [[-0.2831875  -0.20726302]]. Action = [[-0.03620908 -0.15651724 -0.13635331 -0.7399629 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 9269 is [True, False, False, True, False, False]
Current timestep = 9270. State = [[-0.2831875  -0.20726302]]. Action = [[ 0.00621313  0.17007327 -0.18821518 -0.95357794]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 9270 is [True, False, False, True, False, False]
Current timestep = 9271. State = [[-0.2831875  -0.20726302]]. Action = [[-0.20087996  0.2376388   0.14555198 -0.49608278]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 9271 is [True, False, False, True, False, False]
Current timestep = 9272. State = [[-0.2831875  -0.20726302]]. Action = [[-0.04262216 -0.09654862 -0.05922517 -0.89825964]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 9272 is [True, False, False, True, False, False]
Scene graph at timestep 9272 is [True, False, False, True, False, False]
State prediction error at timestep 9272 is tensor(3.0227e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9273. State = [[-0.20992555 -0.01155016]]. Action = [[-0.0361782   0.22732145 -0.14407095 -0.88674986]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 9273 is [True, False, False, True, False, False]
Human Feedback received at timestep 9273 of 1
Current timestep = 9274. State = [[-0.20869033 -0.0126818 ]]. Action = [[ 0.18773407 -0.05957814 -0.15862595 -0.19385737]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 9274 is [True, False, False, False, True, False]
Current timestep = 9275. State = [[-0.20708378 -0.01307031]]. Action = [[ 0.23122567 -0.06578921 -0.1643873  -0.07565159]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 9275 is [True, False, False, False, True, False]
Current timestep = 9276. State = [[-0.20396376 -0.01420396]]. Action = [[ 0.19178009 -0.10217789 -0.16717815  0.0334841 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 9276 is [True, False, False, False, True, False]
Current timestep = 9277. State = [[-0.1990034  -0.01617112]]. Action = [[ 0.2412616  -0.07083161 -0.1884112   0.09628797]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 9277 is [True, False, False, False, True, False]
Current timestep = 9278. State = [[-0.1921122  -0.01840324]]. Action = [[ 0.18005422 -0.10742438 -0.17206354 -0.13901848]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 9278 is [True, False, False, False, True, False]
Human Feedback received at timestep 9278 of 1
Current timestep = 9279. State = [[-0.18422239 -0.02130309]]. Action = [[ 0.2171883  -0.12513846 -0.1965426  -0.09763467]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 9279 is [True, False, False, False, True, False]
Current timestep = 9280. State = [[-0.17657825 -0.0249296 ]]. Action = [[ 0.21395966 -0.09570658 -0.08066326 -0.09107995]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 9280 is [True, False, False, False, True, False]
Current timestep = 9281. State = [[-0.1690844  -0.02892325]]. Action = [[ 0.22903544 -0.06455347 -0.14070216 -0.13277292]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 9281 is [True, False, False, False, True, False]
Human Feedback received at timestep 9281 of 1
Current timestep = 9282. State = [[-0.1607466  -0.03231676]]. Action = [[ 0.22097853 -0.09669587 -0.14233497 -0.1840412 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 9282 is [True, False, False, False, True, False]
Scene graph at timestep 9282 is [True, False, False, False, True, False]
State prediction error at timestep 9282 is tensor(2.4672e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9283. State = [[-0.15185283 -0.03587073]]. Action = [[ 0.21580786 -0.10410738 -0.18788056  0.08944619]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9283 is [True, False, False, False, True, False]
Scene graph at timestep 9283 is [True, False, False, False, True, False]
State prediction error at timestep 9283 is tensor(2.9682e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9284. State = [[-0.14329624 -0.03927456]]. Action = [[ 0.21275026 -0.05751967 -0.18604    -0.03403908]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 9284 is [True, False, False, False, True, False]
Human Feedback received at timestep 9284 of 1
Current timestep = 9285. State = [[-0.13577329 -0.04200974]]. Action = [[ 0.23349425 -0.09121238 -0.16277134 -0.06814551]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 9285 is [True, False, False, False, True, False]
Current timestep = 9286. State = [[-0.12816063 -0.04507606]]. Action = [[ 0.23748288 -0.10251039 -0.1359515   0.10953712]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 9286 is [True, False, False, False, True, False]
Current timestep = 9287. State = [[-0.1198677  -0.04866905]]. Action = [[ 0.22281325 -0.10910867 -0.14794946 -0.05311465]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 9287 is [True, False, False, False, True, False]
Scene graph at timestep 9287 is [True, False, False, False, True, False]
State prediction error at timestep 9287 is tensor(1.4639e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9288. State = [[-0.1117532  -0.05232821]]. Action = [[ 0.22507757 -0.1079478  -0.1440164  -0.06211525]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 9288 is [True, False, False, False, True, False]
Current timestep = 9289. State = [[-0.10337828 -0.05630986]]. Action = [[ 0.21122015 -0.07906277 -0.1801853   0.061203  ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 9289 is [True, False, False, False, True, False]
Current timestep = 9290. State = [[-0.09497518 -0.05968523]]. Action = [[ 0.2245763  -0.06864449 -0.13992171 -0.09941524]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 9290 is [True, False, False, False, True, False]
Scene graph at timestep 9290 is [True, False, False, False, True, False]
State prediction error at timestep 9290 is tensor(2.4424e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9290 of 1
Current timestep = 9291. State = [[-0.08623459 -0.06248067]]. Action = [[ 0.14001787 -0.04571375 -0.14515878 -0.23240292]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 9291 is [True, False, False, False, True, False]
Current timestep = 9292. State = [[-0.07906848 -0.06464303]]. Action = [[ 0.2163795  -0.07648245 -0.16782458 -0.06434077]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 9292 is [True, False, False, False, True, False]
Current timestep = 9293. State = [[-0.07219353 -0.06733947]]. Action = [[ 0.18775472 -0.06414717 -0.13235548 -0.12630296]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 9293 is [True, False, False, False, True, False]
Scene graph at timestep 9293 is [True, False, False, False, True, False]
State prediction error at timestep 9293 is tensor(1.9552e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9294. State = [[-0.06494637 -0.06989677]]. Action = [[ 0.18566674 -0.05166706 -0.15144786 -0.08620435]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 9294 is [True, False, False, False, True, False]
Current timestep = 9295. State = [[-0.05743565 -0.07232927]]. Action = [[ 0.16146874 -0.03540634 -0.14036438 -0.13436311]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 9295 is [True, False, False, False, True, False]
Current timestep = 9296. State = [[-0.05117919 -0.07415875]]. Action = [[ 0.16552585 -0.08057892 -0.15796113 -0.51725394]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 9296 is [True, False, False, False, True, False]
Current timestep = 9297. State = [[-0.04519346 -0.07645132]]. Action = [[ 0.16190061 -0.09392998 -0.16731288  0.0721736 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 9297 is [True, False, False, False, True, False]
Scene graph at timestep 9297 is [False, True, False, False, True, False]
State prediction error at timestep 9297 is tensor(4.6962e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9297 of 1
Current timestep = 9298. State = [[-0.03746313 -0.07941037]]. Action = [[ 0.14729315 -0.09310848 -0.18801521 -0.17884904]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 9298 is [False, True, False, False, True, False]
Current timestep = 9299. State = [[-0.25606096  0.08964701]]. Action = [[-0.01170713 -0.02570543 -0.13397017 -0.00518864]]. Reward = [100.]
Curr episode timestep = 25
Scene graph at timestep 9299 is [False, True, False, False, True, False]
Human Feedback received at timestep 9299 of 1
Current timestep = 9300. State = [[-0.2519554   0.09603405]]. Action = [[ 0.2402075  -0.12222588 -0.19450736  0.02388513]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 9300 is [True, False, False, False, True, False]
Current timestep = 9301. State = [[-0.2494088  0.0962675]]. Action = [[ 0.20709431 -0.11430025 -0.19371481  0.07202232]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 9301 is [True, False, False, False, True, False]
Scene graph at timestep 9301 is [True, False, False, False, True, False]
State prediction error at timestep 9301 is tensor(2.6711e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9302. State = [[-0.24461158  0.09502368]]. Action = [[ 0.24440369 -0.10456598 -0.17306846  0.06528318]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 9302 is [True, False, False, False, True, False]
Current timestep = 9303. State = [[-0.23987424  0.09333458]]. Action = [[ 0.24140596 -0.10442552 -0.18524982 -0.04966867]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 9303 is [True, False, False, False, True, False]
Current timestep = 9304. State = [[-0.23337588  0.09085421]]. Action = [[ 0.24759117 -0.07754424 -0.17533368 -0.08820105]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 9304 is [True, False, False, False, True, False]
Current timestep = 9305. State = [[-0.22625631  0.08864516]]. Action = [[ 0.21735865 -0.11984371 -0.17720793  0.05303419]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 9305 is [True, False, False, False, True, False]
Current timestep = 9306. State = [[-0.21803741  0.08584059]]. Action = [[ 0.22706503 -0.12871614 -0.13976198  0.03436697]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 9306 is [True, False, False, False, True, False]
Current timestep = 9307. State = [[-0.21066546  0.08212487]]. Action = [[ 0.23511454 -0.13425148 -0.15892974 -0.00792718]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 9307 is [True, False, False, False, True, False]
Human Feedback received at timestep 9307 of 1
Current timestep = 9308. State = [[-0.2022938   0.07779984]]. Action = [[ 0.22724926 -0.10935691 -0.16806704 -0.06307828]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 9308 is [True, False, False, False, True, False]
Current timestep = 9309. State = [[-0.19374134  0.07419974]]. Action = [[ 0.24884522 -0.12257692 -0.18900882 -0.06132388]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9309 is [True, False, False, False, True, False]
Current timestep = 9310. State = [[-0.18595977  0.07025557]]. Action = [[ 0.2286861  -0.12141623 -0.17520022 -0.08150208]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 9310 is [True, False, False, False, True, False]
Current timestep = 9311. State = [[-0.17695768  0.06579686]]. Action = [[ 0.24237299 -0.1043334  -0.16961129 -0.03819346]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 9311 is [True, False, False, False, True, False]
Scene graph at timestep 9311 is [True, False, False, False, True, False]
State prediction error at timestep 9311 is tensor(1.8480e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9311 of 1
Current timestep = 9312. State = [[-0.16726007  0.0615368 ]]. Action = [[ 0.23151666 -0.10196397 -0.15577823  0.0680728 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 9312 is [True, False, False, False, True, False]
Current timestep = 9313. State = [[-0.1592396   0.05784062]]. Action = [[ 0.24243778 -0.11482215 -0.15624535  0.0266695 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 9313 is [True, False, False, False, True, False]
Current timestep = 9314. State = [[-0.1503465   0.05394872]]. Action = [[ 0.23702288 -0.07783122 -0.18407103  0.07866478]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 9314 is [True, False, False, False, True, False]
Scene graph at timestep 9314 is [True, False, False, False, True, False]
State prediction error at timestep 9314 is tensor(1.9214e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9314 of 1
Current timestep = 9315. State = [[-0.140894   0.0503693]]. Action = [[ 0.23353207 -0.137487   -0.15555027  0.03076422]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 9315 is [True, False, False, False, True, False]
Current timestep = 9316. State = [[-0.13271675  0.0462155 ]]. Action = [[ 0.23644277 -0.12131867 -0.15544336  0.02211905]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 9316 is [True, False, False, False, True, False]
Human Feedback received at timestep 9316 of 1
Current timestep = 9317. State = [[-0.12423019  0.04182091]]. Action = [[ 0.22601658 -0.10907432 -0.17704096 -0.02369463]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 9317 is [True, False, False, False, True, False]
Scene graph at timestep 9317 is [True, False, False, False, True, False]
State prediction error at timestep 9317 is tensor(2.5350e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9318. State = [[-0.11447979  0.03738165]]. Action = [[ 0.23286816 -0.09224445 -0.14117897  0.12079358]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 9318 is [True, False, False, False, True, False]
Current timestep = 9319. State = [[-0.10728204  0.03368919]]. Action = [[ 0.20330471 -0.10450953 -0.17539597 -0.06424874]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 9319 is [True, False, False, False, True, False]
Human Feedback received at timestep 9319 of 1
Current timestep = 9320. State = [[-0.09865864  0.02996013]]. Action = [[ 0.2331652  -0.13081323 -0.17350703  0.08340144]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 9320 is [True, False, False, False, True, False]
Current timestep = 9321. State = [[-0.08885494  0.02582248]]. Action = [[ 0.2048167  -0.13531461 -0.15712345 -0.14348686]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 9321 is [True, False, False, False, True, False]
Current timestep = 9322. State = [[-0.08129706  0.02121014]]. Action = [[ 0.23596439 -0.10120633 -0.16356581  0.06225693]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 9322 is [True, False, False, False, True, False]
Human Feedback received at timestep 9322 of 1
Current timestep = 9323. State = [[-0.07365325  0.01704734]]. Action = [[ 0.23219883 -0.12483281 -0.18047117  0.16940618]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 9323 is [True, False, False, False, True, False]
Scene graph at timestep 9323 is [True, False, False, False, True, False]
State prediction error at timestep 9323 is tensor(2.1854e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9324. State = [[-0.06402247  0.01224037]]. Action = [[ 0.21778256 -0.09636286 -0.11955544  0.01068163]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 9324 is [True, False, False, False, True, False]
Scene graph at timestep 9324 is [True, False, False, False, True, False]
State prediction error at timestep 9324 is tensor(1.3477e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9325. State = [[-0.05610109  0.00859808]]. Action = [[ 0.20175406 -0.09598753 -0.19365929  0.19379151]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 9325 is [True, False, False, False, True, False]
Human Feedback received at timestep 9325 of 1
Current timestep = 9326. State = [[-0.04786552  0.0045342 ]]. Action = [[ 0.22666466 -0.11014068 -0.16354278  0.12758267]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 9326 is [True, False, False, False, True, False]
Scene graph at timestep 9326 is [False, True, False, False, True, False]
State prediction error at timestep 9326 is tensor(1.4262e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9327. State = [[-0.04055188  0.00049126]]. Action = [[ 0.20437202 -0.10011557 -0.09297052 -0.01168227]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 9327 is [False, True, False, False, True, False]
Current timestep = 9328. State = [[-0.24426974 -0.0665696 ]]. Action = [[ 0.19519436 -0.11349322 -0.15232229 -0.10519075]]. Reward = [100.]
Curr episode timestep = 28
Scene graph at timestep 9328 is [False, True, False, False, True, False]
Human Feedback received at timestep 9328 of 1
Current timestep = 9329. State = [[-0.24286947 -0.07147817]]. Action = [[ 0.15885735 -0.09156743 -0.17625743 -0.0181281 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 9329 is [True, False, False, False, True, False]
Current timestep = 9330. State = [[-0.24125642 -0.07253657]]. Action = [[ 0.21780115 -0.05798227 -0.18208888  0.17121267]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 9330 is [True, False, False, False, True, False]
Current timestep = 9331. State = [[-0.23763622 -0.07447077]]. Action = [[ 0.07451031 -0.07602899 -0.18115345 -0.13431245]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 9331 is [True, False, False, False, True, False]
Current timestep = 9332. State = [[-0.23360546 -0.07681178]]. Action = [[ 0.21438742 -0.08015192 -0.18427782  0.05731821]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 9332 is [True, False, False, False, True, False]
Scene graph at timestep 9332 is [True, False, False, False, True, False]
State prediction error at timestep 9332 is tensor(3.2562e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9333. State = [[-0.22863007 -0.07919893]]. Action = [[ 0.17568913 -0.04237472 -0.15678646 -0.19604707]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 9333 is [True, False, False, False, True, False]
Scene graph at timestep 9333 is [True, False, False, False, True, False]
State prediction error at timestep 9333 is tensor(1.6812e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9334. State = [[-0.22299498 -0.08124342]]. Action = [[ 0.13172743 -0.07293166 -0.1518705   0.04051387]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 9334 is [True, False, False, False, True, False]
Scene graph at timestep 9334 is [True, False, False, False, True, False]
State prediction error at timestep 9334 is tensor(1.7860e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9335. State = [[-0.21731098 -0.08332367]]. Action = [[ 0.24354333 -0.08778548 -0.19877712  0.09372544]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 9335 is [True, False, False, False, True, False]
Current timestep = 9336. State = [[-0.21060036 -0.08589083]]. Action = [[ 0.22192577 -0.02546884 -0.11676091 -0.22451729]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 9336 is [True, False, False, False, True, False]
Current timestep = 9337. State = [[-0.20438321 -0.08800735]]. Action = [[ 0.22200853 -0.05978066 -0.17423582 -0.23423636]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 9337 is [True, False, False, False, True, False]
Scene graph at timestep 9337 is [True, False, False, False, True, False]
State prediction error at timestep 9337 is tensor(1.1054e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9337 of 1
Current timestep = 9338. State = [[-0.19640628 -0.09041618]]. Action = [[ 0.2170459  -0.03017472 -0.11299382  0.22616768]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9338 is [True, False, False, False, True, False]
Current timestep = 9339. State = [[-0.18945746 -0.09215102]]. Action = [[ 0.21851093 -0.04797053 -0.16250138 -0.0943355 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 9339 is [True, False, False, False, True, False]
Current timestep = 9340. State = [[-0.1822055  -0.09382368]]. Action = [[ 0.1224364  -0.0612468  -0.17768534 -0.03479362]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 9340 is [True, False, False, False, True, False]
Current timestep = 9341. State = [[-0.17563328 -0.09576868]]. Action = [[ 0.10826051 -0.03912529 -0.15327302  0.15479898]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 9341 is [True, False, False, False, True, False]
Human Feedback received at timestep 9341 of 1
Current timestep = 9342. State = [[-0.16986223 -0.09746392]]. Action = [[ 0.15328395 -0.0398328  -0.16659914  0.16294193]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 9342 is [True, False, False, False, True, False]
Current timestep = 9343. State = [[-0.16402376 -0.09919089]]. Action = [[ 0.19828257 -0.06792662 -0.17968632 -0.21278399]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 9343 is [True, False, False, False, True, False]
Scene graph at timestep 9343 is [True, False, False, False, True, False]
State prediction error at timestep 9343 is tensor(7.8803e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9344. State = [[-0.15763858 -0.10107619]]. Action = [[ 0.17928618 -0.05950226 -0.07706219 -0.37171626]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 9344 is [True, False, False, False, True, False]
Current timestep = 9345. State = [[-0.15174305 -0.10330616]]. Action = [[ 0.18226194 -0.05792375 -0.17227997 -0.12319672]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 9345 is [True, False, False, False, True, False]
Human Feedback received at timestep 9345 of 1
Current timestep = 9346. State = [[-0.14555351 -0.1053588 ]]. Action = [[ 0.18292695 -0.04768142 -0.12499115  0.08176088]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 9346 is [True, False, False, False, True, False]
Scene graph at timestep 9346 is [True, False, False, False, True, False]
State prediction error at timestep 9346 is tensor(7.4572e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9347. State = [[-0.13823827 -0.10744994]]. Action = [[ 0.1785891  -0.11303154 -0.11993161 -0.14748609]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 9347 is [True, False, False, False, True, False]
Scene graph at timestep 9347 is [True, False, False, False, True, False]
State prediction error at timestep 9347 is tensor(2.5672e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9348. State = [[-0.13082622 -0.11007589]]. Action = [[ 0.16503128 -0.03476104 -0.11449726 -0.01176935]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 9348 is [True, False, False, False, True, False]
Scene graph at timestep 9348 is [True, False, False, False, True, False]
State prediction error at timestep 9348 is tensor(1.7064e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9349. State = [[-0.12411559 -0.11208087]]. Action = [[-0.03902674  0.02545002 -0.15334503  0.03614628]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 9349 is [True, False, False, False, True, False]
Current timestep = 9350. State = [[-0.1210281  -0.11284567]]. Action = [[ 0.08924681  0.00734231 -0.17562105  0.22602189]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 9350 is [True, False, False, False, True, False]
Scene graph at timestep 9350 is [True, False, False, False, True, False]
State prediction error at timestep 9350 is tensor(3.1342e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9351. State = [[-0.11599544 -0.11366978]]. Action = [[ 0.13728663 -0.08938372 -0.06136835 -0.32297182]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 9351 is [True, False, False, False, True, False]
Current timestep = 9352. State = [[-0.11313727 -0.1149241 ]]. Action = [[ 0.08633494 -0.10189644 -0.11306332  0.19992006]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 9352 is [True, False, False, False, True, False]
Scene graph at timestep 9352 is [True, False, False, False, True, False]
State prediction error at timestep 9352 is tensor(2.7224e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9353. State = [[-0.1095772  -0.11717093]]. Action = [[ 0.09175125 -0.09462261 -0.14488249 -0.17992836]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 9353 is [True, False, False, False, True, False]
Scene graph at timestep 9353 is [True, False, False, False, True, False]
State prediction error at timestep 9353 is tensor(5.9115e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9353 of 1
Current timestep = 9354. State = [[-0.1050453  -0.12055518]]. Action = [[ 0.12517214 -0.08771506 -0.09655686 -0.06302631]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 9354 is [True, False, False, False, True, False]
Current timestep = 9355. State = [[-0.10111597 -0.12363768]]. Action = [[ 0.0502378  -0.03737602  0.04352072 -0.43714726]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 9355 is [True, False, False, False, True, False]
Scene graph at timestep 9355 is [True, False, False, False, True, False]
State prediction error at timestep 9355 is tensor(8.3440e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9356. State = [[-0.09731906 -0.12634121]]. Action = [[ 0.13257551 -0.09599009 -0.14742216 -0.21748221]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 9356 is [True, False, False, False, True, False]
Current timestep = 9357. State = [[-0.09461406 -0.12926494]]. Action = [[ 0.02298889 -0.05155116  0.01960891 -0.20458913]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 9357 is [True, False, False, True, False, False]
Current timestep = 9358. State = [[-0.09260242 -0.1317325 ]]. Action = [[-0.01088819  0.01132813 -0.05373147 -0.6028297 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 9358 is [True, False, False, True, False, False]
Current timestep = 9359. State = [[-0.09074692 -0.13291906]]. Action = [[-0.06268811  0.02298534 -0.15797766 -0.6330705 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 9359 is [True, False, False, True, False, False]
Current timestep = 9360. State = [[-0.089471   -0.13360056]]. Action = [[ 0.10211119 -0.03828493 -0.07853842 -0.24192572]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 9360 is [True, False, False, True, False, False]
Current timestep = 9361. State = [[-0.08788807 -0.1341022 ]]. Action = [[-0.03766851  0.04801255 -0.02484103  0.30765975]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 9361 is [True, False, False, True, False, False]
Human Feedback received at timestep 9361 of 1
Current timestep = 9362. State = [[-0.08720928 -0.1342497 ]]. Action = [[-0.03198639  0.07456511 -0.14371352 -0.5438755 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 9362 is [True, False, False, True, False, False]
Current timestep = 9363. State = [[-0.0873487  -0.13423653]]. Action = [[ 0.04149035  0.05314744 -0.02262568 -0.27905828]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 9363 is [True, False, False, True, False, False]
Current timestep = 9364. State = [[-0.08731235 -0.13397515]]. Action = [[-0.08275113 -0.08807504  0.07691538 -0.62651   ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 9364 is [True, False, False, True, False, False]
Current timestep = 9365. State = [[-0.0873273  -0.13423334]]. Action = [[ 0.04879493 -0.04631549 -0.13345848 -0.10838246]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 9365 is [True, False, False, True, False, False]
Current timestep = 9366. State = [[-0.0873273  -0.13423334]]. Action = [[-0.05707327  0.00685346 -0.16503288 -0.32560307]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 9366 is [True, False, False, True, False, False]
Current timestep = 9367. State = [[-0.08734551 -0.13436434]]. Action = [[ 0.03772008  0.06286487 -0.18873909 -0.60843027]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 9367 is [True, False, False, True, False, False]
Current timestep = 9368. State = [[-0.0874247 -0.1343396]]. Action = [[-0.00245522  0.01388624 -0.08964026 -0.62021714]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 9368 is [True, False, False, True, False, False]
Scene graph at timestep 9368 is [True, False, False, True, False, False]
State prediction error at timestep 9368 is tensor(4.6162e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9369. State = [[-0.0874247 -0.1343396]]. Action = [[ 0.02658275  0.0408051  -0.2147644  -0.6469277 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 9369 is [True, False, False, True, False, False]
Scene graph at timestep 9369 is [True, False, False, True, False, False]
State prediction error at timestep 9369 is tensor(4.2711e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9370. State = [[-0.08737142 -0.13429557]]. Action = [[ 0.12174013 -0.1204108  -0.09847417 -0.54711014]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 9370 is [True, False, False, True, False, False]
Current timestep = 9371. State = [[-0.08650048 -0.13451008]]. Action = [[-0.0546577   0.04900336 -0.15209371 -0.41550922]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 9371 is [True, False, False, True, False, False]
Current timestep = 9372. State = [[-0.08624654 -0.13455598]]. Action = [[-0.03052147  0.01537082 -0.05839466 -0.7672678 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 9372 is [True, False, False, True, False, False]
Current timestep = 9373. State = [[-0.08608944 -0.13458519]]. Action = [[-0.00739449 -0.02730848 -0.1559486  -0.5753964 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 9373 is [True, False, False, True, False, False]
Current timestep = 9374. State = [[-0.08611512 -0.1345164 ]]. Action = [[ 0.15175775 -0.00317502 -0.06550047 -0.61607814]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 9374 is [True, False, False, True, False, False]
Current timestep = 9375. State = [[-0.0853042  -0.13464259]]. Action = [[ 0.02186427  0.03341511 -0.0314714  -0.70859677]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 9375 is [True, False, False, True, False, False]
Current timestep = 9376. State = [[-0.08489226 -0.1346895 ]]. Action = [[-0.08968392 -0.11328039 -0.09830625 -0.7401865 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 9376 is [True, False, False, True, False, False]
Current timestep = 9377. State = [[-0.08496252 -0.13474314]]. Action = [[ 0.08189407 -0.05936916 -0.08569196 -0.2356503 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 9377 is [True, False, False, True, False, False]
Current timestep = 9378. State = [[-0.08458263 -0.13584052]]. Action = [[-0.0740847  -0.04049212 -0.08824432 -0.44295096]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 9378 is [True, False, False, True, False, False]
Current timestep = 9379. State = [[-0.08449235 -0.13662882]]. Action = [[ 0.01252255  0.03656539 -0.16133717 -0.58192325]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 9379 is [True, False, False, True, False, False]
Current timestep = 9380. State = [[-0.08443739 -0.13676122]]. Action = [[-0.09301451  0.08360076 -0.13733035 -0.5216507 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 9380 is [True, False, False, True, False, False]
Current timestep = 9381. State = [[-0.08452219 -0.13674225]]. Action = [[ 0.02290678 -0.01030648 -0.09686978 -0.602236  ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 9381 is [True, False, False, True, False, False]
Human Feedback received at timestep 9381 of 1
Current timestep = 9382. State = [[-0.08454677 -0.13675447]]. Action = [[-0.10726678  0.04670933  0.02003998 -0.6006137 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 9382 is [True, False, False, True, False, False]
Current timestep = 9383. State = [[-0.08457147 -0.13676676]]. Action = [[-0.08607107  0.04089978 -0.1709964   0.06048107]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 9383 is [True, False, False, True, False, False]
Current timestep = 9384. State = [[-0.08475282 -0.13674977]]. Action = [[-0.00415671 -0.10993285 -0.04141176 -0.4968754 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 9384 is [True, False, False, True, False, False]
Current timestep = 9385. State = [[-0.08480428 -0.13679701]]. Action = [[-0.06472789  0.07001656 -0.06054062 -0.6092567 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 9385 is [True, False, False, True, False, False]
Current timestep = 9386. State = [[-0.08495759 -0.13675845]]. Action = [[ 0.05405176  0.07985285 -0.03968042 -0.53827274]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 9386 is [True, False, False, True, False, False]
Current timestep = 9387. State = [[-0.08498761 -0.13651416]]. Action = [[ 0.09535629  0.11087942 -0.13380708 -0.6352772 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 9387 is [True, False, False, True, False, False]
Current timestep = 9388. State = [[-0.08490483 -0.13526957]]. Action = [[ 0.08767265 -0.04418778  0.00682932 -0.7540098 ]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 9388 is [True, False, False, True, False, False]
Current timestep = 9389. State = [[-0.08485305 -0.134749  ]]. Action = [[-0.02720648 -0.0052859  -0.02019948 -0.04391599]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 9389 is [True, False, False, True, False, False]
Current timestep = 9390. State = [[-0.08485151 -0.13481285]]. Action = [[-0.07727754  0.04213864 -0.13635114 -0.25617456]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 9390 is [True, False, False, True, False, False]
Scene graph at timestep 9390 is [True, False, False, True, False, False]
State prediction error at timestep 9390 is tensor(1.5872e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9391. State = [[-0.08479838 -0.1343562 ]]. Action = [[ 0.09479928 -0.00166169 -0.05865924 -0.5362475 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 9391 is [True, False, False, True, False, False]
Scene graph at timestep 9391 is [True, False, False, True, False, False]
State prediction error at timestep 9391 is tensor(4.1008e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9392. State = [[-0.08464625 -0.13384044]]. Action = [[ 0.19463563 -0.06544703 -0.13341878 -0.7303263 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 9392 is [True, False, False, True, False, False]
Scene graph at timestep 9392 is [True, False, False, True, False, False]
State prediction error at timestep 9392 is tensor(3.2306e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9393. State = [[-0.0840048 -0.1340134]]. Action = [[ 0.11441785 -0.08736512 -0.16782919 -0.35064948]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 9393 is [True, False, False, True, False, False]
Current timestep = 9394. State = [[-0.08212055 -0.134549  ]]. Action = [[ 0.07963875 -0.00927034 -0.07643108 -0.59633905]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 9394 is [True, False, False, True, False, False]
Current timestep = 9395. State = [[-0.07970695 -0.13522828]]. Action = [[ 0.09980801 -0.08105198 -0.06821939 -0.57087606]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 9395 is [True, False, False, True, False, False]
Current timestep = 9396. State = [[-0.07626546 -0.13608477]]. Action = [[ 0.04275307  0.04938501 -0.15367918 -0.7956936 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 9396 is [True, False, False, True, False, False]
Current timestep = 9397. State = [[-0.07338066 -0.13642576]]. Action = [[-0.07482606 -0.01316021 -0.14657877 -0.481825  ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 9397 is [True, False, False, True, False, False]
Human Feedback received at timestep 9397 of 1
Current timestep = 9398. State = [[-0.07174245 -0.13687067]]. Action = [[-0.12367135  0.01953712 -0.15691231 -0.21074182]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 9398 is [True, False, False, True, False, False]
Current timestep = 9399. State = [[-0.07196686 -0.13688786]]. Action = [[-2.9907078e-03 -2.5239587e-04 -8.8896543e-02 -3.1308603e-01]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 9399 is [True, False, False, True, False, False]
Current timestep = 9400. State = [[-0.07207266 -0.13685535]]. Action = [[-0.06834994 -0.03354795 -0.07809925 -0.6713456 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 9400 is [True, False, False, True, False, False]
Scene graph at timestep 9400 is [True, False, False, True, False, False]
State prediction error at timestep 9400 is tensor(2.8140e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9401. State = [[-0.07230903 -0.13678269]]. Action = [[-0.00933562  0.05027813  0.00270894 -0.62902105]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 9401 is [True, False, False, True, False, False]
Scene graph at timestep 9401 is [True, False, False, True, False, False]
State prediction error at timestep 9401 is tensor(1.5598e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9402. State = [[-0.07238845 -0.13675827]]. Action = [[-0.1394798   0.04570818 -0.06623746 -0.89198184]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 9402 is [True, False, False, True, False, False]
Current timestep = 9403. State = [[-0.07246228 -0.13674325]]. Action = [[-0.09010392  0.05477971 -0.08738393 -0.4492104 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 9403 is [True, False, False, True, False, False]
Current timestep = 9404. State = [[-0.0727629  -0.13644846]]. Action = [[ 0.1044575  -0.03708988 -0.16178666 -0.77960414]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 9404 is [True, False, False, True, False, False]
Current timestep = 9405. State = [[-0.07284217 -0.13635513]]. Action = [[-0.10550359  0.07792693 -0.17430407 -0.3471567 ]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 9405 is [True, False, False, True, False, False]
Scene graph at timestep 9405 is [True, False, False, True, False, False]
State prediction error at timestep 9405 is tensor(3.0050e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9406. State = [[-0.07308467 -0.13583468]]. Action = [[-0.01590218  0.03333247 -0.094193   -0.6674454 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 9406 is [True, False, False, True, False, False]
Current timestep = 9407. State = [[-0.07335312 -0.13551326]]. Action = [[ 0.07033703 -0.01956055 -0.07450274 -0.7840718 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 9407 is [True, False, False, True, False, False]
Current timestep = 9408. State = [[-0.07331081 -0.13536829]]. Action = [[-0.05879319 -0.03034784 -0.0249159  -0.15483081]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 9408 is [True, False, False, True, False, False]
Human Feedback received at timestep 9408 of 1
Current timestep = 9409. State = [[-0.07331081 -0.13536829]]. Action = [[ 0.11912274  0.03917757 -0.10716498 -0.56397396]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 9409 is [True, False, False, True, False, False]
Scene graph at timestep 9409 is [True, False, False, True, False, False]
State prediction error at timestep 9409 is tensor(2.7865e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9410. State = [[-0.07331081 -0.13536829]]. Action = [[ 0.0133222  -0.00038508 -0.09232208 -0.32399762]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 9410 is [True, False, False, True, False, False]
Current timestep = 9411. State = [[-0.07344255 -0.13528328]]. Action = [[ 0.09874165  0.10111713 -0.18705592 -0.41040504]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 9411 is [True, False, False, True, False, False]
Scene graph at timestep 9411 is [True, False, False, True, False, False]
State prediction error at timestep 9411 is tensor(1.7028e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9412. State = [[-0.07327897 -0.13396968]]. Action = [[ 0.1359264   0.01034617  0.04398677 -0.32540107]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 9412 is [True, False, False, True, False, False]
Current timestep = 9413. State = [[-0.07290237 -0.13312934]]. Action = [[ 0.0587092   0.0309934  -0.14162382 -0.14544857]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 9413 is [True, False, False, True, False, False]
Current timestep = 9414. State = [[-0.0723609  -0.13204241]]. Action = [[ 0.00609249  0.08430839  0.0377565  -0.4192412 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 9414 is [True, False, False, True, False, False]
Current timestep = 9415. State = [[-0.07151929 -0.13041055]]. Action = [[ 0.01654351  0.06120005 -0.12501416 -0.6094521 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 9415 is [True, False, False, True, False, False]
Current timestep = 9416. State = [[-0.07056364 -0.12845881]]. Action = [[ 0.03723055 -0.01688758  0.04281241 -0.29477906]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 9416 is [True, False, False, True, False, False]
Human Feedback received at timestep 9416 of 1
Current timestep = 9417. State = [[-0.06980982 -0.12725133]]. Action = [[ 0.0331113   0.0118576  -0.14222024 -0.34404987]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 9417 is [True, False, False, True, False, False]
Current timestep = 9418. State = [[-0.06897662 -0.12660939]]. Action = [[-0.06079432 -0.01347582 -0.13496682 -0.27156454]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 9418 is [True, False, False, True, False, False]
Human Feedback received at timestep 9418 of 1
Current timestep = 9419. State = [[-0.06900816 -0.12616321]]. Action = [[ 0.08708474 -0.01650628 -0.11853206 -0.613569  ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 9419 is [True, False, False, True, False, False]
Current timestep = 9420. State = [[-0.06843901 -0.12580445]]. Action = [[-0.02107587 -0.08819523 -0.08754958 -0.3613943 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 9420 is [True, False, False, True, False, False]
Scene graph at timestep 9420 is [True, False, False, True, False, False]
State prediction error at timestep 9420 is tensor(3.1019e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9420 of 1
Current timestep = 9421. State = [[-0.06837452 -0.12604107]]. Action = [[ 0.1703519   0.05589843 -0.1631048  -0.26733017]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 9421 is [True, False, False, True, False, False]
Human Feedback received at timestep 9421 of 1
Current timestep = 9422. State = [[-0.06694726 -0.12562521]]. Action = [[ 0.14948857  0.00574037 -0.11397243 -0.33376265]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 9422 is [True, False, False, True, False, False]
Current timestep = 9423. State = [[-0.06498313 -0.1253412 ]]. Action = [[ 0.08579499 -0.05339006 -0.0930489  -0.7416966 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 9423 is [True, False, False, True, False, False]
Human Feedback received at timestep 9423 of 1
Current timestep = 9424. State = [[-0.06183701 -0.12561175]]. Action = [[ 0.04221481 -0.00964515 -0.04561006 -0.3653108 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 9424 is [True, False, False, True, False, False]
Current timestep = 9425. State = [[-0.05768903 -0.1261211 ]]. Action = [[ 0.11813933  0.06549567 -0.12060571 -0.71840036]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 9425 is [True, False, False, True, False, False]
Scene graph at timestep 9425 is [True, False, False, True, False, False]
State prediction error at timestep 9425 is tensor(7.8763e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9426. State = [[-0.05435487 -0.1257217 ]]. Action = [[ 0.11640096 -0.04178946 -0.01880696 -0.4813336 ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 9426 is [True, False, False, True, False, False]
Current timestep = 9427. State = [[-0.05050364 -0.12589225]]. Action = [[ 0.15859115  0.05783397 -0.03764451 -0.60788924]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 9427 is [True, False, False, True, False, False]
Current timestep = 9428. State = [[-0.0460411  -0.12540297]]. Action = [[ 0.02750617  0.10271782 -0.10384916 -0.5023751 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 9428 is [True, False, False, True, False, False]
Current timestep = 9429. State = [[-0.04167088 -0.12415491]]. Action = [[ 0.00745162  0.00630707  0.1415323  -0.7326864 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 9429 is [False, True, False, True, False, False]
Current timestep = 9430. State = [[-0.03912633 -0.12325073]]. Action = [[-0.0980562   0.00877753 -0.1474494  -0.79666996]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 9430 is [False, True, False, False, True, False]
Current timestep = 9431. State = [[-0.03881666 -0.12234782]]. Action = [[-0.01242782 -0.00163519 -0.0554892  -0.655799  ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 9431 is [False, True, False, False, True, False]
Scene graph at timestep 9431 is [False, True, False, False, True, False]
State prediction error at timestep 9431 is tensor(4.9000e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9432. State = [[-0.03876138 -0.12163574]]. Action = [[-0.03354627  0.00553703  0.00995624 -0.520501  ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 9432 is [False, True, False, False, True, False]
Current timestep = 9433. State = [[-0.03870845 -0.1214252 ]]. Action = [[-0.16014703  0.0142132  -0.05953979 -0.5009467 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 9433 is [False, True, False, False, True, False]
Human Feedback received at timestep 9433 of 1
Current timestep = 9434. State = [[-0.03874113 -0.12128024]]. Action = [[-0.02344231 -0.02329518 -0.13114941 -0.1658017 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 9434 is [False, True, False, False, True, False]
Current timestep = 9435. State = [[-0.03875832 -0.12134871]]. Action = [[-0.05455692  0.0809955  -0.15787221 -0.34902608]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 9435 is [False, True, False, False, True, False]
Human Feedback received at timestep 9435 of 1
Current timestep = 9436. State = [[-0.03892022 -0.12088016]]. Action = [[ 0.02196902  0.05163264 -0.08474582 -0.6243932 ]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 9436 is [False, True, False, False, True, False]
Scene graph at timestep 9436 is [False, True, False, False, True, False]
State prediction error at timestep 9436 is tensor(4.3829e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9437. State = [[-0.03898221 -0.11989524]]. Action = [[ 0.04654628 -0.09437636 -0.06780151 -0.67303956]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 9437 is [False, True, False, False, True, False]
Scene graph at timestep 9437 is [False, True, False, False, True, False]
State prediction error at timestep 9437 is tensor(4.8174e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9438. State = [[-0.03900246 -0.11989499]]. Action = [[-0.04070722  0.05989856 -0.03647032 -0.6431483 ]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 9438 is [False, True, False, False, True, False]
Current timestep = 9439. State = [[-0.03897448 -0.11961977]]. Action = [[-0.02106719 -0.00336091 -0.09857428 -0.49405915]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 9439 is [False, True, False, False, True, False]
Human Feedback received at timestep 9439 of 1
Current timestep = 9440. State = [[-0.03897448 -0.11961977]]. Action = [[ 0.01307002  0.00133929 -0.05203088 -0.6646109 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 9440 is [False, True, False, False, True, False]
Current timestep = 9441. State = [[-0.03898068 -0.11948265]]. Action = [[-6.8884343e-02  1.9240379e-04 -9.2204809e-02 -7.2829735e-01]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 9441 is [False, True, False, False, True, False]
Current timestep = 9442. State = [[-0.03898382 -0.11941344]]. Action = [[-0.01440835 -0.02444187  0.05867645 -0.65523213]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 9442 is [False, True, False, False, True, False]
Human Feedback received at timestep 9442 of 1
Current timestep = 9443. State = [[-0.03900888 -0.11940684]]. Action = [[-0.15809846  0.00878391 -0.13001111 -0.81658566]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 9443 is [False, True, False, False, True, False]
Current timestep = 9444. State = [[-0.03900888 -0.11940684]]. Action = [[ 0.05611587 -0.02756903 -0.07336348 -0.83308333]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 9444 is [False, True, False, False, True, False]
Human Feedback received at timestep 9444 of 1
Current timestep = 9445. State = [[-0.03902899 -0.11940678]]. Action = [[-0.0340213   0.10565633 -0.08906567 -0.8179426 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 9445 is [False, True, False, False, True, False]
Current timestep = 9446. State = [[-0.03920718 -0.11902689]]. Action = [[ 0.05819139 -0.00264055 -0.14578314 -0.5951311 ]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 9446 is [False, True, False, False, True, False]
Current timestep = 9447. State = [[-0.03931975 -0.11844861]]. Action = [[ 0.09228924  0.03150743 -0.10350931 -0.6755256 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 9447 is [False, True, False, False, True, False]
Current timestep = 9448. State = [[-0.03935389 -0.11781605]]. Action = [[ 0.0144124   0.03381112 -0.01611245 -0.36567914]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 9448 is [False, True, False, False, True, False]
Current timestep = 9449. State = [[-0.03939989 -0.11693736]]. Action = [[ 0.03251913  0.07882047 -0.06486598 -0.64207035]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 9449 is [False, True, False, False, True, False]
Human Feedback received at timestep 9449 of 1
Current timestep = 9450. State = [[-0.03944389 -0.11560064]]. Action = [[ 0.04669356 -0.06062815 -0.12141678 -0.6369404 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 9450 is [False, True, False, False, True, False]
Current timestep = 9451. State = [[-0.03936569 -0.11526377]]. Action = [[ 0.00087056 -0.04705578 -0.13060424 -0.5476514 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 9451 is [False, True, False, False, True, False]
Scene graph at timestep 9451 is [False, True, False, False, True, False]
State prediction error at timestep 9451 is tensor(3.5156e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9452. State = [[-0.03937282 -0.11512608]]. Action = [[ 0.04627648  0.04582372 -0.08930209 -0.57026464]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 9452 is [False, True, False, False, True, False]
Human Feedback received at timestep 9452 of 1
Current timestep = 9453. State = [[-0.03932285 -0.11491894]]. Action = [[ 0.06536224 -0.02226111 -0.04669479 -0.6324082 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 9453 is [False, True, False, False, True, False]
Scene graph at timestep 9453 is [False, True, False, False, True, False]
State prediction error at timestep 9453 is tensor(3.0076e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9454. State = [[-0.03925643 -0.11464319]]. Action = [[ 0.01760307 -0.04860109 -0.13116635 -0.7121858 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 9454 is [False, True, False, False, True, False]
Scene graph at timestep 9454 is [False, True, False, False, True, False]
State prediction error at timestep 9454 is tensor(5.3491e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9454 of 1
Current timestep = 9455. State = [[-0.03925643 -0.11464319]]. Action = [[ 0.08608902 -0.01007751 -0.1151884  -0.2629302 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 9455 is [False, True, False, False, True, False]
Current timestep = 9456. State = [[-0.03925643 -0.11464319]]. Action = [[ 0.00406805  0.00405079 -0.13797617 -0.27876163]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 9456 is [False, True, False, False, True, False]
Current timestep = 9457. State = [[-0.03925643 -0.11464319]]. Action = [[ 0.1358312  -0.01563014 -0.11275116 -0.69208246]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 9457 is [False, True, False, False, True, False]
Current timestep = 9458. State = [[-0.03918303 -0.11457734]]. Action = [[ 0.04646641 -0.05392417 -0.16132462 -0.61153984]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 9458 is [False, True, False, False, True, False]
Human Feedback received at timestep 9458 of 1
Current timestep = 9459. State = [[-0.03845783 -0.11481068]]. Action = [[-0.06877477 -0.05715208 -0.07206863 -0.79226726]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 9459 is [False, True, False, False, True, False]
Current timestep = 9460. State = [[-0.03831887 -0.11533337]]. Action = [[ 0.04755136 -0.07449725 -0.10533451 -0.64185125]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 9460 is [False, True, False, False, True, False]
Current timestep = 9461. State = [[-0.03806832 -0.11633156]]. Action = [[ 0.03593969 -0.03032428 -0.10406238 -0.38230705]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 9461 is [False, True, False, False, True, False]
Human Feedback received at timestep 9461 of 1
Current timestep = 9462. State = [[-0.03746973 -0.11737213]]. Action = [[ 0.15598929  0.03974354 -0.0715621  -0.45518517]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 9462 is [False, True, False, False, True, False]
Current timestep = 9463. State = [[-0.03552733 -0.11779147]]. Action = [[ 0.12716952  0.04737318  0.02475327 -0.50268555]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 9463 is [False, True, False, False, True, False]
Current timestep = 9464. State = [[-0.03226302 -0.11742478]]. Action = [[-0.05194537 -0.0617196  -0.09831321 -0.6336572 ]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 9464 is [False, True, False, False, True, False]
Current timestep = 9465. State = [[-0.02878946 -0.11825909]]. Action = [[ 0.1012744  -0.05860597 -0.060296   -0.5405157 ]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 9465 is [False, True, False, False, True, False]
Current timestep = 9466. State = [[-0.02609252 -0.1191251 ]]. Action = [[ 0.01863801  0.11471742 -0.03061017 -0.33859575]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 9466 is [False, True, False, False, True, False]
Current timestep = 9467. State = [[-0.02341527 -0.11884916]]. Action = [[ 0.03419185 -0.01516935 -0.14326158 -0.80411446]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 9467 is [False, True, False, False, True, False]
Current timestep = 9468. State = [[-0.02120836 -0.11913089]]. Action = [[-0.1068559   0.03213224 -0.13602766 -0.7543199 ]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 9468 is [False, True, False, False, True, False]
Scene graph at timestep 9468 is [False, True, False, False, True, False]
State prediction error at timestep 9468 is tensor(3.5734e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9469. State = [[-0.02042133 -0.11906401]]. Action = [[-0.0143476   0.0501335   0.01507476 -0.8406668 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 9469 is [False, True, False, False, True, False]
Human Feedback received at timestep 9469 of 1
Current timestep = 9470. State = [[-0.02054553 -0.11864233]]. Action = [[-0.10304213 -0.04702529 -0.06020032 -0.71606433]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 9470 is [False, True, False, False, True, False]
Current timestep = 9471. State = [[-0.02083351 -0.11857877]]. Action = [[-0.00166644  0.0655722  -0.0395696  -0.85828453]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 9471 is [False, True, False, False, True, False]
Scene graph at timestep 9471 is [False, True, False, False, True, False]
State prediction error at timestep 9471 is tensor(4.6293e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9472. State = [[-0.02091003 -0.11820441]]. Action = [[ 0.07891214 -0.00431114 -0.03603354 -0.62952054]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 9472 is [False, True, False, False, True, False]
Scene graph at timestep 9472 is [False, True, False, False, True, False]
State prediction error at timestep 9472 is tensor(6.4960e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9473. State = [[-0.02091656 -0.11806292]]. Action = [[-0.06596096  0.10634142 -0.15237409 -0.7983035 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 9473 is [False, True, False, False, True, False]
Scene graph at timestep 9473 is [False, True, False, False, True, False]
State prediction error at timestep 9473 is tensor(1.6372e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9474. State = [[-0.02103761 -0.11690816]]. Action = [[-0.0511255  -0.00631897 -0.12530911 -0.72855246]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 9474 is [False, True, False, False, True, False]
Scene graph at timestep 9474 is [False, True, False, False, True, False]
State prediction error at timestep 9474 is tensor(6.3541e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9474 of 1
Current timestep = 9475. State = [[-0.02106149 -0.11641335]]. Action = [[-0.04662365 -0.03619295 -0.08273612 -0.8762109 ]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 9475 is [False, True, False, False, True, False]
Current timestep = 9476. State = [[-0.02106149 -0.11641335]]. Action = [[-0.07454035  0.00836074 -0.15325037 -0.5511116 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 9476 is [False, True, False, False, True, False]
Current timestep = 9477. State = [[-0.02130324 -0.11635281]]. Action = [[-0.18615577  0.03863576 -0.11515403 -0.46684313]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 9477 is [False, True, False, False, True, False]
Human Feedback received at timestep 9477 of 1
Current timestep = 9478. State = [[-0.02160817 -0.11593854]]. Action = [[ 0.09459737  0.0590432   0.02508157 -0.66651154]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 9478 is [False, True, False, False, True, False]
Scene graph at timestep 9478 is [False, True, False, False, True, False]
State prediction error at timestep 9478 is tensor(2.4961e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9479. State = [[-0.02161558 -0.11509683]]. Action = [[ 0.00981113  0.0556125  -0.06047326 -0.7318281 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 9479 is [False, True, False, False, True, False]
Scene graph at timestep 9479 is [False, True, False, False, True, False]
State prediction error at timestep 9479 is tensor(3.5817e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9480. State = [[-0.02176142 -0.11394184]]. Action = [[ 0.02764669 -0.0099898   0.00952476 -0.82479376]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 9480 is [False, True, False, False, True, False]
Current timestep = 9481. State = [[-0.02189541 -0.11328545]]. Action = [[ 0.04136297  0.05409867 -0.11045825 -0.63659626]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 9481 is [False, True, False, False, True, False]
Human Feedback received at timestep 9481 of 1
Current timestep = 9482. State = [[-0.02196153 -0.11210231]]. Action = [[-0.03249319  0.11451373 -0.10037264 -0.7103676 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 9482 is [False, True, False, False, True, False]
Current timestep = 9483. State = [[-0.02219161 -0.10979496]]. Action = [[-0.01713863  0.00454754 -0.06553826 -0.8590698 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 9483 is [False, True, False, False, True, False]
Human Feedback received at timestep 9483 of 1
Current timestep = 9484. State = [[-0.02220952 -0.10825098]]. Action = [[-0.11530143 -0.02199893 -0.11349469 -0.61424583]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 9484 is [False, True, False, False, True, False]
Current timestep = 9485. State = [[-0.02230512 -0.10733202]]. Action = [[-0.02481353  0.09183842 -0.07063979 -0.7217185 ]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 9485 is [False, True, False, False, True, False]
Human Feedback received at timestep 9485 of 1
Current timestep = 9486. State = [[-0.02241666 -0.10556874]]. Action = [[ 0.02892053  0.07196096 -0.09734321 -0.75538963]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 9486 is [False, True, False, False, True, False]
Current timestep = 9487. State = [[-0.23144439 -0.08324175]]. Action = [[-0.0278151  -0.00998624 -0.10733037 -0.48531377]]. Reward = [100.]
Curr episode timestep = 158
Scene graph at timestep 9487 is [False, True, False, False, True, False]
Human Feedback received at timestep 9487 of 1
Current timestep = 9488. State = [[-0.22876167 -0.08764693]]. Action = [[ 0.16848981 -0.06657064 -0.16016015  0.20948362]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 9488 is [True, False, False, False, True, False]
Scene graph at timestep 9488 is [True, False, False, False, True, False]
State prediction error at timestep 9488 is tensor(3.4913e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9489. State = [[-0.22705115 -0.08836767]]. Action = [[ 0.21906132 -0.06910487 -0.15952481 -0.04662377]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 9489 is [True, False, False, False, True, False]
Current timestep = 9490. State = [[-0.2240357  -0.08970576]]. Action = [[ 0.16936702 -0.0582802  -0.18083055  0.133116  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 9490 is [True, False, False, False, True, False]
Scene graph at timestep 9490 is [True, False, False, False, True, False]
State prediction error at timestep 9490 is tensor(4.2799e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9491. State = [[-0.21902771 -0.09208772]]. Action = [[ 0.18774563 -0.07292494 -0.16741177  0.28232145]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 9491 is [True, False, False, False, True, False]
Current timestep = 9492. State = [[-0.21282929 -0.0945718 ]]. Action = [[ 0.20898649 -0.0654743  -0.18345177  0.17442775]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 9492 is [True, False, False, False, True, False]
Current timestep = 9493. State = [[-0.20611446 -0.0967354 ]]. Action = [[ 0.22712487 -0.07516292 -0.16119641  0.06990516]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 9493 is [True, False, False, False, True, False]
Current timestep = 9494. State = [[-0.19952148 -0.09908387]]. Action = [[ 0.21289921 -0.08880091 -0.15174852  0.03941262]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 9494 is [True, False, False, False, True, False]
Current timestep = 9495. State = [[-0.19212458 -0.10204346]]. Action = [[ 0.17144477 -0.1018461  -0.16619338  0.1559099 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 9495 is [True, False, False, False, True, False]
Scene graph at timestep 9495 is [True, False, False, False, True, False]
State prediction error at timestep 9495 is tensor(2.8004e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9496. State = [[-0.18462883 -0.10529391]]. Action = [[ 0.19317281 -0.0768235  -0.11657611  0.24976361]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 9496 is [True, False, False, False, True, False]
Scene graph at timestep 9496 is [True, False, False, False, True, False]
State prediction error at timestep 9496 is tensor(2.0533e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9497. State = [[-0.17713627 -0.1086656 ]]. Action = [[ 0.17656583 -0.04733671 -0.13757138 -0.12654793]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9497 is [True, False, False, False, True, False]
Human Feedback received at timestep 9497 of 1
Current timestep = 9498. State = [[-0.17053051 -0.11128046]]. Action = [[ 0.11646852 -0.03583439 -0.15667804  0.27185953]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 9498 is [True, False, False, False, True, False]
Scene graph at timestep 9498 is [True, False, False, False, True, False]
State prediction error at timestep 9498 is tensor(1.8930e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9499. State = [[-0.16277197 -0.11339463]]. Action = [[ 0.04318914 -0.02297743 -0.11083576 -0.12436271]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 9499 is [True, False, False, False, True, False]
Current timestep = 9500. State = [[-0.15769862 -0.11487818]]. Action = [[ 0.11505085 -0.06229973 -0.16520211  0.22607017]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 9500 is [True, False, False, False, True, False]
Human Feedback received at timestep 9500 of 1
Current timestep = 9501. State = [[-0.15435572 -0.11659592]]. Action = [[ 0.08629021 -0.13426277 -0.14798586  0.20324945]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 9501 is [True, False, False, False, True, False]
Current timestep = 9502. State = [[-0.15227361 -0.11978579]]. Action = [[ 0.13797534 -0.11354445 -0.15461779  0.42860532]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 9502 is [True, False, False, False, True, False]
Current timestep = 9503. State = [[-0.14935842 -0.12366425]]. Action = [[ 0.02969056 -0.02564879 -0.06477654 -0.13221836]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 9503 is [True, False, False, False, True, False]
Current timestep = 9504. State = [[-0.14724076 -0.12628455]]. Action = [[ 0.13001579 -0.04113042 -0.13799672 -0.13407594]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 9504 is [True, False, False, False, True, False]
Current timestep = 9505. State = [[-0.14373414 -0.12819432]]. Action = [[-0.0838522  -0.02024089 -0.04315467  0.18760371]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 9505 is [True, False, False, True, False, False]
Scene graph at timestep 9505 is [True, False, False, True, False, False]
State prediction error at timestep 9505 is tensor(3.7882e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9505 of 1
Current timestep = 9506. State = [[-0.14017642 -0.12986499]]. Action = [[ 0.13109529  0.00131905 -0.10491505 -0.39122403]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 9506 is [True, False, False, True, False, False]
Current timestep = 9507. State = [[-0.13695666 -0.13072915]]. Action = [[ 0.16267139 -0.08405581 -0.08231485 -0.24268478]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 9507 is [True, False, False, True, False, False]
Human Feedback received at timestep 9507 of -1
Current timestep = 9508. State = [[-0.13355286 -0.13188736]]. Action = [[ 0.01621225 -0.08899859 -0.12027533 -0.06598544]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 9508 is [True, False, False, True, False, False]
Current timestep = 9509. State = [[-0.13005883 -0.13413562]]. Action = [[-0.01093592 -0.12392539 -0.04159515  0.02975821]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 9509 is [True, False, False, True, False, False]
Current timestep = 9510. State = [[-0.12768756 -0.13727184]]. Action = [[ 0.02210316 -0.11912119 -0.05391115 -0.03479415]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 9510 is [True, False, False, True, False, False]
Current timestep = 9511. State = [[-0.12628488 -0.14108919]]. Action = [[ 0.06926584 -0.02911311 -0.13241296 -0.04988462]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 9511 is [True, False, False, True, False, False]
Current timestep = 9512. State = [[-0.12499128 -0.14420827]]. Action = [[-0.01196855 -0.039336   -0.02601451 -0.6117146 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 9512 is [True, False, False, True, False, False]
Scene graph at timestep 9512 is [True, False, False, True, False, False]
State prediction error at timestep 9512 is tensor(7.5751e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9513. State = [[-0.12481265 -0.1463708 ]]. Action = [[ 0.07886261 -0.03456853 -0.08016473 -0.30533946]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 9513 is [True, False, False, True, False, False]
Human Feedback received at timestep 9513 of 1
Current timestep = 9514. State = [[-0.12391708 -0.1481472 ]]. Action = [[-0.01244061  0.02143613  0.00196904 -0.74724704]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 9514 is [True, False, False, True, False, False]
Current timestep = 9515. State = [[-0.12335098 -0.14873222]]. Action = [[ 0.0648132  -0.06006068 -0.02775304 -0.3994975 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 9515 is [True, False, False, True, False, False]
Scene graph at timestep 9515 is [True, False, False, True, False, False]
State prediction error at timestep 9515 is tensor(8.1591e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9516. State = [[-0.12216482 -0.14986594]]. Action = [[-0.08341131  0.04716414 -0.05664754  0.10471964]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 9516 is [True, False, False, True, False, False]
Current timestep = 9517. State = [[-0.12156871 -0.15015332]]. Action = [[ 0.07780012 -0.04676747 -0.02915984 -0.12796259]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 9517 is [True, False, False, True, False, False]
Current timestep = 9518. State = [[-0.12050881 -0.15054736]]. Action = [[-0.00985867 -0.00939228 -0.04602781  0.27740848]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 9518 is [True, False, False, True, False, False]
Current timestep = 9519. State = [[-0.11959759 -0.15110531]]. Action = [[ 0.09560537 -0.02494831 -0.0641737  -0.36561465]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 9519 is [True, False, False, True, False, False]
Current timestep = 9520. State = [[-0.11758896 -0.15195408]]. Action = [[-0.03874186  0.01682177 -0.16380519 -0.05855793]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 9520 is [True, False, False, True, False, False]
Scene graph at timestep 9520 is [True, False, False, True, False, False]
State prediction error at timestep 9520 is tensor(7.2629e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9521. State = [[-0.11636242 -0.15239309]]. Action = [[-0.10465905 -0.04231593 -0.11659881 -0.46194315]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 9521 is [True, False, False, True, False, False]
Scene graph at timestep 9521 is [True, False, False, True, False, False]
State prediction error at timestep 9521 is tensor(1.0843e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9522. State = [[-0.11640752 -0.15284443]]. Action = [[-0.068088    0.00283903 -0.04582499 -0.62552685]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 9522 is [True, False, False, True, False, False]
Current timestep = 9523. State = [[-0.11675851 -0.1532205 ]]. Action = [[-0.16191515  0.00838053 -0.07554916  0.08768463]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 9523 is [True, False, False, True, False, False]
Scene graph at timestep 9523 is [True, False, False, True, False, False]
State prediction error at timestep 9523 is tensor(2.6237e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9524. State = [[-0.11746209 -0.15425213]]. Action = [[-0.00515236  0.02019954 -0.16701037  0.23022985]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 9524 is [True, False, False, True, False, False]
Scene graph at timestep 9524 is [True, False, False, True, False, False]
State prediction error at timestep 9524 is tensor(1.6857e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9525. State = [[-0.11787374 -0.15475745]]. Action = [[-0.12181386 -0.01029235 -0.01809594 -0.74778956]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 9525 is [True, False, False, True, False, False]
Scene graph at timestep 9525 is [True, False, False, True, False, False]
State prediction error at timestep 9525 is tensor(5.4203e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9526. State = [[-0.11838122 -0.15561159]]. Action = [[ 0.02898273 -0.07305455 -0.16475895 -0.1533891 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 9526 is [True, False, False, True, False, False]
Current timestep = 9527. State = [[-0.11903373 -0.15712725]]. Action = [[-0.01393209 -0.00567582 -0.06854798 -0.5477563 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 9527 is [True, False, False, True, False, False]
Scene graph at timestep 9527 is [True, False, False, True, False, False]
State prediction error at timestep 9527 is tensor(1.9904e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9528. State = [[-0.11947396 -0.15834834]]. Action = [[-0.11415976  0.10515401 -0.16969274 -0.47341603]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 9528 is [True, False, False, True, False, False]
Scene graph at timestep 9528 is [True, False, False, True, False, False]
State prediction error at timestep 9528 is tensor(2.3180e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9529. State = [[-0.11993016 -0.1585699 ]]. Action = [[ 0.06246686 -0.02644376 -0.1356613  -0.51837915]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 9529 is [True, False, False, True, False, False]
Scene graph at timestep 9529 is [True, False, False, True, False, False]
State prediction error at timestep 9529 is tensor(2.6030e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9530. State = [[-0.12005705 -0.15859503]]. Action = [[-0.08885774  0.02075532 -0.03920233 -0.36761236]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 9530 is [True, False, False, True, False, False]
Current timestep = 9531. State = [[-0.12025555 -0.1589633 ]]. Action = [[-0.03103168  0.01961824 -0.05688062 -0.59644383]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 9531 is [True, False, False, True, False, False]
Current timestep = 9532. State = [[-0.12069584 -0.15891992]]. Action = [[-0.03955999  0.08666787 -0.12369409 -0.2016043 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 9532 is [True, False, False, True, False, False]
Current timestep = 9533. State = [[-0.12147118 -0.15849164]]. Action = [[-0.03919983 -0.0424484  -0.19019601  0.17598808]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 9533 is [True, False, False, True, False, False]
Current timestep = 9534. State = [[-0.12201731 -0.1585079 ]]. Action = [[-0.01131299  0.02265877 -0.14221422 -0.44953233]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 9534 is [True, False, False, True, False, False]
Current timestep = 9535. State = [[-0.12234557 -0.15844543]]. Action = [[ 0.03845072  0.00103882 -0.10125545 -0.37541234]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 9535 is [True, False, False, True, False, False]
Current timestep = 9536. State = [[-0.12240539 -0.15843108]]. Action = [[-0.02002481  0.00840253  0.01353914 -0.6268393 ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 9536 is [True, False, False, True, False, False]
Scene graph at timestep 9536 is [True, False, False, True, False, False]
State prediction error at timestep 9536 is tensor(1.7005e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9537. State = [[-0.12260714 -0.15839171]]. Action = [[-0.00276585  0.07755163 -0.10983287 -0.53053534]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 9537 is [True, False, False, True, False, False]
Human Feedback received at timestep 9537 of 1
Current timestep = 9538. State = [[-0.1228468  -0.15752403]]. Action = [[-0.08647032 -0.07471964 -0.1806301  -0.8842906 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 9538 is [True, False, False, True, False, False]
Current timestep = 9539. State = [[-0.12329315 -0.15775414]]. Action = [[ 0.0005253   0.02300182 -0.12607709  0.21993387]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 9539 is [True, False, False, True, False, False]
Scene graph at timestep 9539 is [True, False, False, True, False, False]
State prediction error at timestep 9539 is tensor(6.9464e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9540. State = [[-0.12353198 -0.15763637]]. Action = [[-0.03447846  0.03501621 -0.01887025 -0.00219625]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 9540 is [True, False, False, True, False, False]
Current timestep = 9541. State = [[-0.1239788  -0.15734531]]. Action = [[-0.14414304  0.02623653 -0.15975733 -0.8838565 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 9541 is [True, False, False, True, False, False]
Current timestep = 9542. State = [[-0.12525436 -0.15701407]]. Action = [[-0.16153231 -0.0348455  -0.07149467 -0.7240589 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 9542 is [True, False, False, True, False, False]
Current timestep = 9543. State = [[-0.12712601 -0.15744227]]. Action = [[-0.12159279  0.03624949  0.06690726 -0.02394617]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 9543 is [True, False, False, True, False, False]
Current timestep = 9544. State = [[-0.12950198 -0.15746756]]. Action = [[-0.11694306 -0.01505217 -0.06922086 -0.77123374]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 9544 is [True, False, False, True, False, False]
Human Feedback received at timestep 9544 of 1
Current timestep = 9545. State = [[-0.13336463 -0.15741248]]. Action = [[-0.06520793  0.02044347  0.02454376  0.15175581]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 9545 is [True, False, False, True, False, False]
Scene graph at timestep 9545 is [True, False, False, True, False, False]
State prediction error at timestep 9545 is tensor(1.2501e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9546. State = [[-0.13707998 -0.15706721]]. Action = [[-0.01137432 -0.18154477  0.06891468 -0.6312833 ]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 9546 is [True, False, False, True, False, False]
Current timestep = 9547. State = [[-0.13972113 -0.15869595]]. Action = [[-0.06318632 -0.12157579 -0.17829135  0.15887308]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 9547 is [True, False, False, True, False, False]
Scene graph at timestep 9547 is [True, False, False, True, False, False]
State prediction error at timestep 9547 is tensor(8.4526e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9548. State = [[-0.1419942  -0.16097558]]. Action = [[-0.11481813  0.12866962 -0.22538307 -0.2616889 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 9548 is [True, False, False, True, False, False]
Human Feedback received at timestep 9548 of -1
Current timestep = 9549. State = [[-0.14512166 -0.16168097]]. Action = [[-0.18900238 -0.12351172 -0.14639084  0.04973304]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 9549 is [True, False, False, True, False, False]
Human Feedback received at timestep 9549 of -1
Current timestep = 9550. State = [[-0.14957047 -0.16353005]]. Action = [[ 0.14751962 -0.06727079 -0.06528816 -0.63199973]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 9550 is [True, False, False, True, False, False]
Human Feedback received at timestep 9550 of -1
Current timestep = 9551. State = [[-0.15114976 -0.16571973]]. Action = [[-0.04163709  0.09580731 -0.058998   -0.7866351 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 9551 is [True, False, False, True, False, False]
Scene graph at timestep 9551 is [True, False, False, True, False, False]
State prediction error at timestep 9551 is tensor(6.3902e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9551 of -1
Current timestep = 9552. State = [[-0.15220891 -0.1662129 ]]. Action = [[-0.01872718 -0.11573303 -0.04492867 -0.02938908]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 9552 is [True, False, False, True, False, False]
Current timestep = 9553. State = [[-0.15328586 -0.16799669]]. Action = [[-0.20062332 -0.07938792  0.16820985  0.00208223]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 9553 is [True, False, False, True, False, False]
Human Feedback received at timestep 9553 of -1
Current timestep = 9554. State = [[-0.15556842 -0.17013903]]. Action = [[-0.14997004 -0.01174195 -0.07797417  0.62442863]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 9554 is [True, False, False, True, False, False]
Scene graph at timestep 9554 is [True, False, False, True, False, False]
State prediction error at timestep 9554 is tensor(1.0939e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9554 of -1
Current timestep = 9555. State = [[-0.15856314 -0.17220262]]. Action = [[-0.088955   -0.10525763 -0.02161199 -0.53332907]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 9555 is [True, False, False, True, False, False]
Human Feedback received at timestep 9555 of -1
Current timestep = 9556. State = [[-0.16139542 -0.17491592]]. Action = [[-0.05057013 -0.04884464 -0.09994265 -0.07576919]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 9556 is [True, False, False, True, False, False]
Scene graph at timestep 9556 is [True, False, False, True, False, False]
State prediction error at timestep 9556 is tensor(7.3691e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9557. State = [[-0.16427174 -0.177334  ]]. Action = [[-0.08467743  0.14060196 -0.21200337 -0.27950722]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 9557 is [True, False, False, True, False, False]
Scene graph at timestep 9557 is [True, False, False, True, False, False]
State prediction error at timestep 9557 is tensor(5.1024e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9557 of -1
Current timestep = 9558. State = [[-0.16822466 -0.17662422]]. Action = [[-0.13221645  0.03442067 -0.07317066  0.7789328 ]]. Reward = [0.]
Curr episode timestep = 70
Scene graph at timestep 9558 is [True, False, False, True, False, False]
Human Feedback received at timestep 9558 of -1
Current timestep = 9559. State = [[-0.17310956 -0.17537928]]. Action = [[-0.0261347  -0.00798011 -0.11684763 -0.5106955 ]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 9559 is [True, False, False, True, False, False]
Human Feedback received at timestep 9559 of -1
Current timestep = 9560. State = [[-0.17634164 -0.17478058]]. Action = [[ 0.14514297 -0.11021629 -0.04205793  0.27239347]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 9560 is [True, False, False, True, False, False]
Current timestep = 9561. State = [[-0.17729813 -0.17519651]]. Action = [[-0.06187934  0.00373912 -0.18233788  0.03178883]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 9561 is [True, False, False, True, False, False]
Current timestep = 9562. State = [[-0.17838424 -0.17544755]]. Action = [[-0.18980218 -0.03602041 -0.2297837  -0.77232194]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 9562 is [True, False, False, True, False, False]
Scene graph at timestep 9562 is [True, False, False, True, False, False]
State prediction error at timestep 9562 is tensor(1.6673e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9562 of -1
Current timestep = 9563. State = [[-0.18045808 -0.17633851]]. Action = [[-0.07027438  0.19911683 -0.18700238 -0.4501053 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 9563 is [True, False, False, True, False, False]
Human Feedback received at timestep 9563 of -1
Current timestep = 9564. State = [[-0.18253173 -0.17531367]]. Action = [[-0.16437986 -0.1791189  -0.08411208 -0.52203506]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 9564 is [True, False, False, True, False, False]
Scene graph at timestep 9564 is [True, False, False, True, False, False]
State prediction error at timestep 9564 is tensor(4.8264e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9565. State = [[-0.18586512 -0.17610441]]. Action = [[ 0.10249519  0.10216826 -0.18097346 -0.1805132 ]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 9565 is [True, False, False, True, False, False]
Human Feedback received at timestep 9565 of -1
Current timestep = 9566. State = [[-0.18669519 -0.17577645]]. Action = [[-0.03531398 -0.07552487 -0.21700332 -0.15419042]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 9566 is [True, False, False, True, False, False]
Current timestep = 9567. State = [[-0.18754664 -0.17632584]]. Action = [[-0.13915895 -0.03846511 -0.21186854 -0.45060408]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 9567 is [True, False, False, True, False, False]
Scene graph at timestep 9567 is [True, False, False, True, False, False]
State prediction error at timestep 9567 is tensor(1.5644e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9567 of -1
Current timestep = 9568. State = [[-0.18963315 -0.17808683]]. Action = [[-0.09921607 -0.1638159  -0.00652994 -0.1437012 ]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 9568 is [True, False, False, True, False, False]
Current timestep = 9569. State = [[-0.19218121 -0.1810153 ]]. Action = [[-0.18496855 -0.13191943 -0.02488214  0.23340368]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 9569 is [True, False, False, True, False, False]
Scene graph at timestep 9569 is [True, False, False, True, False, False]
State prediction error at timestep 9569 is tensor(7.9830e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9569 of -1
Current timestep = 9570. State = [[-0.19640544 -0.18489707]]. Action = [[-0.18216424 -0.06224337 -0.19750273  0.39760137]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 9570 is [True, False, False, True, False, False]
Current timestep = 9571. State = [[-0.20219877 -0.18794319]]. Action = [[-0.16223368 -0.08216205 -0.07341142 -0.3175602 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 9571 is [True, False, False, True, False, False]
Human Feedback received at timestep 9571 of -1
Current timestep = 9572. State = [[-0.20818833 -0.19101894]]. Action = [[-0.17822681 -0.05504955 -0.15466224  0.19428647]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 9572 is [True, False, False, True, False, False]
Current timestep = 9573. State = [[-0.21495055 -0.19336577]]. Action = [[-0.12225243 -0.188325    0.1461919   0.22789252]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 9573 is [True, False, False, True, False, False]
Current timestep = 9574. State = [[-0.22083849 -0.19686079]]. Action = [[ 0.11529058 -0.01964682 -0.20476002 -0.38449466]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 9574 is [True, False, False, True, False, False]
Human Feedback received at timestep 9574 of -1
Current timestep = 9575. State = [[-0.22359605 -0.19998404]]. Action = [[-0.21632048 -0.0360406   0.17231587 -0.24981612]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 9575 is [True, False, False, True, False, False]
Current timestep = 9576. State = [[-0.22667202 -0.20272684]]. Action = [[-0.21440324  0.12438112 -0.07858673  0.88896036]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 9576 is [True, False, False, True, False, False]
Current timestep = 9577. State = [[-0.23088136 -0.20305178]]. Action = [[ 0.04921335  0.10787565 -0.15731262 -0.23987031]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 9577 is [True, False, False, True, False, False]
Current timestep = 9578. State = [[-0.23473535 -0.20145455]]. Action = [[-0.23246573 -0.06766641 -0.06803045 -0.49007922]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 9578 is [True, False, False, True, False, False]
Current timestep = 9579. State = [[-0.23913841 -0.20109753]]. Action = [[-0.09732819  0.15885049 -0.19464092  0.5779712 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 9579 is [True, False, False, True, False, False]
Human Feedback received at timestep 9579 of -1
Current timestep = 9580. State = [[-0.24384905 -0.19931018]]. Action = [[-0.11460018 -0.20069106  0.00080109  0.3401997 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 9580 is [True, False, False, True, False, False]
Current timestep = 9581. State = [[-0.24821302 -0.20085433]]. Action = [[-0.13066967  0.09826261 -0.20015308 -0.888277  ]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 9581 is [True, False, False, True, False, False]
Human Feedback received at timestep 9581 of -1
Current timestep = 9582. State = [[-0.25161403 -0.20144679]]. Action = [[-0.20802751  0.13307333 -0.24604835  0.8150933 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 9582 is [True, False, False, True, False, False]
Scene graph at timestep 9582 is [True, False, False, True, False, False]
State prediction error at timestep 9582 is tensor(1.2575e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9582 of -1
Current timestep = 9583. State = [[-0.25741804 -0.19999711]]. Action = [[-0.23883434  0.00167832  0.07865292 -0.9006434 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 9583 is [True, False, False, True, False, False]
Scene graph at timestep 9583 is [True, False, False, True, False, False]
State prediction error at timestep 9583 is tensor(8.7748e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9584. State = [[-0.26634434 -0.19776042]]. Action = [[-0.0447638  -0.09204853  0.0903773  -0.91162455]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 9584 is [True, False, False, True, False, False]
Scene graph at timestep 9584 is [True, False, False, True, False, False]
State prediction error at timestep 9584 is tensor(5.7886e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9584 of -1
Current timestep = 9585. State = [[-0.2725541 -0.1974684]]. Action = [[-0.11651802 -0.1116643  -0.01156493  0.10726321]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 9585 is [True, False, False, True, False, False]
Current timestep = 9586. State = [[-0.27608842 -0.19896224]]. Action = [[-0.08683871  0.00939393  0.15276474  0.5503578 ]]. Reward = [0.]
Curr episode timestep = 98
Scene graph at timestep 9586 is [True, False, False, True, False, False]
Scene graph at timestep 9586 is [True, False, False, True, False, False]
State prediction error at timestep 9586 is tensor(6.3234e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9586 of -1
Current timestep = 9587. State = [[-0.2804135  -0.20062546]]. Action = [[-0.22183302  0.04769668  0.11193472  0.6951251 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 9587 is [True, False, False, True, False, False]
Scene graph at timestep 9587 is [True, False, False, True, False, False]
State prediction error at timestep 9587 is tensor(1.8506e-05, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9587 of -1
Current timestep = 9588. State = [[-0.28421843 -0.20142801]]. Action = [[-0.22139537  0.1274702   0.19099987  0.6684189 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 9588 is [True, False, False, True, False, False]
Scene graph at timestep 9588 is [True, False, False, True, False, False]
State prediction error at timestep 9588 is tensor(1.1509e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9589. State = [[-0.28665295 -0.20141189]]. Action = [[-0.21527311 -0.10130468 -0.04490866 -0.5259215 ]]. Reward = [0.]
Curr episode timestep = 101
Scene graph at timestep 9589 is [True, False, False, True, False, False]
Human Feedback received at timestep 9589 of -1
Current timestep = 9590. State = [[-0.2877371  -0.20137158]]. Action = [[ 0.0473218   0.04464778 -0.10442425 -0.9550848 ]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 9590 is [True, False, False, True, False, False]
Current timestep = 9591. State = [[-0.28825402 -0.20135391]]. Action = [[ 0.02415788 -0.1945333   0.10526705  0.46877623]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 9591 is [True, False, False, True, False, False]
Human Feedback received at timestep 9591 of -1
Current timestep = 9592. State = [[-0.28888673 -0.20129126]]. Action = [[-0.03307734  0.08752084  0.1378684   0.5400963 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 9592 is [True, False, False, True, False, False]
Scene graph at timestep 9592 is [True, False, False, True, False, False]
State prediction error at timestep 9592 is tensor(2.0193e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9593. State = [[-0.28946942 -0.20113353]]. Action = [[-0.18061617  0.05856329  0.05372629  0.86187387]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 9593 is [True, False, False, True, False, False]
Human Feedback received at timestep 9593 of -1
Current timestep = 9594. State = [[-0.29003868 -0.2008738 ]]. Action = [[-0.00843023  0.15061677  0.04144672  0.23267615]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 9594 is [True, False, False, True, False, False]
Current timestep = 9595. State = [[-0.29030922 -0.20075472]]. Action = [[-0.18801533 -0.13316956 -0.20188363 -0.77541465]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 9595 is [True, False, False, True, False, False]
Human Feedback received at timestep 9595 of -1
Current timestep = 9596. State = [[-0.29066056 -0.20061949]]. Action = [[-0.13782135 -0.02799772  0.04368934  0.1486373 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 9596 is [True, False, False, True, False, False]
Scene graph at timestep 9596 is [True, False, False, True, False, False]
State prediction error at timestep 9596 is tensor(3.0701e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9597. State = [[-0.2908214  -0.20054796]]. Action = [[-0.1637277   0.05653176  0.16202617 -0.43400228]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 9597 is [True, False, False, True, False, False]
Current timestep = 9598. State = [[-0.29114673 -0.20040554]]. Action = [[-0.09129788 -0.1593595  -0.1207667   0.14049995]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 9598 is [True, False, False, True, False, False]
Current timestep = 9599. State = [[-0.29117736 -0.20045137]]. Action = [[-0.1965261  -0.11204642 -0.05483529  0.7676399 ]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 9599 is [True, False, False, True, False, False]
Current timestep = 9600. State = [[-0.29123864 -0.20042235]]. Action = [[-0.07677823  0.09380746 -0.23954532 -0.10265142]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 9600 is [True, False, False, True, False, False]
Current timestep = 9601. State = [[-0.29123864 -0.20042235]]. Action = [[-0.15998384 -0.06457636  0.04842716  0.47829497]]. Reward = [0.]
Curr episode timestep = 113
Scene graph at timestep 9601 is [True, False, False, True, False, False]
Current timestep = 9602. State = [[-0.29136077 -0.2003645 ]]. Action = [[-0.18306251  0.17147687 -0.04631534  0.37096775]]. Reward = [0.]
Curr episode timestep = 114
Scene graph at timestep 9602 is [True, False, False, True, False, False]
Scene graph at timestep 9602 is [True, False, False, True, False, False]
State prediction error at timestep 9602 is tensor(5.1238e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9603. State = [[-0.29133773 -0.20038001]]. Action = [[-0.10578696 -0.17351751 -0.11494485  0.7144587 ]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 9603 is [True, False, False, True, False, False]
Current timestep = 9604. State = [[-0.29133773 -0.20038001]]. Action = [[-0.14576426 -0.01081949 -0.19093603 -0.23379993]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 9604 is [True, False, False, True, False, False]
Current timestep = 9605. State = [[-0.29133773 -0.20038001]]. Action = [[-0.1401517   0.14817989 -0.0948623  -0.46621203]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 9605 is [True, False, False, True, False, False]
Current timestep = 9606. State = [[-0.29133773 -0.20038001]]. Action = [[ 0.04783905  0.03789592 -0.10250703 -0.7770485 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 9606 is [True, False, False, True, False, False]
Current timestep = 9607. State = [[-0.29133773 -0.20038001]]. Action = [[-0.19252759 -0.12007046 -0.2210082  -0.8555964 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 9607 is [True, False, False, True, False, False]
Current timestep = 9608. State = [[-0.29133773 -0.20038001]]. Action = [[-0.20411746  0.1861878  -0.12874958 -0.6528735 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 9608 is [True, False, False, True, False, False]
Scene graph at timestep 9608 is [True, False, False, True, False, False]
State prediction error at timestep 9608 is tensor(1.3390e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9609. State = [[-0.29133773 -0.20038001]]. Action = [[-0.14454699 -0.03518379 -0.1491155   0.23036146]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 9609 is [True, False, False, True, False, False]
Current timestep = 9610. State = [[-0.29133773 -0.20038001]]. Action = [[ 0.00719082 -0.19420859 -0.0031808   0.8598647 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 9610 is [True, False, False, True, False, False]
Scene graph at timestep 9610 is [True, False, False, True, False, False]
State prediction error at timestep 9610 is tensor(4.7322e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9611. State = [[-0.29133773 -0.20038001]]. Action = [[-0.08091727 -0.20965846 -0.11190327  0.33681762]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 9611 is [True, False, False, True, False, False]
Scene graph at timestep 9611 is [True, False, False, True, False, False]
State prediction error at timestep 9611 is tensor(1.2391e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9612. State = [[-0.29133773 -0.20038001]]. Action = [[-0.15673041  0.13285643 -0.05981082 -0.5076197 ]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 9612 is [True, False, False, True, False, False]
Current timestep = 9613. State = [[-0.29133773 -0.20038001]]. Action = [[-0.12799442 -0.15836453  0.11518252 -0.15216523]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 9613 is [True, False, False, True, False, False]
Scene graph at timestep 9613 is [True, False, False, True, False, False]
State prediction error at timestep 9613 is tensor(1.0921e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9614. State = [[-0.29133773 -0.20038001]]. Action = [[-0.12458271  0.14629728 -0.08441693  0.2885903 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 9614 is [True, False, False, True, False, False]
Scene graph at timestep 9614 is [True, False, False, True, False, False]
State prediction error at timestep 9614 is tensor(9.2504e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9615. State = [[-0.29133773 -0.20038001]]. Action = [[-0.16058521 -0.08271255 -0.20792148  0.05425954]]. Reward = [0.]
Curr episode timestep = 127
Scene graph at timestep 9615 is [True, False, False, True, False, False]
Current timestep = 9616. State = [[-0.29133773 -0.20038001]]. Action = [[-0.05931869 -0.20156196 -0.12946077 -0.60886145]]. Reward = [0.]
Curr episode timestep = 128
Scene graph at timestep 9616 is [True, False, False, True, False, False]
Current timestep = 9617. State = [[-0.29133773 -0.20038001]]. Action = [[-0.14175366  0.03755018 -0.13583708 -0.30167282]]. Reward = [0.]
Curr episode timestep = 129
Scene graph at timestep 9617 is [True, False, False, True, False, False]
Scene graph at timestep 9617 is [True, False, False, True, False, False]
State prediction error at timestep 9617 is tensor(7.4325e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9618. State = [[-0.29133773 -0.20038001]]. Action = [[-0.082133   -0.19696938  0.02483842 -0.8281176 ]]. Reward = [0.]
Curr episode timestep = 130
Scene graph at timestep 9618 is [True, False, False, True, False, False]
Current timestep = 9619. State = [[-0.29133773 -0.20038001]]. Action = [[-0.10047448 -0.21920292 -0.06780095 -0.04506803]]. Reward = [0.]
Curr episode timestep = 131
Scene graph at timestep 9619 is [True, False, False, True, False, False]
Scene graph at timestep 9619 is [True, False, False, True, False, False]
State prediction error at timestep 9619 is tensor(2.5188e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9620. State = [[-0.29133773 -0.20038001]]. Action = [[-0.19737056  0.16962588 -0.23157972  0.72745633]]. Reward = [0.]
Curr episode timestep = 132
Scene graph at timestep 9620 is [True, False, False, True, False, False]
Current timestep = 9621. State = [[-0.29133773 -0.20038001]]. Action = [[-0.18800801 -0.07832275 -0.11463393 -0.00446784]]. Reward = [0.]
Curr episode timestep = 133
Scene graph at timestep 9621 is [True, False, False, True, False, False]
Current timestep = 9622. State = [[-0.29133773 -0.20038001]]. Action = [[-0.09490442 -0.0317011  -0.1000371  -0.16602188]]. Reward = [0.]
Curr episode timestep = 134
Scene graph at timestep 9622 is [True, False, False, True, False, False]
Current timestep = 9623. State = [[-0.29133773 -0.20038001]]. Action = [[-0.20486435  0.0249975  -0.23425902  0.81767774]]. Reward = [0.]
Curr episode timestep = 135
Scene graph at timestep 9623 is [True, False, False, True, False, False]
Current timestep = 9624. State = [[-0.29133773 -0.20038001]]. Action = [[-0.19290027 -0.03134607  0.06109971 -0.85596174]]. Reward = [0.]
Curr episode timestep = 136
Scene graph at timestep 9624 is [True, False, False, True, False, False]
Current timestep = 9625. State = [[-0.29133773 -0.20038001]]. Action = [[-0.01095286 -0.06807062 -0.20415144  0.5435082 ]]. Reward = [0.]
Curr episode timestep = 137
Scene graph at timestep 9625 is [True, False, False, True, False, False]
Current timestep = 9626. State = [[-0.29133773 -0.20038001]]. Action = [[-0.18395825 -0.14768669  0.00398746 -0.02520514]]. Reward = [0.]
Curr episode timestep = 138
Scene graph at timestep 9626 is [True, False, False, True, False, False]
Scene graph at timestep 9626 is [True, False, False, True, False, False]
State prediction error at timestep 9626 is tensor(2.1416e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9627. State = [[-0.29133773 -0.20038001]]. Action = [[-0.10751361  0.02443621  0.16045839  0.92529404]]. Reward = [0.]
Curr episode timestep = 139
Scene graph at timestep 9627 is [True, False, False, True, False, False]
Scene graph at timestep 9627 is [True, False, False, True, False, False]
State prediction error at timestep 9627 is tensor(5.2708e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9628. State = [[-0.29133773 -0.20038001]]. Action = [[-0.03860292 -0.04639067  0.17709664  0.6525562 ]]. Reward = [0.]
Curr episode timestep = 140
Scene graph at timestep 9628 is [True, False, False, True, False, False]
Current timestep = 9629. State = [[-0.29133773 -0.20038001]]. Action = [[-0.22084603  0.16138834 -0.23764147 -0.7560867 ]]. Reward = [0.]
Curr episode timestep = 141
Scene graph at timestep 9629 is [True, False, False, True, False, False]
Current timestep = 9630. State = [[-0.29133773 -0.20038001]]. Action = [[-0.19043711 -0.09480776 -0.09649804 -0.2327342 ]]. Reward = [0.]
Curr episode timestep = 142
Scene graph at timestep 9630 is [True, False, False, True, False, False]
Current timestep = 9631. State = [[-0.29133773 -0.20038001]]. Action = [[-0.12686104 -0.21883512 -0.01914537 -0.65501887]]. Reward = [0.]
Curr episode timestep = 143
Scene graph at timestep 9631 is [True, False, False, True, False, False]
Current timestep = 9632. State = [[-0.29133773 -0.20038001]]. Action = [[-0.01902248  0.12425166  0.23431846 -0.9254992 ]]. Reward = [0.]
Curr episode timestep = 144
Scene graph at timestep 9632 is [True, False, False, True, False, False]
Current timestep = 9633. State = [[-0.29133773 -0.20038001]]. Action = [[-0.00401023 -0.14859708  0.10646108  0.12685621]]. Reward = [0.]
Curr episode timestep = 145
Scene graph at timestep 9633 is [True, False, False, True, False, False]
Scene graph at timestep 9633 is [True, False, False, True, False, False]
State prediction error at timestep 9633 is tensor(5.3881e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9634. State = [[-0.29133773 -0.20038001]]. Action = [[-0.02466571 -0.19712941  0.18881404 -0.89769554]]. Reward = [0.]
Curr episode timestep = 146
Scene graph at timestep 9634 is [True, False, False, True, False, False]
Scene graph at timestep 9634 is [True, False, False, True, False, False]
State prediction error at timestep 9634 is tensor(2.6378e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9635. State = [[-0.29133773 -0.20038001]]. Action = [[ 0.13757685  0.07390839 -0.227793   -0.6053279 ]]. Reward = [0.]
Curr episode timestep = 147
Scene graph at timestep 9635 is [True, False, False, True, False, False]
Scene graph at timestep 9635 is [True, False, False, True, False, False]
State prediction error at timestep 9635 is tensor(7.6784e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9636. State = [[-0.29133773 -0.20038001]]. Action = [[-0.22699519 -0.17379002  0.05653715  0.79553354]]. Reward = [0.]
Curr episode timestep = 148
Scene graph at timestep 9636 is [True, False, False, True, False, False]
Current timestep = 9637. State = [[-0.29133773 -0.20038001]]. Action = [[-0.16264905 -0.14294094 -0.24918996  0.5768237 ]]. Reward = [0.]
Curr episode timestep = 149
Scene graph at timestep 9637 is [True, False, False, True, False, False]
Current timestep = 9638. State = [[-0.29133773 -0.20038001]]. Action = [[-0.18880236 -0.09340388 -0.01306836  0.5456917 ]]. Reward = [0.]
Curr episode timestep = 150
Scene graph at timestep 9638 is [True, False, False, True, False, False]
Current timestep = 9639. State = [[-0.29133773 -0.20038001]]. Action = [[-0.05358627  0.01323763 -0.07149304  0.6759889 ]]. Reward = [0.]
Curr episode timestep = 151
Scene graph at timestep 9639 is [True, False, False, True, False, False]
Current timestep = 9640. State = [[-0.29133773 -0.20038001]]. Action = [[-0.18960005 -0.01820467  0.02486992  0.35545886]]. Reward = [0.]
Curr episode timestep = 152
Scene graph at timestep 9640 is [True, False, False, True, False, False]
Current timestep = 9641. State = [[-0.29133773 -0.20038001]]. Action = [[-0.11244977  0.1028477  -0.11887637 -0.5646404 ]]. Reward = [0.]
Curr episode timestep = 153
Scene graph at timestep 9641 is [True, False, False, True, False, False]
Current timestep = 9642. State = [[-0.29133773 -0.20038001]]. Action = [[-0.22674564  0.16411728 -0.12135932  0.5702429 ]]. Reward = [0.]
Curr episode timestep = 154
Scene graph at timestep 9642 is [True, False, False, True, False, False]
Current timestep = 9643. State = [[-0.29133773 -0.20038001]]. Action = [[-0.16998416 -0.13445212 -0.20089135  0.5044484 ]]. Reward = [0.]
Curr episode timestep = 155
Scene graph at timestep 9643 is [True, False, False, True, False, False]
Current timestep = 9644. State = [[-0.29133773 -0.20038001]]. Action = [[-0.2281577  -0.08213538 -0.23737335 -0.58997387]]. Reward = [0.]
Curr episode timestep = 156
Scene graph at timestep 9644 is [True, False, False, True, False, False]
Scene graph at timestep 9644 is [True, False, False, True, False, False]
State prediction error at timestep 9644 is tensor(3.8745e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9645. State = [[-0.29133773 -0.20038001]]. Action = [[-0.12264565 -0.15979744  0.17890513 -0.6581316 ]]. Reward = [0.]
Curr episode timestep = 157
Scene graph at timestep 9645 is [True, False, False, True, False, False]
Current timestep = 9646. State = [[-0.291399   -0.20035098]]. Action = [[-0.22323737  0.20216244 -0.20441192 -0.8852159 ]]. Reward = [0.]
Curr episode timestep = 158
Scene graph at timestep 9646 is [True, False, False, True, False, False]
Current timestep = 9647. State = [[-0.291399   -0.20035098]]. Action = [[-0.1342544  -0.11873734  0.11720586 -0.78285193]]. Reward = [0.]
Curr episode timestep = 159
Scene graph at timestep 9647 is [True, False, False, True, False, False]
Current timestep = 9648. State = [[-0.291399   -0.20035098]]. Action = [[-0.12468946 -0.14703825  0.10868624  0.39003253]]. Reward = [0.]
Curr episode timestep = 160
Scene graph at timestep 9648 is [True, False, False, True, False, False]
Current timestep = 9649. State = [[-0.291399   -0.20035098]]. Action = [[-0.0918768  -0.2116425   0.06007463 -0.44607908]]. Reward = [0.]
Curr episode timestep = 161
Scene graph at timestep 9649 is [True, False, False, True, False, False]
Current timestep = 9650. State = [[-0.291399   -0.20035098]]. Action = [[-0.19474809 -0.1533643  -0.24034512 -0.97107434]]. Reward = [0.]
Curr episode timestep = 162
Scene graph at timestep 9650 is [True, False, False, True, False, False]
Current timestep = 9651. State = [[-0.291399   -0.20035098]]. Action = [[-0.17014858  0.24033749  0.03927869  0.76669   ]]. Reward = [0.]
Curr episode timestep = 163
Scene graph at timestep 9651 is [True, False, False, True, False, False]
Current timestep = 9652. State = [[-0.291399   -0.20035098]]. Action = [[-0.1348585  -0.12306046 -0.23363343  0.8691683 ]]. Reward = [0.]
Curr episode timestep = 164
Scene graph at timestep 9652 is [True, False, False, True, False, False]
Scene graph at timestep 9652 is [True, False, False, True, False, False]
State prediction error at timestep 9652 is tensor(1.1060e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9653. State = [[-0.291399   -0.20035098]]. Action = [[-0.1971639   0.06754509 -0.02916902  0.8146715 ]]. Reward = [0.]
Curr episode timestep = 165
Scene graph at timestep 9653 is [True, False, False, True, False, False]
Current timestep = 9654. State = [[-0.291399   -0.20035098]]. Action = [[-0.15668695  0.21803516 -0.20540483  0.10416853]]. Reward = [0.]
Curr episode timestep = 166
Scene graph at timestep 9654 is [True, False, False, True, False, False]
Current timestep = 9655. State = [[-0.291399   -0.20035098]]. Action = [[-0.16684939  0.09789437  0.09326917 -0.8229807 ]]. Reward = [0.]
Curr episode timestep = 167
Scene graph at timestep 9655 is [True, False, False, True, False, False]
Scene graph at timestep 9655 is [True, False, False, True, False, False]
State prediction error at timestep 9655 is tensor(1.7280e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9656. State = [[-0.291399   -0.20035098]]. Action = [[-0.05918019 -0.10591918  0.01080191 -0.7375204 ]]. Reward = [0.]
Curr episode timestep = 168
Scene graph at timestep 9656 is [True, False, False, True, False, False]
Current timestep = 9657. State = [[-0.291399   -0.20035098]]. Action = [[-0.11760709  0.09943444 -0.05097292  0.4231081 ]]. Reward = [0.]
Curr episode timestep = 169
Scene graph at timestep 9657 is [True, False, False, True, False, False]
Current timestep = 9658. State = [[-0.291399   -0.20035098]]. Action = [[-0.03586242 -0.13514128 -0.04453416  0.7004287 ]]. Reward = [0.]
Curr episode timestep = 170
Scene graph at timestep 9658 is [True, False, False, True, False, False]
Current timestep = 9659. State = [[-0.291399   -0.20035098]]. Action = [[-0.2152833  -0.21140152 -0.02840644 -0.71444917]]. Reward = [0.]
Curr episode timestep = 171
Scene graph at timestep 9659 is [True, False, False, True, False, False]
Scene graph at timestep 9659 is [True, False, False, True, False, False]
State prediction error at timestep 9659 is tensor(2.0761e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9660. State = [[-0.291399   -0.20035098]]. Action = [[-0.24100052  0.03299239 -0.14794745 -0.8675001 ]]. Reward = [0.]
Curr episode timestep = 172
Scene graph at timestep 9660 is [True, False, False, True, False, False]
Current timestep = 9661. State = [[-0.291399   -0.20035098]]. Action = [[-0.2425065   0.11158288  0.14021754  0.43297958]]. Reward = [0.]
Curr episode timestep = 173
Scene graph at timestep 9661 is [True, False, False, True, False, False]
Scene graph at timestep 9661 is [True, False, False, True, False, False]
State prediction error at timestep 9661 is tensor(3.5484e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9662. State = [[-0.291399   -0.20035098]]. Action = [[-0.07846901 -0.18429665 -0.19503571  0.7141249 ]]. Reward = [0.]
Curr episode timestep = 174
Scene graph at timestep 9662 is [True, False, False, True, False, False]
Current timestep = 9663. State = [[-0.291399   -0.20035098]]. Action = [[-0.15149342 -0.10910158 -0.23025808  0.8203268 ]]. Reward = [0.]
Curr episode timestep = 175
Scene graph at timestep 9663 is [True, False, False, True, False, False]
Current timestep = 9664. State = [[-0.291399   -0.20035098]]. Action = [[-0.21246707 -0.21884811  0.05902401 -0.9658043 ]]. Reward = [0.]
Curr episode timestep = 176
Scene graph at timestep 9664 is [True, False, False, True, False, False]
Current timestep = 9665. State = [[-0.291399   -0.20035098]]. Action = [[-0.18266831 -0.0968121   0.06822798  0.46103907]]. Reward = [0.]
Curr episode timestep = 177
Scene graph at timestep 9665 is [True, False, False, True, False, False]
Current timestep = 9666. State = [[-0.291399   -0.20035098]]. Action = [[-0.21827503 -0.16222996 -0.18766776 -0.9469027 ]]. Reward = [0.]
Curr episode timestep = 178
Scene graph at timestep 9666 is [True, False, False, True, False, False]
Scene graph at timestep 9666 is [True, False, False, True, False, False]
State prediction error at timestep 9666 is tensor(1.8780e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9667. State = [[-0.291399   -0.20035098]]. Action = [[-0.19683471  0.19891134  0.02639642 -0.7323028 ]]. Reward = [0.]
Curr episode timestep = 179
Scene graph at timestep 9667 is [True, False, False, True, False, False]
Current timestep = 9668. State = [[-0.291399   -0.20035098]]. Action = [[-0.03411895 -0.2402434  -0.06362873 -0.4282269 ]]. Reward = [0.]
Curr episode timestep = 180
Scene graph at timestep 9668 is [True, False, False, True, False, False]
Current timestep = 9669. State = [[-0.291399   -0.20035098]]. Action = [[-0.06198987 -0.21090232 -0.22724488  0.91098356]]. Reward = [0.]
Curr episode timestep = 181
Scene graph at timestep 9669 is [True, False, False, True, False, False]
Current timestep = 9670. State = [[-0.291399   -0.20035098]]. Action = [[-0.18299383 -0.1320645  -0.03641285  0.84593844]]. Reward = [0.]
Curr episode timestep = 182
Scene graph at timestep 9670 is [True, False, False, True, False, False]
Current timestep = 9671. State = [[-0.291399   -0.20035098]]. Action = [[-0.15763883 -0.01751144  0.05323884 -0.64586407]]. Reward = [0.]
Curr episode timestep = 183
Scene graph at timestep 9671 is [True, False, False, True, False, False]
Current timestep = 9672. State = [[-0.291399   -0.20035098]]. Action = [[-0.09238532 -0.21216115 -0.1136201  -0.23052436]]. Reward = [0.]
Curr episode timestep = 184
Scene graph at timestep 9672 is [True, False, False, True, False, False]
Current timestep = 9673. State = [[-0.291399   -0.20035098]]. Action = [[-0.21896449 -0.16637853  0.21453464 -0.95841986]]. Reward = [0.]
Curr episode timestep = 185
Scene graph at timestep 9673 is [True, False, False, True, False, False]
Current timestep = 9674. State = [[-0.291399   -0.20035098]]. Action = [[-0.21403466 -0.06761158  0.23393157 -0.40870035]]. Reward = [0.]
Curr episode timestep = 186
Scene graph at timestep 9674 is [True, False, False, True, False, False]
Current timestep = 9675. State = [[-0.291399   -0.20035098]]. Action = [[-0.22288138 -0.11656225 -0.02831157  0.922354  ]]. Reward = [0.]
Curr episode timestep = 187
Scene graph at timestep 9675 is [True, False, False, True, False, False]
Current timestep = 9676. State = [[-0.291399   -0.20035098]]. Action = [[-0.16294813  0.00678143 -0.09408174 -0.40606344]]. Reward = [0.]
Curr episode timestep = 188
Scene graph at timestep 9676 is [True, False, False, True, False, False]
Current timestep = 9677. State = [[-0.291399   -0.20035098]]. Action = [[-0.23471947 -0.1796078  -0.18601269  0.8218664 ]]. Reward = [0.]
Curr episode timestep = 189
Scene graph at timestep 9677 is [True, False, False, True, False, False]
Current timestep = 9678. State = [[-0.291399   -0.20035098]]. Action = [[-0.07279217 -0.06070599 -0.22586831  0.38159978]]. Reward = [0.]
Curr episode timestep = 190
Scene graph at timestep 9678 is [True, False, False, True, False, False]
Current timestep = 9679. State = [[-0.291399   -0.20035098]]. Action = [[-0.19822505  0.09059274  0.10953158  0.39643657]]. Reward = [0.]
Curr episode timestep = 191
Scene graph at timestep 9679 is [True, False, False, True, False, False]
Current timestep = 9680. State = [[-0.291399   -0.20035098]]. Action = [[ 0.09477806 -0.18325152 -0.2373286   0.8924749 ]]. Reward = [0.]
Curr episode timestep = 192
Scene graph at timestep 9680 is [True, False, False, True, False, False]
Current timestep = 9681. State = [[-0.291399   -0.20035098]]. Action = [[-0.16212459  0.06207141 -0.06788227  0.2738062 ]]. Reward = [0.]
Curr episode timestep = 193
Scene graph at timestep 9681 is [True, False, False, True, False, False]
Current timestep = 9682. State = [[-0.291399   -0.20035098]]. Action = [[-0.16921179  0.07593623 -0.0503327  -0.76004356]]. Reward = [0.]
Curr episode timestep = 194
Scene graph at timestep 9682 is [True, False, False, True, False, False]
Current timestep = 9683. State = [[-0.291399   -0.20035098]]. Action = [[0.01881963 0.22772795 0.06132844 0.65301025]]. Reward = [0.]
Curr episode timestep = 195
Scene graph at timestep 9683 is [True, False, False, True, False, False]
Scene graph at timestep 9683 is [True, False, False, True, False, False]
State prediction error at timestep 9683 is tensor(2.1465e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9684. State = [[-0.291399   -0.20035098]]. Action = [[-0.19299164 -0.08122501  0.10572255 -0.427222  ]]. Reward = [0.]
Curr episode timestep = 196
Scene graph at timestep 9684 is [True, False, False, True, False, False]
Current timestep = 9685. State = [[-0.291399   -0.20035098]]. Action = [[-0.12458208 -0.14885105 -0.08377051  0.3194877 ]]. Reward = [0.]
Curr episode timestep = 197
Scene graph at timestep 9685 is [True, False, False, True, False, False]
Current timestep = 9686. State = [[-0.291399   -0.20035098]]. Action = [[-0.19271806 -0.1202974  -0.23411086  0.9547337 ]]. Reward = [0.]
Curr episode timestep = 198
Scene graph at timestep 9686 is [True, False, False, True, False, False]
Current timestep = 9687. State = [[-0.291399   -0.20035098]]. Action = [[ 0.1034162   0.20548055  0.18939182 -0.53253347]]. Reward = [0.]
Curr episode timestep = 199
Scene graph at timestep 9687 is [True, False, False, True, False, False]
Current timestep = 9688. State = [[-0.291399   -0.20035098]]. Action = [[-0.12910287 -0.11257491  0.00475112 -0.8080764 ]]. Reward = [0.]
Curr episode timestep = 200
Scene graph at timestep 9688 is [True, False, False, True, False, False]
Current timestep = 9689. State = [[-0.291399   -0.20035098]]. Action = [[-0.1964187  -0.07119253 -0.09963733 -0.2626624 ]]. Reward = [0.]
Curr episode timestep = 201
Scene graph at timestep 9689 is [True, False, False, True, False, False]
Current timestep = 9690. State = [[-0.291399   -0.20035098]]. Action = [[-0.05102749 -0.08463162 -0.19173218 -0.7901648 ]]. Reward = [0.]
Curr episode timestep = 202
Scene graph at timestep 9690 is [True, False, False, True, False, False]
Current timestep = 9691. State = [[-0.291399   -0.20035098]]. Action = [[-0.1709173  -0.21929619  0.18519253 -0.26140177]]. Reward = [0.]
Curr episode timestep = 203
Scene graph at timestep 9691 is [True, False, False, True, False, False]
Current timestep = 9692. State = [[-0.291399   -0.20035098]]. Action = [[ 0.05942476 -0.12404612 -0.15861559  0.21157455]]. Reward = [0.]
Curr episode timestep = 204
Scene graph at timestep 9692 is [True, False, False, True, False, False]
Current timestep = 9693. State = [[-0.291399   -0.20035098]]. Action = [[-0.21842271  0.15935785  0.00502735 -0.73815805]]. Reward = [0.]
Curr episode timestep = 205
Scene graph at timestep 9693 is [True, False, False, True, False, False]
Current timestep = 9694. State = [[-0.291399   -0.20035098]]. Action = [[-0.21503468 -0.03165932 -0.10034361  0.8569207 ]]. Reward = [0.]
Curr episode timestep = 206
Scene graph at timestep 9694 is [True, False, False, True, False, False]
Current timestep = 9695. State = [[-0.291399   -0.20035098]]. Action = [[-0.04189126  0.11444429 -0.24411298 -0.20024467]]. Reward = [0.]
Curr episode timestep = 207
Scene graph at timestep 9695 is [True, False, False, True, False, False]
Current timestep = 9696. State = [[-0.291399   -0.20035098]]. Action = [[ 0.0134317   0.16820401  0.21934646 -0.8462995 ]]. Reward = [0.]
Curr episode timestep = 208
Scene graph at timestep 9696 is [True, False, False, True, False, False]
Scene graph at timestep 9696 is [True, False, False, True, False, False]
State prediction error at timestep 9696 is tensor(2.9148e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9697. State = [[-0.291399   -0.20035098]]. Action = [[-0.2169032  -0.24033204 -0.24149375  0.33622503]]. Reward = [0.]
Curr episode timestep = 209
Scene graph at timestep 9697 is [True, False, False, True, False, False]
Current timestep = 9698. State = [[-0.291399   -0.20035098]]. Action = [[-0.18659097  0.05478126  0.13201565  0.24376261]]. Reward = [0.]
Curr episode timestep = 210
Scene graph at timestep 9698 is [True, False, False, True, False, False]
Current timestep = 9699. State = [[-0.291399   -0.20035098]]. Action = [[-0.194236   -0.18692222 -0.19841649  0.7410873 ]]. Reward = [0.]
Curr episode timestep = 211
Scene graph at timestep 9699 is [True, False, False, True, False, False]
Current timestep = 9700. State = [[-0.291399   -0.20035098]]. Action = [[-0.12307556  0.15079069 -0.22428516 -0.9190951 ]]. Reward = [0.]
Curr episode timestep = 212
Scene graph at timestep 9700 is [True, False, False, True, False, False]
Current timestep = 9701. State = [[-0.291399   -0.20035098]]. Action = [[-0.22017671  0.1751385  -0.04477665 -0.81005293]]. Reward = [0.]
Curr episode timestep = 213
Scene graph at timestep 9701 is [True, False, False, True, False, False]
Current timestep = 9702. State = [[-0.291399   -0.20035098]]. Action = [[-0.15310727  0.11341721  0.01589435 -0.64661974]]. Reward = [0.]
Curr episode timestep = 214
Scene graph at timestep 9702 is [True, False, False, True, False, False]
Current timestep = 9703. State = [[-0.291399   -0.20035098]]. Action = [[-0.21085662 -0.04398675 -0.23783408 -0.6053237 ]]. Reward = [0.]
Curr episode timestep = 215
Scene graph at timestep 9703 is [True, False, False, True, False, False]
Scene graph at timestep 9703 is [True, False, False, True, False, False]
State prediction error at timestep 9703 is tensor(5.7477e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9704. State = [[-0.291399   -0.20035098]]. Action = [[-0.01900734 -0.21612377 -0.10475218  0.20555615]]. Reward = [0.]
Curr episode timestep = 216
Scene graph at timestep 9704 is [True, False, False, True, False, False]
Scene graph at timestep 9704 is [True, False, False, True, False, False]
State prediction error at timestep 9704 is tensor(1.1482e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9705. State = [[-0.291399   -0.20035098]]. Action = [[-0.17817332  0.13477987  0.0085052   0.9454701 ]]. Reward = [0.]
Curr episode timestep = 217
Scene graph at timestep 9705 is [True, False, False, True, False, False]
Current timestep = 9706. State = [[-0.291399   -0.20035098]]. Action = [[-0.1824749 -0.0354678  0.0063197  0.3756249]]. Reward = [0.]
Curr episode timestep = 218
Scene graph at timestep 9706 is [True, False, False, True, False, False]
Current timestep = 9707. State = [[-0.291399   -0.20035098]]. Action = [[-0.17417349  0.09918988 -0.02758628 -0.15655869]]. Reward = [0.]
Curr episode timestep = 219
Scene graph at timestep 9707 is [True, False, False, True, False, False]
Scene graph at timestep 9707 is [True, False, False, True, False, False]
State prediction error at timestep 9707 is tensor(4.6442e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9708. State = [[-0.291399   -0.20035098]]. Action = [[-0.22097956  0.09820676 -0.20837548  0.11992335]]. Reward = [0.]
Curr episode timestep = 220
Scene graph at timestep 9708 is [True, False, False, True, False, False]
Current timestep = 9709. State = [[-0.291399   -0.20035098]]. Action = [[-0.09107181  0.178969   -0.09703258 -0.9133854 ]]. Reward = [0.]
Curr episode timestep = 221
Scene graph at timestep 9709 is [True, False, False, True, False, False]
Current timestep = 9710. State = [[-0.291399   -0.20035098]]. Action = [[ 3.62753868e-04 -1.12861246e-01 -1.65598780e-01  5.87285280e-01]]. Reward = [0.]
Curr episode timestep = 222
Scene graph at timestep 9710 is [True, False, False, True, False, False]
Current timestep = 9711. State = [[-0.291399   -0.20035098]]. Action = [[-0.09397131 -0.23429541 -0.21907732  0.45128417]]. Reward = [0.]
Curr episode timestep = 223
Scene graph at timestep 9711 is [True, False, False, True, False, False]
Scene graph at timestep 9711 is [True, False, False, True, False, False]
State prediction error at timestep 9711 is tensor(5.9845e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9712. State = [[-0.291399   -0.20035098]]. Action = [[-0.10872823 -0.05639005 -0.24209994 -0.94910216]]. Reward = [0.]
Curr episode timestep = 224
Scene graph at timestep 9712 is [True, False, False, True, False, False]
Current timestep = 9713. State = [[-0.291399   -0.20035098]]. Action = [[-0.08653674 -0.21518809  0.15174985 -0.9517444 ]]. Reward = [0.]
Curr episode timestep = 225
Scene graph at timestep 9713 is [True, False, False, True, False, False]
Current timestep = 9714. State = [[-0.291399   -0.20035098]]. Action = [[-0.22853486  0.16817456 -0.05546235 -0.5210387 ]]. Reward = [0.]
Curr episode timestep = 226
Scene graph at timestep 9714 is [True, False, False, True, False, False]
Current timestep = 9715. State = [[-0.291399   -0.20035098]]. Action = [[-0.07858056 -0.02913114  0.22623712 -0.5191497 ]]. Reward = [0.]
Curr episode timestep = 227
Scene graph at timestep 9715 is [True, False, False, True, False, False]
Current timestep = 9716. State = [[-0.291399   -0.20035098]]. Action = [[-0.2303566   0.16974762 -0.22738248 -0.55401146]]. Reward = [0.]
Curr episode timestep = 228
Scene graph at timestep 9716 is [True, False, False, True, False, False]
Current timestep = 9717. State = [[-0.291399   -0.20035098]]. Action = [[-0.03769004  0.2169767  -0.18018928  0.63444126]]. Reward = [0.]
Curr episode timestep = 229
Scene graph at timestep 9717 is [True, False, False, True, False, False]
Current timestep = 9718. State = [[-0.291399   -0.20035098]]. Action = [[-0.14398454 -0.08652982 -0.12233201 -0.56403214]]. Reward = [0.]
Curr episode timestep = 230
Scene graph at timestep 9718 is [True, False, False, True, False, False]
Scene graph at timestep 9718 is [True, False, False, True, False, False]
State prediction error at timestep 9718 is tensor(1.8511e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9719. State = [[-0.291399   -0.20035098]]. Action = [[-0.21844879  0.19497651 -0.24884371 -0.9350541 ]]. Reward = [0.]
Curr episode timestep = 231
Scene graph at timestep 9719 is [True, False, False, True, False, False]
Scene graph at timestep 9719 is [True, False, False, True, False, False]
State prediction error at timestep 9719 is tensor(2.1336e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9720. State = [[-0.291399   -0.20035098]]. Action = [[-0.19761969 -0.23735791 -0.23769867 -0.96312916]]. Reward = [0.]
Curr episode timestep = 232
Scene graph at timestep 9720 is [True, False, False, True, False, False]
Current timestep = 9721. State = [[-0.291399   -0.20035098]]. Action = [[-0.2227485  -0.07338493 -0.11380106 -0.9175302 ]]. Reward = [0.]
Curr episode timestep = 233
Scene graph at timestep 9721 is [True, False, False, True, False, False]
Current timestep = 9722. State = [[-0.291399   -0.20035098]]. Action = [[-0.22357132 -0.23776793  0.18088365 -0.05212617]]. Reward = [0.]
Curr episode timestep = 234
Scene graph at timestep 9722 is [True, False, False, True, False, False]
Scene graph at timestep 9722 is [True, False, False, True, False, False]
State prediction error at timestep 9722 is tensor(1.0762e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9723. State = [[-0.291399   -0.20035098]]. Action = [[-0.09984589 -0.1803288  -0.01152903  0.69995356]]. Reward = [0.]
Curr episode timestep = 235
Scene graph at timestep 9723 is [True, False, False, True, False, False]
Current timestep = 9724. State = [[-0.291399   -0.20035098]]. Action = [[-0.20394649 -0.06410919 -0.23456821  0.93910193]]. Reward = [0.]
Curr episode timestep = 236
Scene graph at timestep 9724 is [True, False, False, True, False, False]
Scene graph at timestep 9724 is [True, False, False, True, False, False]
State prediction error at timestep 9724 is tensor(9.9075e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9725. State = [[-0.291399   -0.20035098]]. Action = [[-0.09527817  0.03942451  0.15371782 -0.5599733 ]]. Reward = [0.]
Curr episode timestep = 237
Scene graph at timestep 9725 is [True, False, False, True, False, False]
Scene graph at timestep 9725 is [True, False, False, True, False, False]
State prediction error at timestep 9725 is tensor(3.0431e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9726. State = [[-0.291399   -0.20035098]]. Action = [[-0.22379746 -0.1782618   0.22146785  0.23110867]]. Reward = [0.]
Curr episode timestep = 238
Scene graph at timestep 9726 is [True, False, False, True, False, False]
Current timestep = 9727. State = [[-0.291399   -0.20035098]]. Action = [[ 0.05536643 -0.03085825  0.2447373   0.28982782]]. Reward = [0.]
Curr episode timestep = 239
Scene graph at timestep 9727 is [True, False, False, True, False, False]
Current timestep = 9728. State = [[-0.291399   -0.20035098]]. Action = [[-0.14095494 -0.11308616 -0.14639324 -0.87111026]]. Reward = [0.]
Curr episode timestep = 240
Scene graph at timestep 9728 is [True, False, False, True, False, False]
Current timestep = 9729. State = [[-0.291399   -0.20035098]]. Action = [[-0.19704734 -0.10727921 -0.20066233  0.77044773]]. Reward = [0.]
Curr episode timestep = 241
Scene graph at timestep 9729 is [True, False, False, True, False, False]
Current timestep = 9730. State = [[-0.291399   -0.20035098]]. Action = [[ 0.0163047  -0.02856961  0.18202597  0.2661891 ]]. Reward = [0.]
Curr episode timestep = 242
Scene graph at timestep 9730 is [True, False, False, True, False, False]
Current timestep = 9731. State = [[-0.291399   -0.20035098]]. Action = [[-0.03387256  0.0247387  -0.23032701 -0.7151489 ]]. Reward = [0.]
Curr episode timestep = 243
Scene graph at timestep 9731 is [True, False, False, True, False, False]
Current timestep = 9732. State = [[-0.291399   -0.20035098]]. Action = [[-0.2142458  -0.05808708 -0.02859035  0.25335824]]. Reward = [0.]
Curr episode timestep = 244
Scene graph at timestep 9732 is [True, False, False, True, False, False]
Scene graph at timestep 9732 is [True, False, False, True, False, False]
State prediction error at timestep 9732 is tensor(3.0724e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9733. State = [[-0.291399   -0.20035098]]. Action = [[-0.23202208 -0.13406976 -0.11701377  0.35937238]]. Reward = [0.]
Curr episode timestep = 245
Scene graph at timestep 9733 is [True, False, False, True, False, False]
Current timestep = 9734. State = [[-0.291399   -0.20035098]]. Action = [[ 0.03967974 -0.15559946 -0.24491721 -0.7896966 ]]. Reward = [0.]
Curr episode timestep = 246
Scene graph at timestep 9734 is [True, False, False, True, False, False]
Current timestep = 9735. State = [[-0.291399   -0.20035098]]. Action = [[-0.2049982  -0.2406892   0.13442737  0.0583936 ]]. Reward = [0.]
Curr episode timestep = 247
Scene graph at timestep 9735 is [True, False, False, True, False, False]
Current timestep = 9736. State = [[-0.291399   -0.20035098]]. Action = [[-0.16585673  0.18689764 -0.09992144 -0.6725579 ]]. Reward = [0.]
Curr episode timestep = 248
Scene graph at timestep 9736 is [True, False, False, True, False, False]
Current timestep = 9737. State = [[-0.291399   -0.20035098]]. Action = [[-0.14817649  0.01313627 -0.1684165  -0.74596274]]. Reward = [0.]
Curr episode timestep = 249
Scene graph at timestep 9737 is [True, False, False, True, False, False]
Current timestep = 9738. State = [[-0.291399   -0.20035098]]. Action = [[-0.21834375 -0.15703174  0.08637714 -0.50946575]]. Reward = [0.]
Curr episode timestep = 250
Scene graph at timestep 9738 is [True, False, False, True, False, False]
Current timestep = 9739. State = [[-0.291399   -0.20035098]]. Action = [[-0.21895276  0.23453194 -0.15807119 -0.81175494]]. Reward = [0.]
Curr episode timestep = 251
Scene graph at timestep 9739 is [True, False, False, True, False, False]
Current timestep = 9740. State = [[-0.291399   -0.20035098]]. Action = [[-0.22120339  0.16229829 -0.2121167  -0.1466791 ]]. Reward = [0.]
Curr episode timestep = 252
Scene graph at timestep 9740 is [True, False, False, True, False, False]
Current timestep = 9741. State = [[-0.291399   -0.20035098]]. Action = [[-0.23658608 -0.11586174 -0.23955524  0.43814027]]. Reward = [0.]
Curr episode timestep = 253
Scene graph at timestep 9741 is [True, False, False, True, False, False]
Scene graph at timestep 9741 is [True, False, False, True, False, False]
State prediction error at timestep 9741 is tensor(1.6706e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9742. State = [[-0.291399   -0.20035098]]. Action = [[-0.21603227 -0.19933206 -0.24091357  0.21381068]]. Reward = [0.]
Curr episode timestep = 254
Scene graph at timestep 9742 is [True, False, False, True, False, False]
Scene graph at timestep 9742 is [True, False, False, True, False, False]
State prediction error at timestep 9742 is tensor(9.5820e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9743. State = [[-0.291399   -0.20035098]]. Action = [[-0.10940444 -0.10589296  0.02491975 -0.40410852]]. Reward = [0.]
Curr episode timestep = 255
Scene graph at timestep 9743 is [True, False, False, True, False, False]
Current timestep = 9744. State = [[-0.291399   -0.20035098]]. Action = [[-0.23310553 -0.07799286  0.15749258 -0.11160284]]. Reward = [0.]
Curr episode timestep = 256
Scene graph at timestep 9744 is [True, False, False, True, False, False]
Current timestep = 9745. State = [[-0.291399   -0.20035098]]. Action = [[-0.18582113 -0.21282595 -0.11704454 -0.7605273 ]]. Reward = [0.]
Curr episode timestep = 257
Scene graph at timestep 9745 is [True, False, False, True, False, False]
Scene graph at timestep 9745 is [True, False, False, True, False, False]
State prediction error at timestep 9745 is tensor(1.7054e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9746. State = [[-0.291399   -0.20035098]]. Action = [[-0.07103866 -0.17747922  0.2088252  -0.8014367 ]]. Reward = [0.]
Curr episode timestep = 258
Scene graph at timestep 9746 is [True, False, False, True, False, False]
Current timestep = 9747. State = [[-0.291399   -0.20035098]]. Action = [[-0.03021416 -0.17274967  0.08939511  0.26369357]]. Reward = [0.]
Curr episode timestep = 259
Scene graph at timestep 9747 is [True, False, False, True, False, False]
Current timestep = 9748. State = [[-0.291399   -0.20035098]]. Action = [[-0.20044485  0.06434876 -0.2286363   0.50449216]]. Reward = [0.]
Curr episode timestep = 260
Scene graph at timestep 9748 is [True, False, False, True, False, False]
Current timestep = 9749. State = [[-0.291399   -0.20035098]]. Action = [[-0.23158713 -0.22648306 -0.21264598 -0.08253986]]. Reward = [0.]
Curr episode timestep = 261
Scene graph at timestep 9749 is [True, False, False, True, False, False]
Current timestep = 9750. State = [[-0.291399   -0.20035098]]. Action = [[-0.16674085 -0.15167257 -0.10256997  0.6113398 ]]. Reward = [0.]
Curr episode timestep = 262
Scene graph at timestep 9750 is [True, False, False, True, False, False]
Current timestep = 9751. State = [[-0.291399   -0.20035098]]. Action = [[-0.2203268  -0.04738162  0.226421    0.7641027 ]]. Reward = [0.]
Curr episode timestep = 263
Scene graph at timestep 9751 is [True, False, False, True, False, False]
Current timestep = 9752. State = [[-0.291399   -0.20035098]]. Action = [[-0.20516746  0.15389705 -0.17372629 -0.98596656]]. Reward = [0.]
Curr episode timestep = 264
Scene graph at timestep 9752 is [True, False, False, True, False, False]
Scene graph at timestep 9752 is [True, False, False, True, False, False]
State prediction error at timestep 9752 is tensor(1.3226e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9753. State = [[-0.291399   -0.20035098]]. Action = [[0.02744946 0.02808037 0.10780627 0.08257115]]. Reward = [0.]
Curr episode timestep = 265
Scene graph at timestep 9753 is [True, False, False, True, False, False]
Current timestep = 9754. State = [[-0.291399   -0.20035098]]. Action = [[-0.19464912  0.18351698 -0.24460647 -0.9638592 ]]. Reward = [0.]
Curr episode timestep = 266
Scene graph at timestep 9754 is [True, False, False, True, False, False]
Current timestep = 9755. State = [[-0.291399   -0.20035098]]. Action = [[-0.2243814   0.14890054  0.01181236 -0.6121573 ]]. Reward = [0.]
Curr episode timestep = 267
Scene graph at timestep 9755 is [True, False, False, True, False, False]
Current timestep = 9756. State = [[-0.291399   -0.20035098]]. Action = [[-0.22858983  0.02489284 -0.23298298 -0.9512046 ]]. Reward = [0.]
Curr episode timestep = 268
Scene graph at timestep 9756 is [True, False, False, True, False, False]
Current timestep = 9757. State = [[-0.291399   -0.20035098]]. Action = [[-0.21577224 -0.11058503 -0.22738722 -0.8192744 ]]. Reward = [0.]
Curr episode timestep = 269
Scene graph at timestep 9757 is [True, False, False, True, False, False]
Current timestep = 9758. State = [[-0.291399   -0.20035098]]. Action = [[ 0.07307011  0.2342943  -0.23628215 -0.88583887]]. Reward = [0.]
Curr episode timestep = 270
Scene graph at timestep 9758 is [True, False, False, True, False, False]
Current timestep = 9759. State = [[-0.291399   -0.20035098]]. Action = [[-0.24417144 -0.1288788   0.01245159  0.9196913 ]]. Reward = [0.]
Curr episode timestep = 271
Scene graph at timestep 9759 is [True, False, False, True, False, False]
Current timestep = 9760. State = [[-0.291399   -0.20035098]]. Action = [[-0.22211948 -0.22664203 -0.24564779  0.4509151 ]]. Reward = [0.]
Curr episode timestep = 272
Scene graph at timestep 9760 is [True, False, False, True, False, False]
Current timestep = 9761. State = [[-0.291399   -0.20035098]]. Action = [[-0.15374275 -0.20764352 -0.17075804 -0.7650417 ]]. Reward = [0.]
Curr episode timestep = 273
Scene graph at timestep 9761 is [True, False, False, True, False, False]
Scene graph at timestep 9761 is [True, False, False, True, False, False]
State prediction error at timestep 9761 is tensor(1.9597e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9762. State = [[-0.291399   -0.20035098]]. Action = [[-0.22300777 -0.02704176 -0.2474739   0.28427494]]. Reward = [0.]
Curr episode timestep = 274
Scene graph at timestep 9762 is [True, False, False, True, False, False]
Current timestep = 9763. State = [[-0.291399   -0.20035098]]. Action = [[-0.14756174 -0.18810931 -0.11039898  0.20226407]]. Reward = [0.]
Curr episode timestep = 275
Scene graph at timestep 9763 is [True, False, False, True, False, False]
Current timestep = 9764. State = [[-0.291399   -0.20035098]]. Action = [[-0.22952108  0.1844567   0.08833432 -0.079804  ]]. Reward = [0.]
Curr episode timestep = 276
Scene graph at timestep 9764 is [True, False, False, True, False, False]
Current timestep = 9765. State = [[-0.291399   -0.20035098]]. Action = [[-0.16016719 -0.24533078 -0.2234379   0.83641803]]. Reward = [0.]
Curr episode timestep = 277
Scene graph at timestep 9765 is [True, False, False, True, False, False]
Current timestep = 9766. State = [[-0.291399   -0.20035098]]. Action = [[-0.19778988 -0.0750334  -0.18846373 -0.04932177]]. Reward = [0.]
Curr episode timestep = 278
Scene graph at timestep 9766 is [True, False, False, True, False, False]
Current timestep = 9767. State = [[-0.291399   -0.20035098]]. Action = [[ 0.05421278  0.20341346 -0.15063763  0.9843383 ]]. Reward = [0.]
Curr episode timestep = 279
Scene graph at timestep 9767 is [True, False, False, True, False, False]
Current timestep = 9768. State = [[-0.291399   -0.20035098]]. Action = [[-0.2001755   0.10450345 -0.23841675 -0.8534509 ]]. Reward = [0.]
Curr episode timestep = 280
Scene graph at timestep 9768 is [True, False, False, True, False, False]
Scene graph at timestep 9768 is [True, False, False, True, False, False]
State prediction error at timestep 9768 is tensor(3.2702e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9769. State = [[-0.291399   -0.20035098]]. Action = [[-0.20332418 -0.08961013  0.13711095  0.92512083]]. Reward = [0.]
Curr episode timestep = 281
Scene graph at timestep 9769 is [True, False, False, True, False, False]
Current timestep = 9770. State = [[-0.291399   -0.20035098]]. Action = [[-0.20515451  0.02744702 -0.21848696 -0.5165257 ]]. Reward = [0.]
Curr episode timestep = 282
Scene graph at timestep 9770 is [True, False, False, True, False, False]
Scene graph at timestep 9770 is [True, False, False, True, False, False]
State prediction error at timestep 9770 is tensor(1.2298e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9771. State = [[-0.291399   -0.20035098]]. Action = [[-0.18071695 -0.11684081 -0.01175387 -0.9077355 ]]. Reward = [0.]
Curr episode timestep = 283
Scene graph at timestep 9771 is [True, False, False, True, False, False]
Current timestep = 9772. State = [[-0.291399   -0.20035098]]. Action = [[-0.20685028  0.05768859 -0.01529971 -0.8368102 ]]. Reward = [0.]
Curr episode timestep = 284
Scene graph at timestep 9772 is [True, False, False, True, False, False]
Current timestep = 9773. State = [[-0.291399   -0.20035098]]. Action = [[-0.1299597   0.08058235 -0.1429207  -0.8818335 ]]. Reward = [0.]
Curr episode timestep = 285
Scene graph at timestep 9773 is [True, False, False, True, False, False]
Current timestep = 9774. State = [[-0.291399   -0.20035098]]. Action = [[-0.23374131  0.1658892  -0.16187604 -0.89284825]]. Reward = [0.]
Curr episode timestep = 286
Scene graph at timestep 9774 is [True, False, False, True, False, False]
Current timestep = 9775. State = [[-0.291399   -0.20035098]]. Action = [[-0.14705777 -0.01536907  0.02365437  0.98417115]]. Reward = [0.]
Curr episode timestep = 287
Scene graph at timestep 9775 is [True, False, False, True, False, False]
Current timestep = 9776. State = [[-0.291399   -0.20035098]]. Action = [[-0.04221636 -0.02188063  0.04050115 -0.97207135]]. Reward = [0.]
Curr episode timestep = 288
Scene graph at timestep 9776 is [True, False, False, True, False, False]
Current timestep = 9777. State = [[-0.291399   -0.20035098]]. Action = [[-0.20953508 -0.21057926 -0.10514654  0.6766931 ]]. Reward = [0.]
Curr episode timestep = 289
Scene graph at timestep 9777 is [True, False, False, True, False, False]
Scene graph at timestep 9777 is [True, False, False, True, False, False]
State prediction error at timestep 9777 is tensor(3.8020e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9778. State = [[-0.291399   -0.20035098]]. Action = [[-0.16901894 -0.20941624 -0.1694362   0.8285619 ]]. Reward = [0.]
Curr episode timestep = 290
Scene graph at timestep 9778 is [True, False, False, True, False, False]
Current timestep = 9779. State = [[-0.291399   -0.20035098]]. Action = [[-0.22515376 -0.21381009 -0.1789813   0.40891898]]. Reward = [0.]
Curr episode timestep = 291
Scene graph at timestep 9779 is [True, False, False, True, False, False]
Current timestep = 9780. State = [[-0.291399   -0.20035098]]. Action = [[-0.14361863 -0.15809296  0.0250639   0.18854368]]. Reward = [0.]
Curr episode timestep = 292
Scene graph at timestep 9780 is [True, False, False, True, False, False]
Current timestep = 9781. State = [[-0.291399   -0.20035098]]. Action = [[ 0.06579283 -0.21281664  0.12471551 -0.8601584 ]]. Reward = [0.]
Curr episode timestep = 293
Scene graph at timestep 9781 is [True, False, False, True, False, False]
Scene graph at timestep 9781 is [True, False, False, True, False, False]
State prediction error at timestep 9781 is tensor(1.0501e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9782. State = [[-0.291399   -0.20035098]]. Action = [[-0.1939144  -0.109137   -0.12090546 -0.95538396]]. Reward = [0.]
Curr episode timestep = 294
Scene graph at timestep 9782 is [True, False, False, True, False, False]
Scene graph at timestep 9782 is [True, False, False, True, False, False]
State prediction error at timestep 9782 is tensor(1.7642e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9783. State = [[-0.291399   -0.20035098]]. Action = [[-0.1934599   0.16683313 -0.09945068 -0.46020746]]. Reward = [0.]
Curr episode timestep = 295
Scene graph at timestep 9783 is [True, False, False, True, False, False]
Current timestep = 9784. State = [[-0.291399   -0.20035098]]. Action = [[-0.20114465  0.05109948 -0.05003884 -0.3040172 ]]. Reward = [0.]
Curr episode timestep = 296
Scene graph at timestep 9784 is [True, False, False, True, False, False]
Current timestep = 9785. State = [[-0.291399   -0.20035098]]. Action = [[-0.13008453 -0.07489523 -0.16324538 -0.87788665]]. Reward = [0.]
Curr episode timestep = 297
Scene graph at timestep 9785 is [True, False, False, True, False, False]
Current timestep = 9786. State = [[-0.291399   -0.20035098]]. Action = [[-0.1606619   0.04579625 -0.14602682  0.15530443]]. Reward = [0.]
Curr episode timestep = 298
Scene graph at timestep 9786 is [True, False, False, True, False, False]
Scene graph at timestep 9786 is [True, False, False, True, False, False]
State prediction error at timestep 9786 is tensor(4.3737e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9787. State = [[-0.291399   -0.20035098]]. Action = [[-0.08395991 -0.1008085  -0.06064232 -0.27834558]]. Reward = [0.]
Curr episode timestep = 299
Scene graph at timestep 9787 is [True, False, False, True, False, False]
Current timestep = 9788. State = [[-0.291399   -0.20035098]]. Action = [[-0.24217744 -0.22387904 -0.24421903  0.8625171 ]]. Reward = [0.]
Curr episode timestep = 300
Scene graph at timestep 9788 is [True, False, False, True, False, False]
Current timestep = 9789. State = [[-0.291399   -0.20035098]]. Action = [[-0.2189775   0.11748546  0.05811697  0.18888056]]. Reward = [0.]
Curr episode timestep = 301
Scene graph at timestep 9789 is [True, False, False, True, False, False]
Human Feedback received at timestep 9789 of -1
Current timestep = 9790. State = [[-0.291399   -0.20035098]]. Action = [[-0.20044115 -0.08632396 -0.22755192  0.9630555 ]]. Reward = [0.]
Curr episode timestep = 302
Scene graph at timestep 9790 is [True, False, False, True, False, False]
Scene graph at timestep 9790 is [True, False, False, True, False, False]
State prediction error at timestep 9790 is tensor(1.4476e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9791. State = [[-0.291399   -0.20035098]]. Action = [[-0.20114675 -0.03757802  0.00860673  0.06734276]]. Reward = [0.]
Curr episode timestep = 303
Scene graph at timestep 9791 is [True, False, False, True, False, False]
Human Feedback received at timestep 9791 of 1
Current timestep = 9792. State = [[-0.291399   -0.20035098]]. Action = [[-0.23248872  0.03487623  0.03847665 -0.9831858 ]]. Reward = [0.]
Curr episode timestep = 304
Scene graph at timestep 9792 is [True, False, False, True, False, False]
Current timestep = 9793. State = [[-0.291399   -0.20035098]]. Action = [[-0.1690151  -0.23168573  0.12738445 -0.7946133 ]]. Reward = [0.]
Curr episode timestep = 305
Scene graph at timestep 9793 is [True, False, False, True, False, False]
Current timestep = 9794. State = [[-0.291399   -0.20035098]]. Action = [[-0.23585457  0.03482604 -0.2439914   0.68565404]]. Reward = [0.]
Curr episode timestep = 306
Scene graph at timestep 9794 is [True, False, False, True, False, False]
Scene graph at timestep 9794 is [True, False, False, True, False, False]
State prediction error at timestep 9794 is tensor(2.1844e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9795. State = [[-0.291399   -0.20035098]]. Action = [[-0.2029204  -0.13832647 -0.24307826  0.26576638]]. Reward = [0.]
Curr episode timestep = 307
Scene graph at timestep 9795 is [True, False, False, True, False, False]
Scene graph at timestep 9795 is [True, False, False, True, False, False]
State prediction error at timestep 9795 is tensor(1.0148e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9796. State = [[-0.291399   -0.20035098]]. Action = [[-0.20192695 -0.07241458  0.0217379   0.8946779 ]]. Reward = [0.]
Curr episode timestep = 308
Scene graph at timestep 9796 is [True, False, False, True, False, False]
Scene graph at timestep 9796 is [True, False, False, True, False, False]
State prediction error at timestep 9796 is tensor(7.0456e-07, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9796 of -1
Current timestep = 9797. State = [[-0.291399   -0.20035098]]. Action = [[-0.24835886  0.15331155 -0.20694967  0.15466464]]. Reward = [0.]
Curr episode timestep = 309
Scene graph at timestep 9797 is [True, False, False, True, False, False]
Current timestep = 9798. State = [[-0.291399   -0.20035098]]. Action = [[-0.15014036 -0.09017822 -0.17880864  0.10949039]]. Reward = [0.]
Curr episode timestep = 310
Scene graph at timestep 9798 is [True, False, False, True, False, False]
Current timestep = 9799. State = [[-0.291399   -0.20035098]]. Action = [[-0.18359739 -0.1627071  -0.11846758 -0.07191706]]. Reward = [0.]
Curr episode timestep = 311
Scene graph at timestep 9799 is [True, False, False, True, False, False]
Scene graph at timestep 9799 is [True, False, False, True, False, False]
State prediction error at timestep 9799 is tensor(9.1206e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9800. State = [[-0.291399   -0.20035098]]. Action = [[-0.16284998  0.20835444 -0.23936178 -0.11946267]]. Reward = [0.]
Curr episode timestep = 312
Scene graph at timestep 9800 is [True, False, False, True, False, False]
Current timestep = 9801. State = [[-0.291399   -0.20035098]]. Action = [[-0.174878   -0.05961505  0.03303286  0.9728246 ]]. Reward = [0.]
Curr episode timestep = 313
Scene graph at timestep 9801 is [True, False, False, True, False, False]
Current timestep = 9802. State = [[-0.291399   -0.20035098]]. Action = [[-0.18919984 -0.09921429 -0.23502748 -0.54185975]]. Reward = [0.]
Curr episode timestep = 314
Scene graph at timestep 9802 is [True, False, False, True, False, False]
Current timestep = 9803. State = [[-0.291399   -0.20035098]]. Action = [[ 0.06129172 -0.09273213 -0.23324941  0.5239327 ]]. Reward = [0.]
Curr episode timestep = 315
Scene graph at timestep 9803 is [True, False, False, True, False, False]
Current timestep = 9804. State = [[-0.291399   -0.20035098]]. Action = [[-0.22770068 -0.24802633  0.06952977 -0.7863649 ]]. Reward = [0.]
Curr episode timestep = 316
Scene graph at timestep 9804 is [True, False, False, True, False, False]
Current timestep = 9805. State = [[-0.291399   -0.20035098]]. Action = [[-0.18263277 -0.17145896 -0.08753718  0.6589227 ]]. Reward = [0.]
Curr episode timestep = 317
Scene graph at timestep 9805 is [True, False, False, True, False, False]
Scene graph at timestep 9805 is [True, False, False, True, False, False]
State prediction error at timestep 9805 is tensor(1.2903e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9806. State = [[-0.291399   -0.20035098]]. Action = [[-0.19972214  0.14618438 -0.17169233  0.24863112]]. Reward = [0.]
Curr episode timestep = 318
Scene graph at timestep 9806 is [True, False, False, True, False, False]
Current timestep = 9807. State = [[-0.291399   -0.20035098]]. Action = [[-0.20733558  0.1207056   0.23511994 -0.22674781]]. Reward = [0.]
Curr episode timestep = 319
Scene graph at timestep 9807 is [True, False, False, True, False, False]
Current timestep = 9808. State = [[-0.291399   -0.20035098]]. Action = [[-0.20096944 -0.03378829 -0.2070431  -0.5296159 ]]. Reward = [0.]
Curr episode timestep = 320
Scene graph at timestep 9808 is [True, False, False, True, False, False]
Scene graph at timestep 9808 is [True, False, False, True, False, False]
State prediction error at timestep 9808 is tensor(9.0791e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9809. State = [[-0.291399   -0.20035098]]. Action = [[-0.23330683 -0.23620088 -0.16521962  0.6474428 ]]. Reward = [0.]
Curr episode timestep = 321
Scene graph at timestep 9809 is [True, False, False, True, False, False]
Scene graph at timestep 9809 is [True, False, False, True, False, False]
State prediction error at timestep 9809 is tensor(2.8789e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9810. State = [[-0.291399   -0.20035098]]. Action = [[-0.16714258  0.18588287  0.14173618  0.76581097]]. Reward = [0.]
Curr episode timestep = 322
Scene graph at timestep 9810 is [True, False, False, True, False, False]
Current timestep = 9811. State = [[-0.291399   -0.20035098]]. Action = [[-0.21222618  0.08071852  0.20684123  0.5179329 ]]. Reward = [0.]
Curr episode timestep = 323
Scene graph at timestep 9811 is [True, False, False, True, False, False]
Scene graph at timestep 9811 is [True, False, False, True, False, False]
State prediction error at timestep 9811 is tensor(1.5458e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9812. State = [[-0.291399   -0.20035098]]. Action = [[-0.22209494 -0.01155293 -0.05417047 -0.39232498]]. Reward = [0.]
Curr episode timestep = 324
Scene graph at timestep 9812 is [True, False, False, True, False, False]
Scene graph at timestep 9812 is [True, False, False, True, False, False]
State prediction error at timestep 9812 is tensor(1.9149e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9813. State = [[-0.291399   -0.20035098]]. Action = [[-0.20315792 -0.19891268  0.18197477  0.58175254]]. Reward = [0.]
Curr episode timestep = 325
Scene graph at timestep 9813 is [True, False, False, True, False, False]
Scene graph at timestep 9813 is [True, False, False, True, False, False]
State prediction error at timestep 9813 is tensor(4.1919e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9814. State = [[-0.291399   -0.20035098]]. Action = [[-0.13369046 -0.04125082 -0.23871502  0.7649088 ]]. Reward = [0.]
Curr episode timestep = 326
Scene graph at timestep 9814 is [True, False, False, True, False, False]
Current timestep = 9815. State = [[-0.291399   -0.20035098]]. Action = [[-0.05869585 -0.21972221  0.16776675 -0.36305302]]. Reward = [0.]
Curr episode timestep = 327
Scene graph at timestep 9815 is [True, False, False, True, False, False]
Current timestep = 9816. State = [[-0.291399   -0.20035098]]. Action = [[-0.22502132  0.12470412 -0.23790668  0.7582418 ]]. Reward = [0.]
Curr episode timestep = 328
Scene graph at timestep 9816 is [True, False, False, True, False, False]
Current timestep = 9817. State = [[-0.291399   -0.20035098]]. Action = [[-0.24186544 -0.12969367  0.02804112 -0.7098881 ]]. Reward = [0.]
Curr episode timestep = 329
Scene graph at timestep 9817 is [True, False, False, True, False, False]
Scene graph at timestep 9817 is [True, False, False, True, False, False]
State prediction error at timestep 9817 is tensor(1.3093e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9818. State = [[-0.291399   -0.20035098]]. Action = [[-0.17015108 -0.15213375 -0.02122751  0.6491029 ]]. Reward = [0.]
Curr episode timestep = 330
Scene graph at timestep 9818 is [True, False, False, True, False, False]
Scene graph at timestep 9818 is [True, False, False, True, False, False]
State prediction error at timestep 9818 is tensor(9.2263e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9819. State = [[-0.291399   -0.20035098]]. Action = [[-0.23918986  0.08155963 -0.24809961  0.5398295 ]]. Reward = [0.]
Curr episode timestep = 331
Scene graph at timestep 9819 is [True, False, False, True, False, False]
Current timestep = 9820. State = [[-0.291399   -0.20035098]]. Action = [[-0.21572927  0.23909134 -0.105113    0.9189217 ]]. Reward = [0.]
Curr episode timestep = 332
Scene graph at timestep 9820 is [True, False, False, True, False, False]
Current timestep = 9821. State = [[-0.291399   -0.20035098]]. Action = [[-0.21726528 -0.00734423 -0.22950897  0.7206845 ]]. Reward = [0.]
Curr episode timestep = 333
Scene graph at timestep 9821 is [True, False, False, True, False, False]
Current timestep = 9822. State = [[-0.291399   -0.20035098]]. Action = [[-0.06175885  0.1284698   0.23600584 -0.16438949]]. Reward = [0.]
Curr episode timestep = 334
Scene graph at timestep 9822 is [True, False, False, True, False, False]
Current timestep = 9823. State = [[-0.291399   -0.20035098]]. Action = [[-0.17784943 -0.1774549  -0.12602884  0.541461  ]]. Reward = [0.]
Curr episode timestep = 335
Scene graph at timestep 9823 is [True, False, False, True, False, False]
Current timestep = 9824. State = [[-0.291399   -0.20035098]]. Action = [[-0.21568829 -0.15811138 -0.19243246  0.90157163]]. Reward = [0.]
Curr episode timestep = 336
Scene graph at timestep 9824 is [True, False, False, True, False, False]
Current timestep = 9825. State = [[-0.291399   -0.20035098]]. Action = [[-0.21950752  0.08645698 -0.04744849  0.60153675]]. Reward = [0.]
Curr episode timestep = 337
Scene graph at timestep 9825 is [True, False, False, True, False, False]
Current timestep = 9826. State = [[-0.291399   -0.20035098]]. Action = [[-0.21151352 -0.12602544  0.15166101 -0.71431905]]. Reward = [0.]
Curr episode timestep = 338
Scene graph at timestep 9826 is [True, False, False, True, False, False]
Current timestep = 9827. State = [[-0.291399   -0.20035098]]. Action = [[-0.1152519   0.19842306 -0.09350578 -0.91430295]]. Reward = [0.]
Curr episode timestep = 339
Scene graph at timestep 9827 is [True, False, False, True, False, False]
Current timestep = 9828. State = [[-0.291399   -0.20035098]]. Action = [[-0.20749615 -0.1709938  -0.21706943  0.1688242 ]]. Reward = [0.]
Curr episode timestep = 340
Scene graph at timestep 9828 is [True, False, False, True, False, False]
Current timestep = 9829. State = [[-0.291399   -0.20035098]]. Action = [[-0.18976541  0.21498501  0.14389473 -0.8186141 ]]. Reward = [0.]
Curr episode timestep = 341
Scene graph at timestep 9829 is [True, False, False, True, False, False]
Current timestep = 9830. State = [[-0.291399   -0.20035098]]. Action = [[-0.00192793 -0.12550144 -0.21749282 -0.97428626]]. Reward = [0.]
Curr episode timestep = 342
Scene graph at timestep 9830 is [True, False, False, True, False, False]
Scene graph at timestep 9830 is [True, False, False, True, False, False]
State prediction error at timestep 9830 is tensor(1.4004e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9831. State = [[-0.291399   -0.20035098]]. Action = [[-0.1614827  -0.21374416  0.10514742 -0.9871967 ]]. Reward = [0.]
Curr episode timestep = 343
Scene graph at timestep 9831 is [True, False, False, True, False, False]
Current timestep = 9832. State = [[-0.291399   -0.20035098]]. Action = [[-0.17506582  0.16764694  0.18093503 -0.5728034 ]]. Reward = [0.]
Curr episode timestep = 344
Scene graph at timestep 9832 is [True, False, False, True, False, False]
Current timestep = 9833. State = [[-0.291399   -0.20035098]]. Action = [[-0.2332022  -0.12254412 -0.0277348   0.35666692]]. Reward = [0.]
Curr episode timestep = 345
Scene graph at timestep 9833 is [True, False, False, True, False, False]
Scene graph at timestep 9833 is [True, False, False, True, False, False]
State prediction error at timestep 9833 is tensor(1.2932e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9834. State = [[-0.291399   -0.20035098]]. Action = [[-0.23108964  0.1176376  -0.20583472  0.512692  ]]. Reward = [0.]
Curr episode timestep = 346
Scene graph at timestep 9834 is [True, False, False, True, False, False]
Current timestep = 9835. State = [[-0.291399   -0.20035098]]. Action = [[-0.20393217  0.1057024  -0.2250032   0.5903164 ]]. Reward = [0.]
Curr episode timestep = 347
Scene graph at timestep 9835 is [True, False, False, True, False, False]
Current timestep = 9836. State = [[-0.291399   -0.20035098]]. Action = [[-0.19752641 -0.10905904  0.08468983 -0.8526632 ]]. Reward = [0.]
Curr episode timestep = 348
Scene graph at timestep 9836 is [True, False, False, True, False, False]
Current timestep = 9837. State = [[-0.291399   -0.20035098]]. Action = [[-0.08421242 -0.22233413 -0.21332191  0.6379179 ]]. Reward = [0.]
Curr episode timestep = 349
Scene graph at timestep 9837 is [True, False, False, True, False, False]
Current timestep = 9838. State = [[-0.291399   -0.20035098]]. Action = [[-0.21629924 -0.16805285 -0.24048151  0.9868183 ]]. Reward = [0.]
Curr episode timestep = 350
Scene graph at timestep 9838 is [True, False, False, True, False, False]
Current timestep = 9839. State = [[-0.291399   -0.20035098]]. Action = [[-0.21779813 -0.00266306 -0.23729002  0.8980689 ]]. Reward = [0.]
Curr episode timestep = 351
Scene graph at timestep 9839 is [True, False, False, True, False, False]
Current timestep = 9840. State = [[-0.291399   -0.20035098]]. Action = [[-0.19936413 -0.16798055 -0.16733462  0.81242466]]. Reward = [0.]
Curr episode timestep = 352
Scene graph at timestep 9840 is [True, False, False, True, False, False]
Current timestep = 9841. State = [[-0.291399   -0.20035098]]. Action = [[-0.23245716  0.23993564  0.02866396  0.18638575]]. Reward = [0.]
Curr episode timestep = 353
Scene graph at timestep 9841 is [True, False, False, True, False, False]
Current timestep = 9842. State = [[-0.291399   -0.20035098]]. Action = [[-0.14724301 -0.21297464  0.06489959 -0.5759826 ]]. Reward = [0.]
Curr episode timestep = 354
Scene graph at timestep 9842 is [True, False, False, True, False, False]
Current timestep = 9843. State = [[-0.291399   -0.20035098]]. Action = [[-0.03885807 -0.18776157  0.07817784  0.22886407]]. Reward = [0.]
Curr episode timestep = 355
Scene graph at timestep 9843 is [True, False, False, True, False, False]
Current timestep = 9844. State = [[-0.291399   -0.20035098]]. Action = [[-0.09925422 -0.07838674 -0.1741014  -0.79266375]]. Reward = [0.]
Curr episode timestep = 356
Scene graph at timestep 9844 is [True, False, False, True, False, False]
Scene graph at timestep 9844 is [True, False, False, True, False, False]
State prediction error at timestep 9844 is tensor(9.7881e-08, grad_fn=<MseLossBackward0>)
Current timestep = 9845. State = [[-0.291399   -0.20035098]]. Action = [[-0.16945958 -0.19034362  0.1784336  -0.447569  ]]. Reward = [0.]
Curr episode timestep = 357
Scene graph at timestep 9845 is [True, False, False, True, False, False]
Current timestep = 9846. State = [[-0.291399   -0.20035098]]. Action = [[-0.05073366 -0.13206887 -0.18078227  0.12310779]]. Reward = [0.]
Curr episode timestep = 358
Scene graph at timestep 9846 is [True, False, False, True, False, False]
Current timestep = 9847. State = [[-0.291399   -0.20035098]]. Action = [[-0.09895535 -0.20061581  0.20683768  0.9107957 ]]. Reward = [0.]
Curr episode timestep = 359
Scene graph at timestep 9847 is [True, False, False, True, False, False]
Scene graph at timestep 9847 is [True, False, False, True, False, False]
State prediction error at timestep 9847 is tensor(1.5166e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9848. State = [[-0.291399   -0.20035098]]. Action = [[ 0.08303463 -0.01723872 -0.17588241  0.8974047 ]]. Reward = [0.]
Curr episode timestep = 360
Scene graph at timestep 9848 is [True, False, False, True, False, False]
Current timestep = 9849. State = [[-0.291399   -0.20035098]]. Action = [[-0.22957653 -0.18560848  0.1666984   0.9678974 ]]. Reward = [0.]
Curr episode timestep = 361
Scene graph at timestep 9849 is [True, False, False, True, False, False]
Current timestep = 9850. State = [[-0.291399   -0.20035098]]. Action = [[-0.13005915 -0.14375085 -0.05160183  0.32263255]]. Reward = [0.]
Curr episode timestep = 362
Scene graph at timestep 9850 is [True, False, False, True, False, False]
Scene graph at timestep 9850 is [True, False, False, True, False, False]
State prediction error at timestep 9850 is tensor(1.4480e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9851. State = [[-0.291399   -0.20035098]]. Action = [[-0.1944717   0.09741557 -0.22980373 -0.22346753]]. Reward = [0.]
Curr episode timestep = 363
Scene graph at timestep 9851 is [True, False, False, True, False, False]
Current timestep = 9852. State = [[-0.291399   -0.20035098]]. Action = [[-0.22397774 -0.18971746  0.03352898 -0.8242586 ]]. Reward = [0.]
Curr episode timestep = 364
Scene graph at timestep 9852 is [True, False, False, True, False, False]
Current timestep = 9853. State = [[-0.291399   -0.20035098]]. Action = [[-0.15297066 -0.06450291 -0.22083804  0.94382143]]. Reward = [0.]
Curr episode timestep = 365
Scene graph at timestep 9853 is [True, False, False, True, False, False]
Current timestep = 9854. State = [[-0.291399   -0.20035098]]. Action = [[-0.19090588 -0.23840415 -0.15935098 -0.8933229 ]]. Reward = [0.]
Curr episode timestep = 366
Scene graph at timestep 9854 is [True, False, False, True, False, False]
Current timestep = 9855. State = [[-0.291399   -0.20035098]]. Action = [[-0.20245059 -0.1563873  -0.14220037 -0.9955452 ]]. Reward = [0.]
Curr episode timestep = 367
Scene graph at timestep 9855 is [True, False, False, True, False, False]
Scene graph at timestep 9855 is [True, False, False, True, False, False]
State prediction error at timestep 9855 is tensor(1.9823e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9856. State = [[-0.291399   -0.20035098]]. Action = [[-0.22544916 -0.2404503  -0.03296717  0.36525226]]. Reward = [0.]
Curr episode timestep = 368
Scene graph at timestep 9856 is [True, False, False, True, False, False]
Current timestep = 9857. State = [[-0.291399   -0.20035098]]. Action = [[-0.23236547  0.12643415 -0.17118032  0.75341535]]. Reward = [0.]
Curr episode timestep = 369
Scene graph at timestep 9857 is [True, False, False, True, False, False]
Current timestep = 9858. State = [[-0.291399   -0.20035098]]. Action = [[-0.16762269  0.05091107 -0.24723646 -0.32564116]]. Reward = [0.]
Curr episode timestep = 370
Scene graph at timestep 9858 is [True, False, False, True, False, False]
Current timestep = 9859. State = [[-0.291399   -0.20035098]]. Action = [[-0.21026461 -0.18760031  0.04607761  0.96711993]]. Reward = [0.]
Curr episode timestep = 371
Scene graph at timestep 9859 is [True, False, False, True, False, False]
Current timestep = 9860. State = [[-0.291399   -0.20035098]]. Action = [[-0.21961702 -0.17672499 -0.16550596 -0.07562995]]. Reward = [0.]
Curr episode timestep = 372
Scene graph at timestep 9860 is [True, False, False, True, False, False]
Current timestep = 9861. State = [[-0.291399   -0.20035098]]. Action = [[-0.12566872 -0.16802458 -0.2400311  -0.9671151 ]]. Reward = [0.]
Curr episode timestep = 373
Scene graph at timestep 9861 is [True, False, False, True, False, False]
Current timestep = 9862. State = [[-0.291399   -0.20035098]]. Action = [[-0.01008107 -0.00447454 -0.08323757 -0.99269307]]. Reward = [0.]
Curr episode timestep = 374
Scene graph at timestep 9862 is [True, False, False, True, False, False]
Current timestep = 9863. State = [[-0.291399   -0.20035098]]. Action = [[ 0.07427824 -0.20872566 -0.13380349  0.7998769 ]]. Reward = [0.]
Curr episode timestep = 375
Scene graph at timestep 9863 is [True, False, False, True, False, False]
Current timestep = 9864. State = [[-0.291399   -0.20035098]]. Action = [[-0.13162853 -0.07998002 -0.22791578  0.99458385]]. Reward = [0.]
Curr episode timestep = 376
Scene graph at timestep 9864 is [True, False, False, True, False, False]
Current timestep = 9865. State = [[-0.291399   -0.20035098]]. Action = [[-0.21219808 -0.02762118  0.1488846   0.42973173]]. Reward = [0.]
Curr episode timestep = 377
Scene graph at timestep 9865 is [True, False, False, True, False, False]
Current timestep = 9866. State = [[-0.291399   -0.20035098]]. Action = [[-0.2029252  -0.2195996   0.20493367 -0.873151  ]]. Reward = [0.]
Curr episode timestep = 378
Scene graph at timestep 9866 is [True, False, False, True, False, False]
Current timestep = 9867. State = [[-0.291399   -0.20035098]]. Action = [[-0.20624924 -0.15250948  0.0361082  -0.07556731]]. Reward = [0.]
Curr episode timestep = 379
Scene graph at timestep 9867 is [True, False, False, True, False, False]
Current timestep = 9868. State = [[-0.291399   -0.20035098]]. Action = [[-0.22310987  0.1555515  -0.22615428 -0.56882215]]. Reward = [0.]
Curr episode timestep = 380
Scene graph at timestep 9868 is [True, False, False, True, False, False]
Scene graph at timestep 9868 is [True, False, False, True, False, False]
State prediction error at timestep 9868 is tensor(3.0801e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9869. State = [[-0.291399   -0.20035098]]. Action = [[-0.2147721   0.09715274  0.07894576 -0.00276428]]. Reward = [0.]
Curr episode timestep = 381
Scene graph at timestep 9869 is [True, False, False, True, False, False]
Current timestep = 9870. State = [[-0.291399   -0.20035098]]. Action = [[-0.2048496  -0.09243992 -0.24342605  0.265002  ]]. Reward = [0.]
Curr episode timestep = 382
Scene graph at timestep 9870 is [True, False, False, True, False, False]
Current timestep = 9871. State = [[-0.291399   -0.20035098]]. Action = [[-0.17516072  0.23312849 -0.18454447 -0.31923503]]. Reward = [0.]
Curr episode timestep = 383
Scene graph at timestep 9871 is [True, False, False, True, False, False]
Current timestep = 9872. State = [[-0.291399   -0.20035098]]. Action = [[-0.21387832 -0.0270111  -0.20716672  0.5695963 ]]. Reward = [0.]
Curr episode timestep = 384
Scene graph at timestep 9872 is [True, False, False, True, False, False]
Current timestep = 9873. State = [[-0.291399   -0.20035098]]. Action = [[-0.1361222  -0.07078341 -0.23850518  0.92024875]]. Reward = [0.]
Curr episode timestep = 385
Scene graph at timestep 9873 is [True, False, False, True, False, False]
Current timestep = 9874. State = [[-0.291399   -0.20035098]]. Action = [[-0.17006429  0.11900413  0.05320215  0.66421413]]. Reward = [0.]
Curr episode timestep = 386
Scene graph at timestep 9874 is [True, False, False, True, False, False]
Current timestep = 9875. State = [[-0.291399   -0.20035098]]. Action = [[-0.20817532 -0.01847731  0.03195256  0.60027826]]. Reward = [0.]
Curr episode timestep = 387
Scene graph at timestep 9875 is [True, False, False, True, False, False]
Current timestep = 9876. State = [[-0.291399   -0.20035098]]. Action = [[-0.19347283  0.11396855  0.08647618 -0.07122087]]. Reward = [0.]
Curr episode timestep = 388
Scene graph at timestep 9876 is [True, False, False, True, False, False]
Current timestep = 9877. State = [[-0.291399   -0.20035098]]. Action = [[-0.20583436 -0.15132853 -0.15882055  0.96602154]]. Reward = [0.]
Curr episode timestep = 389
Scene graph at timestep 9877 is [True, False, False, True, False, False]
Current timestep = 9878. State = [[-0.291399   -0.20035098]]. Action = [[-0.14313945 -0.10300562 -0.11140077  0.73652244]]. Reward = [0.]
Curr episode timestep = 390
Scene graph at timestep 9878 is [True, False, False, True, False, False]
Current timestep = 9879. State = [[-0.291399   -0.20035098]]. Action = [[-0.11287291  0.00512055 -0.20061664  0.02388346]]. Reward = [0.]
Curr episode timestep = 391
Scene graph at timestep 9879 is [True, False, False, True, False, False]
Current timestep = 9880. State = [[-0.291399   -0.20035098]]. Action = [[-0.23193567  0.13853008 -0.01845115 -0.33999908]]. Reward = [0.]
Curr episode timestep = 392
Scene graph at timestep 9880 is [True, False, False, True, False, False]
Current timestep = 9881. State = [[-0.291399   -0.20035098]]. Action = [[ 0.00314629 -0.13165915  0.15509191 -0.70189726]]. Reward = [0.]
Curr episode timestep = 393
Scene graph at timestep 9881 is [True, False, False, True, False, False]
Current timestep = 9882. State = [[-0.291399   -0.20035098]]. Action = [[-0.23196208 -0.22306192 -0.22012582  0.8158052 ]]. Reward = [0.]
Curr episode timestep = 394
Scene graph at timestep 9882 is [True, False, False, True, False, False]
Current timestep = 9883. State = [[-0.291399   -0.20035098]]. Action = [[-0.22886287 -0.20742407  0.12742692 -0.3184318 ]]. Reward = [0.]
Curr episode timestep = 395
Scene graph at timestep 9883 is [True, False, False, True, False, False]
Current timestep = 9884. State = [[-0.291399   -0.20035098]]. Action = [[-0.19125369  0.01267484  0.21343029  0.9893594 ]]. Reward = [0.]
Curr episode timestep = 396
Scene graph at timestep 9884 is [True, False, False, True, False, False]
Current timestep = 9885. State = [[-0.291399   -0.20035098]]. Action = [[-2.1150032e-01  9.6909881e-02  2.1100044e-05  6.4022923e-01]]. Reward = [0.]
Curr episode timestep = 397
Scene graph at timestep 9885 is [True, False, False, True, False, False]
Current timestep = 9886. State = [[-0.291399   -0.20035098]]. Action = [[-0.18305267  0.0386548  -0.1967278  -0.6325396 ]]. Reward = [0.]
Curr episode timestep = 398
Scene graph at timestep 9886 is [True, False, False, True, False, False]
Current timestep = 9887. State = [[-0.291399   -0.20035098]]. Action = [[-0.1448944  -0.13709761 -0.22672318  0.68178177]]. Reward = [0.]
Curr episode timestep = 399
Scene graph at timestep 9887 is [True, False, False, True, False, False]
Current timestep = 9888. State = [[-0.291399   -0.20035098]]. Action = [[-0.08054578  0.17576021 -0.19292352 -0.27863562]]. Reward = [0.]
Curr episode timestep = 400
Scene graph at timestep 9888 is [True, False, False, True, False, False]
Scene graph at timestep 9888 is [True, False, False, True, False, False]
State prediction error at timestep 9888 is tensor(2.5127e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9889. State = [[-0.291399   -0.20035098]]. Action = [[-0.22358501 -0.06903523 -0.06398353  0.57816243]]. Reward = [0.]
Curr episode timestep = 401
Scene graph at timestep 9889 is [True, False, False, True, False, False]
Current timestep = 9890. State = [[-0.291399   -0.20035098]]. Action = [[-0.19941172 -0.1544155   0.01734132 -0.87577814]]. Reward = [0.]
Curr episode timestep = 402
Scene graph at timestep 9890 is [True, False, False, True, False, False]
Current timestep = 9891. State = [[-0.291399   -0.20035098]]. Action = [[-0.2044635   0.00121784  0.10473824  0.97359693]]. Reward = [0.]
Curr episode timestep = 403
Scene graph at timestep 9891 is [True, False, False, True, False, False]
Current timestep = 9892. State = [[-0.291399   -0.20035098]]. Action = [[-0.1737097   0.13267508 -0.24849449  0.7803346 ]]. Reward = [0.]
Curr episode timestep = 404
Scene graph at timestep 9892 is [True, False, False, True, False, False]
Current timestep = 9893. State = [[-0.291399   -0.20035098]]. Action = [[-0.21171707  0.1169607  -0.13044593  0.49353194]]. Reward = [0.]
Curr episode timestep = 405
Scene graph at timestep 9893 is [True, False, False, True, False, False]
Current timestep = 9894. State = [[-0.291399   -0.20035098]]. Action = [[-0.22455993  0.07135114  0.04442972  0.9346602 ]]. Reward = [0.]
Curr episode timestep = 406
Scene graph at timestep 9894 is [True, False, False, True, False, False]
Scene graph at timestep 9894 is [True, False, False, True, False, False]
State prediction error at timestep 9894 is tensor(3.1927e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9895. State = [[-0.291399   -0.20035098]]. Action = [[-0.2371423   0.09364486 -0.12806872 -0.5661725 ]]. Reward = [0.]
Curr episode timestep = 407
Scene graph at timestep 9895 is [True, False, False, True, False, False]
Current timestep = 9896. State = [[-0.291399   -0.20035098]]. Action = [[-0.13673437  0.12028146 -0.19344829  0.8560388 ]]. Reward = [0.]
Curr episode timestep = 408
Scene graph at timestep 9896 is [True, False, False, True, False, False]
Scene graph at timestep 9896 is [True, False, False, True, False, False]
State prediction error at timestep 9896 is tensor(5.6631e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9897. State = [[-0.291399   -0.20035098]]. Action = [[-0.03754857 -0.22117597 -0.22553447  0.53804874]]. Reward = [0.]
Curr episode timestep = 409
Scene graph at timestep 9897 is [True, False, False, True, False, False]
Scene graph at timestep 9897 is [True, False, False, True, False, False]
State prediction error at timestep 9897 is tensor(7.0049e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9898. State = [[-0.291399   -0.20035098]]. Action = [[-0.21276692 -0.18523563  0.01196599  0.7214184 ]]. Reward = [0.]
Curr episode timestep = 410
Scene graph at timestep 9898 is [True, False, False, True, False, False]
Scene graph at timestep 9898 is [True, False, False, True, False, False]
State prediction error at timestep 9898 is tensor(6.4838e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9899. State = [[-0.291399   -0.20035098]]. Action = [[-0.18896605  0.12881935  0.2263754  -0.42974156]]. Reward = [0.]
Curr episode timestep = 411
Scene graph at timestep 9899 is [True, False, False, True, False, False]
Current timestep = 9900. State = [[-0.291399   -0.20035098]]. Action = [[-0.1848672   0.06955421 -0.01900665  0.91222966]]. Reward = [0.]
Curr episode timestep = 412
Scene graph at timestep 9900 is [True, False, False, True, False, False]
Current timestep = 9901. State = [[-0.291399   -0.20035098]]. Action = [[-0.20710076 -0.16398361 -0.18577409 -0.10924685]]. Reward = [0.]
Curr episode timestep = 413
Scene graph at timestep 9901 is [True, False, False, True, False, False]
Current timestep = 9902. State = [[-0.291399   -0.20035098]]. Action = [[-0.21469937  0.08045891 -0.21830809  0.76789165]]. Reward = [0.]
Curr episode timestep = 414
Scene graph at timestep 9902 is [True, False, False, True, False, False]
Current timestep = 9903. State = [[-0.291399   -0.20035098]]. Action = [[-0.22139348  0.2168056  -0.19373493  0.70766175]]. Reward = [0.]
Curr episode timestep = 415
Scene graph at timestep 9903 is [True, False, False, True, False, False]
Current timestep = 9904. State = [[-0.291399   -0.20035098]]. Action = [[-0.14961989  0.12354717  0.2431671   0.6898782 ]]. Reward = [0.]
Curr episode timestep = 416
Scene graph at timestep 9904 is [True, False, False, True, False, False]
Current timestep = 9905. State = [[-0.291399   -0.20035098]]. Action = [[-0.10723603 -0.05335799 -0.03318208  0.92541313]]. Reward = [0.]
Curr episode timestep = 417
Scene graph at timestep 9905 is [True, False, False, True, False, False]
Scene graph at timestep 9905 is [True, False, False, True, False, False]
State prediction error at timestep 9905 is tensor(3.9140e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9906. State = [[-0.291399   -0.20035098]]. Action = [[-0.19430818 -0.21458025 -0.22967128  0.23878002]]. Reward = [0.]
Curr episode timestep = 418
Scene graph at timestep 9906 is [True, False, False, True, False, False]
Current timestep = 9907. State = [[-0.291399   -0.20035098]]. Action = [[-0.20701966  0.07646319 -0.19062054 -0.37666118]]. Reward = [0.]
Curr episode timestep = 419
Scene graph at timestep 9907 is [True, False, False, True, False, False]
Scene graph at timestep 9907 is [True, False, False, True, False, False]
State prediction error at timestep 9907 is tensor(1.0669e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9908. State = [[-0.291399   -0.20035098]]. Action = [[-0.22410893  0.13624266  0.13808843  0.74048305]]. Reward = [0.]
Curr episode timestep = 420
Scene graph at timestep 9908 is [True, False, False, True, False, False]
Current timestep = 9909. State = [[-0.291399   -0.20035098]]. Action = [[-0.18551578  0.18943322 -0.22684826 -0.21036994]]. Reward = [0.]
Curr episode timestep = 421
Scene graph at timestep 9909 is [True, False, False, True, False, False]
Current timestep = 9910. State = [[-0.291399   -0.20035098]]. Action = [[-0.17640741  0.23206782 -0.00400643 -0.8186569 ]]. Reward = [0.]
Curr episode timestep = 422
Scene graph at timestep 9910 is [True, False, False, True, False, False]
Current timestep = 9911. State = [[-0.291399   -0.20035098]]. Action = [[-0.13523242 -0.10392934 -0.14071664 -0.44230223]]. Reward = [0.]
Curr episode timestep = 423
Scene graph at timestep 9911 is [True, False, False, True, False, False]
Scene graph at timestep 9911 is [True, False, False, True, False, False]
State prediction error at timestep 9911 is tensor(1.8746e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9912. State = [[-0.291399   -0.20035098]]. Action = [[-0.08750479 -0.24719378 -0.1685318  -0.6818257 ]]. Reward = [0.]
Curr episode timestep = 424
Scene graph at timestep 9912 is [True, False, False, True, False, False]
Current timestep = 9913. State = [[-0.291399   -0.20035098]]. Action = [[-0.22150035 -0.2244311   0.22770393 -0.524601  ]]. Reward = [0.]
Curr episode timestep = 425
Scene graph at timestep 9913 is [True, False, False, True, False, False]
Scene graph at timestep 9913 is [True, False, False, True, False, False]
State prediction error at timestep 9913 is tensor(1.0709e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9914. State = [[-0.291399   -0.20035098]]. Action = [[-0.23596387 -0.23711778 -0.13812226  0.6774992 ]]. Reward = [0.]
Curr episode timestep = 426
Scene graph at timestep 9914 is [True, False, False, True, False, False]
Current timestep = 9915. State = [[-0.291399   -0.20035098]]. Action = [[-0.17899115  0.19270045 -0.10224485 -0.93885607]]. Reward = [0.]
Curr episode timestep = 427
Scene graph at timestep 9915 is [True, False, False, True, False, False]
Scene graph at timestep 9915 is [True, False, False, True, False, False]
State prediction error at timestep 9915 is tensor(1.8940e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9916. State = [[-0.291399   -0.20035098]]. Action = [[-0.17712063 -0.10363434 -0.21955855  0.73871374]]. Reward = [0.]
Curr episode timestep = 428
Scene graph at timestep 9916 is [True, False, False, True, False, False]
Current timestep = 9917. State = [[-0.291399   -0.20035098]]. Action = [[-0.19054334  0.12234899 -0.17972843  0.10861957]]. Reward = [0.]
Curr episode timestep = 429
Scene graph at timestep 9917 is [True, False, False, True, False, False]
Current timestep = 9918. State = [[-0.291399   -0.20035098]]. Action = [[-0.21696655 -0.20227574  0.04147071  0.45622587]]. Reward = [0.]
Curr episode timestep = 430
Scene graph at timestep 9918 is [True, False, False, True, False, False]
Current timestep = 9919. State = [[-0.291399   -0.20035098]]. Action = [[-0.21342574 -0.18947689  0.06187609  0.8642694 ]]. Reward = [0.]
Curr episode timestep = 431
Scene graph at timestep 9919 is [True, False, False, True, False, False]
Current timestep = 9920. State = [[-0.291399   -0.20035098]]. Action = [[-0.01564533  0.15546057 -0.11296645 -0.17743665]]. Reward = [0.]
Curr episode timestep = 432
Scene graph at timestep 9920 is [True, False, False, True, False, False]
Current timestep = 9921. State = [[-0.291399   -0.20035098]]. Action = [[-0.17532443  0.23275614  0.01324472  0.54773617]]. Reward = [0.]
Curr episode timestep = 433
Scene graph at timestep 9921 is [True, False, False, True, False, False]
Current timestep = 9922. State = [[-0.291399   -0.20035098]]. Action = [[-0.16354398 -0.1790747   0.05601284 -0.1646496 ]]. Reward = [0.]
Curr episode timestep = 434
Scene graph at timestep 9922 is [True, False, False, True, False, False]
Current timestep = 9923. State = [[-0.291399   -0.20035098]]. Action = [[ 0.04096019 -0.18839726  0.11037463 -0.6305311 ]]. Reward = [0.]
Curr episode timestep = 435
Scene graph at timestep 9923 is [True, False, False, True, False, False]
Current timestep = 9924. State = [[-0.291399   -0.20035098]]. Action = [[-0.13955434  0.15975523 -0.19010358  0.6069932 ]]. Reward = [0.]
Curr episode timestep = 436
Scene graph at timestep 9924 is [True, False, False, True, False, False]
Current timestep = 9925. State = [[-0.291399   -0.20035098]]. Action = [[-0.23029897  0.1125617   0.2415011  -0.97891176]]. Reward = [0.]
Curr episode timestep = 437
Scene graph at timestep 9925 is [True, False, False, True, False, False]
Scene graph at timestep 9925 is [True, False, False, True, False, False]
State prediction error at timestep 9925 is tensor(2.7776e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9926. State = [[-0.291399   -0.20035098]]. Action = [[-0.13461313  0.00381345 -0.1164355   0.6140734 ]]. Reward = [0.]
Curr episode timestep = 438
Scene graph at timestep 9926 is [True, False, False, True, False, False]
Current timestep = 9927. State = [[-0.291399   -0.20035098]]. Action = [[-0.22971053 -0.04492165  0.1428752   0.44383812]]. Reward = [0.]
Curr episode timestep = 439
Scene graph at timestep 9927 is [True, False, False, True, False, False]
Current timestep = 9928. State = [[-0.291399   -0.20035098]]. Action = [[-0.23178934 -0.20142566  0.14047104 -0.9145919 ]]. Reward = [0.]
Curr episode timestep = 440
Scene graph at timestep 9928 is [True, False, False, True, False, False]
Current timestep = 9929. State = [[-0.291399   -0.20035098]]. Action = [[-0.08544934  0.17027968  0.07700163  0.61780655]]. Reward = [0.]
Curr episode timestep = 441
Scene graph at timestep 9929 is [True, False, False, True, False, False]
Scene graph at timestep 9929 is [True, False, False, True, False, False]
State prediction error at timestep 9929 is tensor(6.0762e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9930. State = [[-0.291399   -0.20035098]]. Action = [[-0.12105265 -0.22634655  0.1585365  -0.53589875]]. Reward = [0.]
Curr episode timestep = 442
Scene graph at timestep 9930 is [True, False, False, True, False, False]
Scene graph at timestep 9930 is [True, False, False, True, False, False]
State prediction error at timestep 9930 is tensor(2.2350e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9931. State = [[-0.291399   -0.20035098]]. Action = [[-0.23416372 -0.22625344 -0.11428797  0.81390524]]. Reward = [0.]
Curr episode timestep = 443
Scene graph at timestep 9931 is [True, False, False, True, False, False]
Current timestep = 9932. State = [[-0.291399   -0.20035098]]. Action = [[-0.20755237  0.14620823 -0.16883709  0.09813678]]. Reward = [0.]
Curr episode timestep = 444
Scene graph at timestep 9932 is [True, False, False, True, False, False]
Current timestep = 9933. State = [[-0.291399   -0.20035098]]. Action = [[-0.23453371  0.05854642  0.19917661 -0.44269425]]. Reward = [0.]
Curr episode timestep = 445
Scene graph at timestep 9933 is [True, False, False, True, False, False]
Scene graph at timestep 9933 is [True, False, False, True, False, False]
State prediction error at timestep 9933 is tensor(2.1282e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9934. State = [[-0.291399   -0.20035098]]. Action = [[-0.05986314 -0.17825544  0.09231678 -0.939049  ]]. Reward = [0.]
Curr episode timestep = 446
Scene graph at timestep 9934 is [True, False, False, True, False, False]
Current timestep = 9935. State = [[-0.291399   -0.20035098]]. Action = [[-0.17518656 -0.05164707 -0.06992029 -0.9597117 ]]. Reward = [0.]
Curr episode timestep = 447
Scene graph at timestep 9935 is [True, False, False, True, False, False]
Current timestep = 9936. State = [[-0.291399   -0.20035098]]. Action = [[-0.22435005 -0.24254489 -0.1355493  -0.692243  ]]. Reward = [0.]
Curr episode timestep = 448
Scene graph at timestep 9936 is [True, False, False, True, False, False]
Current timestep = 9937. State = [[-0.291399   -0.20035098]]. Action = [[-0.13264465  0.0625512   0.20866692 -0.6682211 ]]. Reward = [0.]
Curr episode timestep = 449
Scene graph at timestep 9937 is [True, False, False, True, False, False]
Scene graph at timestep 9937 is [True, False, False, True, False, False]
State prediction error at timestep 9937 is tensor(1.4104e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9938. State = [[-0.291399   -0.20035098]]. Action = [[-0.17523111 -0.2033809  -0.16457853 -0.36281687]]. Reward = [0.]
Curr episode timestep = 450
Scene graph at timestep 9938 is [True, False, False, True, False, False]
Current timestep = 9939. State = [[-0.291399   -0.20035098]]. Action = [[-0.2186944   0.15104836 -0.14260824  0.6738632 ]]. Reward = [0.]
Curr episode timestep = 451
Scene graph at timestep 9939 is [True, False, False, True, False, False]
Current timestep = 9940. State = [[-0.291399   -0.20035098]]. Action = [[-0.1899898   0.10388052 -0.09428141  0.7134831 ]]. Reward = [0.]
Curr episode timestep = 452
Scene graph at timestep 9940 is [True, False, False, True, False, False]
Current timestep = 9941. State = [[-0.291399   -0.20035098]]. Action = [[ 0.09164798  0.22097439 -0.20439424 -0.9344601 ]]. Reward = [0.]
Curr episode timestep = 453
Scene graph at timestep 9941 is [True, False, False, True, False, False]
Current timestep = 9942. State = [[-0.291399   -0.20035098]]. Action = [[-0.13305935  0.24773484  0.11108243 -0.60413694]]. Reward = [0.]
Curr episode timestep = 454
Scene graph at timestep 9942 is [True, False, False, True, False, False]
Current timestep = 9943. State = [[-0.291399   -0.20035098]]. Action = [[-0.17415899  0.09661615  0.10341528 -0.05390245]]. Reward = [0.]
Curr episode timestep = 455
Scene graph at timestep 9943 is [True, False, False, True, False, False]
Scene graph at timestep 9943 is [True, False, False, True, False, False]
State prediction error at timestep 9943 is tensor(5.5324e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9944. State = [[-0.291399   -0.20035098]]. Action = [[-0.14852828  0.22046569  0.06910253  0.00508213]]. Reward = [0.]
Curr episode timestep = 456
Scene graph at timestep 9944 is [True, False, False, True, False, False]
Scene graph at timestep 9944 is [True, False, False, True, False, False]
State prediction error at timestep 9944 is tensor(2.1527e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9945. State = [[-0.291399   -0.20035098]]. Action = [[-0.12160969 -0.23503049  0.05208343 -0.8740396 ]]. Reward = [0.]
Curr episode timestep = 457
Scene graph at timestep 9945 is [True, False, False, True, False, False]
Current timestep = 9946. State = [[-0.291399   -0.20035098]]. Action = [[-0.08131585 -0.23208767  0.08203468  0.874251  ]]. Reward = [0.]
Curr episode timestep = 458
Scene graph at timestep 9946 is [True, False, False, True, False, False]
Current timestep = 9947. State = [[-0.291399   -0.20035098]]. Action = [[-0.23504435  0.10331032 -0.12123406  0.9363663 ]]. Reward = [0.]
Curr episode timestep = 459
Scene graph at timestep 9947 is [True, False, False, True, False, False]
Current timestep = 9948. State = [[-0.291399   -0.20035098]]. Action = [[-0.11836755 -0.09877074 -0.13808006  0.38213015]]. Reward = [0.]
Curr episode timestep = 460
Scene graph at timestep 9948 is [True, False, False, True, False, False]
Current timestep = 9949. State = [[-0.291399   -0.20035098]]. Action = [[-0.15315634 -0.21557024 -0.15821546  0.15655255]]. Reward = [0.]
Curr episode timestep = 461
Scene graph at timestep 9949 is [True, False, False, True, False, False]
Current timestep = 9950. State = [[-0.291399   -0.20035098]]. Action = [[-0.16391458 -0.2031485   0.01786792  0.961249  ]]. Reward = [0.]
Curr episode timestep = 462
Scene graph at timestep 9950 is [True, False, False, True, False, False]
Current timestep = 9951. State = [[-0.291399   -0.20035098]]. Action = [[-0.19360043  0.06743219 -0.14571103 -0.79790646]]. Reward = [0.]
Curr episode timestep = 463
Scene graph at timestep 9951 is [True, False, False, True, False, False]
Current timestep = 9952. State = [[-0.291399   -0.20035098]]. Action = [[-0.23020199  0.02252465 -0.15838704  0.4238689 ]]. Reward = [0.]
Curr episode timestep = 464
Scene graph at timestep 9952 is [True, False, False, True, False, False]
Current timestep = 9953. State = [[-0.291399   -0.20035098]]. Action = [[ 0.09904253  0.18661767 -0.15156841 -0.99292535]]. Reward = [0.]
Curr episode timestep = 465
Scene graph at timestep 9953 is [True, False, False, True, False, False]
Current timestep = 9954. State = [[-0.291399   -0.20035098]]. Action = [[-0.1628866  -0.14616355 -0.21534851 -0.59638745]]. Reward = [0.]
Curr episode timestep = 466
Scene graph at timestep 9954 is [True, False, False, True, False, False]
Current timestep = 9955. State = [[-0.291399   -0.20035098]]. Action = [[-0.21550481 -0.17935616  0.20924309 -0.8902503 ]]. Reward = [0.]
Curr episode timestep = 467
Scene graph at timestep 9955 is [True, False, False, True, False, False]
Scene graph at timestep 9955 is [True, False, False, True, False, False]
State prediction error at timestep 9955 is tensor(3.0809e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9956. State = [[-0.291399   -0.20035098]]. Action = [[-0.18274133  0.19152367 -0.06645855  0.8667003 ]]. Reward = [0.]
Curr episode timestep = 468
Scene graph at timestep 9956 is [True, False, False, True, False, False]
Scene graph at timestep 9956 is [True, False, False, True, False, False]
State prediction error at timestep 9956 is tensor(2.4494e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9957. State = [[-0.291399   -0.20035098]]. Action = [[-0.08245245 -0.08248371  0.19324315 -0.82702774]]. Reward = [0.]
Curr episode timestep = 469
Scene graph at timestep 9957 is [True, False, False, True, False, False]
Current timestep = 9958. State = [[-0.291399   -0.20035098]]. Action = [[-0.17059425  0.01066023 -0.09947062  0.12415445]]. Reward = [0.]
Curr episode timestep = 470
Scene graph at timestep 9958 is [True, False, False, True, False, False]
Current timestep = 9959. State = [[-0.291399   -0.20035098]]. Action = [[-0.23188181 -0.19225211  0.22228912 -0.00604427]]. Reward = [0.]
Curr episode timestep = 471
Scene graph at timestep 9959 is [True, False, False, True, False, False]
Scene graph at timestep 9959 is [True, False, False, True, False, False]
State prediction error at timestep 9959 is tensor(3.5718e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9960. State = [[-0.291399   -0.20035098]]. Action = [[-0.18703054  0.14110231 -0.05801491 -0.272326  ]]. Reward = [0.]
Curr episode timestep = 472
Scene graph at timestep 9960 is [True, False, False, True, False, False]
Scene graph at timestep 9960 is [True, False, False, True, False, False]
State prediction error at timestep 9960 is tensor(2.2587e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9961. State = [[-0.291399   -0.20035098]]. Action = [[-0.19536892 -0.1926036  -0.14962074  0.86709046]]. Reward = [0.]
Curr episode timestep = 473
Scene graph at timestep 9961 is [True, False, False, True, False, False]
Current timestep = 9962. State = [[-0.291399   -0.20035098]]. Action = [[-0.20058486 -0.07338977 -0.12035984  0.6961763 ]]. Reward = [0.]
Curr episode timestep = 474
Scene graph at timestep 9962 is [True, False, False, True, False, False]
Current timestep = 9963. State = [[-0.291399   -0.20035098]]. Action = [[-0.2214945  -0.23459242  0.04636404 -0.7583976 ]]. Reward = [0.]
Curr episode timestep = 475
Scene graph at timestep 9963 is [True, False, False, True, False, False]
Scene graph at timestep 9963 is [True, False, False, True, False, False]
State prediction error at timestep 9963 is tensor(5.8205e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9964. State = [[-0.291399   -0.20035098]]. Action = [[-0.15116262 -0.04617563 -0.19062735 -0.7892698 ]]. Reward = [0.]
Curr episode timestep = 476
Scene graph at timestep 9964 is [True, False, False, True, False, False]
Scene graph at timestep 9964 is [True, False, False, True, False, False]
State prediction error at timestep 9964 is tensor(8.4312e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9965. State = [[-0.291399   -0.20035098]]. Action = [[-0.035014   -0.16433528  0.21881914  0.3390727 ]]. Reward = [0.]
Curr episode timestep = 477
Scene graph at timestep 9965 is [True, False, False, True, False, False]
Current timestep = 9966. State = [[-0.291399   -0.20035098]]. Action = [[-0.226838   -0.23839559 -0.01944269  0.04563987]]. Reward = [0.]
Curr episode timestep = 478
Scene graph at timestep 9966 is [True, False, False, True, False, False]
Current timestep = 9967. State = [[-0.291399   -0.20035098]]. Action = [[-0.183693   -0.19477704 -0.24794103 -0.68629974]]. Reward = [0.]
Curr episode timestep = 479
Scene graph at timestep 9967 is [True, False, False, True, False, False]
Scene graph at timestep 9967 is [True, False, False, True, False, False]
State prediction error at timestep 9967 is tensor(6.9780e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9968. State = [[-0.291399   -0.20035098]]. Action = [[-0.16911148  0.14356375 -0.15879098  0.50787425]]. Reward = [0.]
Curr episode timestep = 480
Scene graph at timestep 9968 is [True, False, False, True, False, False]
Current timestep = 9969. State = [[-0.291399   -0.20035098]]. Action = [[-0.2122411  -0.242007   -0.12774459 -0.9118108 ]]. Reward = [0.]
Curr episode timestep = 481
Scene graph at timestep 9969 is [True, False, False, True, False, False]
Current timestep = 9970. State = [[-0.291399   -0.20035098]]. Action = [[-0.18428744  0.16989672  0.01251     0.7785735 ]]. Reward = [0.]
Curr episode timestep = 482
Scene graph at timestep 9970 is [True, False, False, True, False, False]
Current timestep = 9971. State = [[-0.291399   -0.20035098]]. Action = [[-0.12037626 -0.15012239 -0.07368204 -0.793706  ]]. Reward = [0.]
Curr episode timestep = 483
Scene graph at timestep 9971 is [True, False, False, True, False, False]
Current timestep = 9972. State = [[-0.291399   -0.20035098]]. Action = [[-0.21250471  0.24773297 -0.22326972 -0.40328026]]. Reward = [0.]
Curr episode timestep = 484
Scene graph at timestep 9972 is [True, False, False, True, False, False]
Scene graph at timestep 9972 is [True, False, False, True, False, False]
State prediction error at timestep 9972 is tensor(5.4229e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9973. State = [[-0.291399   -0.20035098]]. Action = [[-0.2114738  -0.23216435 -0.16925964 -0.722554  ]]. Reward = [0.]
Curr episode timestep = 485
Scene graph at timestep 9973 is [True, False, False, True, False, False]
Current timestep = 9974. State = [[-0.291399   -0.20035098]]. Action = [[-0.20952348 -0.01489855 -0.07805625 -0.96886784]]. Reward = [0.]
Curr episode timestep = 486
Scene graph at timestep 9974 is [True, False, False, True, False, False]
Current timestep = 9975. State = [[-0.291399   -0.20035098]]. Action = [[-0.05891699  0.18649465 -0.2435536  -0.38991308]]. Reward = [0.]
Curr episode timestep = 487
Scene graph at timestep 9975 is [True, False, False, True, False, False]
Scene graph at timestep 9975 is [True, False, False, True, False, False]
State prediction error at timestep 9975 is tensor(2.6211e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9976. State = [[-0.291399   -0.20035098]]. Action = [[-0.20908129 -0.06068796  0.0039677   0.25814152]]. Reward = [0.]
Curr episode timestep = 488
Scene graph at timestep 9976 is [True, False, False, True, False, False]
Current timestep = 9977. State = [[-0.291399   -0.20035098]]. Action = [[-0.2063269   0.09924299 -0.24153921 -0.9907502 ]]. Reward = [0.]
Curr episode timestep = 489
Scene graph at timestep 9977 is [True, False, False, True, False, False]
Current timestep = 9978. State = [[-0.291399   -0.20035098]]. Action = [[-0.09560889 -0.24343027  0.04638976 -0.7464518 ]]. Reward = [0.]
Curr episode timestep = 490
Scene graph at timestep 9978 is [True, False, False, True, False, False]
Current timestep = 9979. State = [[-0.291399   -0.20035098]]. Action = [[-0.20574535  0.1287862  -0.20481811  0.15233684]]. Reward = [0.]
Curr episode timestep = 491
Scene graph at timestep 9979 is [True, False, False, True, False, False]
Current timestep = 9980. State = [[-0.291399   -0.20035098]]. Action = [[-0.09602982 -0.15262483 -0.221606   -0.17908794]]. Reward = [0.]
Curr episode timestep = 492
Scene graph at timestep 9980 is [True, False, False, True, False, False]
Current timestep = 9981. State = [[-0.291399   -0.20035098]]. Action = [[-0.22864285 -0.12690279 -0.15341696  0.770669  ]]. Reward = [0.]
Curr episode timestep = 493
Scene graph at timestep 9981 is [True, False, False, True, False, False]
Scene graph at timestep 9981 is [True, False, False, True, False, False]
State prediction error at timestep 9981 is tensor(2.4398e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9982. State = [[-0.291399   -0.20035098]]. Action = [[-0.23892331 -0.06599635  0.08421415 -0.77281487]]. Reward = [0.]
Curr episode timestep = 494
Scene graph at timestep 9982 is [True, False, False, True, False, False]
Current timestep = 9983. State = [[-0.291399   -0.20035098]]. Action = [[-0.12323007 -0.1607776   0.11190084  0.09730983]]. Reward = [0.]
Curr episode timestep = 495
Scene graph at timestep 9983 is [True, False, False, True, False, False]
Scene graph at timestep 9983 is [True, False, False, True, False, False]
State prediction error at timestep 9983 is tensor(2.4600e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9984. State = [[-0.291399   -0.20035098]]. Action = [[-0.19622108  0.06369925  0.09890854  0.90401363]]. Reward = [0.]
Curr episode timestep = 496
Scene graph at timestep 9984 is [True, False, False, True, False, False]
Current timestep = 9985. State = [[-0.291399   -0.20035098]]. Action = [[-0.05307032  0.02966833  0.13954246 -0.7672155 ]]. Reward = [0.]
Curr episode timestep = 497
Scene graph at timestep 9985 is [True, False, False, True, False, False]
Current timestep = 9986. State = [[-0.291399   -0.20035098]]. Action = [[-0.23686753  0.2326842  -0.15469569 -0.512941  ]]. Reward = [0.]
Curr episode timestep = 498
Scene graph at timestep 9986 is [True, False, False, True, False, False]
Current timestep = 9987. State = [[-0.291399   -0.20035098]]. Action = [[-0.23483397 -0.24187113 -0.09000778 -0.44939208]]. Reward = [0.]
Curr episode timestep = 499
Scene graph at timestep 9987 is [True, False, False, True, False, False]
Scene graph at timestep 9987 is [True, False, False, True, False, False]
State prediction error at timestep 9987 is tensor(1.3734e-06, grad_fn=<MseLossBackward0>)
Current timestep = 9988. State = [[-0.291399   -0.20035098]]. Action = [[-0.05562989  0.1691756  -0.17655943  0.9857609 ]]. Reward = [0.]
Curr episode timestep = 500
Scene graph at timestep 9988 is [True, False, False, True, False, False]
Current timestep = 9989. State = [[-0.20543796  0.00804753]]. Action = [[-0.05366693  0.06212282  0.09987459 -0.8940558 ]]. Reward = [0.]
Curr episode timestep = 501
Scene graph at timestep 9989 is [True, False, False, True, False, False]
Human Feedback received at timestep 9989 of 1
Current timestep = 9990. State = [[-0.2026629   0.00890831]]. Action = [[ 0.23038492 -0.11267194 -0.18736082  0.03624666]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 9990 is [True, False, False, False, True, False]
Current timestep = 9991. State = [[-0.20070215  0.0086592 ]]. Action = [[ 0.21162707 -0.09784129 -0.14490339  0.032956  ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 9991 is [True, False, False, False, True, False]
Current timestep = 9992. State = [[-0.19650619  0.00764174]]. Action = [[ 0.22970825 -0.08397853 -0.1795141   0.10454082]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 9992 is [True, False, False, False, True, False]
Scene graph at timestep 9992 is [True, False, False, False, True, False]
State prediction error at timestep 9992 is tensor(2.8292e-07, grad_fn=<MseLossBackward0>)
Current timestep = 9993. State = [[-0.1888291   0.00624358]]. Action = [[ 0.23479122 -0.09005694 -0.18608744 -0.03289908]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 9993 is [True, False, False, False, True, False]
Current timestep = 9994. State = [[-0.1816688  0.004731 ]]. Action = [[ 0.20733893 -0.08063076 -0.17191508  0.08922362]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 9994 is [True, False, False, False, True, False]
Human Feedback received at timestep 9994 of 1
Current timestep = 9995. State = [[-0.17370209  0.00294406]]. Action = [[ 0.21355087 -0.09624213 -0.1355175  -0.08327949]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 9995 is [True, False, False, False, True, False]
Current timestep = 9996. State = [[-1.6517717e-01  9.8864883e-05]]. Action = [[ 0.19662818 -0.09947073 -0.13956307  0.0887574 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 9996 is [True, False, False, False, True, False]
Human Feedback received at timestep 9996 of 1
Current timestep = 9997. State = [[-0.15843795 -0.00247132]]. Action = [[ 0.23753542 -0.06337288 -0.1875542  -0.02817297]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 9997 is [True, False, False, False, True, False]
Current timestep = 9998. State = [[-0.15033197 -0.00553601]]. Action = [[ 0.14207989 -0.12658705 -0.10762975  0.05783081]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 9998 is [True, False, False, False, True, False]
Scene graph at timestep 9998 is [True, False, False, False, True, False]
State prediction error at timestep 9998 is tensor(1.7270e-05, grad_fn=<MseLossBackward0>)
Current timestep = 9999. State = [[-0.14404012 -0.00856514]]. Action = [[ 0.21156275 -0.10173765 -0.18285297  0.08656216]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 9999 is [True, False, False, False, True, False]
Scene graph at timestep 9999 is [True, False, False, False, True, False]
State prediction error at timestep 9999 is tensor(8.6550e-06, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 9999 of 1
