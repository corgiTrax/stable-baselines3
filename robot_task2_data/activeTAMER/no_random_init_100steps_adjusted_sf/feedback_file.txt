Current timestep = 0. State = [[-0.23978324 -0.00544006  0.2415634   1.        ]]. Action = [[ 0.6752809  -0.29345167  0.86048007  0.00107884]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3909, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.22909243 -0.01452905  0.25405484  1.        ]]. Action = [[-0.59492475  0.0530982  -0.18350077  0.4943217 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3758, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1 of -1
Current timestep = 2. State = [[-0.23415454 -0.01347774  0.25271478  1.        ]]. Action = [[-0.971198    0.6809635   0.8618622   0.14697981]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3455, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.25139433  0.00236523  0.23211688  1.        ]]. Action = [[-0.67421544  0.44791985 -0.6830506  -0.7495307 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 4. State = [[-0.25171503  0.00167736  0.23131397  1.        ]]. Action = [[-0.5211358  -0.16710323 -0.3016026  -0.58337736]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 4 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 4 is tensor(0.2805, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 4 of -1
Current timestep = 5. State = [[-0.25173873  0.00140307  0.23132461  1.        ]]. Action = [[-0.79291356  0.23677981 -0.01264232  0.22843361]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2688, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.25173873  0.00140307  0.23132461  1.        ]]. Action = [[-0.56718063 -0.92920697  0.06791079 -0.7182951 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2082, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-0.25173873  0.00140307  0.23132461  1.        ]]. Action = [[-0.78218275 -0.97638696 -0.76644665 -0.34022707]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.1944, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 7 of -1
Current timestep = 8. State = [[-0.25173873  0.00140307  0.23132461  1.        ]]. Action = [[-0.80551445 -0.44777858 -0.07581699 -0.4546733 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1822, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-2.5016978e-01  7.1695645e-04  2.3237443e-01  1.0000000e+00]]. Action = [[ 0.18230343 -0.62651163 -0.5954716  -0.7866285 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 10. State = [[-0.23899736 -0.01181131  0.21360137  1.        ]]. Action = [[ 0.9095961   0.12738872 -0.20371717  0.30173707]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1283, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.20972987 -0.01790366  0.20314167  1.        ]]. Action = [[ 0.68635666 -0.02891475  0.07081652  0.64602256]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1150, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.19559912 -0.01904507  0.20094022  1.        ]]. Action = [[ 0.6788497  0.9779593  0.5261209 -0.8540199]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 12 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 12 is tensor(0.0955, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.19755453 -0.02700449  0.20120521  1.        ]]. Action = [[-0.6183653 -0.4992593  0.1871655  0.826293 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.0811, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.20405336 -0.04315145  0.21173199  1.        ]]. Action = [[-0.1381613  -0.18309557  0.9153781   0.7369511 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0583, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.25089315  0.00254634  0.23242818  1.        ]]. Action = [[ 0.6528727   0.9239626  -0.04956061 -0.11320758]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 16. State = [[-0.24712563 -0.0075463   0.24302402  1.        ]]. Action = [[ 0.361763   -0.6158788   0.8719082   0.98883605]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0269, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.23573843 -0.01359018  0.2559649   1.        ]]. Action = [[ 0.9265251  0.6150117 -0.9160235  0.1170013]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0283, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.25110635  0.00215616  0.2324186   1.        ]]. Action = [[-0.47361016  0.2922833  -0.19548434 -0.66037524]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 19. State = [[-0.2512434   0.00246445  0.23240356  1.        ]]. Action = [[-0.8267425  -0.8549263   0.2455405   0.05382013]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 19 of -1
Current timestep = 20. State = [[-0.2510281   0.00217637  0.23311047  1.        ]]. Action = [[-0.19130886 -0.8451162   0.41635227 -0.38852757]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 21. State = [[-0.24784009 -0.02166259  0.24325395  1.        ]]. Action = [[ 0.3449129  -0.21454716  0.73203516  0.96241355]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 21 of 0
Current timestep = 22. State = [[-0.24320266 -0.02443096  0.25914553  1.        ]]. Action = [[-0.09044868  0.82027245  0.10526097  0.30724764]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 22 of 0
Current timestep = 23. State = [[-0.250995    0.00263333  0.23225263  1.        ]]. Action = [[ 0.59197974 -0.49000394 -0.76217043 -0.31182265]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 24. State = [[-2.5032461e-01 -2.2467932e-04  2.2957420e-01  1.0000000e+00]]. Action = [[-0.8191117   0.87845135 -0.01370114 -0.12751675]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0181, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 24 of -1
Current timestep = 25. State = [[-0.24586722 -0.00939182  0.23702575  1.        ]]. Action = [[ 0.13753855 -0.48428458  0.97780895  0.5864389 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 25 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.23374309 -0.02094269  0.24658528  1.        ]]. Action = [[ 0.6500325   0.08872032 -0.55405176  0.2965821 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 26 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0241, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.25118667  0.00223321  0.23209348  1.        ]]. Action = [[-0.4723804   0.42838168  0.74975514 -0.38775933]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 28. State = [[-0.25340465 -0.0017663   0.231509    1.        ]]. Action = [[-0.33688867 -0.3043878   0.09082556  0.9453418 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 28 of -1
Current timestep = 29. State = [[-0.26032457 -0.0191803   0.2268585   1.        ]]. Action = [[-0.22018808 -0.7511584  -0.45793712  0.83596206]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 29 of -1
Current timestep = 30. State = [[-0.25992996 -0.03462873  0.222338    1.        ]]. Action = [[ 0.81465065  0.07032955 -0.16655517  0.3951671 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0404, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.23850687 -0.03426442  0.22104777  1.        ]]. Action = [[0.9372319  0.10922909 0.85070324 0.31995082]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0312, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.21166983 -0.02432039  0.23672299  1.        ]]. Action = [[0.39626765 0.54891217 0.1094147  0.7919867 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0442, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 32 of 1
Current timestep = 33. State = [[-0.25092438  0.00253361  0.23230986  1.        ]]. Action = [[-0.37356293  0.456591    0.93025255 -0.6720422 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 34. State = [[-0.25112563  0.00215527  0.23237856  1.        ]]. Action = [[-0.38545704 -0.6765627  -0.8569599  -0.7883685 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0352, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 34 of -1
Current timestep = 35. State = [[-0.24699917 -0.00940728  0.23485929  1.        ]]. Action = [[ 0.47106087 -0.7277996   0.27295923  0.8393253 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0478, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.24659255 -0.01870558  0.24758035  1.        ]]. Action = [[-0.46307886  0.39807355  0.8288368   0.7613734 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0471, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 36 of 0
Current timestep = 37. State = [[-0.25392205 -0.02703143  0.26193017  1.        ]]. Action = [[ 0.23060834 -0.9973975  -0.52566814  0.38950014]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0510, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 37 of 0
Current timestep = 38. State = [[-0.25112405  0.00258943  0.23227042  1.        ]]. Action = [[ 0.06230307 -0.77457035 -0.1245144  -0.289541  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 39. State = [[-0.2511708   0.00260612  0.23220985  1.        ]]. Action = [[ 0.55145895 -0.24628448  0.64663506 -0.7687316 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 40. State = [[-0.22599155 -0.01125359  0.2395379   1.        ]]. Action = [[ 0.928457   -0.44705212 -0.14825857  0.16827047]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 40 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0646, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 40 of 1
Current timestep = 41. State = [[-0.20379615 -0.01544587  0.23827799  1.        ]]. Action = [[-0.37642348  0.5573931  -0.4087162   0.9833336 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0674, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 41 of -1
Current timestep = 42. State = [[-0.25136042  0.00239611  0.23241663  1.        ]]. Action = [[ 0.04774678 -0.30359977 -0.407467   -0.40348732]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 43. State = [[-0.2449118   0.00237938  0.22571765  1.        ]]. Action = [[ 0.96360826  0.2258209  -0.7884212   0.49124193]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 43 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0662, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.25078958  0.00229393  0.23235027  1.        ]]. Action = [[ 0.7071283  -0.64209306 -0.6021165  -0.2666204 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 45. State = [[-0.25124097  0.00227975  0.23205535  1.        ]]. Action = [[ 0.8213537   0.92049325  0.596828   -0.67289305]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 46. State = [[-0.25108093  0.00258115  0.22319418  1.        ]]. Action = [[ 0.15539145  0.13721967 -0.81385803  0.46921086]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0843, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 46 of 0
Current timestep = 47. State = [[-0.2510376   0.00260263  0.23233491  1.        ]]. Action = [[ 0.21255255 -0.26883817  0.6862329  -0.14420009]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 48. State = [[-0.25091812  0.00239454  0.23228545  1.        ]]. Action = [[ 0.8370279   0.77647066  0.5216367  -0.6309074 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 49. State = [[-0.24668752 -0.00360589  0.23512429  1.        ]]. Action = [[ 0.2697904  -0.38158876  0.45311952  0.6982002 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.0931, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.2506869   0.00259446  0.23273797  1.        ]]. Action = [[ 0.8546796  -0.5270228  -0.24974906 -0.40361536]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 51. State = [[-0.25050518  0.00187086  0.23231977  1.        ]]. Action = [[-0.060462   -0.7272738  -0.36770904 -0.2699957 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 52. State = [[-0.24542353 -0.02483335  0.2328952   1.        ]]. Action = [[ 0.6362889  -0.5424567   0.58075666  0.27924097]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.0961, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.22359134 -0.04494162  0.24872513  1.        ]]. Action = [[0.96729684 0.02060175 0.70212317 0.2207402 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.0939, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 53 of 1
Current timestep = 54. State = [[-0.25096256  0.00260329  0.23253272  1.        ]]. Action = [[-0.08077449  0.8288641  -0.71778744 -0.07632142]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 55. State = [[-0.2417838   0.00279037  0.24038613  1.        ]]. Action = [[ 0.7089629  -0.00277877  0.94519377  0.8212514 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.0924, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.23296615  0.0118679   0.26191416  1.        ]]. Action = [[-0.72169805  0.57925105  0.7209252   0.8464067 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.0882, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 56 of -1
Current timestep = 57. State = [[-0.24982049  0.02192715  0.29208553  1.        ]]. Action = [[-0.07462817 -0.25196874  0.9131608   0.8098068 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.1018, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 57 of 0
Current timestep = 58. State = [[-0.25060043  0.0027423   0.23259668  1.        ]]. Action = [[ 0.33113146 -0.7781986  -0.3881309  -0.64654917]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 59. State = [[-0.2508057   0.00258817  0.23242195  1.        ]]. Action = [[ 0.92146516  0.53991055  0.9019015  -0.19315445]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 60. State = [[-0.2508845   0.00213183  0.23244269  1.        ]]. Action = [[-0.66696066 -0.4460678   0.01040995  0.8909917 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.1083, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.25093955  0.00259054  0.23230179  1.        ]]. Action = [[ 0.6521685  -0.81187713  0.40612411 -0.17327476]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 62. State = [[-0.24030916 -0.00105795  0.23084465  1.        ]]. Action = [[ 0.8656225  -0.15770566 -0.36031544  0.57879615]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 62 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.1121, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 62 of 1
Current timestep = 63. State = [[-0.2511627   0.0022022   0.23238735  1.        ]]. Action = [[-0.51863164  0.83375263 -0.152444   -0.36574054]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 64. State = [[-0.24278098  0.01129602  0.23784529  1.        ]]. Action = [[0.8629792 0.5426674 0.4347216 0.8252485]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.1132, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.23451605  0.02774813  0.24180493  1.        ]]. Action = [[-0.32881463  0.40897417 -0.5612562   0.67143786]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.1115, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 65 of -1
Current timestep = 66. State = [[-0.2511556   0.00229483  0.23229828  1.        ]]. Action = [[ 0.11141574  0.95299697 -0.20297688 -0.11728096]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 67. State = [[-0.24657963  0.01607362  0.22907005  1.        ]]. Action = [[ 0.6439023   0.76168776 -0.29540324  0.95098186]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.1114, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.22337936  0.03521932  0.22363198  1.        ]]. Action = [[0.81189024 0.315722   0.02831364 0.24429786]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.1258, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 68 of 1
Current timestep = 69. State = [[-0.19638534  0.04258774  0.22455585  1.        ]]. Action = [[ 0.65539527 -0.05358356  0.16032207  0.74880934]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.1289, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.186732    0.031297    0.22942004  1.        ]]. Action = [[-0.9651858 -0.8380193  0.4336474  0.400136 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.0974, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.19513024  0.01426494  0.23390289  1.        ]]. Action = [[ 0.9858146   0.12834978 -0.86309373  0.6997763 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.1063, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 71 of -1
Current timestep = 72. State = [[-0.19979697  0.00586861  0.24430883  1.        ]]. Action = [[-0.5944934  -0.50120324  0.8290999   0.831597  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.1066, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 72 of 0
Current timestep = 73. State = [[-0.21023001 -0.00655487  0.2588881   1.        ]]. Action = [[ 0.5081365  -0.19859171 -0.83857495  0.47523737]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.1128, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 73 of 0
Current timestep = 74. State = [[-0.21360227  0.0018403   0.2511971   1.        ]]. Action = [[-0.559745    0.6780313   0.12596726  0.16916084]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.1202, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 74 of 0
Current timestep = 75. State = [[-0.21576302  0.01930829  0.24909867  1.        ]]. Action = [[ 0.589165    0.47215438 -0.26278007  0.4233154 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.1217, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 75 of 0
Current timestep = 76. State = [[-0.20112664  0.0169987   0.24365893  1.        ]]. Action = [[ 0.88211083 -0.77108985 -0.09380609  0.56075764]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.1039, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.18144658 -0.0099359   0.23941915  1.        ]]. Action = [[-0.35179853 -0.96608996  0.0065037   0.08400261]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.1056, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 77 of 0
Current timestep = 78. State = [[-0.18491347 -0.04708904  0.24436902  1.        ]]. Action = [[-0.09445554 -0.94355434  0.00494027  0.6559379 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.1094, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 78 of 0
Current timestep = 79. State = [[-0.18596433 -0.06655171  0.24937625  1.        ]]. Action = [[ 0.5666429  -0.1589734  -0.54542804 -0.27712768]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Scene graph at timestep 79 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 79 is tensor(0.1029, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.2510347   0.00259973  0.23247837  1.        ]]. Action = [[ 0.9935366  -0.37529582  0.73725915 -0.61268294]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 81. State = [[-0.24507605 -0.00972434  0.23965883  1.        ]]. Action = [[ 0.5574925  -0.608709    0.79310083  0.76654196]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.0969, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 81 of 1
Current timestep = 82. State = [[-0.22865517 -0.03520014  0.2524672   1.        ]]. Action = [[ 0.687973   -0.6390866  -0.21484315  0.84488475]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.0926, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.2509812   0.00254832  0.23226237  1.        ]]. Action = [[ 0.45720243  0.5779121   0.5150094  -0.05534637]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 84. State = [[-0.25111213  0.00286443  0.22983313  1.        ]]. Action = [[-0.77365816 -0.32661778  0.22509456  0.7962413 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 84 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 84 is tensor(0.0879, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 84 of -1
Current timestep = 85. State = [[-0.2509715   0.0021558   0.23218451  1.        ]]. Action = [[ 0.47232985  0.79992867  0.44360805 -0.54492885]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 86. State = [[-0.25060222  0.00259355  0.23278746  1.        ]]. Action = [[ 0.15248966 -0.626497    0.31035078 -0.6029395 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 87. State = [[-0.25042415  0.00290987  0.23415369  1.        ]]. Action = [[-0.3009405  -0.44257188  0.6994035  -0.6696047 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 88. State = [[-0.23843433  0.00751152  0.24145344  1.        ]]. Action = [[0.83463216 0.30422282 0.78694654 0.7732568 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.0798, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.2512262   0.00238744  0.23226014  1.        ]]. Action = [[ 0.21662343 -0.8644988  -0.323848   -0.72827023]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 90. State = [[-0.23850049 -0.00722883  0.2394881   1.        ]]. Action = [[ 0.93049073 -0.61350024  0.86335146  0.47419   ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0788, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.20803934 -0.02501631  0.2645367   1.        ]]. Action = [[ 0.8208494  -0.32400334  0.9850172   0.90034246]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.0749, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.25129458  0.00234006  0.23219842  1.        ]]. Action = [[ 0.7351936   0.15971112 -0.03930736 -0.10918456]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 93. State = [[-2.4716699e-01  8.3548098e-04  2.3965089e-01  1.0000000e+00]]. Action = [[ 0.25625205 -0.15989089  0.95972776  0.87096834]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0835, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.23245901 -0.00617404  0.2635627   1.        ]]. Action = [[ 0.61278486 -0.27858037  0.89871454  0.89559674]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0863, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.2164985  -0.02265154  0.29953158  1.        ]]. Action = [[-0.3080777  -0.65287143  0.8243296   0.93080306]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0893, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 95 of 0
Current timestep = 96. State = [[-0.21612431 -0.04812084  0.33220738  1.        ]]. Action = [[ 0.6345736  -0.75468063  0.8187305   0.7508267 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.0954, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.19967607 -0.07295553  0.35191402  1.        ]]. Action = [[ 0.25957954 -0.46256816 -0.94874257  0.4455291 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 97 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.1038, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 97 of 0
Current timestep = 98. State = [[-0.25080833  0.00254752  0.23266935  1.        ]]. Action = [[-0.8349357   0.6475439   0.94766104 -0.08948046]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 99. State = [[-0.24659605  0.00279792  0.22479038  1.        ]]. Action = [[ 0.7564578   0.06071186 -0.5876604   0.82486904]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.1010, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 99 of 0
Current timestep = 100. State = [[-0.25016195  0.00255389  0.23262773  1.        ]]. Action = [[ 0.5120902 -0.9788491  0.5462005 -0.6327665]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 101. State = [[-0.2451769   0.00121234  0.22992623  1.        ]]. Action = [[ 0.84155345 -0.01413763 -0.65984017  0.902437  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 101 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.0993, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 101 of 0
Current timestep = 102. State = [[-0.25048265  0.00229368  0.23240837  1.        ]]. Action = [[-0.5929865   0.48064506 -0.8246613  -0.78886396]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 103. State = [[-0.2514201   0.00242065  0.23254827  1.        ]]. Action = [[-0.49063635  0.45954537  0.05706513  0.6169375 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 103 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 103 is tensor(0.1146, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 103 of -1
Current timestep = 104. State = [[-0.2516478   0.00210998  0.2325793   1.        ]]. Action = [[-0.8667865  -0.91898084 -0.09218752  0.84317136]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0974, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.2516641   0.00215643  0.2325793   1.        ]]. Action = [[-0.3753606  -0.6808823   0.43613124  0.71385705]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 105 is tensor(0.1129, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 105 of -1
Current timestep = 106. State = [[-0.25072446  0.00269513  0.2323601   1.        ]]. Action = [[ 0.47016764  0.0208993  -0.43070114 -0.8273111 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 107. State = [[-0.2485427   0.00291928  0.22775023  1.        ]]. Action = [[-0.42951667 -0.89821166  0.92667353  0.9384476 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0964, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 107 of -1
Current timestep = 108. State = [[-0.23954272  0.01610967  0.22448424  1.        ]]. Action = [[ 0.81551147  0.8622236  -0.30890822  0.7963624 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 108 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 108 is tensor(0.1069, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 108 of 0
Current timestep = 109. State = [[-0.20944242  0.03956175  0.209257    1.        ]]. Action = [[ 0.83376026  0.5332358  -0.7126673   0.5801629 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 109 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 109 is tensor(0.1075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.18908694  0.04807309  0.18015704  1.        ]]. Action = [[-0.36265123 -0.42475462 -0.7984213   0.78975534]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 110 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 110 is tensor(0.1128, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.18994106  0.04298966  0.16154091  1.        ]]. Action = [[ 0.9113337  -0.07963026  0.86131644  0.9573312 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 111 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 111 is tensor(0.1091, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.18990844  0.04215368  0.16156743  1.        ]]. Action = [[ 0.44420457 -0.65834147  0.9775393   0.7849461 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 112 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 112 is tensor(0.1088, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.18990844  0.04215368  0.16156743  1.        ]]. Action = [[ 0.6553235  -0.01064765  0.18224585  0.9635949 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 113 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 113 is tensor(0.1187, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 113 of -1
Current timestep = 114. State = [[-0.18990844  0.04215368  0.16156743  1.        ]]. Action = [[ 0.56158125 -0.43780035 -0.07795668  0.5276768 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 114 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 114 is tensor(0.1209, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 114 of -1
Current timestep = 115. State = [[-0.18990844  0.04215368  0.16156743  1.        ]]. Action = [[ 0.9688604   0.3777268  -0.7958791  -0.00198251]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 115 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 115 is tensor(0.0988, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 115 of -1
Current timestep = 116. State = [[-0.19767615  0.04320299  0.15216792  1.        ]]. Action = [[-0.4990393   0.17494416 -0.6968535   0.24873805]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.1082, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 116 of -1
Current timestep = 117. State = [[-0.20688005  0.05031535  0.13310401  1.        ]]. Action = [[-0.04456282  0.26298833  0.40004015  0.4175942 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.1221, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 117 of -1
Current timestep = 118. State = [[-0.21184179  0.05654973  0.13358106  1.        ]]. Action = [[ 0.9042177   0.7878485  -0.08294523  0.27818906]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.1093, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.21633531  0.0673109   0.13346004  1.        ]]. Action = [[-0.3860796   0.6665263  -0.02845991  0.15875351]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.1123, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 119 of -1
Current timestep = 120. State = [[-0.22651692  0.07898263  0.13742153  1.        ]]. Action = [[-0.1629594  -0.25606346  0.3003769   0.8505292 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.1072, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.2502601   0.00247663  0.2324971   1.        ]]. Action = [[ 0.40308475  0.8485627   0.34138715 -0.35145068]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 122. State = [[-0.2513101   0.00223333  0.23206232  1.        ]]. Action = [[ 0.95095146  0.91588235  0.21623826 -0.19040805]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 123. State = [[-0.25069484 -0.00995248  0.2400558   1.        ]]. Action = [[-0.31578815 -0.8692028   0.9276149   0.9088769 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0783, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.24457473 -0.04242923  0.2551535   1.        ]]. Action = [[ 0.8768735  -0.92278606 -0.11572528  0.61775184]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0809, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.22652045 -0.05686865  0.27004543  1.        ]]. Action = [[0.31597638 0.33800232 0.7180308  0.93865705]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0850, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.20565322 -0.04293682  0.27698728  1.        ]]. Action = [[ 0.8852737   0.71600604 -0.75661236  0.8784555 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0820, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.2509638   0.00253339  0.23239931  1.        ]]. Action = [[ 0.35751677 -0.6435009   0.66386676 -0.7592524 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 128. State = [[-0.2506254   0.00367023  0.22364049  1.        ]]. Action = [[ 0.39151192  0.25879228 -0.9075251   0.4119252 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0841, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.25090903  0.0129619   0.19969276  1.        ]]. Action = [[-0.0569824   0.44215107 -0.4874282   0.8674035 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0806, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 129 of -1
Current timestep = 130. State = [[-0.23840883  0.02037105  0.1926301   1.        ]]. Action = [[ 0.96662354 -0.21943259  0.79526734  0.7217252 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0758, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.20504819  0.00656434  0.20862272  1.        ]]. Action = [[ 0.8378173  -0.857402    0.17715108  0.48426104]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0759, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.18706524 -0.00144887  0.2098952   1.        ]]. Action = [[ 0.35118246  0.6276636  -0.9791169   0.7288904 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0690, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 132 of -1
Current timestep = 133. State = [[-0.1695835   0.0050728   0.18521672  1.        ]]. Action = [[0.23833048 0.72990537 0.9352304  0.6833576 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: No entry zone
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0700, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 133 of -1
Current timestep = 134. State = [[-0.1693762   0.00514153  0.18530698  1.        ]]. Action = [[ 0.55104136 -0.8652928   0.01534593  0.5423863 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0818, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 134 of -1
Current timestep = 135. State = [[-0.1691015   0.00521128  0.18542877  1.        ]]. Action = [[ 0.7156602 -0.8966545 -0.6032745  0.504446 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0713, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.1689616   0.00521333  0.18549399  1.        ]]. Action = [[ 0.9015924  -0.9978186   0.2630371   0.00671673]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.0735, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 136 of -1
Current timestep = 137. State = [[-0.16889018  0.00528016  0.18552117  1.        ]]. Action = [[ 0.78821707 -0.35758638  0.82842064  0.98167276]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0676, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 137 of -1
Current timestep = 138. State = [[-0.16889018  0.00528016  0.18552117  1.        ]]. Action = [[ 0.61662364 -0.8636496   0.92112374 -0.2754035 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Scene graph at timestep 138 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 138 is tensor(0.0781, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 138 of -1
Current timestep = 139. State = [[-0.1714897   0.01370695  0.18665743  1.        ]]. Action = [[-0.33993655  0.552142    0.43779945  0.8027828 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0739, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.17681305  0.02541897  0.18523015  1.        ]]. Action = [[-0.03777146 -0.36241078 -0.5074078   0.08344698]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 140 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.0925, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 140 of -1
Current timestep = 141. State = [[-0.17715828  0.01979427  0.18504691  1.        ]]. Action = [[ 0.8335322  -0.5100442   0.7863393   0.84240675]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Scene graph at timestep 141 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0678, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 141 of -1
Current timestep = 142. State = [[-0.17707309  0.01954923  0.1851157   1.        ]]. Action = [[ 0.61451864  0.66299427 -0.45468616  0.59901893]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Scene graph at timestep 142 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0717, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 142 of -1
Current timestep = 143. State = [[-0.17707795  0.01935134  0.18513274  1.        ]]. Action = [[ 0.2159208  -0.22569436  0.7519593   0.36654627]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Scene graph at timestep 143 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.0855, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 143 of -1
Current timestep = 144. State = [[-0.17707959  0.01928538  0.18513843  1.        ]]. Action = [[0.9493997  0.61518526 0.17826903 0.8090737 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Scene graph at timestep 144 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0661, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 144 of -1
Current timestep = 145. State = [[-0.17707959  0.01928538  0.18513843  1.        ]]. Action = [[ 0.80990934 -0.43614042  0.76592743  0.9592266 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Scene graph at timestep 145 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0638, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.17667806  0.00729496  0.18696865  1.        ]]. Action = [[-0.0426836  -0.74747777  0.3748746   0.23707068]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 146 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.0843, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 146 of 0
Current timestep = 147. State = [[-0.25015727  0.00259016  0.23271298  1.        ]]. Action = [[-0.57725936 -0.48736262 -0.26243806 -0.66065377]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 148. State = [[-0.2506009   0.00265752  0.23378535  1.        ]]. Action = [[ 0.67453194 -0.4220754   0.59700024 -0.31899607]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 149. State = [[-0.23859864 -0.0107993   0.24288188  1.        ]]. Action = [[-0.47358942 -0.06592101  0.01960826  0.7445729 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 149 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 149 is tensor(0.0751, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 149 of -1
Current timestep = 150. State = [[-0.23196617 -0.01369522  0.24273483  1.        ]]. Action = [[ 0.26494074  0.32992554 -0.70375746  0.9308655 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 150 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 150 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 150 of -1
Current timestep = 151. State = [[-2.3286851e-01  3.2861328e-05  2.3407647e-01  1.0000000e+00]]. Action = [[-0.43187743  0.6591959   0.10022128  0.45054722]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 151 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 151 is tensor(0.0742, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 151 of -1
Current timestep = 152. State = [[-0.23657268  0.02469021  0.22571857  1.        ]]. Action = [[ 0.5390768   0.48690438 -0.9033768   0.23998296]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 152 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 152 is tensor(0.0574, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.2510789   0.00254805  0.23264979  1.        ]]. Action = [[ 0.96679854  0.9757149   0.1482284  -0.4629473 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 154. State = [[-0.25060913  0.00263917  0.23360793  1.        ]]. Action = [[ 0.6995585  -0.9342724   0.73034227 -0.4210359 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 155. State = [[-0.22424738 -0.00915624  0.26285428  1.        ]]. Action = [[0.9161341  0.65623343 0.9329318  0.92689943]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.0449, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.19086868 -0.01245963  0.29999614  1.        ]]. Action = [[ 0.17668748 -0.46467352  0.18763971  0.9336176 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 156 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0708, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.19214445 -0.01675461  0.3172748   1.        ]]. Action = [[-0.9079515   0.22231174  0.7063153   0.35504293]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 157 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 157 is tensor(0.0698, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 157 of -1
Current timestep = 158. State = [[-0.20980538 -0.02217359  0.32684264  1.        ]]. Action = [[ 0.54866993 -0.5830359  -0.8181931   0.6232871 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.0625, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 158 of 0
Current timestep = 159. State = [[-0.20743482 -0.03656957  0.32008117  1.        ]]. Action = [[-0.8892944  -0.13149643  0.25142097  0.38596165]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.0738, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.21337819 -0.05350338  0.31820768  1.        ]]. Action = [[ 0.77014446 -0.7690338  -0.535169    0.7095361 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.0589, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 160 of 0
Current timestep = 161. State = [[-0.21556087 -0.06132669  0.30550703  1.        ]]. Action = [[-0.9344301   0.600003   -0.46734744  0.9682094 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.0558, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.22783868 -0.04876931  0.2877736   1.        ]]. Action = [[ 0.26798224  0.30931175 -0.47160506  0.66543746]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.0761, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.22330159 -0.04245276  0.2671136   1.        ]]. Action = [[ 0.13812006 -0.12278223 -0.7043441   0.9021225 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.0694, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.2182798  -0.04482871  0.2481641   1.        ]]. Action = [[ 0.1746254  -0.17165828  0.16491139  0.47365785]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.0883, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 164 of -1
Current timestep = 165. State = [[-0.20739043 -0.05123641  0.25672805  1.        ]]. Action = [[ 0.7445581  -0.3506583   0.79784036  0.8517822 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.0665, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.18649215 -0.06584677  0.2693341   1.        ]]. Action = [[ 0.7331722 -0.4781313 -0.0413906  0.8818337]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.0715, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.17470293 -0.08365455  0.2732938   1.        ]]. Action = [[-0.23049903 -0.4828403   0.19748378  0.9464412 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.0803, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 167 of -1
Current timestep = 168. State = [[-0.16922191 -0.08550768  0.286115    1.        ]]. Action = [[0.5737338  0.73714745 0.7820116  0.9351114 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.0613, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.14328453 -0.08173703  0.3144229   1.        ]]. Action = [[ 0.6479968  -0.5889292   0.26043665  0.9835094 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.0720, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.13573399 -0.07852934  0.33219075  1.        ]]. Action = [[-0.8223799   0.7214134   0.7601876   0.49803388]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.0740, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.15024118 -0.07494018  0.34032336  1.        ]]. Action = [[-0.06902075 -0.49807894 -0.602001    0.65279484]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.0771, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.1447237  -0.08112614  0.34070492  1.        ]]. Action = [[ 0.9193344  -0.01263511  0.30405152  0.8722975 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.2509953   0.00263953  0.23257498  1.        ]]. Action = [[ 0.78708506 -0.35536218 -0.8599614  -0.1813457 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 174. State = [[-0.25539038 -0.00217105  0.23163243  1.        ]]. Action = [[-0.27274036 -0.10941625  0.20079565  0.47767234]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 174 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.0750, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 174 of -1
Current timestep = 175. State = [[-0.2534992   0.00441103  0.24003452  1.        ]]. Action = [[0.9250059  0.84300315 0.79225266 0.8545574 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 175 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 175 is tensor(0.0423, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.2507906   0.00264645  0.23245761  1.        ]]. Action = [[ 0.32457137  0.5789902   0.40456438 -0.37643677]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 177. State = [[-0.24433589 -0.00691499  0.22429547  1.        ]]. Action = [[ 0.71806407 -0.5951131  -0.53887624  0.7896699 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 177 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.0497, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 177 of 0
Current timestep = 178. State = [[-0.22607107 -0.01487708  0.20794983  1.        ]]. Action = [[ 0.44736814  0.44662964 -0.25238502  0.6713296 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 178 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.0619, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.25040197  0.00268899  0.2324785   1.        ]]. Action = [[-0.24622488 -0.6737059   0.9825449  -0.25916123]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 180. State = [[-0.25100148 -0.00145216  0.23377964  1.        ]]. Action = [[-0.4683739  -0.8658884  -0.73086184  0.91726565]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 180 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0467, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.23918086 -0.00287501  0.24200955  1.        ]]. Action = [[0.91833436 0.09635687 0.5742073  0.74812937]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 181 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0549, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.25114092  0.00236462  0.23215923  1.        ]]. Action = [[-0.7537431   0.6073265   0.9482515  -0.19648719]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 183. State = [[-2.513729e-01 -3.646515e-04  2.322699e-01  1.000000e+00]]. Action = [[ 0.05273378 -0.12337601  0.1765387   0.9853716 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 183 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0600, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.2517284  -0.00367569  0.23241393  1.        ]]. Action = [[-0.39802706 -0.8441012  -0.06477809  0.971184  ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 184 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.0527, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.2427218  -0.00524836  0.24076097  1.        ]]. Action = [[ 0.63480794 -0.08854467  0.90193486  0.93194747]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 185 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.22512498  0.00473788  0.2679589   1.        ]]. Action = [[0.11375165 0.81032896 0.8605499  0.3676455 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 186 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.0572, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 186 of 0
Current timestep = 187. State = [[-0.2198503   0.01905007  0.28993323  1.        ]]. Action = [[ 0.20183182 -0.29039764 -0.38400245  0.78030586]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 187 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.0631, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 187 of 0
Current timestep = 188. State = [[-0.25139615  0.00234972  0.23205067  1.        ]]. Action = [[-0.14558476 -0.05757606  0.7919986  -0.00579047]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 189. State = [[-0.2515187   0.00118354  0.23207982  1.        ]]. Action = [[-0.75242853 -0.5268237   0.54464257  0.94178534]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 189 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0544, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.24522336 -0.00597876  0.22797827  1.        ]]. Action = [[ 0.68509936 -0.41089332 -0.33941424  0.58847487]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 190 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0597, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 190 of 0
Current timestep = 191. State = [[-0.23304777 -0.01316417  0.21510519  1.        ]]. Action = [[-0.01761085  0.15796256 -0.524548    0.62328196]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 191 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.22273909 -0.01892876  0.19955903  1.        ]]. Action = [[ 0.7776878  -0.4876821  -0.19560492  0.92214084]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 192 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0542, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 192 of 0
Current timestep = 193. State = [[-0.20538884 -0.02530235  0.19217551  1.        ]]. Action = [[0.92173624 0.8503536  0.3029735  0.97326255]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 193 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0467, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.1982813  -0.02419793  0.19549125  1.        ]]. Action = [[0.4512738  0.21438158 0.42737055 0.84720135]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 194 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0621, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 194 of 0
Current timestep = 195. State = [[-0.182936   -0.02871145  0.20676938  1.        ]]. Action = [[ 0.30840135 -0.45267773  0.65960026  0.86318743]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 195 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0621, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.15708145 -0.05017437  0.23124933  1.        ]]. Action = [[ 0.9725046  -0.8280189   0.56730866  0.5788293 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 196 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0518, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.13024601 -0.06614754  0.25907314  1.        ]]. Action = [[0.37525535 0.14337265 0.6471698  0.95197964]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 197 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0631, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.11918751 -0.06083246  0.28564098  1.        ]]. Action = [[-0.27790457  0.26785827  0.9171953   0.8730259 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 198 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0654, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.11408506 -0.0651086   0.31444982  1.        ]]. Action = [[ 0.4301021  -0.5699633   0.42174304  0.14490366]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 199 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0802, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 199 of 0
Current timestep = 200. State = [[-0.10682015 -0.06199574  0.32648078  1.        ]]. Action = [[ 0.12478387  0.9221115  -0.17902303  0.7023587 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 200 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0705, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.10335566 -0.04310325  0.3181842   1.        ]]. Action = [[ 0.82763195  0.05009866 -0.7867574   0.8289902 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 201 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0627, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.07317606 -0.03138712  0.29863453  1.        ]]. Action = [[0.38585663 0.71400845 0.00512516 0.49635446]]. Reward = [0.]
Curr episode timestep = 13
Above hoop
Scene graph at timestep 202 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0760, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 202 of 0
Current timestep = 203. State = [[-0.05340925 -0.01340991  0.3045126   1.        ]]. Action = [[0.55853593 0.1327517  0.4730785  0.222875  ]]. Reward = [0.]
Curr episode timestep = 14
Above hoop
Scene graph at timestep 203 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0817, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 203 of 0
Current timestep = 204. State = [[-0.04504375 -0.02399398  0.322232    1.        ]]. Action = [[-0.49396372 -0.9132159   0.38742125  0.62259364]]. Reward = [0.]
Curr episode timestep = 15
Above hoop
Scene graph at timestep 204 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0660, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 204 of 0
Current timestep = 205. State = [[-0.04951723 -0.05485789  0.31898978  1.        ]]. Action = [[ 0.05527997 -0.87669504 -0.7539426   0.88306   ]]. Reward = [0.]
Curr episode timestep = 16
Above hoop
Scene graph at timestep 205 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0556, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 205 of 0
Current timestep = 206. State = [[-0.05376092 -0.08163159  0.30514124  1.        ]]. Action = [[-0.50751626 -0.5835086  -0.49578524  0.9231628 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 206 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0638, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.05779592 -0.09456914  0.30225316  1.        ]]. Action = [[-0.36754388  0.09730625  0.51033306  0.456259  ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 207 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0815, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.06217246 -0.09711971  0.31234485  1.        ]]. Action = [[-0.1720779   0.1007309   0.47871184  0.9329175 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 208 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0716, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[-0.07494554 -0.10438662  0.33564743  1.        ]]. Action = [[-0.74368656 -0.53450257  0.8886218   0.84259033]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 209 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0606, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.1030388  -0.12487646  0.35196567  1.        ]]. Action = [[-0.1299482 -0.703809  -0.8411833  0.6223383]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 210 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0644, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 210 of -1
Current timestep = 211. State = [[-0.1105073  -0.14282721  0.33460334  1.        ]]. Action = [[ 0.45555377 -0.32017815 -0.41066068  0.5945107 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 211 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0781, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.10867405 -0.1577275   0.31034997  1.        ]]. Action = [[-0.3496095 -0.533917  -0.8819422  0.5527344]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 212 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.0654, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.11882846 -0.1590702   0.292557    1.        ]]. Action = [[-0.798789    0.9120796   0.4497403   0.48579717]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 213 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.0708, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.13558526 -0.1347095   0.2967286   1.        ]]. Action = [[-0.19342148  0.625371    0.11097395  0.9581344 ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 214 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 214 is tensor(0.0694, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 214 of -1
Current timestep = 215. State = [[-0.13399033 -0.12788607  0.3096345   1.        ]]. Action = [[ 0.8662931 -0.6739357  0.8064798  0.9260948]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 215 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 215 is tensor(0.0536, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.13036144 -0.13343908  0.31353593  1.        ]]. Action = [[-0.17353296  0.4694525  -0.99628395  0.8010819 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 216 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 216 is tensor(0.0585, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.13507357 -0.1136625   0.30335665  1.        ]]. Action = [[-0.76553106  0.8884158   0.32204425  0.18182361]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 217 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 217 is tensor(0.0700, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.15672143 -0.10126789  0.30490193  1.        ]]. Action = [[-0.6730509 -0.5803625 -0.2034486  0.7110684]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 218 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 218 is tensor(0.0568, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.1661911  -0.11294904  0.30379328  1.        ]]. Action = [[ 0.8599615  -0.22678274  0.1855998   0.82808447]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 219 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 219 is tensor(0.0640, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.14986247 -0.11985844  0.30415154  1.        ]]. Action = [[ 0.90438807 -0.34600824 -0.28234518  0.4593612 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 220 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 220 is tensor(0.0637, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.12282282 -0.13292806  0.28571603  1.        ]]. Action = [[ 0.87736845 -0.35552716 -0.836798    0.60254836]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 221 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 221 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.08122037 -0.13647088  0.2742442   1.        ]]. Action = [[0.94066215 0.3633653  0.8925457  0.76029944]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 222 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 222 is tensor(0.0607, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.05986951 -0.12076055  0.29350582  1.        ]]. Action = [[-0.08203262  0.6819285   0.06161141  0.9454621 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 223 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 223 is tensor(0.0627, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.05258505 -0.11255702  0.29252315  1.        ]]. Action = [[ 0.8369702  -0.32570362 -0.26877558  0.50701666]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 224 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 224 is tensor(0.0643, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.03582874 -0.12612428  0.29157776  1.        ]]. Action = [[-0.30236495 -0.83486706  0.42195022  0.6409075 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 225 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 225 is tensor(0.0578, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.04879208 -0.14745122  0.29213873  1.        ]]. Action = [[-0.8594509  -0.19140172 -0.89074105  0.38319445]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 226 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 226 is tensor(0.0520, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 226 of 0
Current timestep = 227. State = [[-0.06378749 -0.15945329  0.27886492  1.        ]]. Action = [[ 0.1024344  -0.33797747 -0.07105839  0.8658862 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 227 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 227 is tensor(0.0618, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 227 of -1
Current timestep = 228. State = [[-0.07076738 -0.15723728  0.2729279   1.        ]]. Action = [[-0.9489722   0.85673714 -0.32750523  0.98301804]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 228 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 228 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.08616444 -0.14480771  0.260578    1.        ]]. Action = [[ 0.6613554  -0.5163196  -0.53887016  0.7324599 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Scene graph at timestep 229 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 229 is tensor(0.0490, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.08616444 -0.14480771  0.260578    1.        ]]. Action = [[ 0.98613024  0.9899845  -0.59208673  0.68265176]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Scene graph at timestep 230 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 230 is tensor(0.0424, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.08578375 -0.15523677  0.26704285  1.        ]]. Action = [[ 0.17037451 -0.79447323  0.53118896  0.5412984 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 231 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 231 is tensor(0.0549, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.25073436  0.00244996  0.23268297  1.        ]]. Action = [[ 0.34699464  0.78600264  0.75066924 -0.08237219]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 233. State = [[-0.24903947  0.00400069  0.23329586  1.        ]]. Action = [[0.27202582 0.1344465  0.8558186  0.8930092 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 234. State = [[-0.24789006  0.00473244  0.23779789  1.        ]]. Action = [[-0.43042678 -0.19392413  0.47763622  0.85262406]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 235. State = [[-0.24237554 -0.00383001  0.24065733  1.        ]]. Action = [[ 0.47219598 -0.6140896   0.06036663  0.7007034 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 236. State = [[-0.22435962 -0.01191047  0.2567398   1.        ]]. Action = [[0.7383487  0.17107713 0.97274864 0.98246217]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 237. State = [[-1.9567463e-01  8.2577276e-04  2.8337672e-01  1.0000000e+00]]. Action = [[0.8041965 0.8270005 0.4048879 0.8935374]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 238. State = [[-0.17052831  0.02208201  0.305198    1.        ]]. Action = [[0.8216816  0.44732904 0.19548225 0.18257272]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 239. State = [[-0.15748364  0.0363609   0.303695    1.        ]]. Action = [[-0.72435486  0.11330593 -0.7078203   0.51266193]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 240. State = [[-0.15298167  0.04416813  0.29438862  1.        ]]. Action = [[0.8046335  0.28525364 0.02244246 0.26182342]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 241. State = [[-0.13529755  0.04180045  0.29047284  1.        ]]. Action = [[ 0.44788432 -0.56907755 -0.00898707  0.15169144]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 242. State = [[-0.11413263  0.03264308  0.28839776  1.        ]]. Action = [[ 0.9069798  -0.12561774 -0.12136585  0.58908284]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 243. State = [[-0.08914862  0.01494743  0.29531857  1.        ]]. Action = [[ 0.15174937 -0.8959816   0.79922485  0.18408453]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 244. State = [[-0.08284191  0.00880937  0.32413036  1.        ]]. Action = [[-0.3695929   0.67895365  0.9732621   0.915313  ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 245. State = [[-0.07599644  0.0027711   0.34935284  1.        ]]. Action = [[ 0.7704463  -0.86005116  0.06692266  0.83551717]]. Reward = [0.]
Curr episode timestep = 12
Above hoop
Current timestep = 246. State = [[-0.0616658  -0.01627883  0.3592254   1.        ]]. Action = [[ 0.4454447  -0.44271022  0.20625448  0.23507917]]. Reward = [0.]
Curr episode timestep = 13
Above hoop
Current timestep = 247. State = [[-0.04902881 -0.01604505  0.3746243   1.        ]]. Action = [[0.1420002  0.64358044 0.69684434 0.5407827 ]]. Reward = [0.]
Curr episode timestep = 14
Above hoop
Scene graph at timestep 247 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 247 is tensor(0.0553, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.03637966  0.00319145  0.3927177   1.        ]]. Action = [[ 0.926566    0.7122886  -0.5481945   0.01169634]]. Reward = [0.]
Curr episode timestep = 15
Above hoop
Scene graph at timestep 248 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 248 is tensor(0.0524, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 248 of 0
Current timestep = 249. State = [[-0.00128735  0.03189693  0.37440607  1.        ]]. Action = [[ 0.58797026  0.978459   -0.04251212  0.74114   ]]. Reward = [0.]
Curr episode timestep = 16
Above hoop
Scene graph at timestep 249 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 249 is tensor(0.0500, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 249 of 0
Current timestep = 250. State = [[0.01850318 0.04321386 0.3711538  1.        ]]. Action = [[-0.7353651  -0.9923232  -0.4237761   0.84653735]]. Reward = [0.]
Curr episode timestep = 17
Above hoop
Scene graph at timestep 250 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 250 is tensor(0.0439, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 250 of 0
Current timestep = 251. State = [[0.01269852 0.0109449  0.36372414 1.        ]]. Action = [[ 0.2917273 -0.8894204 -0.4234621  0.6885059]]. Reward = [0.]
Curr episode timestep = 18
Above hoop
Scene graph at timestep 251 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 251 is tensor(0.0469, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 251 of 0
Current timestep = 252. State = [[ 0.01768368 -0.00931323  0.35362265  1.        ]]. Action = [[ 0.8936039  -0.0844571  -0.18170768  0.8371352 ]]. Reward = [0.]
Curr episode timestep = 19
Above hoop
Scene graph at timestep 252 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 252 is tensor(0.0474, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 252 of 0
Current timestep = 253. State = [[ 0.03341672 -0.02137025  0.33031675  1.        ]]. Action = [[-0.8308001  -0.53554237 -0.72320044  0.76235366]]. Reward = [0.]
Curr episode timestep = 20
Above hoop
Scene graph at timestep 253 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 253 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 253 of 0
Current timestep = 254. State = [[ 0.0192692  -0.02851536  0.30954427  1.        ]]. Action = [[-0.8034004   0.32206154 -0.62959313  0.09558785]]. Reward = [0.]
Curr episode timestep = 21
Above hoop
Scene graph at timestep 254 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 254 is tensor(0.0511, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 254 of 0
Current timestep = 255. State = [[ 0.00487806 -0.02090921  0.28940982  1.        ]]. Action = [[0.07797551 0.22933722 0.26564503 0.9192486 ]]. Reward = [0.]
Curr episode timestep = 22
Above hoop
Scene graph at timestep 255 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 255 is tensor(0.0436, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 255 of 0
Current timestep = 256. State = [[ 0.00539599 -0.00611129  0.2816137   1.        ]]. Action = [[ 0.5882845   0.64470387 -0.85477346  0.90161586]]. Reward = [0.]
Curr episode timestep = 23
Above hoop
Scene graph at timestep 256 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 256 is tensor(0.0349, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 256 of 0
Current timestep = 257. State = [[0.0072917  0.00519995 0.26317877 1.        ]]. Action = [[-0.42248833 -0.48759294 -0.8687405   0.6923479 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Above hoop
Scene graph at timestep 257 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 257 is tensor(0.0369, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 257 of 0
Current timestep = 258. State = [[ 9.7759077e-03 -9.5774216e-04  2.7310637e-01  1.0000000e+00]]. Action = [[-0.08702612 -0.48289037  0.86043     0.6727996 ]]. Reward = [0.]
Curr episode timestep = 25
Above hoop
Scene graph at timestep 258 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 258 is tensor(0.0394, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 258 of 0
Current timestep = 259. State = [[ 0.0170457  -0.02182439  0.2840909   1.        ]]. Action = [[ 0.943398   -0.8730677  -0.17563671  0.3093722 ]]. Reward = [0.]
Curr episode timestep = 26
Above hoop
Scene graph at timestep 259 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 259 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 259 of 0
Current timestep = 260. State = [[ 0.03164868 -0.04204353  0.2800795   1.        ]]. Action = [[ 0.65013456 -0.19642621 -0.8698629   0.5408447 ]]. Reward = [0.]
Curr episode timestep = 27
Above hoop
Scene graph at timestep 260 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 260 is tensor(0.0418, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 260 of 0
Current timestep = 261. State = [[ 0.05677741 -0.0474559   0.24997298  1.        ]]. Action = [[ 0.96871686 -0.75507164 -0.5977995   0.9206748 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Action ignored: No entry zone
Above hoop
Scene graph at timestep 261 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 261 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[ 0.05982425 -0.05757323  0.25087413  1.        ]]. Action = [[ 0.49578035 -0.7021278   0.01942158  0.01332355]]. Reward = [0.]
Curr episode timestep = 29
Above hoop
Scene graph at timestep 262 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 262 is tensor(0.0501, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[ 0.07486773 -0.07485733  0.2501999   1.        ]]. Action = [[ 0.27625442 -0.5319657  -0.38738358  0.91780376]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: No entry zone
Scene graph at timestep 263 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 263 is tensor(0.0461, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 263 of -1
Current timestep = 264. State = [[ 0.077263   -0.08964387  0.2607632   1.        ]]. Action = [[ 0.19831848 -0.9624664   0.74115     0.3394003 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 264 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 264 is tensor(0.0478, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[ 0.07935983 -0.11031061  0.2757757   1.        ]]. Action = [[ 0.51367986 -0.13465619 -0.51810163  0.86552   ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Scene graph at timestep 265 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 265 is tensor(0.0479, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[ 0.0794065  -0.11037287  0.27582538  1.        ]]. Action = [[ 0.93943167  0.8047173  -0.20116973  0.9550456 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 266 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 266 is tensor(0.0380, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[ 0.07933047 -0.1103396   0.2758213   1.        ]]. Action = [[ 0.8017497  -0.46200663  0.74417937  0.47027695]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Scene graph at timestep 267 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 267 is tensor(0.0470, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[ 0.07983634 -0.12418342  0.2897751   1.        ]]. Action = [[-0.0409956  -0.79586256  0.9659101   0.7048675 ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 268 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 268 is tensor(0.0481, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[ 0.08507233 -0.1320796   0.32347402  1.        ]]. Action = [[0.27936637 0.69422364 0.5884032  0.6623622 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 269 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 269 is tensor(0.0499, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[ 0.09095053 -0.12946148  0.3503858   1.        ]]. Action = [[-0.5038     -0.4778207   0.83331645  0.96753955]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 270 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 270 is tensor(0.0518, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 270 of -1
Current timestep = 271. State = [[ 0.08110677 -0.15453179  0.36770052  1.        ]]. Action = [[-0.3183419  -0.8416984  -0.33603477  0.6808014 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 271 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 271 is tensor(0.0526, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[ 0.07808496 -0.15757649  0.3773453   1.        ]]. Action = [[-0.42201793  0.86796606  0.726187    0.8131733 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 272 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 272 is tensor(0.0477, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 272 of 0
Current timestep = 273. State = [[ 0.07243332 -0.14221391  0.39382526  1.        ]]. Action = [[-0.5254636  -0.9406954   0.55565083  0.9295156 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 273 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 273 is tensor(0.0483, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[ 0.07314342 -0.14615783  0.39508072  1.        ]]. Action = [[ 0.36161065 -0.35275942  0.14797175  0.418689  ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 274 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 274 is tensor(0.0574, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[ 0.07518367 -0.14207928  0.39348647  1.        ]]. Action = [[ 0.24215794  0.52142024 -0.3155046   0.91172767]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 275 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 275 is tensor(0.0462, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 275 of 0
Current timestep = 276. State = [[ 0.07731545 -0.13657202  0.38689384  1.        ]]. Action = [[ 0.715281   -0.16589165  0.6660361   0.9141871 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 276 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 276 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[ 0.07731545 -0.13657202  0.38689384  1.        ]]. Action = [[ 0.53281534 -0.61158884  0.7333603   0.4314344 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 277 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 277 is tensor(0.0484, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[-0.25067893  0.00244968  0.23268296  1.        ]]. Action = [[-0.9581054  -0.5861455  -0.07909834 -0.01243228]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 279. State = [[-0.25171328  0.00424357  0.20897871  1.        ]]. Action = [[ 0.26156032  0.18177998 -0.98382246  0.4537208 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 280. State = [[-0.2427252  -0.0067509   0.18712223  1.        ]]. Action = [[ 0.5558262  -0.9596749   0.8602052   0.22841418]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 281. State = [[-0.23174796 -0.02944988  0.20315988  1.        ]]. Action = [[ 0.08132911 -0.46024477  0.91361666  0.98448086]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 282. State = [[-0.22759789 -0.05610527  0.22728115  1.        ]]. Action = [[-0.07966828 -0.9260794   0.4088707   0.939708  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 283. State = [[-0.22854137 -0.07520472  0.23749378  1.        ]]. Action = [[-0.864069   -0.25019282  0.39904916  0.6514144 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 284. State = [[-0.21875678 -0.0768863   0.24829587  1.        ]]. Action = [[0.71064377 0.21282268 0.64695024 0.6921017 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 285. State = [[-0.19815181 -0.0735971   0.2564549   1.        ]]. Action = [[ 0.6650348  0.0742203 -0.9909584  0.7456317]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 286. State = [[-0.17533906 -0.07570266  0.23999736  1.        ]]. Action = [[ 0.64732885 -0.2789147  -0.3838297   0.86351347]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 287. State = [[-0.16284807 -0.06996099  0.23385648  1.        ]]. Action = [[-0.68623513  0.5501547   0.402956    0.8353331 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 288. State = [[-0.15673004 -0.05000789  0.24300507  1.        ]]. Action = [[0.8838215 0.7903452 0.6296735 0.6668819]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 289. State = [[-0.14435862 -0.03356417  0.2567973   1.        ]]. Action = [[-0.27344942  0.59761333 -0.7559147   0.5714731 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 290. State = [[-0.13768025 -0.04111575  0.26701024  1.        ]]. Action = [[ 0.20516014 -0.63270354  0.5385138   0.1955092 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 291. State = [[-0.1323205  -0.05090217  0.2892656   1.        ]]. Action = [[-0.1256668   0.01747096  0.77532756  0.7087134 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 292. State = [[-0.12408987 -0.04117793  0.3154012   1.        ]]. Action = [[0.67618275 0.71348786 0.56604624 0.5579362 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 293. State = [[-0.11106498 -0.03304144  0.32880256  1.        ]]. Action = [[ 0.13157332 -0.3266623  -0.4099502   0.86529493]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 294. State = [[-0.10771651 -0.05224395  0.3351187   1.        ]]. Action = [[-0.14454526 -0.98528504  0.7311634   0.58695614]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 295. State = [[-0.11273456 -0.07372238  0.3511397   1.        ]]. Action = [[-0.4833691  -0.1230377   0.31722462  0.781095  ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 296. State = [[-0.12562984 -0.09326671  0.35608655  1.        ]]. Action = [[-0.6551078  -0.7107169  -0.506437    0.13628066]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 297. State = [[-0.13573822 -0.09286547  0.34512028  1.        ]]. Action = [[ 0.13753498  0.894675   -0.80708194  0.6369562 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 298. State = [[-0.13183764 -0.09110464  0.34432495  1.        ]]. Action = [[ 0.5198457  -0.80416226  0.97926104  0.8348267 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 299. State = [[-0.1235256  -0.11364127  0.35404593  1.        ]]. Action = [[ 0.7067157  -0.79077965 -0.03386372  0.969512  ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 300. State = [[-0.10111954 -0.13219866  0.34696153  1.        ]]. Action = [[ 0.99575543 -0.19666034 -0.702152    0.58047295]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 301. State = [[-0.08212874 -0.15334971  0.3337984   1.        ]]. Action = [[-0.6175976  -0.796326    0.01880825  0.7506319 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 302. State = [[-0.09173105 -0.18502733  0.34743524  1.        ]]. Action = [[-0.5970182  -0.8717001   0.83451486  0.7180872 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 303. State = [[-0.09022926 -0.19461644  0.36988208  1.        ]]. Action = [[0.87988067 0.6761849  0.7704135  0.5276153 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 304. State = [[-0.07083852 -0.1791729   0.3782029   1.        ]]. Action = [[ 0.9362626   0.39224267 -0.70458287  0.36216664]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 305. State = [[-0.04839827 -0.17237088  0.36722782  1.        ]]. Action = [[-0.11488658  0.07361424  0.82145894  0.03606856]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 305 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 305 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 305 of -1
Current timestep = 306. State = [[-0.04357542 -0.16659719  0.35565302  1.        ]]. Action = [[ 0.17086244  0.20009351 -0.8629726   0.0837816 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 306 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 306 is tensor(0.0290, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 306 of 1
Current timestep = 307. State = [[-0.02855797 -0.16454546  0.34518367  1.        ]]. Action = [[ 0.5446396  -0.21685398  0.50777996  0.89402246]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 307 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 307 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-0.2508495   0.00256958  0.23266274  1.        ]]. Action = [[-0.71277285  0.5721214   0.96155477 -0.4034174 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 309. State = [[-2.4929409e-01 -3.0577875e-04  2.3016827e-01  1.0000000e+00]]. Action = [[ 0.31419945 -0.15030396  0.41230822  0.76845384]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 310. State = [[-0.24786495 -0.00443574  0.23121388  1.        ]]. Action = [[-0.8928137  -0.53792596  0.15396237  0.03668714]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 311. State = [[-0.24808066 -0.0051816   0.23118545  1.        ]]. Action = [[-0.8701686  -0.36552763  0.2769413   0.5175152 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 312. State = [[-0.23921485 -0.01518437  0.22549307  1.        ]]. Action = [[ 0.8548187  -0.6615633  -0.8110526   0.84655285]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 313. State = [[-0.2245588  -0.03615347  0.2166208   1.        ]]. Action = [[-0.45245016 -0.59737337  0.83722484  0.7511076 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 314. State = [[-0.22044192 -0.04845433  0.2184687   1.        ]]. Action = [[ 0.7660681   0.1894939  -0.65646434  0.85576177]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 315. State = [[-0.20121923 -0.04941539  0.21285631  1.        ]]. Action = [[ 0.5375824  -0.12590754  0.00726759  0.4814638 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 316. State = [[-0.18957292 -0.05038349  0.21072505  1.        ]]. Action = [[0.7268622  0.56870174 0.31500447 0.8218031 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 317. State = [[-0.18869969 -0.05043975  0.21116583  1.        ]]. Action = [[ 0.58775735 -0.39571333  0.05416298  0.8312975 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: No entry zone
Current timestep = 318. State = [[-0.18837875 -0.05045426  0.21131302  1.        ]]. Action = [[ 0.42545974  0.6539475  -0.8857892   0.44706106]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 319. State = [[-0.18191458 -0.03798689  0.21692482  1.        ]]. Action = [[0.35300732 0.880239   0.618474   0.81050825]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 320. State = [[-0.18037604 -0.01165879  0.2332313   1.        ]]. Action = [[-0.7439312  0.6912224  0.6001458  0.9625747]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 321. State = [[-0.19070973  0.00498114  0.24586016  1.        ]]. Action = [[ 0.5164758   0.37960112 -0.27056408  0.7681148 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 322. State = [[-0.1928838  0.0071318  0.2467875  1.       ]]. Action = [[ 0.5525849  -0.07194012 -0.9691008  -0.05693471]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 323. State = [[-0.19280036  0.00737942  0.24736664  1.        ]]. Action = [[ 0.8282285  -0.61359024 -0.907032    0.8870902 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 324. State = [[-0.18675672 -0.00696674  0.2584124   1.        ]]. Action = [[ 0.3694805  -0.95119613  0.77465105  0.935725  ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 325. State = [[-0.17355828 -0.03010898  0.27714333  1.        ]]. Action = [[ 0.6337372  -0.43157077 -0.19058782  0.61084795]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 326. State = [[-0.16568537 -0.0535676   0.27012432  1.        ]]. Action = [[-0.18229222 -0.8096658  -0.9493493   0.65745616]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 327. State = [[-0.16074497 -0.06089032  0.26143253  1.        ]]. Action = [[0.13775659 0.56540704 0.7123592  0.8593297 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 328. State = [[-0.1617343  -0.05472262  0.25977725  1.        ]]. Action = [[-0.6224451   0.1506573  -0.75155383  0.59871435]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 329. State = [[-0.16604063 -0.067141    0.25682017  1.        ]]. Action = [[ 0.2769009  -0.918072    0.08414805  0.3449651 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 330. State = [[-0.16705544 -0.07922516  0.25802892  1.        ]]. Action = [[ 0.73695254 -0.40431923 -0.606779   -0.26531905]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: No entry zone
Current timestep = 331. State = [[-0.17626844 -0.07374948  0.25506747  1.        ]]. Action = [[-0.94810915  0.51681376  0.02121973  0.8784001 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 332. State = [[-0.18838578 -0.06780208  0.24991201  1.        ]]. Action = [[ 0.66169107 -0.4640069  -0.45153135  0.5060842 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 333. State = [[-0.19639163 -0.05603783  0.24768603  1.        ]]. Action = [[-0.5457405   0.6625291  -0.05772281  0.742234  ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 334. State = [[-0.20185556 -0.04879056  0.24901307  1.        ]]. Action = [[ 0.7050127  -0.40400386 -0.04759359  0.86000276]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 335. State = [[-0.19509102 -0.0579714   0.24114805  1.        ]]. Action = [[ 0.2586733  -0.33149898 -0.92490107  0.8153682 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 336. State = [[-0.19304813 -0.05302461  0.2234142   1.        ]]. Action = [[-0.44576168  0.74227786  0.35997772  0.96735716]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 337. State = [[-0.19615386 -0.04328019  0.22305915  1.        ]]. Action = [[ 0.7164787 -0.555263  -0.0873788  0.5843761]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 338. State = [[-0.2509323   0.00244033  0.2325138   1.        ]]. Action = [[ 0.64447117  0.8413253   0.78095984 -0.05379361]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 339. State = [[-0.2567892  -0.00860958  0.22940303  1.        ]]. Action = [[-0.30797958 -0.68351555 -0.45991635  0.61336315]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 340. State = [[-0.26161996 -0.02122318  0.2276336   1.        ]]. Action = [[-0.67014295 -0.83817285  0.65483236  0.6344669 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 341. State = [[-0.25708285 -0.0133569   0.23104043  1.        ]]. Action = [[0.62184787 0.7349155  0.67949367 0.2918136 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 342. State = [[-0.25499496 -0.00157745  0.2339796   1.        ]]. Action = [[-0.6682209  -0.6489541   0.7483934  -0.49646157]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 343. State = [[-0.24227685  0.01463626  0.24115966  1.        ]]. Action = [[0.9694629 0.9109912 0.2899245 0.8933569]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 344. State = [[-0.22106722  0.04410184  0.25849605  1.        ]]. Action = [[0.3646599  0.7518599  0.318753   0.62330675]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 345. State = [[-0.2004887   0.04955677  0.27204585  1.        ]]. Action = [[ 0.7211509 -0.7533728  0.3313887  0.6098914]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 346. State = [[-0.2507299   0.00260154  0.23255785  1.        ]]. Action = [[ 0.36882317 -0.20276928 -0.18095744 -0.01859933]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 347. State = [[-0.24872772 -0.0128217   0.22580414  1.        ]]. Action = [[ 0.47957158 -0.9028304  -0.657994    0.6493193 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 348. State = [[-0.24726991 -0.0278717   0.21362364  1.        ]]. Action = [[-0.9232602  -0.01798481  0.6173961   0.06050205]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 349. State = [[-0.23777924 -0.02705374  0.21728222  1.        ]]. Action = [[0.7316892  0.39844513 0.22640908 0.8435626 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 350. State = [[-0.22311409 -0.01217074  0.22496004  1.        ]]. Action = [[-0.02494067  0.7162385   0.8291192   0.14220536]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 351. State = [[-0.22250696  0.01355205  0.23388012  1.        ]]. Action = [[-0.45535684  0.5901537  -0.33946145  0.565207  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 352. State = [[-0.23658273  0.0137389   0.22479907  1.        ]]. Action = [[-0.64621073 -0.92591995 -0.8176944   0.67917204]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 353. State = [[-0.24775723  0.01342013  0.21801446  1.        ]]. Action = [[0.15796375 0.8164457  0.5448532  0.7317798 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 354. State = [[-0.2484099   0.01537322  0.22049883  1.        ]]. Action = [[-0.00317472 -0.5853854  -0.01041502  0.49732947]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 355. State = [[-0.23807125 -0.00376498  0.22626162  1.        ]]. Action = [[ 0.9145968  -0.72544223  0.17194283  0.9282352 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 356. State = [[-0.2205913  -0.02911681  0.22773446  1.        ]]. Action = [[ 0.43036914 -0.77656776 -0.4353956   0.89382577]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 357. State = [[-0.2500174   0.00250587  0.23259206  1.        ]]. Action = [[-0.627193   -0.6012109  -0.05405521 -0.09054673]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 358. State = [[-0.24369512  0.01138018  0.23104654  1.        ]]. Action = [[ 0.93398833  0.92331254 -0.33774793  0.18731356]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 359. State = [[-0.23061189  0.01896229  0.22141954  1.        ]]. Action = [[ 0.06692851 -0.431306   -0.34726965  0.24668908]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 360. State = [[-0.21899015  0.01692415  0.20848139  1.        ]]. Action = [[ 0.81740236  0.05713618 -0.6697703   0.80147576]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 361. State = [[-0.19212839  0.00427284  0.20072317  1.        ]]. Action = [[ 0.40461743 -0.8499532   0.94546914  0.81822634]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 362. State = [[-0.17989968 -0.01202283  0.21010485  1.        ]]. Action = [[ 0.8754139  -0.05900222  0.3442701   0.5256152 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 363. State = [[-0.2508392   0.00261666  0.23262426  1.        ]]. Action = [[-0.5614131   0.7653189   0.8498386  -0.02470618]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 364. State = [[-0.2449348  -0.01077172  0.22765307  1.        ]]. Action = [[ 0.8223275  -0.79987997 -0.6728438   0.7480309 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 365. State = [[-0.2284466 -0.02049    0.2274283  1.       ]]. Action = [[0.1328963  0.25242138 0.8712864  0.9460149 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 366. State = [[-0.2162509  -0.01825158  0.23624296  1.        ]]. Action = [[0.70425105 0.24740624 0.01164269 0.63487935]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 367. State = [[-0.19923651 -0.01783078  0.23901351  1.        ]]. Action = [[ 0.34353876 -0.25232196 -0.38930327  0.7514055 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 368. State = [[-0.17760552 -0.02634044  0.24620132  1.        ]]. Action = [[ 0.8940848  -0.49275094  0.97531176  0.8475044 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 369. State = [[-0.14771257 -0.04523067  0.26334733  1.        ]]. Action = [[ 0.75406194 -0.6362968  -0.22336137  0.79693604]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 370. State = [[-0.1206672  -0.07161999  0.25822896  1.        ]]. Action = [[ 0.78803635 -0.8890999  -0.52607733  0.75008655]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 371. State = [[-0.10607733 -0.09357388  0.24625947  1.        ]]. Action = [[-0.92984    -0.04287338 -0.01566035  0.76240945]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 372. State = [[-0.11757486 -0.11062623  0.251248    1.        ]]. Action = [[-0.617687  -0.5244647  0.5383165  0.5708902]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 373. State = [[-0.12021808 -0.11219972  0.2637071   1.        ]]. Action = [[0.81802464 0.60961604 0.42591786 0.6536701 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 374. State = [[-0.11974338 -0.09270337  0.2827368   1.        ]]. Action = [[-0.6206421   0.5750363   0.70602036  0.7166586 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 375. State = [[-0.11967123 -0.0870073   0.30360478  1.        ]]. Action = [[ 0.7327471  -0.48068178  0.12258637  0.91908514]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 376. State = [[-0.10760321 -0.07999019  0.3232422   1.        ]]. Action = [[0.39447677 0.7670131  0.9807774  0.4287361 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 377. State = [[-0.09575295 -0.06193719  0.34675863  1.        ]]. Action = [[-0.06332505  0.36829472  0.11445212  0.34039116]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 378. State = [[-0.09992347 -0.06648368  0.36706632  1.        ]]. Action = [[-0.6896416  -0.7611124   0.95241404  0.91005325]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 379. State = [[-0.11804534 -0.07290894  0.3780226   1.        ]]. Action = [[-0.6185393   0.2666781  -0.8863073   0.86272454]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 380. State = [[-0.12632088 -0.05935739  0.3793876   1.        ]]. Action = [[0.11279225 0.68423486 0.6764873  0.93768907]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 381. State = [[-0.13905546 -0.05517454  0.37844518  1.        ]]. Action = [[-0.7797436  -0.5191388  -0.68032223  0.25251162]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 382. State = [[-0.15407117 -0.06385031  0.36321265  1.        ]]. Action = [[-0.13562965 -0.15981829 -0.8043603   0.460742  ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 383. State = [[-0.16162083 -0.06000508  0.3552963   1.        ]]. Action = [[0.22520947 0.48066878 0.8293381  0.8672359 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 384. State = [[-0.16777778 -0.03929726  0.35229942  1.        ]]. Action = [[-0.59278524  0.8547063  -0.80792326  0.81737614]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 385. State = [[-0.16867489 -0.00965098  0.35205626  1.        ]]. Action = [[0.97203195 0.79041445 0.56338906 0.8840561 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 386. State = [[-0.16574688  0.00874809  0.35769886  1.        ]]. Action = [[-0.8660726  0.0101012  0.1914177  0.6770985]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 387. State = [[-0.18021241  0.02116528  0.36453173  1.        ]]. Action = [[-0.6440388   0.37817848  0.20307302  0.60281825]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 388. State = [[-0.19694103  0.02359116  0.38163194  1.        ]]. Action = [[-0.16923153 -0.33984387  0.7045629   0.79310274]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 389. State = [[-0.20026818  0.01506058  0.39291078  1.        ]]. Action = [[ 0.84466505 -0.27846515 -0.12311476  0.6554235 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 390. State = [[-0.19473527  0.01165529  0.3936551   1.        ]]. Action = [[-0.79353195 -0.6022009   0.96039367  0.7638984 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 391. State = [[-0.25096008  0.0025427   0.23261172  1.        ]]. Action = [[-0.42910707  0.63759744  0.2558272  -0.0302223 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 392. State = [[-0.24449976  0.00872987  0.21446037  1.        ]]. Action = [[ 0.950734    0.49539447 -0.94804156  0.5199282 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 393. State = [[-0.22795017  0.01483382  0.18532382  1.        ]]. Action = [[-0.8875129   0.7804781  -0.7674729   0.68472445]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 394. State = [[-0.23404282  0.02893899  0.1749891   1.        ]]. Action = [[-0.65322495  0.82362247 -0.5759048   0.7108809 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 395. State = [[-0.23830566  0.03578588  0.16072063  1.        ]]. Action = [[ 0.42128468 -0.73401606 -0.04990447  0.5044339 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 396. State = [[-0.2501279   0.00264486  0.23259705  1.        ]]. Action = [[ 0.6380304  -0.67523754 -0.7302537  -0.06180537]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 397. State = [[-0.25072166  0.00104793  0.2327811   1.        ]]. Action = [[-0.8419702  0.9188013  0.9484861  0.925997 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 398. State = [[-2.5081348e-01  4.4611277e-04  2.3288786e-01  1.0000000e+00]]. Action = [[-0.5134218   0.45565987 -0.8004523   0.9136567 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 399. State = [[-0.24599957 -0.00647573  0.24185267  1.        ]]. Action = [[ 0.4125278  -0.4614514   0.77548647  0.6830368 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 400. State = [[-0.24335623 -0.02948279  0.25379214  1.        ]]. Action = [[-0.11658394 -0.9457154  -0.21499199  0.7809315 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 401. State = [[-0.24689572 -0.04845746  0.25521308  1.        ]]. Action = [[-5.5751735e-01  7.2170746e-01 -7.0458651e-04  8.8195550e-01]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Current timestep = 402. State = [[-0.2476642  -0.05199728  0.25550538  1.        ]]. Action = [[-0.55578595  0.2388165   0.8861325   0.8606076 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 403. State = [[-0.2383756  -0.0382015   0.25749087  1.        ]]. Action = [[ 0.798046    0.9307573  -0.04384041  0.70444727]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 404. State = [[-0.22781351 -0.02699361  0.25093025  1.        ]]. Action = [[ 0.0186069  -0.16742796 -0.6199263   0.34316623]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 405. State = [[-0.2106533  -0.02098514  0.23621644  1.        ]]. Action = [[ 0.90270376  0.37911415 -0.4473933   0.65794086]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 406. State = [[-0.19132057 -0.01986094  0.22881833  1.        ]]. Action = [[-0.26177812 -0.3718536   0.48237598  0.72793615]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 407. State = [[-0.19366726 -0.03304239  0.22939576  1.        ]]. Action = [[-0.49128562 -0.56439036 -0.31709933  0.811455  ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 408. State = [[-0.19799668 -0.05786697  0.23091318  1.        ]]. Action = [[-0.12956369 -0.96451825  0.08493841  0.57547   ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 409. State = [[-0.202212   -0.07790526  0.23385581  1.        ]]. Action = [[ 0.84918594 -0.40523112 -0.8413775   0.18054438]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: No entry zone
Current timestep = 410. State = [[-0.21626627 -0.07655638  0.22641286  1.        ]]. Action = [[-0.9172045   0.41730475 -0.5790887   0.93944526]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 411. State = [[-0.2396863  -0.06262045  0.21887597  1.        ]]. Action = [[-0.36725777  0.6747227   0.6024823   0.87577343]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 412. State = [[-0.24537693 -0.05495714  0.22038873  1.        ]]. Action = [[ 0.5699966 -0.5887274 -0.5564263  0.7245363]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 413. State = [[-0.2367939  -0.07306199  0.2193322   1.        ]]. Action = [[ 0.35666704 -0.8645878   0.34792817  0.43600297]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 414. State = [[-0.22722785 -0.10309489  0.23165667  1.        ]]. Action = [[ 0.33928442 -0.76859564  0.7393768   0.9567914 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 415. State = [[-0.2130285  -0.12114037  0.23472379  1.        ]]. Action = [[ 0.7799481   0.00804186 -0.96211004  0.89903605]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 416. State = [[-0.19990976 -0.12434598  0.21770586  1.        ]]. Action = [[-0.5086369   0.18594861 -0.13010436  0.6758511 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 417. State = [[-0.20147422 -0.12293597  0.21085474  1.        ]]. Action = [[ 0.10063064  0.14924991 -0.40638983  0.28943574]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 418. State = [[-0.25022513  0.00256045  0.2326081   1.        ]]. Action = [[-0.57050896 -0.8474743  -0.23614848 -0.2315138 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 419. State = [[-0.25059235 -0.00191263  0.23282227  1.        ]]. Action = [[-0.60921156 -0.14563644 -0.0316543   0.22122502]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 420. State = [[-0.24593088  0.00177111  0.2339804   1.        ]]. Action = [[ 0.60360575  0.44384098 -0.16119188  0.5619569 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 421. State = [[-0.24260196  0.00981274  0.23055224  1.        ]]. Action = [[-0.22521794  0.15004659 -0.33788157  0.61012053]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 422. State = [[-0.23532987  0.00742437  0.22786918  1.        ]]. Action = [[ 0.6971444  -0.4997316   0.17456305  0.862833  ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 423. State = [[-2.2192147e-01  6.3421711e-04  2.2930893e-01  1.0000000e+00]]. Action = [[ 0.07770419 -0.1192351   0.23745596  0.6150243 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 424. State = [[-0.22240907  0.00988148  0.24029101  1.        ]]. Action = [[-0.44795352  0.9298284   0.8947797   0.5625355 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 425. State = [[-0.25078005  0.00261741  0.23268239  1.        ]]. Action = [[-0.68033075  0.53430676 -0.15134293 -0.26320374]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 426. State = [[-0.2411169   0.0143427   0.24001731  1.        ]]. Action = [[0.77802134 0.7476611  0.86822176 0.90404344]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 427. State = [[-0.2365904   0.03999278  0.2539179   1.        ]]. Action = [[-0.49441957  0.7370044   0.2826525   0.08458173]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 428. State = [[-0.24329689  0.05628838  0.2587342   1.        ]]. Action = [[-0.80910534  0.30669987 -0.61624235  0.942569  ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 429. State = [[-0.23337644  0.06674565  0.26886895  1.        ]]. Action = [[0.9020252  0.5433384  0.36906052 0.42449582]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 430. State = [[-0.22253361  0.07026257  0.2756563   1.        ]]. Action = [[-0.5073399  -0.49895144 -0.7352826   0.7943076 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 431. State = [[-0.215181    0.04944999  0.26719978  1.        ]]. Action = [[ 0.91573906 -0.95041364 -0.01169288  0.78364336]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 432. State = [[-0.2003564   0.01945828  0.25260982  1.        ]]. Action = [[ 0.24778008 -0.74331856 -0.8871981   0.711432  ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 433. State = [[-1.9122736e-01  3.6307354e-04  2.3941274e-01  1.0000000e+00]]. Action = [[-0.20939052 -0.17258018  0.64210474  0.57350624]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 434. State = [[-0.18853907 -0.01622428  0.24386716  1.        ]]. Action = [[ 0.19215417 -0.57848895  0.06346095  0.8461976 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 435. State = [[-0.17758968 -0.01831561  0.2547007   1.        ]]. Action = [[0.6049533  0.6460843  0.46109426 0.26720548]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 436. State = [[-0.15771098 -0.01518834  0.27226636  1.        ]]. Action = [[ 0.58582544 -0.29662758  0.45584488  0.76638937]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 437. State = [[-0.14830638 -0.01704915  0.27518362  1.        ]]. Action = [[-0.2060417  -0.03874129 -0.6671102   0.8894222 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 438. State = [[-0.15479933 -0.01928166  0.27116457  1.        ]]. Action = [[-0.95117694 -0.03181285  0.38846326  0.12848604]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 439. State = [[-0.17411616 -0.02162326  0.26803377  1.        ]]. Action = [[-0.7803709  -0.05323756 -0.45222872  0.7713493 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 440. State = [[-0.19941603 -0.0155272   0.2541187   1.        ]]. Action = [[-0.5828799   0.46293986 -0.7610018   0.6989169 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 441. State = [[-0.21658269  0.00293991  0.24216022  1.        ]]. Action = [[0.13108826 0.72786057 0.31043112 0.65528286]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 442. State = [[-0.21612923  0.02980668  0.24754034  1.        ]]. Action = [[0.40868056 0.7698176  0.4351678  0.9306624 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 443. State = [[-0.20917886  0.05687339  0.24680965  1.        ]]. Action = [[ 0.5581356   0.695611   -0.8272832   0.94832313]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 444. State = [[-0.19136102  0.08735887  0.24229285  1.        ]]. Action = [[0.47650278 0.94583297 0.6328695  0.72279716]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 445. State = [[-0.18692932  0.11032363  0.24060246  1.        ]]. Action = [[-0.10146224  0.21775043 -0.8223482   0.28849387]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 446. State = [[-0.18161695  0.11810258  0.22703806  1.        ]]. Action = [[ 0.79589534  0.94366264 -0.5641391   0.704918  ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: No entry zone
Current timestep = 447. State = [[-0.18601185  0.10934378  0.2234119   1.        ]]. Action = [[-0.6552442  -0.82988685 -0.3441385   0.779147  ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 448. State = [[-0.19314528  0.09811197  0.21820219  1.        ]]. Action = [[ 0.71495533  0.62477636 -0.16875911  0.87304235]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: No entry zone
Current timestep = 449. State = [[-0.1953163   0.09634221  0.21692926  1.        ]]. Action = [[ 0.5317142   0.24718058 -0.2554195   0.875931  ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: No entry zone
Current timestep = 450. State = [[-0.1965685   0.09580694  0.21591064  1.        ]]. Action = [[0.9434389  0.00689077 0.3286389  0.5871035 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: No entry zone
Current timestep = 451. State = [[-0.1873057   0.10283384  0.22331409  1.        ]]. Action = [[0.92924786 0.6560843  0.8231908  0.82023335]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 452. State = [[-0.1752515   0.12228721  0.23929094  1.        ]]. Action = [[-0.11230564  0.65157914  0.53754973  0.31046987]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 453. State = [[-0.17288488  0.13916978  0.25181207  1.        ]]. Action = [[ 0.8524473   0.25863218 -0.21825075  0.28130853]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: No entry zone
Scene graph at timestep 453 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 453 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 453 of -1
Current timestep = 454. State = [[-0.16492012  0.13643372  0.25669485  1.        ]]. Action = [[ 0.5918546  -0.39753377 -0.00859296  0.81892   ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 454 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 454 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 454 of 1
Current timestep = 455. State = [[-0.14913495  0.12551376  0.26884285  1.        ]]. Action = [[ 0.6665964  -0.24798578  0.6210284   0.4554912 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 455 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 455 is tensor(0.0176, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.13533741  0.13303994  0.2884935   1.        ]]. Action = [[-0.34877175  0.8668337   0.28207338  0.6970053 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 456 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 456 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 456 of -1
Current timestep = 457. State = [[-0.13602857  0.14777519  0.28882974  1.        ]]. Action = [[ 0.87743974  0.04003143 -0.69323224  0.852988  ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 457 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 457 is tensor(0.0235, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 457 of 0
Current timestep = 458. State = [[-0.11658806  0.16781934  0.2641339   1.        ]]. Action = [[-0.3278203   0.8994169  -0.79229313  0.780887  ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 458 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 458 is tensor(0.0210, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.10743476  0.18193658  0.24881834  1.        ]]. Action = [[ 0.9234731  -0.20543718  0.23441625  0.867579  ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 459 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 459 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 459 of 0
Current timestep = 460. State = [[-0.0821625   0.19446902  0.26057723  1.        ]]. Action = [[0.5178046  0.8940108  0.6610285  0.65934324]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 460 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 460 is tensor(0.0242, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 460 of 0
Current timestep = 461. State = [[-0.05246818  0.2057904   0.2854166   1.        ]]. Action = [[ 0.8713714  -0.5295616   0.66318154  0.71713805]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 461 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 461 is tensor(0.0165, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 461 of 0
Current timestep = 462. State = [[-0.03927061  0.20310566  0.29401562  1.        ]]. Action = [[-0.966125    0.23138106 -0.72867     0.79077804]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 462 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 462 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.04779922  0.2114313   0.2939316   1.        ]]. Action = [[-0.39816284  0.03291035  0.86581635  0.79321516]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 463 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 463 is tensor(0.0158, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.05746204  0.22435433  0.3207177   1.        ]]. Action = [[0.21791542 0.7292681  0.8450651  0.9321494 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 465. State = [[-0.06997643  0.25015375  0.34655684  1.        ]]. Action = [[-0.9091613  0.8167763  0.8009877  0.8155428]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 466. State = [[-0.08969917  0.25597948  0.38107404  1.        ]]. Action = [[-0.23131698 -0.8535591   0.68538713  0.96333766]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 467. State = [[-0.09269775  0.2336601   0.40721655  1.        ]]. Action = [[ 0.23226106 -0.77547324  0.3159728   0.69805837]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 468. State = [[-0.09119449  0.21942529  0.41913125  1.        ]]. Action = [[0.26556695 0.2311399  0.5229889  0.47101116]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Current timestep = 469. State = [[-0.09142331  0.21504772  0.42102128  1.        ]]. Action = [[-0.2833321   0.1661017   0.95954084  0.5129628 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Current timestep = 470. State = [[-0.09155747  0.2144937   0.42090678  1.        ]]. Action = [[-0.46588147  0.8114135   0.59674287  0.64281225]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Current timestep = 471. State = [[-0.09344586  0.21421228  0.40913978  1.        ]]. Action = [[ 0.1193037   0.17728424 -0.9171574   0.19171286]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 472. State = [[-0.0942988   0.21554999  0.39316955  1.        ]]. Action = [[-0.37317777 -0.10742855 -0.21736419  0.8630446 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 473. State = [[-0.09803192  0.20861344  0.38039836  1.        ]]. Action = [[ 0.21157968 -0.26756096 -0.6458411   0.8120105 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 474. State = [[-0.09779655  0.21539812  0.36823428  1.        ]]. Action = [[0.5204985  0.95520914 0.12205088 0.9242351 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 475. State = [[-0.08734445  0.23852527  0.35029185  1.        ]]. Action = [[ 0.9209819   0.7267072  -0.84497494  0.5589093 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 476. State = [[-0.06201389  0.2456483   0.33469164  1.        ]]. Action = [[ 0.07442653 -0.918495    0.9006648   0.7443502 ]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 477. State = [[-0.05182737  0.23014997  0.34076414  1.        ]]. Action = [[ 0.99315476 -0.17093593 -0.8342604   0.6826786 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 478. State = [[-0.02307662  0.20992433  0.31564698  1.        ]]. Action = [[-0.13345808 -0.97734004 -0.39226323  0.32617342]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 478 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 478 is tensor(0.0276, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 478 of 0
Current timestep = 479. State = [[-0.01247394  0.19268665  0.30269638  1.        ]]. Action = [[ 0.52523685  0.21162128 -0.6745746   0.4943154 ]]. Reward = [0.]
Curr episode timestep = 53
Scene graph at timestep 479 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 479 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 479 of 0
Current timestep = 480. State = [[0.0016603  0.18093732 0.28659165 1.        ]]. Action = [[-0.4551474  -0.96418035  0.68402123  0.6466254 ]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 480 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 480 is tensor(0.0162, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 480 of 1
Current timestep = 481. State = [[-0.00170781  0.15206984  0.2926423   1.        ]]. Action = [[-0.05788416 -0.5537247  -0.7893519   0.83477056]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 481 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 481 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 481 of 1
Current timestep = 482. State = [[-0.00235265  0.13342734  0.27780366  1.        ]]. Action = [[ 0.87659335 -0.4731294  -0.2555954   0.8451158 ]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 482 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 482 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 482 of 1
Current timestep = 483. State = [[0.01358357 0.11928567 0.26406634 1.        ]]. Action = [[ 0.73800135 -0.1736908  -0.90859604  0.74006176]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: No entry zone
Current timestep = 484. State = [[0.01358357 0.11928567 0.26406634 1.        ]]. Action = [[ 0.9625579  -0.6937616  -0.62968564  0.6447866 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Current timestep = 485. State = [[0.02045373 0.11465221 0.27011883 1.        ]]. Action = [[ 0.8294072  -0.31194276  0.4894501   0.32757664]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 486. State = [[0.03825559 0.11190133 0.2748667  1.        ]]. Action = [[-0.19022524  0.4094113  -0.2687763   0.8953123 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 487. State = [[0.04477231 0.13147333 0.26683062 1.        ]]. Action = [[ 0.42532182  0.9167464  -0.47430336  0.77117157]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 487 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 487 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[0.05614178 0.15625359 0.24803221 1.        ]]. Action = [[-0.1946097   0.20645773 -0.10020542  0.20984173]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 488 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 488 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[0.05011574 0.16266052 0.25742182 1.        ]]. Action = [[-0.9275923   0.12811375  0.77479315  0.31004548]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 489 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 489 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[0.04140208 0.17935    0.28793415 1.        ]]. Action = [[0.92081356 0.57604635 0.8427055  0.782146  ]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 490 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 490 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 490 of -1
Current timestep = 491. State = [[0.04805883 0.2007891  0.29966444 1.        ]]. Action = [[ 0.64156675  0.636888   -0.80924433  0.76757836]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 492. State = [[0.07327933 0.2136041  0.28243017 1.        ]]. Action = [[0.7124462  0.1667993  0.4660616  0.71134996]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 492 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 492 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 492 of -1
Current timestep = 493. State = [[0.09732487 0.22039749 0.28874993 1.        ]]. Action = [[ 0.8209704  -0.28457606 -0.11604249  0.9243605 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 493 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 493 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[0.09848909 0.20692001 0.29946855 1.        ]]. Action = [[-0.2511521  -0.97401476  0.78490436  0.4224075 ]]. Reward = [0.]
Curr episode timestep = 68
Scene graph at timestep 494 is [False, False, True, False, False, True, False, True, True, False]
State prediction error at timestep 494 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 494 of 0
Current timestep = 495. State = [[0.10039438 0.19102074 0.3191335  1.        ]]. Action = [[ 0.8068688 -0.657637  -0.9696617  0.6422529]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Current timestep = 496. State = [[0.09818548 0.1852578  0.32503256 1.        ]]. Action = [[-0.55817866 -0.24175763  0.3273859   0.7765069 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 497. State = [[0.08783652 0.19242156 0.34942824 1.        ]]. Action = [[-0.67647344  0.8368056   0.92167616  0.4890561 ]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 498. State = [[0.06978732 0.20891039 0.36739242 1.        ]]. Action = [[ 0.93210864 -0.2993667   0.8349011   0.5662749 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Current timestep = 499. State = [[0.06849788 0.2091974  0.3677172  1.        ]]. Action = [[ 0.4127468  -0.07666022 -0.28376782  0.7809794 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 500. State = [[0.07038245 0.21146397 0.36891314 1.        ]]. Action = [[0.3031838  0.21565485 0.23800254 0.42720687]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 501. State = [[0.07115872 0.21297197 0.37006345 1.        ]]. Action = [[ 0.47730708 -0.8037655   0.93307185  0.63981795]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 502. State = [[0.07264768 0.21099715 0.3805892  1.        ]]. Action = [[ 0.08782911 -0.16461903  0.8516958   0.90814877]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 503. State = [[0.07440659 0.2088335  0.39325348 1.        ]]. Action = [[-0.82409596 -0.11172396  0.7361727   0.5678513 ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 504. State = [[0.06951857 0.2161297  0.39867678 1.        ]]. Action = [[-0.57948726  0.4591303   0.05554378  0.52951026]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 505. State = [[0.06603369 0.22176687 0.40426046 1.        ]]. Action = [[ 0.60889494  0.45433927 -0.65222275 -0.10747445]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 506. State = [[0.06559906 0.22230293 0.4043096  1.        ]]. Action = [[ 0.9688492  -0.84825015 -0.14156419  0.94144416]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 507. State = [[0.06559906 0.22230293 0.4043096  1.        ]]. Action = [[-0.40698695  0.3881936   0.49142265  0.6630442 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 508. State = [[0.06599297 0.22278635 0.39690682 1.        ]]. Action = [[-0.01325989 -0.03078026 -0.7175039   0.65931916]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 509. State = [[0.06011445 0.22932343 0.39088723 1.        ]]. Action = [[-0.7072825   0.21999252  0.28261554  0.7177782 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 510. State = [[0.04699004 0.23540166 0.39025077 1.        ]]. Action = [[0.77851427 0.7074549  0.08447897 0.44880795]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Scene graph at timestep 510 is [False, True, False, False, False, True, False, True, True, False]
State prediction error at timestep 510 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[0.0431895  0.23723903 0.39691854 1.        ]]. Action = [[-0.15913141  0.16279626  0.45881557  0.8076612 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 512. State = [[0.0393481  0.24046934 0.40485364 1.        ]]. Action = [[0.21969414 0.6264169  0.74444354 0.6559305 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Current timestep = 513. State = [[0.0386492  0.24107043 0.407947   1.        ]]. Action = [[-0.6318533  -0.5134967   0.57050884  0.8844093 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Current timestep = 514. State = [[0.03851328 0.24107228 0.4081848  1.        ]]. Action = [[-0.11911541  0.31912947  0.64926183  0.798373  ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 515. State = [[0.03851217 0.24101202 0.40818492 1.        ]]. Action = [[0.6332693  0.71931887 0.2788732  0.63982916]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 516. State = [[0.03848948 0.2410316  0.40836647 1.        ]]. Action = [[ 0.63261735 -0.1988554   0.6225326   0.91639614]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Current timestep = 517. State = [[0.04303322 0.23698963 0.3970836  1.        ]]. Action = [[ 0.9396832 -0.2134912 -0.9837973  0.3734498]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 518. State = [[0.06238694 0.24000816 0.3761197  1.        ]]. Action = [[0.92397356 0.40260315 0.19115448 0.75141907]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 519. State = [[0.07511164 0.24244882 0.3695706  1.        ]]. Action = [[ 0.4407382   0.5233017  -0.66159654  0.9027847 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 520. State = [[0.07756002 0.24452047 0.35875034 1.        ]]. Action = [[ 0.87681997 -0.48516786  0.73274016  0.7875434 ]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 521. State = [[0.07440858 0.2572408  0.3588274  1.        ]]. Action = [[-0.6997969   0.55654144  0.0619725   0.59186983]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 522. State = [[0.06925005 0.25825462 0.36961508 1.        ]]. Action = [[-0.04153734 -0.91827035  0.9492152   0.6158589 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 523. State = [[0.06941041 0.24803786 0.3805696  1.        ]]. Action = [[-0.54455245  0.8611963   0.35569     0.341681  ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 524. State = [[0.06939934 0.2463455  0.38236666 1.        ]]. Action = [[-0.14687544  0.14859891  0.89856434 -0.12148213]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 525. State = [[0.06950194 0.24538659 0.38254815 1.        ]]. Action = [[0.5058422 0.7068405 0.8504101 0.7369454]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Current timestep = 526. State = [[0.07430932 0.23400934 0.37432334 1.        ]]. Action = [[ 0.48499072 -0.76058143 -0.5766751   0.794752  ]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 527. State = [[-0.25242022  0.00234584  0.23209825  1.        ]]. Action = [[-0.40800261  0.1411184  -0.18069202  0.4036827 ]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 528. State = [[-0.2505985  -0.00122575  0.21919493  1.        ]]. Action = [[-0.33805656 -0.17654204  0.096452    0.5433918 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 529. State = [[-0.25165072 -0.00605972  0.21772105  1.        ]]. Action = [[-0.87747246  0.725793    0.45818198  0.68921494]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 530. State = [[-0.24095893 -0.02053509  0.22425048  1.        ]]. Action = [[ 0.9678042  -0.87806314  0.73667645  0.5480554 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 531. State = [[-0.21830401 -0.0329819   0.24302223  1.        ]]. Action = [[0.68679523 0.30182195 0.9301989  0.90555596]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 532. State = [[-0.18567425 -0.04280618  0.27192223  1.        ]]. Action = [[ 0.9112524 -0.7228275  0.6831422  0.8488661]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 533. State = [[-0.15298986 -0.04580558  0.30478382  1.        ]]. Action = [[0.86735284 0.63442373 0.7120955  0.64395857]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 534. State = [[-0.12531182 -0.0302669   0.32215467  1.        ]]. Action = [[ 0.6202338   0.549983   -0.28027904  0.9506588 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 535. State = [[-0.10496001 -0.01194972  0.3171454   1.        ]]. Action = [[ 0.11128497  0.44306946 -0.1411022   0.08317864]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 536. State = [[-0.10047422 -0.01441509  0.31271252  1.        ]]. Action = [[-0.40980828 -0.8476748  -0.32418478  0.6632426 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 537. State = [[-0.10310255 -0.02402501  0.2991067   1.        ]]. Action = [[ 0.06244838  0.04408514 -0.9466367   0.9219024 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 538. State = [[-0.09997821 -0.04120118  0.27351445  1.        ]]. Action = [[ 0.2734145  -0.9405399  -0.24710011  0.6881337 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 539. State = [[-0.09897909 -0.07039208  0.27234954  1.        ]]. Action = [[-0.54067117 -0.731101    0.61439633  0.8138417 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 540. State = [[-0.09795227 -0.09982049  0.2784679   1.        ]]. Action = [[ 0.7089274  -0.7660051  -0.22557896  0.75136614]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 541. State = [[-0.08471896 -0.10854825  0.27367514  1.        ]]. Action = [[ 0.804103    0.4733975  -0.24676502  0.8472216 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 542. State = [[-0.07016826 -0.11033151  0.2645793   1.        ]]. Action = [[-0.47047788 -0.29438597 -0.27310693  0.81671   ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 543. State = [[-0.07816084 -0.10355548  0.26321074  1.        ]]. Action = [[-0.88328624  0.66857266  0.27320814  0.7621932 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 544. State = [[-0.09595629 -0.10592723  0.27134347  1.        ]]. Action = [[-0.64467275 -0.5517885   0.39192533  0.63349724]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 545. State = [[-0.10801218 -0.10627965  0.27354005  1.        ]]. Action = [[ 0.5563965   0.4630803  -0.49643147  0.82209814]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 546. State = [[-0.10207626 -0.08883484  0.27801946  1.        ]]. Action = [[0.34119058 0.7733011  0.8783691  0.7333437 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 547. State = [[-0.08897116 -0.07880929  0.28620005  1.        ]]. Action = [[ 0.9521949  -0.6656092  -0.13116956  0.4529209 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 548. State = [[-0.0760875  -0.08013836  0.28038225  1.        ]]. Action = [[-0.17499971  0.34223127 -0.59691346  0.92716277]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 549. State = [[-0.08089    -0.09380072  0.27309787  1.        ]]. Action = [[-0.844634   -0.8416189   0.11730564  0.8714274 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 550. State = [[-0.07941075 -0.10707279  0.2865839   1.        ]]. Action = [[ 0.74215496 -0.06317842  0.9884263   0.71802247]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 551. State = [[-0.2508689   0.00252636  0.23264177  1.        ]]. Action = [[ 0.8288164   0.80953836  0.88750553 -0.09362864]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 552. State = [[-0.24657407  0.00993448  0.23629713  1.        ]]. Action = [[0.7873006  0.56668806 0.56707025 0.3421191 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 553. State = [[-0.23914666  0.01854079  0.24168013  1.        ]]. Action = [[-0.87023985 -0.21777564  0.9385569   0.73290753]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 554. State = [[-0.24281162  0.02591508  0.2526242   1.        ]]. Action = [[-0.5950149   0.28530514  0.8294964   0.55760264]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 555. State = [[-0.25251254  0.03268059  0.26888445  1.        ]]. Action = [[-0.9622621  -0.88267887  0.23466063  0.94172287]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 556. State = [[-0.25437236  0.02291789  0.27142912  1.        ]]. Action = [[-0.33575845 -0.71911657  0.01619661  0.90157807]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 557. State = [[-0.25893694  0.00974139  0.27471885  1.        ]]. Action = [[-0.30342495 -0.7482522   0.8456687   0.59814   ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 558. State = [[-0.25978073  0.00626127  0.27503982  1.        ]]. Action = [[-0.40120542  0.23330998 -0.57129985  0.6930332 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Current timestep = 559. State = [[-0.26033852  0.0061819   0.2699313   1.        ]]. Action = [[ 0.30175436  0.08993912 -0.6959162   0.61462   ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 560. State = [[-0.24891083 -0.00658234  0.2753804   1.        ]]. Action = [[ 0.7150024  -0.95525444  0.95751977  0.9454452 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 561. State = [[-0.23717353 -0.03456637  0.29034048  1.        ]]. Action = [[-0.04886311 -0.80756944  0.3002317   0.6556139 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 562. State = [[-0.2339376  -0.0535967   0.29992965  1.        ]]. Action = [[-0.76648474 -0.62753063 -0.10040718  0.5044224 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Current timestep = 563. State = [[-0.22685371 -0.04377144  0.30727485  1.        ]]. Action = [[0.52409506 0.85732913 0.46441674 0.61857784]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 564. State = [[-0.21138899 -0.02776631  0.3182581   1.        ]]. Action = [[ 0.5785947   0.2748587  -0.18434757  0.88434696]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 565. State = [[-0.19413744 -0.01312236  0.31760588  1.        ]]. Action = [[ 0.30479777  0.4457786  -0.01776707  0.78041863]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 566. State = [[-0.18590829 -0.01216578  0.3091264   1.        ]]. Action = [[-0.18665755 -0.66023934 -0.720813    0.3510896 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 567. State = [[-0.17477734 -0.0167015   0.299018    1.        ]]. Action = [[0.89475286 0.15528738 0.0485903  0.7931502 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 568. State = [[-0.16425706 -0.02548019  0.30667984  1.        ]]. Action = [[-0.8730966  -0.5446179   0.9585862   0.44229293]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 569. State = [[-0.16700871 -0.02625777  0.33526453  1.        ]]. Action = [[0.5221572  0.6102514  0.91015697 0.76583517]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 570. State = [[-0.1631964  -0.02526156  0.35175225  1.        ]]. Action = [[-0.33548582 -0.52993655 -0.4159019   0.85257924]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 571. State = [[-0.16855465 -0.04508631  0.36197332  1.        ]]. Action = [[-0.10073322 -0.717887    0.7588681   0.12663364]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 572. State = [[-0.1637698  -0.06118536  0.3844996   1.        ]]. Action = [[ 0.80254674 -0.1368742   0.74724936  0.7429199 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 573. State = [[-0.15349711 -0.06508771  0.4052512   1.        ]]. Action = [[ 0.9007485 -0.3545456  0.6286787  0.859874 ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Current timestep = 574. State = [[-0.1529428  -0.06546398  0.40713665  1.        ]]. Action = [[ 0.98912764 -0.6712065   0.93381476  0.6597724 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Current timestep = 575. State = [[-0.14392538 -0.07582929  0.40743855  1.        ]]. Action = [[ 0.8623402  -0.652268   -0.05795747  0.46320748]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 576. State = [[-0.12438077 -0.08739948  0.40711117  1.        ]]. Action = [[ 0.51489806 -0.46859932  0.7741864   0.8827938 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Current timestep = 577. State = [[-0.11882679 -0.07937115  0.39758655  1.        ]]. Action = [[ 0.05992496  0.82606244 -0.9265236   0.7182038 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 578. State = [[-0.10380151 -0.07065234  0.3861436   1.        ]]. Action = [[ 0.87719095 -0.31090915  0.35415268  0.3178234 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 579. State = [[-0.09081309 -0.07485795  0.388568    1.        ]]. Action = [[-0.4448384  -0.16060352  0.02371097  0.7337103 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 580. State = [[-0.09680613 -0.07121424  0.38401738  1.        ]]. Action = [[-0.55623776  0.4513588  -0.541519    0.9660752 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 581. State = [[-0.10166182 -0.06603144  0.37934107  1.        ]]. Action = [[0.64078593 0.28098977 0.69973516 0.2507205 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 582. State = [[-0.1106945  -0.05885962  0.3698727   1.        ]]. Action = [[-0.6051926   0.35771346 -0.58469963  0.16538465]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 583. State = [[-0.11496439 -0.04755295  0.35391548  1.        ]]. Action = [[ 0.97314143  0.2767967  -0.21122408  0.69772005]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 584. State = [[-0.11178834 -0.03166155  0.33453575  1.        ]]. Action = [[-0.732239    0.7022041  -0.8167214   0.35403514]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 585. State = [[-0.11171801 -0.02064451  0.32853177  1.        ]]. Action = [[ 0.9736074  -0.36154485  0.72348976  0.8532715 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 586. State = [[-0.10225064 -0.03457817  0.3321932   1.        ]]. Action = [[-0.04318225 -0.7863894  -0.23119569  0.8908279 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 587. State = [[-0.10107046 -0.04977691  0.3352456   1.        ]]. Action = [[-0.2231996   0.01060331  0.576138    0.59728503]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 588. State = [[-0.25084782  0.00253852  0.2326396   1.        ]]. Action = [[ 0.26869226  0.35883427  0.6925136  -0.23885113]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 589. State = [[-0.25217128  0.00154339  0.23676752  1.        ]]. Action = [[-0.15578908 -0.07504696  0.67600393  0.7420795 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 590. State = [[-0.25000542  0.00139476  0.23873617  1.        ]]. Action = [[ 0.7906195   0.22415388 -0.4826601   0.74626505]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 591. State = [[-0.24169818  0.0112517   0.23472153  1.        ]]. Action = [[0.19458413 0.48009014 0.00894415 0.7661185 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 592. State = [[-0.23855874  0.01942973  0.23627405  1.        ]]. Action = [[-0.62336105  0.81206846 -0.14928102  0.45376444]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Current timestep = 593. State = [[-0.2301469   0.02301362  0.2451767   1.        ]]. Action = [[0.40333033 0.17619014 0.98131466 0.80980086]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 594. State = [[-0.21752517  0.02197501  0.25484937  1.        ]]. Action = [[-0.03273082 -0.5463486  -0.43997765  0.82897186]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 595. State = [[-0.20576116  0.02527806  0.26391643  1.        ]]. Action = [[0.9992976  0.7520499  0.7636554  0.58310366]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 596. State = [[-0.17559771  0.03467891  0.2921386   1.        ]]. Action = [[ 0.78950167 -0.07234699  0.8471801   0.8575747 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 597. State = [[-0.14212221  0.03458735  0.32851762  1.        ]]. Action = [[ 0.990154   -0.20973033  0.97188735  0.7575426 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 598. State = [[-0.12510404  0.02718908  0.36306664  1.        ]]. Action = [[-0.9338756  -0.32174206  0.8445287   0.8582306 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 599. State = [[-0.14018741  0.00686018  0.3722504   1.        ]]. Action = [[-0.9502223  -0.8530605  -0.94573593  0.59856105]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 600. State = [[-1.6318528e-01  2.8574603e-04  3.5750720e-01  1.0000000e+00]]. Action = [[-0.71766967  0.55718446 -0.5415092   0.50241137]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 601. State = [[-0.18365678 -0.00507882  0.34428877  1.        ]]. Action = [[-0.2084446  -0.6870188   0.01322424  0.10554409]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 602. State = [[-0.19535673 -0.02848247  0.34517047  1.        ]]. Action = [[-0.2947386  -0.7771239   0.22362614  0.70613384]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 603. State = [[-0.21023625 -0.05261159  0.3604767   1.        ]]. Action = [[-0.77041286 -0.36541826  0.9640502   0.8011091 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 604. State = [[-0.23999164 -0.07569124  0.39331034  1.        ]]. Action = [[-0.5141159  -0.69114196  0.83667135  0.44182765]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 605. State = [[-0.2610068  -0.09193151  0.4089229   1.        ]]. Action = [[-0.2546128  -0.08144397 -0.42387217  0.59764874]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 606. State = [[-0.26268017 -0.10785671  0.4032898   1.        ]]. Action = [[ 0.70649815 -0.8186772  -0.736775    0.6807952 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 607. State = [[-0.25474355 -0.12260421  0.39008325  1.        ]]. Action = [[-0.74372876 -0.18620902 -0.7905356   0.8008385 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 608. State = [[-0.25336763 -0.11625828  0.38940606  1.        ]]. Action = [[-0.06685686  0.6786711   0.23244083  0.8323604 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 609. State = [[-0.2557464  -0.11744691  0.39180157  1.        ]]. Action = [[-0.0069865  -0.73241186  0.13413215  0.8188486 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 610. State = [[-0.2565656  -0.13622455  0.39767104  1.        ]]. Action = [[ 0.3342383 -0.606065   0.1395266  0.7973187]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 611. State = [[-0.24931824 -0.16193943  0.39198038  1.        ]]. Action = [[ 0.52449644 -0.96426153 -0.91036797  0.7960453 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 612. State = [[-0.23198648 -0.19452293  0.36338025  1.        ]]. Action = [[ 0.0848242  -0.51400274 -0.6006424   0.72416306]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 613. State = [[-0.22458683 -0.20612864  0.3579853   1.        ]]. Action = [[-0.1368767   0.2626413   0.9643152   0.69066465]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 614. State = [[-0.22356707 -0.20693007  0.3720304   1.        ]]. Action = [[ 0.19838285 -0.03724182  0.6883087   0.5658214 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 615. State = [[-0.21900381 -0.20809746  0.38696206  1.        ]]. Action = [[-0.13178957  0.50781095  0.9632952   0.89469326]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Current timestep = 616. State = [[-0.21267763 -0.20382507  0.38293323  1.        ]]. Action = [[ 0.82171917  0.20372975 -0.86606723  0.5589254 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 617. State = [[-0.19248669 -0.1900076   0.3788626   1.        ]]. Action = [[0.37398732 0.6307641  0.5422244  0.8040165 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 618. State = [[-0.17843074 -0.18105994  0.39308885  1.        ]]. Action = [[ 0.46128058 -0.36751622  0.6031165   0.38205004]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 619. State = [[-0.15973684 -0.18504786  0.4015892   1.        ]]. Action = [[ 0.88701093 -0.23596025 -0.26023465  0.60283923]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 620. State = [[-0.12367076 -0.17769176  0.3996389   1.        ]]. Action = [[0.88040423 0.80157137 0.16566885 0.7038785 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 621. State = [[-0.09967654 -0.16732189  0.40320367  1.        ]]. Action = [[ 0.65312755 -0.1150142   0.9624146   0.9160321 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Current timestep = 622. State = [[-0.09650442 -0.16609289  0.40403694  1.        ]]. Action = [[-0.55323035 -0.49228597  0.6194267   0.80502605]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 623. State = [[-0.09652223 -0.16595209  0.40403548  1.        ]]. Action = [[-0.48853648 -0.1787898   0.72263384  0.89525616]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Current timestep = 624. State = [[-0.0969586  -0.17170578  0.4057268   1.        ]]. Action = [[-0.00723535 -0.47600138  0.12703931  0.94566274]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 625. State = [[-0.09680126 -0.16517025  0.40728566  1.        ]]. Action = [[-0.23438102  0.83199763 -0.09808201  0.70393157]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 626. State = [[-0.09678166 -0.1551171   0.40738016  1.        ]]. Action = [[-0.871976    0.1048106   0.82324314  0.40401185]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 627. State = [[-0.09699503 -0.15412909  0.40733254  1.        ]]. Action = [[ 0.85728693 -0.61333233  0.40974402  0.8341198 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Current timestep = 628. State = [[-0.09698132 -0.1458187   0.40145686  1.        ]]. Action = [[-0.14670485  0.5144156  -0.69349664  0.71147156]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 629. State = [[-0.1022453  -0.14522561  0.3827504   1.        ]]. Action = [[-0.37353384 -0.73480916 -0.95761114  0.9465908 ]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 630. State = [[-0.11018573 -0.15363662  0.36065486  1.        ]]. Action = [[0.22740996 0.7276807  0.7034216  0.64367485]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 631. State = [[-0.11136277 -0.15539795  0.35588074  1.        ]]. Action = [[ 0.25639367 -0.05859542 -0.12546378  0.74138606]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 632. State = [[-0.11192014 -0.16791123  0.35828325  1.        ]]. Action = [[-0.32156503 -0.6670103   0.482167    0.5263423 ]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 633. State = [[-0.12865323 -0.19663304  0.37834528  1.        ]]. Action = [[-0.8031014  -0.91838914  0.9629381   0.86925936]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 634. State = [[-0.15158568 -0.22569245  0.40226203  1.        ]]. Action = [[-0.57131714 -0.5295827   0.29094315  0.6690767 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 635. State = [[-0.16544949 -0.23861939  0.41068897  1.        ]]. Action = [[0.29161072 0.8194468  0.76885414 0.85732245]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Current timestep = 636. State = [[-0.16286038 -0.23524547  0.40043473  1.        ]]. Action = [[ 0.7346271   0.30669641 -0.9200698   0.6681652 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 637. State = [[-0.15232776 -0.23195551  0.38396624  1.        ]]. Action = [[-0.02739346  0.85796213  0.88624203  0.57361233]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Current timestep = 638. State = [[-0.14096063 -0.22074312  0.37511894  1.        ]]. Action = [[ 0.8457434   0.61736655 -0.51450163  0.66609097]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 639. State = [[-0.10900296 -0.2036117   0.37013283  1.        ]]. Action = [[0.7578038  0.50813127 0.5965166  0.43334925]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 640. State = [[-0.09852708 -0.18355602  0.36939842  1.        ]]. Action = [[-0.5724776  0.6385317 -0.6517145  0.7222146]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 641. State = [[-0.10778648 -0.1726313   0.35423276  1.        ]]. Action = [[-0.50293624 -0.05747616 -0.98118967  0.44732022]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 642. State = [[-0.12494249 -0.17778616  0.33086127  1.        ]]. Action = [[-0.5302283  -0.4712072  -0.08936471  0.6648928 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 643. State = [[-0.1328384  -0.19450106  0.33400586  1.        ]]. Action = [[ 0.3018272  -0.71326184  0.85545003  0.66997445]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 644. State = [[-0.13033883 -0.20804043  0.35359362  1.        ]]. Action = [[ 0.265885   -0.23540139  0.851336    0.7387451 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 645. State = [[-0.13557112 -0.21580514  0.38023353  1.        ]]. Action = [[-0.33946562  0.00489902  0.9465027   0.76642704]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 646. State = [[-0.14860548 -0.23212373  0.39836094  1.        ]]. Action = [[-0.5530083  -0.8743826  -0.4416809   0.20238233]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 647. State = [[-0.15531446 -0.24665985  0.3972802   1.        ]]. Action = [[0.1132524 0.603564  0.6502969 0.7501935]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Current timestep = 648. State = [[-0.14853208 -0.23884395  0.3869154   1.        ]]. Action = [[ 0.834002    0.65636086 -0.8404049   0.64354444]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 649. State = [[-0.13624544 -0.23311327  0.3721791   1.        ]]. Action = [[-0.21165526  0.62770605  0.69785964  0.6077137 ]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Current timestep = 650. State = [[-0.13554205 -0.21927667  0.3629769   1.        ]]. Action = [[-0.4161129   0.8368598  -0.76358986  0.9223993 ]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 651. State = [[-0.15001488 -0.21749686  0.34623176  1.        ]]. Action = [[-0.63265544 -0.89799356 -0.63262063  0.8736019 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 652. State = [[-0.16754879 -0.21658067  0.3191203   1.        ]]. Action = [[-0.45051593  0.8679936  -0.97857726  0.5369828 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 653. State = [[-0.16984798 -0.19653343  0.293695    1.        ]]. Action = [[ 0.38582754  0.6073278  -0.05551416  0.688787  ]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 654. State = [[-0.16555831 -0.17182443  0.29360998  1.        ]]. Action = [[-0.28513867  0.78377056  0.8657738   0.65984046]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 655. State = [[-0.17069699 -0.1422758   0.31222495  1.        ]]. Action = [[0.04855013 0.6487849  0.91008484 0.45833123]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 656. State = [[-0.1683974 -0.11255    0.3300451  1.       ]]. Action = [[ 0.631104    0.67085266 -0.06052172  0.635731  ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 657. State = [[-0.14922096 -0.08573724  0.33433935  1.        ]]. Action = [[0.897851   0.7567053  0.00704765 0.17147183]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 658. State = [[-0.13587227 -0.06306917  0.34136933  1.        ]]. Action = [[-0.45233685  0.37018466  0.5428307   0.870049  ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 659. State = [[-0.25084916  0.00264293  0.23258124  1.        ]]. Action = [[-0.33194262 -0.04776275  0.9895382  -0.10988146]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 660. State = [[-0.24598348  0.00465457  0.2157064   1.        ]]. Action = [[ 0.8447969  -0.0246731  -0.254102    0.61105657]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 661. State = [[-0.22208548  0.00216106  0.20048392  1.        ]]. Action = [[ 0.936566   -0.2878096  -0.20440245  0.20461118]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 662. State = [[-0.19508837  0.0058444   0.18514909  1.        ]]. Action = [[ 0.63470984  0.49347723 -0.65873486  0.81052315]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 663. State = [[-0.18483907  0.02160992  0.17420381  1.        ]]. Action = [[-0.78652376  0.7049513   0.5481485   0.52300787]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 664. State = [[-0.19418411  0.0384516   0.17681675  1.        ]]. Action = [[0.8619826  0.47843063 0.67768836 0.4886973 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: No entry zone
Current timestep = 665. State = [[-0.193909    0.02772624  0.18896355  1.        ]]. Action = [[-0.06279349 -0.84841496  0.89692295  0.710197  ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 666. State = [[-0.19417667  0.01296228  0.20769274  1.        ]]. Action = [[ 0.66662085 -0.48195338  0.7517055   0.88413477]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 667. State = [[-0.1978048   0.00825925  0.2192453   1.        ]]. Action = [[-0.49269587 -0.15224975  0.7628286   0.4231634 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 668. State = [[-0.20464143 -0.00728485  0.24155147  1.        ]]. Action = [[-0.04989463 -0.75148344  0.1776936   0.465361  ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 669. State = [[-0.20701133 -0.02521438  0.26079452  1.        ]]. Action = [[ 0.10648894 -0.26104558  0.96465635  0.75716424]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 670. State = [[-0.19952308 -0.033087    0.2810399   1.        ]]. Action = [[ 0.9045615  -0.09120125 -0.261833    0.7285795 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 671. State = [[-0.18999392 -0.0263554   0.27223212  1.        ]]. Action = [[-0.04331112  0.67830217 -0.81067836  0.2695923 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 672. State = [[-0.18385457 -0.03061488  0.24809588  1.        ]]. Action = [[ 0.07149649 -0.89748687 -0.9247442   0.46572554]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 673. State = [[-0.17988731 -0.04170386  0.22641174  1.        ]]. Action = [[ 0.8036401  0.3602667 -0.528173   0.7133895]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 674. State = [[-0.17880964 -0.04286676  0.22452766  1.        ]]. Action = [[ 0.88249195 -0.72249395 -0.6664712   0.90166783]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 675. State = [[-0.1818105  -0.03317271  0.22643778  1.        ]]. Action = [[-0.56062526  0.7194928   0.3777423   0.8058934 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 676. State = [[-0.19189629 -0.00743658  0.22069182  1.        ]]. Action = [[-0.31398785  0.9674058  -0.7620031   0.7003008 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 677. State = [[-0.20291677  0.01180257  0.21206966  1.        ]]. Action = [[ 0.84922016 -0.19744766 -0.7081237   0.90327287]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: No entry zone
Current timestep = 678. State = [[-0.21452874  0.0138548   0.20274246  1.        ]]. Action = [[-0.765712   -0.04053247 -0.5963345   0.5277629 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 679. State = [[-0.23736559  0.00780655  0.18757103  1.        ]]. Action = [[-0.3672166  -0.45216793 -0.00691086  0.89629936]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 680. State = [[-0.23841394 -0.01063366  0.19110489  1.        ]]. Action = [[ 0.7391882  -0.8451269   0.7189369   0.44959593]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 681. State = [[-0.22434302 -0.02928028  0.19599889  1.        ]]. Action = [[ 0.55867255 -0.16378784 -0.47353864  0.75169146]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 682. State = [[-0.21530865 -0.04418711  0.20278431  1.        ]]. Action = [[-0.38253492 -0.4323852   0.9838021   0.6762147 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 683. State = [[-0.21837221 -0.04093278  0.21112844  1.        ]]. Action = [[ 0.08906019  0.9043951  -0.5447401   0.740034  ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 684. State = [[-0.21432196 -0.03368052  0.20318149  1.        ]]. Action = [[ 0.6771574  -0.32879347 -0.5156054   0.84395075]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 685. State = [[-0.20119457 -0.04811528  0.1893537   1.        ]]. Action = [[-0.14858806 -0.93212587 -0.28641343  0.6427758 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 686. State = [[-0.19822298 -0.06561124  0.18413457  1.        ]]. Action = [[ 0.72473836 -0.07656896 -0.8484583   0.8041984 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: No entry zone
Current timestep = 687. State = [[-0.20549709 -0.06627525  0.17970899  1.        ]]. Action = [[-0.68438977  0.31913173 -0.36253017  0.4454596 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 688. State = [[-0.21363251 -0.06504209  0.17322952  1.        ]]. Action = [[ 0.8160894  -0.11655486  0.7049364   0.87285066]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: No entry zone
Current timestep = 689. State = [[-0.20538078 -0.05194507  0.17770736  1.        ]]. Action = [[0.83202386 0.72695136 0.79217803 0.72585344]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 690. State = [[-0.19052729 -0.02085657  0.17699087  1.        ]]. Action = [[ 0.69227386  0.9799578  -0.64379096  0.66503096]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 691. State = [[-0.17688659 -0.00183229  0.16906247  1.        ]]. Action = [[ 0.57095337 -0.8259554   0.9699745   0.62526727]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: No entry zone
Current timestep = 692. State = [[-0.18106638 -0.00127479  0.16756019  1.        ]]. Action = [[-0.89197063 -0.18653512  0.15192866  0.83505535]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 693. State = [[-0.20176241  0.0117075   0.16180067  1.        ]]. Action = [[-0.84881186  0.9212347  -0.3898523   0.3662181 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 694. State = [[-0.2171384  0.0424721  0.1496108  1.       ]]. Action = [[ 0.45456302  0.8406286  -0.8415524   0.10673809]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 695. State = [[-0.22420019  0.07314379  0.1246862   1.        ]]. Action = [[-0.6358594   0.80852437 -0.6634408   0.38688016]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 696. State = [[-0.22356573  0.09664587  0.10095326  1.        ]]. Action = [[ 0.9035802   0.31556857 -0.59611344  0.8574939 ]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 697. State = [[-0.19819205  0.10168908  0.09165957  1.        ]]. Action = [[ 0.95847297 -0.4279803   0.444916    0.28526974]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 698. State = [[-0.17795749  0.08953103  0.10073083  1.        ]]. Action = [[ 0.42701435 -0.6275395   0.46394157  0.35995245]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 699. State = [[-0.16635701  0.08379394  0.11746213  1.        ]]. Action = [[-0.14077723  0.3649987   0.76188374  0.6835985 ]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 700. State = [[-0.16249777  0.08545175  0.13043946  1.        ]]. Action = [[0.82130265 0.6940191  0.0121783  0.5965551 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 701. State = [[-0.1620959   0.08567265  0.13141938  1.        ]]. Action = [[ 0.12631547  0.99635506 -0.78358245  0.77385926]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: No entry zone
Current timestep = 702. State = [[-0.1620959   0.08567265  0.13141938  1.        ]]. Action = [[ 0.9723532  -0.23929602  0.64611006  0.75549173]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Current timestep = 703. State = [[-0.1620959   0.08567265  0.13141938  1.        ]]. Action = [[ 0.84018683 -0.92180103  0.9714291   0.67752624]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 704. State = [[-0.16518842  0.08453901  0.12964816  1.        ]]. Action = [[-0.48081422 -0.05901843 -0.30831194  0.24670649]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 705. State = [[-0.16701305  0.08384052  0.12871853  1.        ]]. Action = [[0.16244435 0.60059667 0.96497    0.9047518 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Current timestep = 706. State = [[-0.16727048  0.08374248  0.12859423  1.        ]]. Action = [[ 0.91209054  0.365484   -0.86820024  0.82181585]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: No entry zone
Current timestep = 707. State = [[-0.16727048  0.08374248  0.12859423  1.        ]]. Action = [[ 0.80150414 -0.01160347 -0.796042    0.49212396]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: No entry zone
Current timestep = 708. State = [[-0.16777684  0.07006334  0.13203898  1.        ]]. Action = [[-0.16670585 -0.9403982   0.3419094   0.68187237]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 709. State = [[-0.17097731  0.05360632  0.13652498  1.        ]]. Action = [[ 0.6601789   0.79370856 -0.29606783  0.71820974]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: No entry zone
Current timestep = 710. State = [[-0.17843896  0.0406559   0.130874    1.        ]]. Action = [[-0.31603372 -0.6419121  -0.69130474  0.05016232]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 711. State = [[-0.19702148  0.01106801  0.12491339  1.        ]]. Action = [[-0.942871   -0.91503364 -0.05942929  0.73135734]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 712. State = [[-0.21339655 -0.00482579  0.12066866  1.        ]]. Action = [[ 0.9765786 -0.5815015  0.2888124  0.4814558]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: No entry zone
Current timestep = 713. State = [[-0.231334    0.01753454  0.11820994  1.        ]]. Action = [[-0.8612231   0.19445074 -0.7500074   0.3931644 ]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 714. State = [[-0.24155088  0.04480588  0.11365598  1.        ]]. Action = [[ 0.8597417   0.74065804 -0.20786166  0.9489062 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 715. State = [[-0.23160094  0.04688497  0.09959634  1.        ]]. Action = [[ 0.41830182 -0.78176683 -0.43123746  0.77567434]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 716. State = [[-0.21499752  0.03637591  0.09817124  1.        ]]. Action = [[ 0.37030864 -0.14627475  0.839262    0.8199203 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 717. State = [[-0.19375847  0.01624153  0.12055137  1.        ]]. Action = [[ 0.78347707 -0.7535626   0.88272476  0.6610049 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 718. State = [[-0.17800532 -0.0042146   0.13273163  1.        ]]. Action = [[ 0.2928157  -0.08846974 -0.90933233  0.73004746]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 719. State = [[-0.1686409  -0.00635904  0.11808053  1.        ]]. Action = [[-0.34684646  0.08975029  0.38324344  0.57083845]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 720. State = [[-0.17709197 -0.00501164  0.11871243  1.        ]]. Action = [[-0.9591656  -0.14339489  0.02042019  0.3336842 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 721. State = [[-0.1901357  -0.00655249  0.12299471  1.        ]]. Action = [[ 0.9637766   0.02098501 -0.67625237  0.65381217]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 722. State = [[-0.19638477  0.00641182  0.11804949  1.        ]]. Action = [[-0.2555802   0.9607091  -0.55742824  0.13266301]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 723. State = [[-0.20617755  0.0292121   0.11424949  1.        ]]. Action = [[ 0.75724864 -0.85804313 -0.3015951   0.7755884 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: No entry zone
Current timestep = 724. State = [[-0.2084735   0.03524932  0.11425005  1.        ]]. Action = [[ 0.9248197  -0.7631896   0.05303228  0.88966537]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 725. State = [[-0.2084735   0.03524932  0.11425005  1.        ]]. Action = [[ 0.9676765  -0.19467193  0.6665664   0.74892044]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 726. State = [[-0.21657121  0.03476732  0.11925414  1.        ]]. Action = [[-0.9955745  -0.11350667  0.63251173  0.7957479 ]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 727. State = [[-0.24683149  0.04055108  0.12769021  1.        ]]. Action = [[-0.6067966  -0.47076523 -0.25883043  0.49296784]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 728. State = [[-0.26158553  0.03730801  0.14423     1.        ]]. Action = [[ 0.258121   -0.4948544   0.81628454  0.6220038 ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 729. State = [[-0.25876144  0.01539689  0.17043042  1.        ]]. Action = [[ 0.7266365  -0.93511724  0.60234046  0.8361418 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 730. State = [[-0.25355196 -0.0046539   0.19343105  1.        ]]. Action = [[-0.45330268  0.8092334  -0.45198858  0.7375567 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Current timestep = 731. State = [[-0.24392942 -0.01059646  0.19483085  1.        ]]. Action = [[ 0.98637605  0.77251863 -0.9574117   0.81324744]]. Reward = [0.]
Curr episode timestep = 71
Current timestep = 732. State = [[-0.22130157  0.00106479  0.17166646  1.        ]]. Action = [[-0.14819819  0.3669492   0.49934745  0.79711056]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 733. State = [[-0.21764709 -0.00173147  0.178913    1.        ]]. Action = [[-0.7673379  -0.8286005   0.44680452  0.7158053 ]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 734. State = [[-0.23007552 -0.01609825  0.19039354  1.        ]]. Action = [[-0.29844224 -0.7376233   0.04301333  0.6222    ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 735. State = [[-0.23838751 -0.02923054  0.20175043  1.        ]]. Action = [[ 0.01042509 -0.88657457  0.7572243   0.7615664 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 736. State = [[-0.24630117 -0.05985055  0.23862839  1.        ]]. Action = [[-0.12678003 -0.8846641   0.9628804   0.7187176 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 737. State = [[-0.25912926 -0.08681834  0.26784894  1.        ]]. Action = [[ 0.19750357 -0.14675325 -0.7042241   0.69979596]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 738. State = [[-0.25678027 -0.09377293  0.2578329   1.        ]]. Action = [[-0.41680598 -0.5731757   0.11903536  0.69221246]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 739. State = [[-0.24654335 -0.09712803  0.24715787  1.        ]]. Action = [[ 0.5392628  -0.67895216 -0.39603388  0.6813352 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 740. State = [[-0.23390456 -0.12143363  0.24601915  1.        ]]. Action = [[-0.9051073  -0.9076322   0.13915765  0.86873233]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Current timestep = 741. State = [[-0.2167779  -0.14421913  0.24032356  1.        ]]. Action = [[ 0.7726824  -0.42740226  0.89207315  0.687739  ]]. Reward = [0.]
Curr episode timestep = 81
Current timestep = 742. State = [[-0.19582956 -0.18396531  0.2342784   1.        ]]. Action = [[ 0.95160663 -0.13252711 -0.8988334   0.84393024]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 743. State = [[-0.16538611 -0.20938799  0.21282858  1.        ]]. Action = [[0.46527767 0.50648665 0.3088969  0.6538482 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 744. State = [[-0.14782307 -0.202023    0.21129352  1.        ]]. Action = [[0.3590405  0.43730378 0.10728407 0.59188604]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 745. State = [[-0.13845414 -0.20207496  0.20247015  1.        ]]. Action = [[ 0.19020724 -0.64650077 -0.8787778   0.7214761 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 746. State = [[-0.124666   -0.22092257  0.19449     1.        ]]. Action = [[ 0.43455315 -0.48296976  0.85518026  0.6159136 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 747. State = [[-0.11721287 -0.23382784  0.21255268  1.        ]]. Action = [[-0.43833148 -0.14883202  0.92967546  0.41154528]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 748. State = [[-0.11155278 -0.2431291   0.23820218  1.        ]]. Action = [[ 0.9068774  -0.37704086  0.3833741   0.7117765 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 749. State = [[-0.09613995 -0.2432904   0.2570852   1.        ]]. Action = [[-0.19009662  0.7385738   0.6349871   0.53312945]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 750. State = [[-0.09483673 -0.22157422  0.2639239   1.        ]]. Action = [[-0.07537866  0.8581544  -0.8238452   0.7374754 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 751. State = [[-0.08961821 -0.20993678  0.252431    1.        ]]. Action = [[ 0.6658969  -0.63165665 -0.99742293  0.6602726 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 752. State = [[-0.07870196 -0.2131335   0.23298958  1.        ]]. Action = [[-0.16816413  0.2975123   0.6620989   0.46866465]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 753. State = [[-0.07157938 -0.20186038  0.23046061  1.        ]]. Action = [[ 0.3809464   0.6663203  -0.4830258   0.62913907]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 754. State = [[-0.06620999 -0.18888836  0.22923909  1.        ]]. Action = [[ 0.03444958  0.6739657  -0.8957199   0.88435173]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: No entry zone
Current timestep = 755. State = [[-0.06498694 -0.18668096  0.22902147  1.        ]]. Action = [[-0.4184878   0.43788326 -0.04688013  0.50984645]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: No entry zone
Current timestep = 756. State = [[-0.05459613 -0.18156359  0.22983357  1.        ]]. Action = [[0.7870066  0.14125264 0.22456586 0.8030143 ]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 757. State = [[-0.04208893 -0.1753078   0.23062594  1.        ]]. Action = [[-0.14484096  0.08821559  0.19693136  0.782248  ]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 757 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 757 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 757 of 0
Current timestep = 758. State = [[-0.0404798  -0.17393766  0.23112229  1.        ]]. Action = [[-0.10308194  0.83841467 -0.24527335  0.82220006]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Scene graph at timestep 758 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 758 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 758 of -1
Current timestep = 759. State = [[-0.04861157 -0.1797987   0.22492737  1.        ]]. Action = [[-0.6484247  -0.3148347  -0.7753902   0.56162906]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 759 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 759 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 759 of -1
Current timestep = 760. State = [[-0.07123666 -0.19943388  0.20832154  1.        ]]. Action = [[-0.8560378  -0.7150255  -0.34918058  0.79591393]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 761. State = [[-0.25057068  0.00276851  0.23289517  1.        ]]. Action = [[-0.8358229   0.96467125  0.47565126  0.63108647]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: No entry zone
Current timestep = 762. State = [[-0.24075863  0.06838505  0.22595054  1.        ]]. Action = [[ 0.77527475 -0.7605815   0.47648358  0.92886233]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 763. State = [[-0.20847747  0.09472077  0.23108141  1.        ]]. Action = [[ 0.92630553 -0.25585186  0.21427548  0.84209657]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 764. State = [[-0.18249795  0.08417946  0.22894685  1.        ]]. Action = [[ 0.34543574 -0.74166083 -0.4908424   0.88208175]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 765. State = [[-0.1747864   0.06514447  0.21886867  1.        ]]. Action = [[-0.5991865  -0.54576486 -0.11877358  0.13818038]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 766. State = [[-0.17861634  0.03861118  0.2072735   1.        ]]. Action = [[ 0.07628059 -0.8710974  -0.96996075  0.7954993 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 767. State = [[-0.17984147  0.00583578  0.18202174  1.        ]]. Action = [[-0.1011945  -0.9478043  -0.19297874  0.8194516 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 768. State = [[-0.18029362 -0.01327266  0.17611893  1.        ]]. Action = [[ 0.37239456  0.6720116  -0.8667012   0.7476516 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Current timestep = 769. State = [[-0.17983517 -0.01526143  0.17683023  1.        ]]. Action = [[ 0.5204505   0.50604033 -0.8566059   0.61391866]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: No entry zone
Current timestep = 770. State = [[-0.18802167 -0.03324839  0.1827373   1.        ]]. Action = [[-0.9461464  -0.9254994   0.72710955  0.8843415 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 771. State = [[-0.20579617 -0.0523887   0.1918571   1.        ]]. Action = [[ 0.593544   -0.5101196   0.98397183  0.80152774]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 772. State = [[-0.20493087 -0.06895737  0.1861019   1.        ]]. Action = [[ 0.20478654 -0.81661266 -0.83650464  0.6807294 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 773. State = [[-0.21313389 -0.09336776  0.18036784  1.        ]]. Action = [[-0.84388965 -0.4493032   0.7258065   0.81043744]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 774. State = [[-0.22676653 -0.11971695  0.19272241  1.        ]]. Action = [[ 0.22215986 -0.8858059   0.12255788  0.67049277]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 775. State = [[-0.22736073 -0.1398639   0.19304377  1.        ]]. Action = [[-0.06162691 -0.07391357 -0.64214104  0.5698885 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 776. State = [[-0.23801716 -0.14410937  0.19195248  1.        ]]. Action = [[-0.55252266  0.16446841  0.9068183   0.5775857 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 777. State = [[-0.2466845  -0.14429832  0.2016262   1.        ]]. Action = [[-0.8491087   0.7034855  -0.03765345  0.41371918]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 778. State = [[-0.23973069 -0.15859725  0.21306106  1.        ]]. Action = [[ 0.86036134 -0.98468935  0.79056716  0.65293264]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 779. State = [[-0.23651575 -0.17273057  0.22432534  1.        ]]. Action = [[-0.4897318   0.14500952 -0.503446    0.84224415]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 780. State = [[-0.24682741 -0.1729204   0.21852632  1.        ]]. Action = [[-0.45156032  0.24068975 -0.5605925   0.8555461 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 781. State = [[-0.24744067 -0.17355838  0.20773117  1.        ]]. Action = [[ 0.94517684 -0.329185   -0.16054744  0.83941627]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 782. State = [[-0.22943895 -0.18985868  0.21127622  1.        ]]. Action = [[ 0.21862888 -0.80671716  0.99349284  0.3561207 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 783. State = [[-0.22708379 -0.21991616  0.22134106  1.        ]]. Action = [[-0.14292532 -0.9352971  -0.3518545   0.7173016 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 784. State = [[-0.21670868 -0.23558505  0.21955447  1.        ]]. Action = [[ 0.94099426  0.26166534 -0.21840036  0.9011376 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 785. State = [[-0.20449986 -0.24392828  0.21968788  1.        ]]. Action = [[-0.33371615 -0.48964274  0.68996215  0.8613864 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 786. State = [[-0.20166567 -0.24673353  0.23671459  1.        ]]. Action = [[0.0481621  0.42532003 0.7361007  0.7714758 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 787. State = [[-0.19835232 -0.23316893  0.25210705  1.        ]]. Action = [[ 0.03643537  0.58095515 -0.00778157  0.44495058]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 788. State = [[-0.1844496 -0.2088236  0.2613271  1.       ]]. Action = [[0.9725292  0.8454826  0.43711603 0.8460877 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 789. State = [[-0.16963197 -0.19832003  0.28434235  1.        ]]. Action = [[-0.2001139  -0.4149518   0.83716416  0.6776979 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 790. State = [[-0.15699989 -0.20117532  0.30539078  1.        ]]. Action = [[ 0.8943198  -0.04151201  0.01249707  0.8220303 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 791. State = [[-0.1387212  -0.18737596  0.30145946  1.        ]]. Action = [[ 0.33314776  0.97261727 -0.842301    0.6127975 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 792. State = [[-0.13597555 -0.18036611  0.29038018  1.        ]]. Action = [[-0.9005893  -0.43191546  0.04937518  0.45138705]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 793. State = [[-0.1454181  -0.17500077  0.2846712   1.        ]]. Action = [[-0.03540671  0.7192874  -0.5663991   0.3622756 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 794. State = [[-0.14640422 -0.16006841  0.27502424  1.        ]]. Action = [[ 0.42245317  0.16714597 -0.14145517  0.70712256]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 795. State = [[-0.14057115 -0.14081694  0.27080432  1.        ]]. Action = [[-0.0405165   0.844877    0.50890565  0.66357756]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 796. State = [[-0.13287023 -0.11098618  0.2721557   1.        ]]. Action = [[ 0.72766435  0.6111796  -0.4894458   0.39782357]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 797. State = [[-0.12333641 -0.09445979  0.2650943   1.        ]]. Action = [[-0.34878862  0.15881681  0.054268    0.7182548 ]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 798. State = [[-0.11528777 -0.08131379  0.2689767   1.        ]]. Action = [[0.79277873 0.48891473 0.19930792 0.21698284]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 799. State = [[-0.09460252 -0.06993227  0.28088436  1.        ]]. Action = [[ 0.6722839  -0.09708619  0.6374136   0.91434467]]. Reward = [0.]
Curr episode timestep = 37
Current timestep = 800. State = [[-0.08504828 -0.0534569   0.28377217  1.        ]]. Action = [[-0.09395987  0.9803895  -0.6560726   0.7187247 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 801. State = [[-0.07677743 -0.0209709   0.27963015  1.        ]]. Action = [[0.05652058 0.96107125 0.48479462 0.46251535]]. Reward = [0.]
Curr episode timestep = 39
Above hoop
Current timestep = 802. State = [[-0.06706909  0.0043303   0.2918993   1.        ]]. Action = [[0.9835936  0.14018667 0.47494006 0.81390095]]. Reward = [0.]
Curr episode timestep = 40
Above hoop
Current timestep = 803. State = [[-4.3175485e-02 -1.8054248e-05  2.9667056e-01  1.0000000e+00]]. Action = [[ 0.5365039  -0.92695814 -0.39822626  0.74811304]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Current timestep = 804. State = [[-0.02790931 -0.02529001  0.28295663  1.        ]]. Action = [[-0.32322097 -0.68534344 -0.6327265   0.6793418 ]]. Reward = [0.]
Curr episode timestep = 42
Above hoop
Current timestep = 805. State = [[-0.02389818 -0.03405436  0.26776576  1.        ]]. Action = [[ 0.3472402   0.46765256 -0.27533782  0.46005356]]. Reward = [0.]
Curr episode timestep = 43
Above hoop
Current timestep = 806. State = [[-0.01727559 -0.030023    0.25863644  1.        ]]. Action = [[ 0.66808903  0.0473659  -0.73834777  0.8412374 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: No entry zone
Above hoop
Current timestep = 807. State = [[-0.01649413 -0.02953261  0.2591544   1.        ]]. Action = [[ 0.9923179   0.2966919  -0.9799525   0.15488374]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Above hoop
Current timestep = 808. State = [[-0.00919079 -0.02559669  0.26713192  1.        ]]. Action = [[0.5708864  0.17682886 0.6440134  0.45302367]]. Reward = [0.]
Curr episode timestep = 46
Above hoop
Current timestep = 809. State = [[-5.3371117e-04 -2.2278346e-02  2.7203912e-01  1.0000000e+00]]. Action = [[ 0.3450284  -0.0687722  -0.64000726  0.2803347 ]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Current timestep = 810. State = [[ 0.00917551 -0.02544575  0.2511976   1.        ]]. Action = [[-0.23052967 -0.28021586 -0.37733048  0.63604856]]. Reward = [0.]
Curr episode timestep = 48
Above hoop
Current timestep = 811. State = [[ 0.01645047 -0.01902219  0.24973105  1.        ]]. Action = [[0.2599287  0.80924296 0.4975134  0.7214229 ]]. Reward = [0.]
Curr episode timestep = 49
Above hoop
Current timestep = 812. State = [[ 0.02656973 -0.00838359  0.26694605  1.        ]]. Action = [[ 0.32948673 -0.0904721   0.65611506  0.5395694 ]]. Reward = [0.]
Curr episode timestep = 50
Above hoop
Current timestep = 813. State = [[ 0.02739527 -0.0174027   0.29647335  1.        ]]. Action = [[-0.79684424 -0.5994066   0.92226005  0.73828566]]. Reward = [0.]
Curr episode timestep = 51
Above hoop
Current timestep = 814. State = [[ 0.01672431 -0.01911165  0.3104864   1.        ]]. Action = [[-0.58442855  0.6039293  -0.73044914  0.5566282 ]]. Reward = [0.]
Curr episode timestep = 52
Above hoop
Current timestep = 815. State = [[-5.1416276e-04 -2.4275137e-02  3.1401938e-01  1.0000000e+00]]. Action = [[-0.39291656 -0.83847755  0.73581743  0.6282034 ]]. Reward = [0.]
Curr episode timestep = 53
Above hoop
Current timestep = 816. State = [[-0.01893484 -0.05109429  0.3197963   1.        ]]. Action = [[-0.63067526 -0.9315868  -0.55653495  0.7571595 ]]. Reward = [0.]
Curr episode timestep = 54
Above hoop
Current timestep = 817. State = [[-0.03922137 -0.08201697  0.31446788  1.        ]]. Action = [[ 0.0672853  -0.8753447   0.08850706  0.6847111 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 818. State = [[-0.04094847 -0.11241183  0.3165132   1.        ]]. Action = [[ 0.2774427  -0.71916544 -0.02070892  0.7905078 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 819. State = [[-0.03827962 -0.1220025   0.3141186   1.        ]]. Action = [[ 0.12090516  0.6402583  -0.15193987  0.6258831 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 820. State = [[-0.04246786 -0.11348276  0.30093852  1.        ]]. Action = [[-0.6779677   0.31275356 -0.86496127  0.88040996]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 821. State = [[-0.05425565 -0.09623531  0.28744364  1.        ]]. Action = [[-0.43131506  0.62866235 -0.18770844  0.73413765]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 822. State = [[-0.05642464 -0.08453633  0.2899068   1.        ]]. Action = [[ 0.6724026  -0.15847129  0.90260863  0.6872499 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 823. State = [[-0.05204917 -0.09530932  0.29556346  1.        ]]. Action = [[ 0.20219696 -0.88847667 -0.11283606  0.52504945]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 824. State = [[-0.04084997 -0.10143632  0.29236764  1.        ]]. Action = [[ 0.7818897   0.5072553  -0.45600575  0.871856  ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 825. State = [[-0.02377355 -0.09357427  0.27296227  1.        ]]. Action = [[ 0.1465205   0.12855124 -0.4245646   0.80683863]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 826. State = [[-0.01625919 -0.08996231  0.26133907  1.        ]]. Action = [[-0.9703944  -0.5346436  -0.840974    0.65745366]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 827. State = [[-0.01610828 -0.08993889  0.26124084  1.        ]]. Action = [[ 0.93077743  0.42927253 -0.8850071   0.4686681 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 828. State = [[-0.0176311  -0.08864605  0.26945686  1.        ]]. Action = [[-0.6249984   0.21761394  0.69832826  0.84067833]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 829. State = [[-0.01407733 -0.07742628  0.2899148   1.        ]]. Action = [[0.6319033  0.49218392 0.7960727  0.87889695]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 830. State = [[ 4.8915617e-04 -6.1391786e-02  3.0990386e-01  1.0000000e+00]]. Action = [[0.9642855  0.27939045 0.21335149 0.6650214 ]]. Reward = [0.]
Curr episode timestep = 68
Above hoop
Current timestep = 831. State = [[ 0.00908179 -0.04798207  0.3101911   1.        ]]. Action = [[-0.81086993  0.38462746 -0.91299665  0.59157073]]. Reward = [0.]
Curr episode timestep = 69
Above hoop
Current timestep = 832. State = [[ 0.01346134 -0.04700236  0.30608428  1.        ]]. Action = [[ 0.7054446  -0.3939246   0.83099437  0.75556815]]. Reward = [0.]
Curr episode timestep = 70
Above hoop
Current timestep = 833. State = [[ 0.02570641 -0.0358833   0.32099643  1.        ]]. Action = [[0.9587927  0.75194466 0.7182219  0.68334293]]. Reward = [0.]
Curr episode timestep = 71
Above hoop
Current timestep = 834. State = [[ 0.04890504 -0.01984815  0.341416    1.        ]]. Action = [[0.2767315  0.23367381 0.0720191  0.82189465]]. Reward = [0.]
Curr episode timestep = 72
Above hoop
Current timestep = 835. State = [[ 0.06004458 -0.01576454  0.33510423  1.        ]]. Action = [[ 0.17378902 -0.10458761 -0.7733839   0.6028478 ]]. Reward = [0.]
Curr episode timestep = 73
Above hoop
Scene graph at timestep 835 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 835 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[ 0.07221493 -0.0080846   0.31436127  1.        ]]. Action = [[ 0.07697034  0.5687139  -0.19871372  0.58355904]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 836 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 836 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 836 of -1
Current timestep = 837. State = [[0.07798433 0.00115468 0.30942073 1.        ]]. Action = [[ 0.6614809  -0.48961103 -0.09167916  0.85855246]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 837 is [False, False, True, False, True, False, False, True, True, False]
State prediction error at timestep 837 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 837 of -1
Current timestep = 838. State = [[ 0.08036143 -0.00804928  0.314772    1.        ]]. Action = [[ 0.35052598 -0.7461004   0.48756123  0.58924794]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 839. State = [[ 0.08085281 -0.01269341  0.3352117   1.        ]]. Action = [[-0.82228285  0.567907    0.74958014  0.81601477]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 840. State = [[ 0.07547114 -0.0070345   0.3552292   1.        ]]. Action = [[0.6232462  0.78848004 0.1423769  0.8381777 ]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Current timestep = 841. State = [[ 0.07406571 -0.00638088  0.35794473  1.        ]]. Action = [[ 0.85875106 -0.14544511 -0.96642244  0.26400983]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Current timestep = 842. State = [[ 0.06692985 -0.01189416  0.36673185  1.        ]]. Action = [[-0.75800383 -0.28927755  0.4504125   0.61667466]]. Reward = [0.]
Curr episode timestep = 80
Current timestep = 843. State = [[ 4.6881489e-02 -8.0874440e-04  3.9072627e-01  1.0000000e+00]]. Action = [[-0.7732998   0.8700862   0.74092066  0.38036263]]. Reward = [0.]
Curr episode timestep = 81
Above hoop
Current timestep = 844. State = [[0.02285088 0.01774159 0.40583727 1.        ]]. Action = [[-0.8742854  -0.67634416  0.8903971   0.3998865 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Above hoop
Current timestep = 845. State = [[0.02049916 0.02965101 0.4084013  1.        ]]. Action = [[0.445549   0.66185236 0.09388757 0.723488  ]]. Reward = [0.]
Curr episode timestep = 83
Above hoop
Current timestep = 846. State = [[0.02583598 0.04290698 0.4064529  1.        ]]. Action = [[ 0.2563057   0.18126929 -0.3228128   0.6087556 ]]. Reward = [0.]
Curr episode timestep = 84
Above hoop
Current timestep = 847. State = [[0.03568387 0.04000359 0.3892714  1.        ]]. Action = [[ 0.98986137 -0.804009   -0.64350885  0.24659705]]. Reward = [0.]
Curr episode timestep = 85
Above hoop
Current timestep = 848. State = [[0.05998744 0.02610726 0.3721751  1.        ]]. Action = [[-0.3744977 -0.254995   0.702981   0.5833392]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Above hoop
Current timestep = 849. State = [[0.06161475 0.02463713 0.37038594 1.        ]]. Action = [[ 0.95469594  0.33723998 -0.02716994  0.6823759 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Above hoop
Current timestep = 850. State = [[0.06833756 0.0383503  0.3702781  1.        ]]. Action = [[0.4043888  0.83743906 0.07317567 0.3524648 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 851. State = [[0.07599725 0.05295523 0.3720796  1.        ]]. Action = [[-0.5096401   0.03564751  0.89470315  0.72973585]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Current timestep = 852. State = [[0.08020929 0.06907351 0.37248245 1.        ]]. Action = [[0.10756242 0.85378575 0.01689899 0.23053777]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 853. State = [[0.08432898 0.0810856  0.38172644 1.        ]]. Action = [[-0.08289552 -0.19233757  0.7026104   0.16221666]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 854. State = [[0.08187969 0.08248188 0.39280272 1.        ]]. Action = [[ 0.40154517 -0.18005645 -0.9725454   0.00428212]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Current timestep = 855. State = [[0.08131982 0.08275715 0.39454865 1.        ]]. Action = [[ 0.34457242 -0.84619766 -0.6708626   0.45439672]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Current timestep = 856. State = [[0.08136524 0.08282805 0.3946313  1.        ]]. Action = [[ 0.6242523   0.73800325 -0.15973943  0.53850734]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Current timestep = 857. State = [[0.08416446 0.07584067 0.39555594 1.        ]]. Action = [[ 0.21232605 -0.4174193   0.13447547  0.6489501 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 858. State = [[0.08849774 0.06798997 0.3970347  1.        ]]. Action = [[-0.23539019  0.8267468   0.5000355   0.5047456 ]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: Workspace boundary
Current timestep = 859. State = [[0.08915611 0.06692223 0.39762342 1.        ]]. Action = [[ 0.25981474  0.51677966 -0.35304427  0.916808  ]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 860. State = [[0.08331461 0.0513734  0.4004259  1.        ]]. Action = [[-0.9459609  -0.9790027  -0.02322716  0.7743186 ]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 861. State = [[0.0722308  0.03147359 0.39543214 1.        ]]. Action = [[-0.15220118 -0.02942705 -0.73151684  0.8417454 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 862. State = [[0.06749288 0.02667361 0.38847023 1.        ]]. Action = [[-0.37104577  0.42165685  0.46168983  0.5621295 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Current timestep = 863. State = [[-0.25075024  0.00263366  0.23269002  1.        ]]. Action = [[-0.72790253 -0.36116654  0.37246966  0.8018789 ]]. Reward = [0.]
Curr episode timestep = 101
Above hoop
Current timestep = 864. State = [[-0.24699071  0.01053566  0.22239761  1.        ]]. Action = [[0.35081017 0.59425235 0.38954222 0.6694498 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 865. State = [[-0.24010198  0.02914298  0.22669232  1.        ]]. Action = [[0.30747175 0.68816185 0.20594358 0.68773127]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 866. State = [[-0.22724012  0.02881484  0.2243397   1.        ]]. Action = [[ 0.84736085 -0.89318395 -0.8515747   0.6821437 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 867. State = [[-0.20087017  0.02406585  0.21155685  1.        ]]. Action = [[0.17802012 0.35698938 0.77411175 0.6193459 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 868. State = [[-0.19881243  0.03694076  0.22876461  1.        ]]. Action = [[-0.6467864   0.64132607  0.9800184   0.82068014]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 869. State = [[-0.21513052  0.0617768   0.24107566  1.        ]]. Action = [[-0.39659512  0.7703848  -0.97627467  0.3458948 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 870. State = [[-0.22211938  0.07312997  0.2225184   1.        ]]. Action = [[ 0.20274246 -0.34758526 -0.66064554  0.57263863]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 871. State = [[-0.23028976  0.06100988  0.20558976  1.        ]]. Action = [[-0.90466547 -0.5705288  -0.01763874  0.76774216]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 872. State = [[-0.2355897   0.05206101  0.21000475  1.        ]]. Action = [[0.45093417 0.09412038 0.9139962  0.6047766 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 873. State = [[-0.23932298  0.0600403   0.22912943  1.        ]]. Action = [[-0.601369    0.4914297   0.4474448   0.69738483]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 874. State = [[-0.24435155  0.06289013  0.2348248   1.        ]]. Action = [[ 0.66291976 -0.38799125 -0.7567304   0.55525863]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 875. State = [[-0.22921962  0.06487487  0.22370279  1.        ]]. Action = [[ 0.8909819   0.48246002 -0.35369134  0.45585155]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 876. State = [[-0.21551903  0.07742586  0.20239669  1.        ]]. Action = [[-0.30008972  0.44976783 -0.9001379   0.6838336 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 877. State = [[-0.20628326  0.07490274  0.18515816  1.        ]]. Action = [[ 0.723068   -0.73551697  0.1940583   0.6450546 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 878. State = [[-0.19023117  0.06438086  0.18959737  1.        ]]. Action = [[ 0.14741635 -0.17422593  0.39709294  0.49776435]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 879. State = [[-0.18616064  0.06023212  0.19232148  1.        ]]. Action = [[ 0.60268927  0.38818944 -0.42554665  0.60985315]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 880. State = [[-0.18186161  0.04956314  0.19819608  1.        ]]. Action = [[ 0.16761458 -0.54620934  0.4544983   0.84554875]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 881. State = [[-0.17626773  0.03721282  0.21335195  1.        ]]. Action = [[-0.09303612 -0.0853408   0.63535476  0.7620032 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 882. State = [[-0.18048458  0.03434419  0.22018401  1.        ]]. Action = [[-0.6035695   0.05636859 -0.76751024  0.68380976]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 883. State = [[-0.19562091  0.03339305  0.2239363   1.        ]]. Action = [[-0.8819769  -0.12348318  0.84616375  0.7827866 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 884. State = [[-0.20867099  0.04386336  0.24384755  1.        ]]. Action = [[0.7590332 0.7532017 0.6271008 0.5046387]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 885. State = [[-0.20954536  0.04186571  0.2638185   1.        ]]. Action = [[-0.9262457  -0.94033974  0.49160123  0.6979275 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 886. State = [[-0.22255729  0.02783909  0.29176557  1.        ]]. Action = [[0.04697609 0.00137007 0.83091664 0.7678169 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 887. State = [[-0.2263916   0.01447983  0.31857213  1.        ]]. Action = [[ 0.27079713 -0.5663606   0.6100762   0.43793762]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 888. State = [[-0.21243353  0.00941027  0.34140748  1.        ]]. Action = [[0.8762729  0.39879692 0.49035478 0.7779987 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 889. State = [[-0.19362983  0.00662953  0.36706847  1.        ]]. Action = [[ 0.27442646 -0.5122096   0.5230992   0.70955443]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 890. State = [[-0.18397179  0.00347732  0.39277887  1.        ]]. Action = [[-0.04840165  0.23678374  0.90210867  0.8138615 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 891. State = [[-0.1775506   0.00424233  0.413774    1.        ]]. Action = [[ 0.78853345 -0.52465224  0.72976613  0.6749673 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Current timestep = 892. State = [[-0.17315353 -0.00689609  0.40926814  1.        ]]. Action = [[ 0.95795226 -0.8785491  -0.9175385   0.6769185 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 893. State = [[-0.15282835 -0.02193222  0.39346018  1.        ]]. Action = [[0.31603038 0.41123688 0.6612979  0.8182695 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Current timestep = 894. State = [[-0.14920403 -0.03041776  0.38487574  1.        ]]. Action = [[-0.35295355 -0.3093909  -0.73287195  0.8070967 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 895. State = [[-0.14421712 -0.03556697  0.3656807   1.        ]]. Action = [[ 0.72163737  0.14354086 -0.9749965   0.74846447]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 896. State = [[-0.11960404 -0.03923522  0.34312224  1.        ]]. Action = [[ 0.92262447 -0.3484379   0.5243745   0.5116012 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 897. State = [[-0.09667167 -0.03184199  0.33678007  1.        ]]. Action = [[ 0.63304985  0.9132602  -0.47113836  0.81044483]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 898. State = [[-0.0723912  -0.0189606   0.32167658  1.        ]]. Action = [[ 0.5420475   0.04590011 -0.433182    0.7647083 ]]. Reward = [0.]
Curr episode timestep = 34
Above hoop
Current timestep = 899. State = [[-0.06421939 -0.02364518  0.30310932  1.        ]]. Action = [[-0.8561371  -0.6715608  -0.77775216  0.53500736]]. Reward = [0.]
Curr episode timestep = 35
Above hoop
Current timestep = 900. State = [[-0.07572686 -0.0334739   0.28553215  1.        ]]. Action = [[-2.0862627e-01 -4.7874451e-04 -5.1915759e-01  6.4547801e-01]]. Reward = [0.]
Curr episode timestep = 36
Above hoop
Current timestep = 901. State = [[-0.07456397 -0.04472362  0.2635887   1.        ]]. Action = [[ 0.8464409  -0.52493423 -0.9198121   0.7859218 ]]. Reward = [0.]
Curr episode timestep = 37
Above hoop
Current timestep = 902. State = [[-0.06356018 -0.04590098  0.22902237  1.        ]]. Action = [[ 0.04247522  0.6580925  -0.26277053  0.6892774 ]]. Reward = [0.]
Curr episode timestep = 38
Above hoop
Current timestep = 903. State = [[-0.06114525 -0.03847713  0.2194662   1.        ]]. Action = [[-0.8781486  -0.86824286 -0.47308505 -0.04239827]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: No entry zone
Current timestep = 904. State = [[-0.06089978 -0.03703318  0.21854657  1.        ]]. Action = [[ 0.9199505  -0.8944321   0.27620006  0.1863867 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: No entry zone
Current timestep = 905. State = [[-0.04683684 -0.02360979  0.22786099  1.        ]]. Action = [[0.9539678  0.7749144  0.74235964 0.623634  ]]. Reward = [0.]
Curr episode timestep = 41
Above hoop
Current timestep = 906. State = [[-0.03110225 -0.01052571  0.23998176  1.        ]]. Action = [[ 0.5548439   0.78299344 -0.84925956  0.6495092 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: No entry zone
Above hoop
Current timestep = 907. State = [[-0.0238035   0.00719858  0.24812914  1.        ]]. Action = [[0.10069478 0.8933593  0.49565315 0.6464474 ]]. Reward = [0.]
Curr episode timestep = 43
Above hoop
Current timestep = 908. State = [[-0.02365452  0.01109077  0.2582339   1.        ]]. Action = [[-0.58677846 -0.9171464   0.08077168  0.92010427]]. Reward = [0.]
Curr episode timestep = 44
Above hoop
Current timestep = 909. State = [[-2.5703996e-02  4.5147528e-05  2.6356825e-01  1.0000000e+00]]. Action = [[-0.96090883 -0.50358087 -0.7056917   0.47566986]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: No entry zone
Above hoop
Current timestep = 910. State = [[-0.01923709 -0.00714123  0.27378932  1.        ]]. Action = [[ 0.81727886 -0.35124052  0.8170407   0.50563407]]. Reward = [0.]
Curr episode timestep = 46
Above hoop
Current timestep = 911. State = [[-0.01441496 -0.01930689  0.28168362  1.        ]]. Action = [[-0.16428435 -0.39596218 -0.6366201   0.7324995 ]]. Reward = [0.]
Curr episode timestep = 47
Above hoop
Current timestep = 912. State = [[-0.00942338 -0.04142401  0.2807244   1.        ]]. Action = [[ 0.5305977  -0.8628162   0.29916918  0.62096953]]. Reward = [0.]
Curr episode timestep = 48
Above hoop
Current timestep = 913. State = [[-0.00517567 -0.05001789  0.27356172  1.        ]]. Action = [[-0.12870526  0.718434   -0.48440439  0.76696515]]. Reward = [0.]
Curr episode timestep = 49
Above hoop
Current timestep = 914. State = [[ 0.00675546 -0.03130057  0.26230153  1.        ]]. Action = [[ 0.7056799   0.69077456 -0.21187937  0.86031175]]. Reward = [0.]
Curr episode timestep = 50
Above hoop
Current timestep = 915. State = [[ 0.0240132  -0.02015365  0.2631812   1.        ]]. Action = [[-0.04761016 -0.23586476  0.64163303  0.6102226 ]]. Reward = [0.]
Curr episode timestep = 51
Above hoop
Current timestep = 916. State = [[ 0.02479173 -0.02477061  0.2784144   1.        ]]. Action = [[-0.37923276 -0.22208488  0.5192046   0.81812835]]. Reward = [0.]
Curr episode timestep = 52
Above hoop
Current timestep = 917. State = [[ 0.01890216 -0.0234528   0.28105953  1.        ]]. Action = [[-0.16368502  0.37165105 -0.85287976  0.74000573]]. Reward = [0.]
Curr episode timestep = 53
Above hoop
Current timestep = 918. State = [[ 0.01803733 -0.00861742  0.26791197  1.        ]]. Action = [[ 0.3515402   0.57113767 -0.2156198   0.75930214]]. Reward = [0.]
Curr episode timestep = 54
Above hoop
Current timestep = 919. State = [[0.02193746 0.00176543 0.26226348 1.        ]]. Action = [[ 0.9210738  -0.823525   -0.7496026   0.11189556]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: No entry zone
Above hoop
Current timestep = 920. State = [[0.02469358 0.01356123 0.259651   1.        ]]. Action = [[ 0.5210502   0.5947137  -0.44681036  0.6825141 ]]. Reward = [0.]
Curr episode timestep = 56
Above hoop
Current timestep = 921. State = [[0.0427273  0.02096994 0.25691977 1.        ]]. Action = [[ 0.38491058 -0.35769796  0.9088056   0.5491288 ]]. Reward = [0.]
Curr episode timestep = 57
Above hoop
Current timestep = 922. State = [[0.04944215 0.02057595 0.2698025  1.        ]]. Action = [[ 0.43820202 -0.251436   -0.5745342   0.72866046]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: No entry zone
Above hoop
Current timestep = 923. State = [[0.05706757 0.01652251 0.28456694 1.        ]]. Action = [[ 0.60944843 -0.19351727  0.8633745   0.8190421 ]]. Reward = [0.]
Curr episode timestep = 59
Above hoop
Current timestep = 924. State = [[0.0668574  0.02414566 0.29220414 1.        ]]. Action = [[-0.37016636  0.72744083 -0.64376086  0.5312128 ]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 925. State = [[0.06418411 0.04605573 0.2840646  1.        ]]. Action = [[-0.48519695  0.7534528  -0.11609429  0.48729777]]. Reward = [0.]
Curr episode timestep = 61
Above hoop
Current timestep = 926. State = [[0.05181631 0.0764354  0.27560183 1.        ]]. Action = [[-0.5636732   0.7517936  -0.72096     0.83361053]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 927. State = [[0.04639032 0.08230144 0.2509562  1.        ]]. Action = [[ 0.8167269  -0.7867211  -0.69577205  0.45690942]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 928. State = [[0.06206256 0.08663209 0.2359838  1.        ]]. Action = [[0.64317584 0.8907354  0.62931705 0.80107045]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 929. State = [[0.07247657 0.09950692 0.24307416 1.        ]]. Action = [[ 0.02384865 -0.09839326 -0.93141115  0.7417252 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 930. State = [[0.07601228 0.1020001  0.24311176 1.        ]]. Action = [[ 0.8121741  -0.8399512  -0.75335604  0.53479993]]. Reward = [0.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 931. State = [[0.08015885 0.09158429 0.24965173 1.        ]]. Action = [[ 0.08109832 -0.690547    0.3444178   0.8360071 ]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 932. State = [[0.07812757 0.07471015 0.25575432 1.        ]]. Action = [[-0.8867741  -0.3439623  -0.1256414   0.56109416]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 933. State = [[0.07201777 0.06548601 0.25689924 1.        ]]. Action = [[ 0.52161     0.4417447  -0.88132626  0.6068406 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 934. State = [[0.06179018 0.05861519 0.26843238 1.        ]]. Action = [[-0.97983    -0.25224328  0.7692814   0.57895744]]. Reward = [0.]
Curr episode timestep = 70
Above hoop
Current timestep = 935. State = [[0.04071054 0.05608375 0.28293094 1.        ]]. Action = [[0.8901696  0.13213158 0.09708536 0.51080513]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Above hoop
Current timestep = 936. State = [[0.02511112 0.03566812 0.30066782 1.        ]]. Action = [[-0.9372394  -0.98880917  0.7393155   0.02600777]]. Reward = [0.]
Curr episode timestep = 72
Above hoop
Current timestep = 937. State = [[-0.00708054  0.01208163  0.33108103  1.        ]]. Action = [[-0.90638864 -0.33350617  0.9636737   0.8041531 ]]. Reward = [0.]
Curr episode timestep = 73
Above hoop
Current timestep = 938. State = [[-0.03812566  0.00202771  0.3540293   1.        ]]. Action = [[-0.15813726 -0.07990295  0.0245769   0.89240336]]. Reward = [0.]
Curr episode timestep = 74
Above hoop
Current timestep = 939. State = [[-0.03995996  0.00582425  0.3696511   1.        ]]. Action = [[0.8823348  0.3811772  0.98936605 0.70431435]]. Reward = [0.]
Curr episode timestep = 75
Above hoop
Current timestep = 940. State = [[-0.04221439  0.02058934  0.396013    1.        ]]. Action = [[-0.65871096  0.52831364  0.5582228   0.54373455]]. Reward = [0.]
Curr episode timestep = 76
Above hoop
Current timestep = 941. State = [[-0.0458853   0.0381206   0.40693495  1.        ]]. Action = [[ 0.56720495  0.51305926 -0.55867594  0.6682869 ]]. Reward = [0.]
Curr episode timestep = 77
Above hoop
Current timestep = 942. State = [[-0.04324641  0.06187471  0.403487    1.        ]]. Action = [[-0.603054    0.82951117  0.17039561  0.888257  ]]. Reward = [0.]
Curr episode timestep = 78
Above hoop
Current timestep = 943. State = [[-0.05084473  0.07720006  0.40283057  1.        ]]. Action = [[ 0.10094321 -0.24016118 -0.17201316  0.38855314]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 944. State = [[-0.05829811  0.07092373  0.401438    1.        ]]. Action = [[-0.7557813  -0.35663223 -0.06603366  0.85453033]]. Reward = [0.]
Curr episode timestep = 80
Above hoop
Current timestep = 945. State = [[-0.0682702   0.05286501  0.38816106  1.        ]]. Action = [[ 0.5684359  -0.8122562  -0.992653    0.76813245]]. Reward = [0.]
Curr episode timestep = 81
Above hoop
Current timestep = 946. State = [[-0.05545459  0.03177429  0.36499342  1.        ]]. Action = [[ 0.9131831  -0.4480613  -0.75523823  0.7696066 ]]. Reward = [0.]
Curr episode timestep = 82
Above hoop
Current timestep = 947. State = [[-0.03006464  0.03270533  0.34504667  1.        ]]. Action = [[0.26972342 0.9106748  0.5727825  0.7726636 ]]. Reward = [0.]
Curr episode timestep = 83
Above hoop
Current timestep = 948. State = [[-0.02679706  0.05572677  0.35265148  1.        ]]. Action = [[-0.74828327  0.620795    0.3099246   0.46195722]]. Reward = [0.]
Curr episode timestep = 84
Above hoop
Current timestep = 949. State = [[-0.03272639  0.07780869  0.36958316  1.        ]]. Action = [[0.33403277 0.4372095  0.8299049  0.44486487]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 950. State = [[-0.03315348  0.08062217  0.3940599   1.        ]]. Action = [[ 0.10530949 -0.41929853  0.71793866  0.567281  ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 951. State = [[-0.02253742  0.07677648  0.40835142  1.        ]]. Action = [[ 0.9275944  -0.04684806 -0.01891792  0.49469924]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 952. State = [[-0.00330283  0.07344202  0.41363952  1.        ]]. Action = [[-0.1898737  -0.18380648  0.8715799   0.8590245 ]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Above hoop
Current timestep = 953. State = [[-0.00371509  0.06419843  0.4098868   1.        ]]. Action = [[-0.46023196 -0.6229679  -0.61777997  0.46306646]]. Reward = [0.]
Curr episode timestep = 89
Above hoop
Current timestep = 954. State = [[3.2348567e-04 6.7257963e-02 3.9570466e-01 1.0000000e+00]]. Action = [[ 0.94298506  0.76007366 -0.95892966  0.91586864]]. Reward = [0.]
Curr episode timestep = 90
Above hoop
Current timestep = 955. State = [[0.01094955 0.06390599 0.35972914 1.        ]]. Action = [[-0.70921755 -0.8049346  -0.49325907  0.88994634]]. Reward = [0.]
Curr episode timestep = 91
Above hoop
Current timestep = 956. State = [[0.01433646 0.05927517 0.33705455 1.        ]]. Action = [[ 0.94029176  0.37501442 -0.92995536  0.84915876]]. Reward = [0.]
Curr episode timestep = 92
Above hoop
Current timestep = 957. State = [[0.03658067 0.05499501 0.30025116 1.        ]]. Action = [[ 0.95103884 -0.6044275  -0.35960543  0.4248531 ]]. Reward = [0.]
Curr episode timestep = 93
Above hoop
Current timestep = 958. State = [[0.05892828 0.03064607 0.30055565 1.        ]]. Action = [[-0.60139716 -0.55240107  0.9018128   0.7496773 ]]. Reward = [0.]
Curr episode timestep = 94
Above hoop
Current timestep = 959. State = [[0.05347875 0.00723203 0.3080614  1.        ]]. Action = [[ 0.12648284 -0.92515147 -0.32279688  0.5810051 ]]. Reward = [0.]
Curr episode timestep = 95
Above hoop
Current timestep = 960. State = [[ 0.06001589 -0.00959251  0.30521315  1.        ]]. Action = [[ 0.3096509   0.21828616 -0.16773355  0.6887133 ]]. Reward = [0.]
Curr episode timestep = 96
Above hoop
Current timestep = 961. State = [[0.06546326 0.0060504  0.29540804 1.        ]]. Action = [[ 0.21943927  0.9385624  -0.8153591   0.3399197 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 962. State = [[0.07066098 0.03395074 0.28458717 1.        ]]. Action = [[-0.68567306  0.95056343  0.58325434  0.86447144]]. Reward = [0.]
Curr episode timestep = 98
Current timestep = 963. State = [[0.05331614 0.06186592 0.281605   1.        ]]. Action = [[-0.6643952   0.23116887 -0.8788757   0.56642914]]. Reward = [0.]
Curr episode timestep = 99
Above hoop
Current timestep = 964. State = [[0.03158681 0.08240001 0.2730051  1.        ]]. Action = [[-0.8220817   0.76084995  0.394495    0.19379687]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 965. State = [[-0.25285476  0.00240531  0.23186713  1.        ]]. Action = [[0.5034126  0.4182272  0.984416   0.62056994]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 966. State = [[-0.2428507  -0.0104593   0.20786642  1.        ]]. Action = [[ 0.5242555 -0.7743569 -0.936567   0.8015144]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 967. State = [[-0.21897551 -0.03706978  0.17227253  1.        ]]. Action = [[ 0.84618807 -0.82176095 -0.53021276  0.53041863]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 968. State = [[-0.20813443 -0.04900071  0.14825091  1.        ]]. Action = [[-0.87218136  0.40977383 -0.6099429   0.7089398 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 969. State = [[-0.22990131 -0.03819355  0.13454291  1.        ]]. Action = [[-0.98713523  0.6432011   0.32729888  0.8631408 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 970. State = [[-0.2376277  -0.0375467   0.12441366  1.        ]]. Action = [[ 0.92353666 -0.8405328  -0.93777436  0.8392755 ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 971. State = [[-0.22980365 -0.0628577   0.11743619  1.        ]]. Action = [[ 0.0597223 -0.8500529  0.8535416  0.8582258]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 972. State = [[-0.2183632  -0.08853285  0.11673028  1.        ]]. Action = [[ 0.7812154  -0.6047628  -0.95460594  0.6432272 ]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 973. State = [[-0.1967366  -0.09538941  0.10107111  1.        ]]. Action = [[ 0.46425092  0.6196003  -0.28176296  0.7818892 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 974. State = [[-0.18781649 -0.08149321  0.093788    1.        ]]. Action = [[-0.24081153  0.5093038   0.0498122   0.46697843]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 975. State = [[-0.18895525 -0.0732477   0.09338613  1.        ]]. Action = [[ 0.82529783 -0.29875755 -0.4404202   0.781319  ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: No entry zone
Current timestep = 976. State = [[-0.19281635 -0.07627186  0.09854418  1.        ]]. Action = [[-0.57378614 -0.31326258  0.7237303   0.5913769 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 977. State = [[-0.20514864 -0.07711815  0.0993577   1.        ]]. Action = [[-0.36280775  0.1468873  -0.7664563   0.67827296]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 978. State = [[-0.2206752  -0.0826762   0.08625477  1.        ]]. Action = [[-0.7642258  -0.4876976  -0.54693866  0.5991409 ]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 979. State = [[-0.23537727 -0.0814011   0.07558051  1.        ]]. Action = [[0.39848363 0.5217246  0.20775056 0.6341131 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 980. State = [[-0.23799595 -0.06984174  0.06667879  1.        ]]. Action = [[-0.21142864  0.41141176 -0.8722088   0.691561  ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 981. State = [[-0.24297087 -0.06176015  0.0488433   1.        ]]. Action = [[ 0.31183112 -0.7151412  -0.914453    0.6437361 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Current timestep = 982. State = [[-0.24166095 -0.06597499  0.04540786  1.        ]]. Action = [[ 0.19189596 -0.5304915  -0.1996361   0.54891896]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 983. State = [[-0.24163078 -0.07072393  0.04136614  1.        ]]. Action = [[-0.8632397  -0.8433774   0.30239058  0.23976159]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Current timestep = 984. State = [[-0.24157818 -0.07101789  0.04115226  1.        ]]. Action = [[-0.5388512  -0.17937177 -0.801433    0.542665  ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 985. State = [[-0.23080401 -0.0704459   0.04370977  1.        ]]. Action = [[0.83768594 0.20665884 0.47864938 0.53242326]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 986. State = [[-0.21834867 -0.07078824  0.04596696  1.        ]]. Action = [[-0.37991428  0.7819667  -0.9145608   0.44788957]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Current timestep = 987. State = [[-0.22233784 -0.08435996  0.04752129  1.        ]]. Action = [[-0.81504565 -0.89544064  0.23950303  0.6576998 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 988. State = [[-0.22567645 -0.09317566  0.05481931  1.        ]]. Action = [[0.41017044 0.31613922 0.43171823 0.5920963 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 989. State = [[-0.22912645 -0.10388762  0.07294037  1.        ]]. Action = [[-0.7979579  -0.6974101   0.69460714  0.4671365 ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 990. State = [[-0.23694706 -0.12801914  0.09392507  1.        ]]. Action = [[ 0.48088443 -0.9033592   0.24728775  0.68788123]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 991. State = [[-0.23534295 -0.1457497   0.1013599   1.        ]]. Action = [[-0.74505496 -0.7164418   0.7227725   0.5749564 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Current timestep = 992. State = [[-0.23882069 -0.14552772  0.09861398  1.        ]]. Action = [[-0.3713361   0.35944176 -0.54203475  0.71830976]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 993. State = [[-0.23328702 -0.1472906   0.0934777   1.        ]]. Action = [[ 0.85919464 -0.39705735 -0.32951784  0.6652787 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 994. State = [[-0.21091361 -0.15938999  0.07845493  1.        ]]. Action = [[ 0.87510586 -0.5969854  -0.9124376   0.6249304 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 995. State = [[-0.1925083  -0.160954    0.05369326  1.        ]]. Action = [[-0.09760612  0.81098294 -0.10694224  0.7470722 ]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 996. State = [[-0.17647254 -0.16403443  0.0503878   1.        ]]. Action = [[ 0.93926334 -0.9609312  -0.04782271  0.75269353]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 997. State = [[-0.14607643 -0.18351877  0.05499695  1.        ]]. Action = [[ 0.9462458  -0.4919014   0.9828942   0.42913842]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 998. State = [[-0.11057911 -0.20614152  0.07459167  1.        ]]. Action = [[ 0.763937   -0.616749    0.21974945  0.41775393]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 999. State = [[-0.09907103 -0.21749915  0.07864527  1.        ]]. Action = [[-0.6918952   0.23385978 -0.5019993   0.73372483]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1000. State = [[-0.09126651 -0.2263791   0.08142866  1.        ]]. Action = [[ 0.9620478  -0.6336705   0.68192124  0.40023172]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1001. State = [[-0.06743187 -0.22967964  0.08762204  1.        ]]. Action = [[ 0.83310366  0.3813082  -0.37171936  0.68091846]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1002. State = [[-0.03874929 -0.22755927  0.0789524   1.        ]]. Action = [[ 0.9919909   0.03762197 -0.29059124  0.80362105]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 1002 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1002 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1002 of -1
Current timestep = 1003. State = [[ 0.00517196 -0.22886527  0.0633919   1.        ]]. Action = [[ 0.9747639  -0.08490461 -0.60041696  0.05186391]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 1003 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1003 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1003 of -1
Current timestep = 1004. State = [[ 0.03556078 -0.22700037  0.04563188  1.        ]]. Action = [[-0.5957192   0.47658014  0.2936449   0.78929806]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 1004 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1004 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1004 of -1
Current timestep = 1005. State = [[ 0.03809305 -0.2314549   0.06149068  1.        ]]. Action = [[ 0.6253638  -0.7586396   0.80844355  0.6900637 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 1005 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1005 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1005 of 0
Current timestep = 1006. State = [[ 0.04701295 -0.24786998  0.09314999  1.        ]]. Action = [[-0.8213949   0.09775698  0.8717396   0.6775601 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 1006 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1006 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[ 0.0404533  -0.23760946  0.10376614  1.        ]]. Action = [[ 0.45479465  0.6175816  -0.97076756  0.7426686 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 1007 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1007 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1007 of -1
Current timestep = 1008. State = [[ 0.0411628  -0.23080127  0.08491389  1.        ]]. Action = [[ 0.23172832 -0.7054139  -0.24061102  0.6609225 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 1008 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 1008 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1008 of -1
Current timestep = 1009. State = [[ 0.03807245 -0.24648806  0.08562517  1.        ]]. Action = [[-0.93733734 -0.08745456  0.62855864  0.61261725]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1010. State = [[ 0.04384516 -0.24401183  0.10193206  1.        ]]. Action = [[0.88430357 0.5926454  0.7529558  0.50553083]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1011. State = [[ 0.04938847 -0.22210559  0.1220134   1.        ]]. Action = [[-0.50031674  0.6664617   0.29540122  0.6271455 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1012. State = [[ 0.03927259 -0.21243042  0.14401092  1.        ]]. Action = [[-0.90313214 -0.08262616  0.7562063   0.70836127]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1013. State = [[ 0.01530402 -0.20496024  0.16123378  1.        ]]. Action = [[ 0.00672567  0.1422658  -0.1871438   0.42729485]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1014. State = [[ 0.01228711 -0.2026321   0.16232497  1.        ]]. Action = [[-0.80149996  0.8178772   0.5921862   0.6370847 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: No entry zone
Current timestep = 1015. State = [[ 0.00242983 -0.21775469  0.16539319  1.        ]]. Action = [[-0.54452795 -0.9284718   0.12905872  0.36890757]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1016. State = [[-0.00912934 -0.23434573  0.17061642  1.        ]]. Action = [[0.36270225 0.8564222  0.40377533 0.4107362 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: No entry zone
Current timestep = 1017. State = [[-0.01793231 -0.25349465  0.17860703  1.        ]]. Action = [[-0.27707076 -0.8933209   0.39162993  0.6635952 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1018. State = [[-0.03001961 -0.2615815   0.17664848  1.        ]]. Action = [[-0.15024132  0.5092281  -0.9849057   0.5725186 ]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1019. State = [[-0.03469548 -0.25877792  0.16411984  1.        ]]. Action = [[-0.00708383 -0.13027751  0.06611085  0.79109573]]. Reward = [0.]
Curr episode timestep = 53
Current timestep = 1020. State = [[-0.04480615 -0.27301225  0.15748431  1.        ]]. Action = [[-0.3563347 -0.8826909 -0.4954661  0.897593 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1021. State = [[-0.05467928 -0.2839158   0.14648521  1.        ]]. Action = [[ 0.04772925  0.28471196 -0.2611289   0.8128536 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1022. State = [[-0.05555345 -0.2742033   0.13673644  1.        ]]. Action = [[-0.27373445  0.7509688   0.13870871  0.6209656 ]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1023. State = [[-0.0548877  -0.25928542  0.13657616  1.        ]]. Action = [[ 0.49763083 -0.10558116  0.30785203  0.6380898 ]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1024. State = [[-0.0433663  -0.26076663  0.14552525  1.        ]]. Action = [[ 0.69563305 -0.40626287  0.44353497  0.8367305 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1025. State = [[-0.03273591 -0.2585546   0.14615828  1.        ]]. Action = [[ 0.26041543  0.37927723 -0.67524654  0.59728146]]. Reward = [0.]
Curr episode timestep = 59
Current timestep = 1026. State = [[-0.03265198 -0.24603878  0.12877946  1.        ]]. Action = [[-0.66303974  0.6499318  -0.73544794  0.48755956]]. Reward = [0.]
Curr episode timestep = 60
Current timestep = 1027. State = [[-0.0253287  -0.23875192  0.12397789  1.        ]]. Action = [[ 0.8689375  -0.30975455  0.9096739   0.65743876]]. Reward = [0.]
Curr episode timestep = 61
Current timestep = 1028. State = [[-0.02167189 -0.2475965   0.14013292  1.        ]]. Action = [[-0.86771667 -0.3211763   0.6276021   0.6303611 ]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1029. State = [[-0.03462842 -0.2433594   0.15535372  1.        ]]. Action = [[-0.7178847   0.9529078   0.13681257  0.76236844]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1030. State = [[-0.04589494 -0.20857693  0.16449475  1.        ]]. Action = [[0.2835517  0.98643255 0.2178998  0.78966963]]. Reward = [0.]
Curr episode timestep = 64
Current timestep = 1031. State = [[-0.04404985 -0.18588999  0.17233974  1.        ]]. Action = [[ 0.6704216   0.8059442   0.93199134 -0.09945774]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: No entry zone
Current timestep = 1032. State = [[-0.04147754 -0.19105439  0.1740139   1.        ]]. Action = [[ 0.72984767 -0.77731943 -0.23016882  0.76834786]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1033. State = [[-0.03824779 -0.20769955  0.16504537  1.        ]]. Action = [[ 0.34819126 -0.54053086 -0.60963345  0.50942945]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1034. State = [[-0.02271869 -0.23102435  0.14666183  1.        ]]. Action = [[ 0.64206576 -0.8133847  -0.25025642  0.825649  ]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1035. State = [[ 0.0037212  -0.23956068  0.12763336  1.        ]]. Action = [[ 0.8808441   0.48410547 -0.9517946   0.5891497 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1036. State = [[ 0.03136249 -0.23115033  0.0896813   1.        ]]. Action = [[ 0.71384025  0.07321739 -0.9263454   0.5173633 ]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1037. State = [[ 0.0643701  -0.24253882  0.07606939  1.        ]]. Action = [[ 0.8437412  -0.84036046  0.8800931   0.87333155]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1037 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 1037 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1037 of -1
Current timestep = 1038. State = [[ 0.09028288 -0.2659019   0.09858528  1.        ]]. Action = [[-0.84599113  0.19435275  0.62856793  0.49676073]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 1038 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 1038 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1038 of 1
Current timestep = 1039. State = [[ 0.08383618 -0.2689355   0.11335663  1.        ]]. Action = [[ 0.7134671   0.22256601 -0.69940615  0.52218604]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Current timestep = 1040. State = [[ 0.07663213 -0.2663259   0.12529829  1.        ]]. Action = [[-0.9732233   0.42051017  0.82410693  0.9312098 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1041. State = [[ 0.05407924 -0.25357622  0.14211245  1.        ]]. Action = [[-0.9425604  -0.93485045 -0.39311635  0.57352614]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Current timestep = 1042. State = [[ 0.04157247 -0.24142812  0.1590183   1.        ]]. Action = [[-0.7195898   0.761127    0.8305596   0.59951854]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1043. State = [[ 0.01611018 -0.21560921  0.17044173  1.        ]]. Action = [[-0.5825607  0.5425873 -0.491956   0.58078  ]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1044. State = [[ 0.00667983 -0.19762364  0.16925855  1.        ]]. Action = [[0.88958085 0.02741659 0.12973154 0.59689856]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1045. State = [[ 0.01165705 -0.19385028  0.17049459  1.        ]]. Action = [[ 0.3562771   0.43355978 -0.9779823   0.35563135]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: No entry zone
Current timestep = 1046. State = [[ 0.01272518 -0.19298396  0.17083734  1.        ]]. Action = [[ 0.6395793  0.6669769 -0.5641891  0.5200504]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 1047. State = [[ 0.01325487 -0.19258437  0.17095557  1.        ]]. Action = [[ 0.10126352  0.7939544  -0.8163046   0.88826776]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 1048. State = [[ 0.01331043 -0.19254643  0.17097366  1.        ]]. Action = [[0.58013546 0.49907327 0.55385566 0.7199782 ]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: No entry zone
Current timestep = 1049. State = [[ 0.02129334 -0.19155392  0.1833195   1.        ]]. Action = [[ 0.5411445  -0.04232281  0.9151151   0.6486161 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1050. State = [[ 0.02846986 -0.18959615  0.19960216  1.        ]]. Action = [[-0.01323956  0.8429997  -0.5853163   0.7584499 ]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: No entry zone
Current timestep = 1051. State = [[ 0.02784445 -0.200522    0.19310154  1.        ]]. Action = [[ 0.1988014  -0.81925577 -0.88850266  0.57852983]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1052. State = [[ 0.04382426 -0.22653762  0.16547236  1.        ]]. Action = [[ 0.7695428  -0.61558837 -0.9232347   0.7224933 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1053. State = [[ 0.06921352 -0.24327871  0.14414673  1.        ]]. Action = [[0.2593639  0.03096473 0.33154476 0.66857517]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1054. State = [[ 0.07578122 -0.2437163   0.14383738  1.        ]]. Action = [[ 0.62279105  0.74928546 -0.13760936  0.69420993]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Current timestep = 1055. State = [[ 0.07312859 -0.23946752  0.15699154  1.        ]]. Action = [[-0.8624201   0.56869316  0.8711686   0.7798674 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1056. State = [[ 0.05753471 -0.24506383  0.1760139   1.        ]]. Action = [[-0.34846854 -0.82048386  0.15018964  0.7818512 ]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1057. State = [[ 0.04046627 -0.2504299   0.18327723  1.        ]]. Action = [[-0.66259634  0.55743694  0.08346581  0.7698704 ]]. Reward = [0.]
Curr episode timestep = 91
Current timestep = 1058. State = [[ 0.03402839 -0.23375976  0.18651909  1.        ]]. Action = [[ 0.7722875   0.3898871  -0.08545613  0.4683932 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1059. State = [[ 0.04958285 -0.21479675  0.18789445  1.        ]]. Action = [[0.76546633 0.44904184 0.12550676 0.7021539 ]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1060. State = [[ 0.06912076 -0.19638604  0.19834635  1.        ]]. Action = [[0.37006712 0.4705093  0.62188864 0.69356513]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1061. State = [[ 0.07569342 -0.18434367  0.21740021  1.        ]]. Action = [[-0.53135735  0.21908855  0.4038198   0.47624588]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1062. State = [[ 0.07471541 -0.18090837  0.22769634  1.        ]]. Action = [[-0.22320545  0.4723407  -0.4897871   0.65323234]]. Reward = [0.]
Curr episode timestep = 96
Action ignored: No entry zone
Current timestep = 1063. State = [[ 0.07463048 -0.1805673   0.2277692   1.        ]]. Action = [[0.92592645 0.8690598  0.263646   0.85612345]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Current timestep = 1064. State = [[ 0.07463048 -0.1805673   0.2277692   1.        ]]. Action = [[0.63607883 0.05899858 0.8690071  0.7934761 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Current timestep = 1065. State = [[ 0.07463048 -0.1805673   0.2277692   1.        ]]. Action = [[-0.39252847  0.24341726 -0.4373501   0.47270906]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: No entry zone
Current timestep = 1066. State = [[ 0.07463048 -0.1805673   0.2277692   1.        ]]. Action = [[ 0.97237575  0.8451147  -0.64764     0.8256638 ]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Action ignored: No entry zone
Current timestep = 1067. State = [[-0.25080475  0.00229979  0.23252709  1.        ]]. Action = [[ 0.65867853 -0.30335426 -0.61529255  0.5125835 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Current timestep = 1068. State = [[-0.24652734 -0.00136124  0.22406621  1.        ]]. Action = [[0.0702579 0.1748116 0.6251025 0.6722616]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1069. State = [[-0.23361436 -0.01542481  0.22313261  1.        ]]. Action = [[ 0.9409466  -0.9160437  -0.41741514  0.8316324 ]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1070. State = [[-0.20334369 -0.03151796  0.22269812  1.        ]]. Action = [[ 0.84424496 -0.03653681  0.46710408  0.7140398 ]]. Reward = [0.]
Curr episode timestep = 2
Current timestep = 1071. State = [[-0.18877071 -0.034602    0.22619347  1.        ]]. Action = [[-0.29823923  0.08621359 -0.28712344  0.8717787 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1072. State = [[-0.19147733 -0.01938792  0.213228    1.        ]]. Action = [[ 0.12896931  0.93742645 -0.7737784   0.72533846]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1073. State = [[-1.8390627e-01 -9.6256932e-04  2.0196331e-01  1.0000000e+00]]. Action = [[-0.00114459  0.23961508  0.42434764  0.72592807]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1074. State = [[-0.1842924   0.01588502  0.20703939  1.        ]]. Action = [[-0.04486495  0.58560896  0.40546727  0.15875995]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1075. State = [[-0.19438799  0.04042688  0.20414501  1.        ]]. Action = [[-0.71889746  0.73914814 -0.8674497   0.5275723 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1076. State = [[-0.19393167  0.06880329  0.20471905  1.        ]]. Action = [[0.9787774 0.7747388 0.8049209 0.7655287]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1077. State = [[-0.16827871  0.07652433  0.22546698  1.        ]]. Action = [[ 0.8981949 -0.6085847  0.7404058  0.5791409]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1078. State = [[-0.14718898  0.06959731  0.24458475  1.        ]]. Action = [[ 0.89326835 -0.7883248  -0.83978826  0.47217607]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 1079. State = [[-0.14794789  0.06804131  0.24894506  1.        ]]. Action = [[-0.8171611  -0.05346757  0.04855406  0.8976048 ]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1080. State = [[-0.16006376  0.0830055   0.2621005   1.        ]]. Action = [[-0.22329426  0.92119443  0.96414757  0.65522385]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1081. State = [[-0.17929395  0.09508773  0.275142    1.        ]]. Action = [[-0.92855066 -0.3411165  -0.41550088  0.33811975]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1082. State = [[-0.20280252  0.10740367  0.26887533  1.        ]]. Action = [[-0.49242765  0.89014244 -0.4994334   0.7190143 ]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1083. State = [[-0.22422501  0.13386448  0.2707893   1.        ]]. Action = [[-0.5787129   0.55241275  0.61251855  0.4316795 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 1083 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 1083 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Human Feedback received at timestep 1083 of -1
Current timestep = 1084. State = [[-0.24830145  0.15047543  0.2701471   1.        ]]. Action = [[-0.25703323  0.16600573 -0.938248    0.4971149 ]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1085. State = [[-0.24767208  0.15989608  0.2619256   1.        ]]. Action = [[0.60790753 0.4999473  0.6341722  0.7770374 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1086. State = [[-0.24401452  0.16746815  0.26638544  1.        ]]. Action = [[-0.732675   -0.9526753   0.20839274  0.4731927 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Current timestep = 1087. State = [[-0.24401376  0.16783033  0.2665437   1.        ]]. Action = [[-0.7054703  -0.41467607  0.630224    0.5441394 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Current timestep = 1088. State = [[-0.23071541  0.17067693  0.2763788   1.        ]]. Action = [[0.87376773 0.05078125 0.5694237  0.67873394]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1089. State = [[-0.21793194  0.18393159  0.28137773  1.        ]]. Action = [[-0.02916455  0.71962786 -0.3488503   0.6117642 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1090. State = [[-0.21573737  0.18707508  0.2704317   1.        ]]. Action = [[-0.6048203  -0.7850594  -0.51742756  0.7478857 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1091. State = [[-0.21581107  0.17383578  0.2724408   1.        ]]. Action = [[ 0.58408785 -0.19242907  0.80967665  0.16486454]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1092. State = [[-0.1990882   0.15349045  0.29339007  1.        ]]. Action = [[ 0.7267852  -0.8760965   0.98164034  0.49593163]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1093. State = [[-0.19330394  0.13792926  0.32469445  1.        ]]. Action = [[-0.7469205   0.04110253  0.49986982  0.6851208 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1094. State = [[-0.19781446  0.11840924  0.34522158  1.        ]]. Action = [[ 0.32777262 -0.97942024  0.41672456  0.65908885]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1095. State = [[-0.20297013  0.09454525  0.37372872  1.        ]]. Action = [[-0.91164416 -0.41681045  0.9728539   0.6059494 ]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1096. State = [[-0.22303258  0.09311122  0.39454818  1.        ]]. Action = [[-0.33849466  0.53276014 -0.29492724  0.6483145 ]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1097. State = [[-0.24060231  0.09780012  0.39340988  1.        ]]. Action = [[-0.85011303 -0.29353023 -0.29896128  0.86167634]]. Reward = [0.]
Curr episode timestep = 29
Current timestep = 1098. State = [[-0.26109484  0.093195    0.38981682  1.        ]]. Action = [[0.6935637  0.70259    0.6175568  0.55274403]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Current timestep = 1099. State = [[-0.2627951   0.09212871  0.3903107   1.        ]]. Action = [[-0.32031548 -0.28388882 -0.41306877  0.7905617 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Current timestep = 1100. State = [[-0.25796604  0.082091    0.39258203  1.        ]]. Action = [[ 0.69209504 -0.5623143   0.03206718  0.46792555]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1101. State = [[-0.25240082  0.07413122  0.3962919   1.        ]]. Action = [[-0.567188   -0.5184029   0.97750664  0.56844854]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Current timestep = 1102. State = [[-0.2507035   0.07213211  0.39588845  1.        ]]. Action = [[0.33846843 0.08552527 0.10965002 0.5123687 ]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1103. State = [[-0.24596688  0.07245387  0.3940123   1.        ]]. Action = [[-0.25770295 -0.48947805  0.6730108   0.70489955]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Current timestep = 1104. State = [[-0.24493904  0.07629328  0.3900976   1.        ]]. Action = [[ 0.07483745  0.39264262 -0.27998316  0.40297234]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1105. State = [[-0.24040434  0.08084492  0.3826577   1.        ]]. Action = [[ 0.41859686 -0.74898493  0.7835424   0.5123652 ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 1106. State = [[-0.23167525  0.08705533  0.3827886   1.        ]]. Action = [[ 0.8094424   0.3613932  -0.04282135  0.38608682]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1107. State = [[-0.20385838  0.08876291  0.3796574   1.        ]]. Action = [[ 0.97715807 -0.53570604  0.02760327  0.68131185]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1108. State = [[-0.1794566   0.08099333  0.3764331   1.        ]]. Action = [[0.4241972  0.74050117 0.83348835 0.48458076]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Current timestep = 1109. State = [[-0.17790882  0.07964621  0.3773293   1.        ]]. Action = [[ 0.38239002 -0.22189707  0.82602096  0.5629238 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Current timestep = 1110. State = [[-0.16865745  0.0691911   0.37021297  1.        ]]. Action = [[ 0.8835081  -0.73135257 -0.7500688   0.7721102 ]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1111. State = [[-0.14212482  0.04717539  0.34268597  1.        ]]. Action = [[ 0.07957375 -0.2815143  -0.7240835   0.76192224]]. Reward = [0.]
Curr episode timestep = 43
Current timestep = 1112. State = [[-0.13243498  0.04696184  0.3194106   1.        ]]. Action = [[ 0.33229518  0.44862676 -0.6509856   0.7725382 ]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1113. State = [[-0.11684032  0.05554806  0.29320195  1.        ]]. Action = [[ 0.78661776  0.21126723 -0.97363186  0.76442313]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1114. State = [[-0.10261746  0.04956076  0.25712633  1.        ]]. Action = [[-0.88368696 -0.88699025 -0.95229495  0.5542488 ]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1115. State = [[-0.10705318  0.04252502  0.24446231  1.        ]]. Action = [[0.36027718 0.546612   0.7757962  0.6925638 ]]. Reward = [0.]
Curr episode timestep = 47
Current timestep = 1116. State = [[-0.1073737   0.03166149  0.2564835   1.        ]]. Action = [[-0.41214067 -0.9343377   0.4942484   0.74002385]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1117. State = [[-0.11149983  0.00227601  0.27828082  1.        ]]. Action = [[-0.04665744 -0.9565203   0.7696935   0.4895941 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1118. State = [[-0.11836366 -0.01604013  0.28670493  1.        ]]. Action = [[ 0.11849189  0.12708998 -0.8342994   0.70829296]]. Reward = [0.]
Curr episode timestep = 50
Current timestep = 1119. State = [[-0.11746807 -0.01389619  0.28347525  1.        ]]. Action = [[0.06086493 0.2755611  0.5517845  0.7908815 ]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1120. State = [[-0.25072733  0.00246466  0.23266482  1.        ]]. Action = [[ 0.743978   -0.6113269  -0.8722963  -0.11115348]]. Reward = [0.]
Curr episode timestep = 52
Current timestep = 1121. State = [[-0.24870001  0.00318579  0.21886471  1.        ]]. Action = [[ 0.47868192 -0.06430799  0.14049852  0.8803067 ]]. Reward = [0.]
Curr episode timestep = 0
Current timestep = 1122. State = [[-0.24808466  0.00735277  0.20925306  1.        ]]. Action = [[-0.07356012  0.35958028 -0.320203    0.70316184]]. Reward = [0.]
Curr episode timestep = 1
Current timestep = 1123. State = [[-0.24876274  0.01179679  0.20298854  1.        ]]. Action = [[-0.8134527  -0.9485947  -0.16561973  0.5048677 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1124. State = [[-0.24581899  0.02788709  0.20548597  1.        ]]. Action = [[0.24786448 0.9677582  0.3481655  0.58034897]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1125. State = [[-0.24435884  0.06096407  0.21219729  1.        ]]. Action = [[-0.09428495  0.8650594   0.3378731   0.76052165]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1126. State = [[-0.23990662  0.07312077  0.21079639  1.        ]]. Action = [[ 0.57302547 -0.49099463 -0.7496937   0.6766058 ]]. Reward = [0.]
Curr episode timestep = 5
Current timestep = 1127. State = [[-0.22042926  0.0815195   0.19455966  1.        ]]. Action = [[ 0.5388752   0.7959403  -0.334603    0.49525177]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1128. State = [[-0.19956832  0.08610405  0.1756123   1.        ]]. Action = [[ 0.64005685 -0.45794928 -0.8414872   0.70144844]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1129. State = [[-0.17522693  0.0906469   0.15538254  1.        ]]. Action = [[ 0.58519673  0.55189157 -0.15949863  0.6340662 ]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1130. State = [[-0.16271393  0.11001527  0.15132739  1.        ]]. Action = [[-0.14160305  0.72646093  0.24090874  0.66613114]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1131. State = [[-0.16493155  0.12533443  0.15425111  1.        ]]. Action = [[ 0.30932605 -0.69426674 -0.75513107  0.7017282 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: No entry zone
Current timestep = 1132. State = [[-0.1643475   0.12744021  0.15498644  1.        ]]. Action = [[ 0.472574    0.2312454  -0.89899766  0.61462164]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: No entry zone
Current timestep = 1133. State = [[-0.17087927  0.13276456  0.14888903  1.        ]]. Action = [[-0.7587084   0.17857528 -0.49698102  0.36009324]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1134. State = [[-0.1789158   0.13837695  0.14189738  1.        ]]. Action = [[ 0.485708   -0.03970027 -0.5995928   0.82748866]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: No entry zone
Current timestep = 1135. State = [[-0.1798988   0.13908446  0.14112669  1.        ]]. Action = [[ 0.84637594 -0.5674847  -0.77599293  0.85120106]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: No entry zone
Current timestep = 1136. State = [[-0.18015334  0.13906705  0.14057048  1.        ]]. Action = [[ 0.7679454  -0.9151158   0.6451111   0.29972267]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: No entry zone
Current timestep = 1137. State = [[-0.18007782  0.1390876   0.14062195  1.        ]]. Action = [[ 0.646152   -0.77703744  0.39183342  0.75524926]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: No entry zone
Current timestep = 1138. State = [[-0.18732956  0.1356132   0.13484186  1.        ]]. Action = [[-0.5350113 -0.257447  -0.386261   0.616145 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1139. State = [[-0.2114224   0.14503293  0.11835246  1.        ]]. Action = [[-0.87016207  0.9622102  -0.74089044  0.7208741 ]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1140. State = [[-0.23619807  0.1517176   0.10369477  1.        ]]. Action = [[-0.82942057 -0.8359327   0.6604247   0.5240537 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1141. State = [[-0.24968912  0.14334325  0.1002313   1.        ]]. Action = [[ 0.6097106   0.22997439 -0.8251691   0.60008216]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1142. State = [[-0.24144483  0.13368157  0.09323899  1.        ]]. Action = [[ 0.35630727 -0.70798796  0.19553351  0.22955787]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1143. State = [[-0.23490173  0.10741311  0.09583782  1.        ]]. Action = [[-0.21216416 -0.98718077  0.033741    0.9268358 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1144. State = [[-0.22774196  0.08311623  0.09719264  1.        ]]. Action = [[ 0.71146    -0.29582965 -0.04952013  0.7213    ]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1145. State = [[-0.2087558   0.0601336   0.09606411  1.        ]]. Action = [[ 0.7400029  -0.77381057 -0.11253572  0.5708766 ]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1146. State = [[-0.19443242  0.04130268  0.09427292  1.        ]]. Action = [[ 0.8274487  -0.505309    0.26314652  0.757879  ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: No entry zone
Current timestep = 1147. State = [[-0.19002663  0.03815255  0.09503331  1.        ]]. Action = [[ 0.11921918 -0.07888997  0.03055561  0.5602815 ]]. Reward = [0.]
Curr episode timestep = 26
Current timestep = 1148. State = [[-0.1832922   0.02435217  0.09918968  1.        ]]. Action = [[ 0.22640443 -0.73525333  0.5267668   0.03809154]]. Reward = [0.]
Curr episode timestep = 27
Current timestep = 1149. State = [[-0.18193126  0.00273313  0.09514231  1.        ]]. Action = [[-0.48997486 -0.4741646  -0.80655503  0.73290944]]. Reward = [0.]
Curr episode timestep = 28
Current timestep = 1150. State = [[-0.18407221 -0.00900039  0.08522184  1.        ]]. Action = [[ 0.7048483   0.41257977 -0.25040036  0.47785926]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: No entry zone
Current timestep = 1151. State = [[-0.18996958 -0.01583142  0.08601056  1.        ]]. Action = [[-0.6939748  -0.27564144  0.42726982  0.6270906 ]]. Reward = [0.]
Curr episode timestep = 30
Current timestep = 1152. State = [[-0.21114199 -0.03791258  0.09989159  1.        ]]. Action = [[-0.89873576 -0.96058244  0.8467063   0.6085286 ]]. Reward = [0.]
Curr episode timestep = 31
Current timestep = 1153. State = [[-0.23187864 -0.05327429  0.11143398  1.        ]]. Action = [[ 0.07913876  0.23333359 -0.7416383   0.5181589 ]]. Reward = [0.]
Curr episode timestep = 32
Current timestep = 1154. State = [[-0.24094391 -0.04260026  0.09860419  1.        ]]. Action = [[-0.17641401  0.7361572  -0.5715913   0.9257295 ]]. Reward = [0.]
Curr episode timestep = 33
Current timestep = 1155. State = [[-0.23595242 -0.03529507  0.09136484  1.        ]]. Action = [[ 0.85151243 -0.43070853  0.77927065  0.66825235]]. Reward = [0.]
Curr episode timestep = 34
Current timestep = 1156. State = [[-0.23207998 -0.05052953  0.1008055   1.        ]]. Action = [[-0.6037882  -0.6834645   0.32454848  0.78091145]]. Reward = [0.]
Curr episode timestep = 35
Current timestep = 1157. State = [[-0.23489058 -0.05139538  0.10472207  1.        ]]. Action = [[ 0.41108406  0.92151666 -0.5493982   0.82790995]]. Reward = [0.]
Curr episode timestep = 36
Current timestep = 1158. State = [[-0.23324886 -0.0377274   0.10080364  1.        ]]. Action = [[-0.7286408   0.83209896 -0.8654334   0.54518306]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Current timestep = 1159. State = [[-0.23733824 -0.0304592   0.09756689  1.        ]]. Action = [[-0.56769854  0.29604816 -0.24625552  0.9056909 ]]. Reward = [0.]
Curr episode timestep = 38
Current timestep = 1160. State = [[-0.23336823 -0.02726284  0.09819095  1.        ]]. Action = [[ 0.7576524  -0.13054717  0.47261882  0.58084035]]. Reward = [0.]
Curr episode timestep = 39
Current timestep = 1161. State = [[-0.21629336 -0.0369486   0.09319501  1.        ]]. Action = [[ 0.8808805 -0.6276973 -0.7944372  0.8048785]]. Reward = [0.]
Curr episode timestep = 40
Current timestep = 1162. State = [[-0.2032027  -0.06067071  0.08980317  1.        ]]. Action = [[-0.5927682 -0.771734   0.8144672  0.0410949]]. Reward = [0.]
Curr episode timestep = 41
Current timestep = 1163. State = [[-0.20046055 -0.08723307  0.09775852  1.        ]]. Action = [[ 5.1180780e-01 -8.0101770e-01  4.1365623e-04  7.9577088e-01]]. Reward = [0.]
Curr episode timestep = 42
Current timestep = 1164. State = [[-0.19584282 -0.10687746  0.09921458  1.        ]]. Action = [[0.9459379  0.6162176  0.6115017  0.67260945]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: No entry zone
Current timestep = 1165. State = [[-0.20447169 -0.12556264  0.09394857  1.        ]]. Action = [[-0.9344722  -0.9748306  -0.7435091   0.45875752]]. Reward = [0.]
Curr episode timestep = 44
Current timestep = 1166. State = [[-0.22162026 -0.15702458  0.08576129  1.        ]]. Action = [[-0.547019   -0.2953533  -0.04069513  0.6432731 ]]. Reward = [0.]
Curr episode timestep = 45
Current timestep = 1167. State = [[-0.23586267 -0.16282986  0.08753242  1.        ]]. Action = [[-0.40605998  0.33753812  0.63977826  0.61066866]]. Reward = [0.]
Curr episode timestep = 46
Current timestep = 1168. State = [[-0.24774289 -0.15900058  0.0952547   1.        ]]. Action = [[-0.96010685 -0.18726361  0.9769192   0.8678801 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Current timestep = 1169. State = [[-0.24757083 -0.16456547  0.10546403  1.        ]]. Action = [[ 0.05606794 -0.45170403  0.8324778   0.6651716 ]]. Reward = [0.]
Curr episode timestep = 48
Current timestep = 1170. State = [[-0.24660864 -0.17044783  0.11535977  1.        ]]. Action = [[ 0.38098788 -0.05756223 -0.837412    0.5461024 ]]. Reward = [0.]
Curr episode timestep = 49
Current timestep = 1171. State = [[-0.24513113 -0.17282602  0.10918912  1.        ]]. Action = [[-0.79916465  0.04262078  0.54135334  0.5637047 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Current timestep = 1172. State = [[-0.2402627  -0.1673999   0.11259467  1.        ]]. Action = [[0.36203122 0.4165063  0.6474972  0.80199575]]. Reward = [0.]
Curr episode timestep = 51
Current timestep = 1173. State = [[-0.23680933 -0.1613459   0.11661875  1.        ]]. Action = [[-0.9150188  -0.7803526  -0.2769301   0.72720075]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Current timestep = 1174. State = [[-0.23603553 -0.16084124  0.11726502  1.        ]]. Action = [[-0.7700605  -0.46278638  0.6654246   0.49098682]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Current timestep = 1175. State = [[-0.23479368 -0.16716784  0.12790896  1.        ]]. Action = [[-0.04639345 -0.5478051   0.82488227  0.6938796 ]]. Reward = [0.]
Curr episode timestep = 54
Current timestep = 1176. State = [[-0.2330539  -0.16834533  0.13978742  1.        ]]. Action = [[ 0.19768596  0.38064635 -0.5840866   0.3536632 ]]. Reward = [0.]
Curr episode timestep = 55
Current timestep = 1177. State = [[-0.21970695 -0.16935758  0.14438553  1.        ]]. Action = [[ 0.77836514 -0.49921262  0.70762897  0.75649047]]. Reward = [0.]
Curr episode timestep = 56
Current timestep = 1178. State = [[-0.19333482 -0.17198312  0.14533669  1.        ]]. Action = [[ 0.885633    0.19832885 -0.76381713  0.76750004]]. Reward = [0.]
Curr episode timestep = 57
Current timestep = 1179. State = [[-0.17775893 -0.16422342  0.13389343  1.        ]]. Action = [[-0.07583582  0.5974004  -0.16858923  0.5708319 ]]. Reward = [0.]
Curr episode timestep = 58
Current timestep = 1180. State = [[-0.17579813 -0.15400587  0.13020809  1.        ]]. Action = [[0.85875285 0.99268615 0.8569883  0.63656163]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: No entry zone
Current timestep = 1181. State = [[-0.17510925 -0.15191025  0.12973364  1.        ]]. Action = [[0.8080268  0.8321221  0.9815881  0.22507012]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: No entry zone
Current timestep = 1182. State = [[-0.17493121 -0.15138508  0.12959036  1.        ]]. Action = [[0.17806077 0.52761793 0.74742377 0.71515703]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: No entry zone
Current timestep = 1183. State = [[-0.17426704 -0.14952926  0.12934971  1.        ]]. Action = [[ 0.09081292  0.13185489 -0.02720577  0.52412605]]. Reward = [0.]
Curr episode timestep = 62
Current timestep = 1184. State = [[-0.1748054  -0.1529239   0.12973289  1.        ]]. Action = [[-0.37282354 -0.3702532   0.2588681   0.8554654 ]]. Reward = [0.]
Curr episode timestep = 63
Current timestep = 1185. State = [[-0.17610647 -0.1569046   0.13072452  1.        ]]. Action = [[ 0.95541525 -0.21140444  0.9296293   0.23179972]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: No entry zone
Current timestep = 1186. State = [[-0.18422979 -0.16056134  0.12832314  1.        ]]. Action = [[-0.6593728  -0.14391524 -0.26662576  0.62085557]]. Reward = [0.]
Curr episode timestep = 65
Current timestep = 1187. State = [[-0.19776522 -0.17792043  0.12873659  1.        ]]. Action = [[-0.420341   -0.742147    0.10481846  0.17379141]]. Reward = [0.]
Curr episode timestep = 66
Current timestep = 1188. State = [[-0.20718656 -0.1853055   0.12233242  1.        ]]. Action = [[ 0.18582523  0.45192206 -0.6624106   0.33477545]]. Reward = [0.]
Curr episode timestep = 67
Current timestep = 1189. State = [[-0.21229632 -0.17232674  0.11543471  1.        ]]. Action = [[-0.5410371   0.6725397   0.4868164   0.82922626]]. Reward = [0.]
Curr episode timestep = 68
Current timestep = 1190. State = [[-0.2259708  -0.15043847  0.10984202  1.        ]]. Action = [[-0.43148488  0.74765563 -0.91300607  0.7628269 ]]. Reward = [0.]
Curr episode timestep = 69
Current timestep = 1191. State = [[-0.23718658 -0.13244326  0.08903898  1.        ]]. Action = [[ 0.03241205  0.00111794 -0.56908834  0.61190414]]. Reward = [0.]
Curr episode timestep = 70
Current timestep = 1192. State = [[-0.23927985 -0.12804677  0.07447287  1.        ]]. Action = [[-0.69643176  0.08642614 -0.47126698  0.37591004]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Current timestep = 1193. State = [[-0.23831111 -0.11979236  0.07614522  1.        ]]. Action = [[-0.18090022  0.45961726  0.6048982   0.83898664]]. Reward = [0.]
Curr episode timestep = 72
Current timestep = 1194. State = [[-0.2309156  -0.10889872  0.0812436   1.        ]]. Action = [[0.96343553 0.13246071 0.13631666 0.74479544]]. Reward = [0.]
Curr episode timestep = 73
Current timestep = 1195. State = [[-0.21953043 -0.10762671  0.08567397  1.        ]]. Action = [[ 0.3926443  -0.3882681  -0.05175877  0.7988596 ]]. Reward = [0.]
Curr episode timestep = 74
Current timestep = 1196. State = [[-0.21578614 -0.10047185  0.08693916  1.        ]]. Action = [[-0.55230683  0.7798269   0.18323874  0.7544575 ]]. Reward = [0.]
Curr episode timestep = 75
Current timestep = 1197. State = [[-0.21503425 -0.0812897   0.08559537  1.        ]]. Action = [[ 0.5107868   0.4471662  -0.45318985  0.8676021 ]]. Reward = [0.]
Curr episode timestep = 76
Current timestep = 1198. State = [[-0.2037471  -0.05716991  0.07913201  1.        ]]. Action = [[ 0.68295074  0.8684406  -0.44023407  0.21775389]]. Reward = [0.]
Curr episode timestep = 77
Current timestep = 1199. State = [[-0.19516294 -0.03081707  0.07210368  1.        ]]. Action = [[-0.44208235  0.42222917  0.47879076  0.68998575]]. Reward = [0.]
Curr episode timestep = 78
Current timestep = 1200. State = [[-0.19338214 -0.02162782  0.07867694  1.        ]]. Action = [[ 0.32900167 -0.23349929  0.3986795   0.6490369 ]]. Reward = [0.]
Curr episode timestep = 79
Current timestep = 1201. State = [[-0.1909349  -0.02388018  0.08411702  1.        ]]. Action = [[ 0.84807575  0.5181382  -0.73760206  0.6059706 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: No entry zone
Current timestep = 1202. State = [[-0.19001569 -0.02402622  0.0849471   1.        ]]. Action = [[0.6655978  0.97611046 0.87400293 0.6315994 ]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: No entry zone
Current timestep = 1203. State = [[-0.18890496 -0.01939309  0.09139378  1.        ]]. Action = [[-0.00546038  0.26869297  0.49902093  0.6186402 ]]. Reward = [0.]
Curr episode timestep = 82
Current timestep = 1204. State = [[-0.19779424 -0.02703174  0.09967948  1.        ]]. Action = [[-0.899328   -0.7560547  -0.50877434  0.6640043 ]]. Reward = [0.]
Curr episode timestep = 83
Current timestep = 1205. State = [[-0.21410899 -0.03823682  0.08885752  1.        ]]. Action = [[-0.3756019   0.14339757 -0.89515984  0.5102711 ]]. Reward = [0.]
Curr episode timestep = 84
Current timestep = 1206. State = [[-0.2275104  -0.03519064  0.07740559  1.        ]]. Action = [[-0.56659156  0.30573523  0.5667275   0.6753063 ]]. Reward = [0.]
Curr episode timestep = 85
Current timestep = 1207. State = [[-0.23006342 -0.01764245  0.08088874  1.        ]]. Action = [[ 0.90460134  0.47965705 -0.18438953  0.6358447 ]]. Reward = [0.]
Curr episode timestep = 86
Current timestep = 1208. State = [[-0.2252441  -0.01554538  0.08921719  1.        ]]. Action = [[-0.4649433  -0.35911977  0.90747404  0.57585835]]. Reward = [0.]
Curr episode timestep = 87
Current timestep = 1209. State = [[-0.23701034 -0.01595084  0.09834889  1.        ]]. Action = [[-0.62639904  0.24634707 -0.25771374  0.8035438 ]]. Reward = [0.]
Curr episode timestep = 88
Current timestep = 1210. State = [[-0.23824167 -0.00641188  0.09973823  1.        ]]. Action = [[0.72839856 0.42756164 0.18219829 0.7695937 ]]. Reward = [0.]
Curr episode timestep = 89
Current timestep = 1211. State = [[-0.22791587  0.00272496  0.09457138  1.        ]]. Action = [[ 0.6222968   0.19739711 -0.88305956  0.43626392]]. Reward = [0.]
Curr episode timestep = 90
Current timestep = 1212. State = [[-0.21671367  0.00969462  0.08388298  1.        ]]. Action = [[-0.8722776   0.27172375 -0.65408695  0.52020884]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Current timestep = 1213. State = [[-0.21421817  0.01029587  0.08903744  1.        ]]. Action = [[-0.31342506  0.07851434  0.9380789   0.7888006 ]]. Reward = [0.]
Curr episode timestep = 92
Current timestep = 1214. State = [[-0.21667044  0.01123606  0.0948486   1.        ]]. Action = [[-0.37921655 -0.3135748  -0.57742864  0.49588013]]. Reward = [0.]
Curr episode timestep = 93
Current timestep = 1215. State = [[-0.21729195  0.00231001  0.09552711  1.        ]]. Action = [[ 0.92809725 -0.6926667   0.13145566  0.61895764]]. Reward = [0.]
Curr episode timestep = 94
Current timestep = 1216. State = [[-0.20458393 -0.02117543  0.10941774  1.        ]]. Action = [[ 0.20955896 -0.7590848   0.8967798   0.6705935 ]]. Reward = [0.]
Curr episode timestep = 95
Current timestep = 1217. State = [[-0.18856193 -0.04461864  0.12772968  1.        ]]. Action = [[ 0.6729436  -0.23763323  0.3792243   0.69453704]]. Reward = [0.]
Curr episode timestep = 96
Current timestep = 1218. State = [[-0.17982556 -0.05066719  0.12926637  1.        ]]. Action = [[-0.21141142  0.41877925 -0.8691701   0.6044166 ]]. Reward = [0.]
Curr episode timestep = 97
Current timestep = 1219. State = [[-0.18013228 -0.04932967  0.1198455   1.        ]]. Action = [[ 0.6334747  -0.8677594  -0.32613927  0.72722876]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: No entry zone
Current timestep = 1220. State = [[-0.17859228 -0.05487217  0.12187426  1.        ]]. Action = [[ 0.03672016 -0.49759185  0.38531113  0.7117026 ]]. Reward = [0.]
Curr episode timestep = 99
Current timestep = 1221. State = [[-0.18292601 -0.05425319  0.12187153  1.        ]]. Action = [[-0.77602047  0.4264629   0.00848222  0.80134296]]. Reward = [0.]
Curr episode timestep = 100
Current timestep = 1222. State = [[-0.25041777  0.00253964  0.23262674  1.        ]]. Action = [[-0.14315736 -0.34952152 -0.56231314  0.57792664]]. Reward = [0.]
Curr episode timestep = 101
Current timestep = 1223. State = [[-0.2505874   0.00162495  0.23254924  1.        ]]. Action = [[-0.5097485  -0.545707   -0.27152437  0.729185  ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Current timestep = 1224. State = [[-0.25064594  0.00136887  0.2325484   1.        ]]. Action = [[-0.653502   -0.22666371  0.23824513  0.5859914 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Current timestep = 1225. State = [[-0.25064594  0.00136887  0.2325484   1.        ]]. Action = [[-0.86026037  0.89500904 -0.31437135  0.7339802 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Current timestep = 1226. State = [[-0.25069538  0.00838738  0.22529049  1.        ]]. Action = [[ 0.21386325  0.56741023 -0.76654285  0.7181853 ]]. Reward = [0.]
Curr episode timestep = 3
Current timestep = 1227. State = [[-0.24317965  0.01181684  0.21670415  1.        ]]. Action = [[ 0.17598474 -0.3714826   0.80017877  0.271775  ]]. Reward = [0.]
Curr episode timestep = 4
Current timestep = 1228. State = [[-0.23806584  0.0074759   0.2201856   1.        ]]. Action = [[-0.7586531   0.43480277 -0.809329    0.807111  ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Current timestep = 1229. State = [[-0.23417424 -0.00210362  0.22549967  1.        ]]. Action = [[ 0.16317165 -0.5830464   0.41813612  0.27569532]]. Reward = [0.]
Curr episode timestep = 6
Current timestep = 1230. State = [[-0.22819747 -0.02246954  0.24300686  1.        ]]. Action = [[ 0.10619962 -0.61195534  0.5593405   0.7065811 ]]. Reward = [0.]
Curr episode timestep = 7
Current timestep = 1231. State = [[-0.21264496 -0.05100628  0.26760378  1.        ]]. Action = [[ 0.89573336 -0.9634019   0.64596486  0.65915537]]. Reward = [0.]
Curr episode timestep = 8
Current timestep = 1232. State = [[-0.20155765 -0.06399821  0.28228682  1.        ]]. Action = [[-0.44628274  0.4420402  -0.5235364   0.5561583 ]]. Reward = [0.]
Curr episode timestep = 9
Current timestep = 1233. State = [[-0.20168445 -0.04877889  0.2701978   1.        ]]. Action = [[ 0.5312853   0.86481047 -0.8883399   0.6435077 ]]. Reward = [0.]
Curr episode timestep = 10
Current timestep = 1234. State = [[-0.186125   -0.03002606  0.25747743  1.        ]]. Action = [[0.37737405 0.22449946 0.49353826 0.74983287]]. Reward = [0.]
Curr episode timestep = 11
Current timestep = 1235. State = [[-0.17655227 -0.00910314  0.26679614  1.        ]]. Action = [[0.14190936 0.8473325  0.52292395 0.22667086]]. Reward = [0.]
Curr episode timestep = 12
Current timestep = 1236. State = [[-0.16195391  0.01672734  0.2847013   1.        ]]. Action = [[0.7596011  0.46071625 0.5395794  0.6501038 ]]. Reward = [0.]
Curr episode timestep = 13
Current timestep = 1237. State = [[-0.1557406   0.03972862  0.28973296  1.        ]]. Action = [[-0.579356   0.7312918 -0.9779742  0.4872414]]. Reward = [0.]
Curr episode timestep = 14
Current timestep = 1238. State = [[-0.15699889  0.06842289  0.2643652   1.        ]]. Action = [[ 0.6036136  0.810992  -0.7486371  0.590729 ]]. Reward = [0.]
Curr episode timestep = 15
Current timestep = 1239. State = [[-0.14673714  0.08115111  0.24084286  1.        ]]. Action = [[-0.4517011  -0.35135114 -0.29205036  0.78401995]]. Reward = [0.]
Curr episode timestep = 16
Current timestep = 1240. State = [[-0.15594721  0.08546315  0.23280267  1.        ]]. Action = [[-0.81075025  0.21542478  0.03813791  0.6893282 ]]. Reward = [0.]
Curr episode timestep = 17
Current timestep = 1241. State = [[-0.16360986  0.08097585  0.23451656  1.        ]]. Action = [[ 0.37842464 -0.46015555  0.3505509   0.45395494]]. Reward = [0.]
Curr episode timestep = 18
Current timestep = 1242. State = [[-0.17107187  0.07252234  0.24733649  1.        ]]. Action = [[-0.9976408  -0.13260835  0.65138173  0.6732482 ]]. Reward = [0.]
Curr episode timestep = 19
Current timestep = 1243. State = [[-0.19317412  0.06452754  0.26463753  1.        ]]. Action = [[-0.25392604 -0.25667965  0.20649433  0.7717501 ]]. Reward = [0.]
Curr episode timestep = 20
Current timestep = 1244. State = [[-0.19947697  0.0470013   0.27536106  1.        ]]. Action = [[ 0.15449452 -0.75823486  0.31631756  0.7014613 ]]. Reward = [0.]
Curr episode timestep = 21
Current timestep = 1245. State = [[-0.20290111  0.01554883  0.28053415  1.        ]]. Action = [[-0.20831716 -0.99057484 -0.36241114  0.7121773 ]]. Reward = [0.]
Curr episode timestep = 22
Current timestep = 1246. State = [[-0.20756    -0.01862318  0.2775062   1.        ]]. Action = [[ 0.33909404 -0.87422526 -0.4462756   0.58747685]]. Reward = [0.]
Curr episode timestep = 23
Current timestep = 1247. State = [[-0.20073913 -0.02684214  0.26109123  1.        ]]. Action = [[ 0.3977418   0.871395   -0.49190623  0.66461337]]. Reward = [0.]
Curr episode timestep = 24
Current timestep = 1248. State = [[-0.19707945 -0.01718704  0.24390131  1.        ]]. Action = [[-0.27538776  0.12379336 -0.30473077  0.7708818 ]]. Reward = [0.]
Curr episode timestep = 25
Current timestep = 1249. State = [[-0.18883562 -0.01074169  0.24373396  1.        ]]. Action = [[0.8369155  0.20118928 0.5971322  0.8033316 ]]. Reward = [0.]
Curr episode timestep = 26
