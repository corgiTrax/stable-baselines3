Current timestep = 0. State = [[-0.23581691 -0.004702    0.24779452  1.        ]]. Action = [[ 0.6752536  -0.29346895  0.8604896   0.00108433]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3910, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 0 is 1
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.2083036  -0.0310175   0.26831102  1.        ]]. Action = [[ 0.80132127 -0.9856681  -0.15820175  0.15501964]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3650, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1 is 1
Human Feedback received at timestep 1 of 1
Current timestep = 2. State = [[-0.2506009   0.00243264  0.23256092  1.        ]]. Action = [[ 0.23433149  0.98531294  0.9645599  -0.2529559 ]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3223, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 2 is -1
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.2508298   0.00276461  0.232419    1.        ]]. Action = [[-0.8121195   0.45546556 -0.20534599  0.1635704 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 3 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 3 is tensor(0.3187, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 3 is 1
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.24892567 -0.01577752  0.23383455  1.        ]]. Action = [[ 0.1476419  -0.9528265   0.21161222  0.9620935 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 4 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 4 is tensor(0.2887, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 4 is 1
Human Feedback received at timestep 4 of 1
Current timestep = 5. State = [[-0.25052163 -0.03553037  0.235626    1.        ]]. Action = [[-0.54992664  0.5284207  -0.87297314  0.00533319]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2516, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 5 is -1
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.25042343  0.00265926  0.23281008  1.        ]]. Action = [[ 0.2984048   0.78703666 -0.23517662 -0.8910731 ]]. Reward = [-100.]
Curr episode timestep = 3
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 6 is 1
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.24654602  0.00126543  0.21895014  1.        ]]. Action = [[-0.12094986 -0.10273498 -0.8749628   0.23087525]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.2077, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 7 is 1
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-2.4582711e-01  4.9019676e-05  1.9195700e-01  1.0000000e+00]]. Action = [[-0.58510625  0.5450041   0.36670923  0.5163168 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1930, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 8 is -1
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.24995354  0.01342796  0.18622994  1.        ]]. Action = [[-0.24983454  0.7548213  -0.17184049  0.07638621]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 9 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 9 is tensor(0.1739, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 9 is 1
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.25625926  0.02748092  0.1796391   1.        ]]. Action = [[-0.8658661  -0.6618159  -0.14517593  0.647861  ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1414, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 10 is 1
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.25673082  0.02951193  0.17938931  1.        ]]. Action = [[-0.7206418   0.48894453  0.704041   -0.06801546]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1301, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 11 is 1
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.25667283  0.0300028   0.17937434  1.        ]]. Action = [[-0.29205453 -0.800999    0.67145514 -0.04063982]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 12 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 12 is tensor(0.0922, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 12 is -1
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-0.25667208  0.03006057  0.17937194  1.        ]]. Action = [[-0.74971575  0.8577244   0.09105074 -0.17764705]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.1029, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 13 is -1
Human Feedback received at timestep 13 of -1
Current timestep = 14. State = [[-0.25667208  0.03006057  0.17937194  1.        ]]. Action = [[-0.24282366 -0.32665622  0.96600294  0.03104353]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0711, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 14 is 1
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.25034958  0.0025003   0.2327329   1.        ]]. Action = [[ 0.46191406  0.54289746 -0.92540854 -0.01489276]]. Reward = [-100.]
Curr episode timestep = 8
Scene graph at timestep 15 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 15 is tensor(0.0600, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 15 is 1
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.23496653  0.00508904  0.24185547  1.        ]]. Action = [[0.94120574 0.13763964 0.5383959  0.19147539]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 16 is 1
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.25113133  0.00242484  0.23241022  1.        ]]. Action = [[ 0.688545    0.6566329   0.11472321 -0.11000001]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0569, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 17 is 1
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.25126222  0.00235739  0.23238856  1.        ]]. Action = [[-0.6689321  -0.639929    0.19439363 -0.5667016 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 18 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 18 is tensor(0.0287, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 18 is 1
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.24734823 -0.00685241  0.22998202  1.        ]]. Action = [[ 0.38205266 -0.47426218 -0.19101757  0.65904295]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0338, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 19 is 1
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.25049326  0.00292334  0.23275001  1.        ]]. Action = [[ 0.2350657  -0.06802917 -0.80849445 -0.01427567]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 20 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 20 is tensor(0.0319, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 20 is 1
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-2.5053254e-01  2.8163390e-04  2.3315774e-01  1.0000000e+00]]. Action = [[-0.3520465  -0.08709228  0.86585927 -0.8660446 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 21 is 1
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.25068802  0.00243452  0.23251581  1.        ]]. Action = [[ 0.575428    0.7015071   0.41959107 -0.42129594]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 22 is 1
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.25021157  0.0024666   0.23275189  1.        ]]. Action = [[-0.9576406   0.7684177   0.42617333  0.5239215 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 23 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 23 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 23 is -1
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.2500983   0.00242548  0.23279954  1.        ]]. Action = [[-0.78454524 -0.44663072  0.49488914 -0.27692175]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0145, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 24 is 1
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.2514824   0.00957078  0.23827241  1.        ]]. Action = [[-0.14749908  0.33371723  0.46805346  0.4842552 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 25 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0290, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 25 is 1
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.2424733   0.01506564  0.25036058  1.        ]]. Action = [[ 0.77519464 -0.12017375  0.1328479   0.52818334]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 26 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0283, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 26 is 1
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.22915742  0.0147501   0.25956073  1.        ]]. Action = [[-0.8165976   0.39058554 -0.4342574  -0.32226598]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 27 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 27 is tensor(0.0323, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 27 is 1
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.2505977   0.00264483  0.23276386  1.        ]]. Action = [[ 0.01788485 -0.6467551  -0.62986404 -0.22095579]]. Reward = [-100.]
Curr episode timestep = 5
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0331, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 28 is 1
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.2407005  -0.01463942  0.24209353  1.        ]]. Action = [[ 0.57975316 -0.79687905  0.77336717  0.88660705]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 29 is 1
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.22839409 -0.05069394  0.25829878  1.        ]]. Action = [[ 0.09994066 -0.90010285  0.19072187  0.1727879 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 30 is 1
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.25084594  0.0026925   0.23272619  1.        ]]. Action = [[ 0.4539925  -0.24751854 -0.7061611  -0.6229187 ]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 31 is 1
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.25115764  0.00167969  0.23201506  1.        ]]. Action = [[-0.7580835   0.43355322 -0.9934835   0.3903185 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0409, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 32 is -1
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.25030816  0.00268944  0.23269327  1.        ]]. Action = [[ 0.4677863  -0.70031697 -0.5961815  -0.62576556]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 33 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 33 is tensor(0.0372, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 33 is 1
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-2.4966930e-01  3.3699747e-04  2.3324081e-01  1.0000000e+00]]. Action = [[-0.6231679   0.2441268   0.7057884   0.30018115]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0521, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 34 is 1
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.25244007  0.00870868  0.2271121   1.        ]]. Action = [[ 0.01366448  0.51576304 -0.55565965  0.8894696 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0523, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 35 is 1
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.25043625  0.00236043  0.23248023  1.        ]]. Action = [[ 0.6131811   0.46835673  0.35905504 -0.3090495 ]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0705, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 36 is 1
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.25630265  0.01130555  0.22202653  1.        ]]. Action = [[-0.28653014  0.5103774  -0.949374    0.7697902 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0546, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 37 is -1
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.2606953   0.02257418  0.20064904  1.        ]]. Action = [[-0.24436224 -0.80560297  0.8777933  -0.99561554]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 38 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 38 is tensor(0.0398, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 38 is 1
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.2508324   0.00243105  0.23245403  1.        ]]. Action = [[ 0.00364935 -0.30469626  0.25027323 -0.37262017]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 39 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 39 is tensor(0.0859, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 39 is 1
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.25397128  0.01793183  0.23648264  1.        ]]. Action = [[-0.13067126  0.8372781   0.2331413   0.9333564 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 40 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0619, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 40 is -1
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.25865012  0.03405814  0.2456103   1.        ]]. Action = [[-0.00534827 -0.14020526  0.3417697   0.8019004 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0849, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 41 is 1
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.25078693  0.0365745   0.25129813  1.        ]]. Action = [[ 0.6948185   0.20006108 -0.47436243  0.07067227]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 42 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 42 is tensor(0.0895, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 42 is 1
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.25056607  0.00247921  0.23275094  1.        ]]. Action = [[ 0.74848795 -0.81276083 -0.59081936 -0.61326367]]. Reward = [-100.]
Curr episode timestep = 3
Scene graph at timestep 43 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0678, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 43 is -1
Human Feedback received at timestep 43 of -1
Current timestep = 44. State = [[-0.24975745  0.0029647   0.23298812  1.        ]]. Action = [[ 0.91448593  0.86989915 -0.43616402 -0.7317392 ]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 44 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 44 is tensor(0.0668, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 44 is 1
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.24869487 -0.01560238  0.24417548  1.        ]]. Action = [[-0.03200269 -0.9853638   0.7462859   0.68761194]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 45 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 45 is tensor(0.0752, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 45 is 1
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.25378877 -0.03937287  0.2618533   1.        ]]. Action = [[-0.90864366 -0.75088596  0.69889045 -0.49958992]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0647, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 46 is 1
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.24163236 -0.05376808  0.27733013  1.        ]]. Action = [[ 0.7112286  -0.53423566  0.7530037   0.3037541 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 47 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 47 is tensor(0.0898, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 47 is 1
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.22609541 -0.06734266  0.2985035   1.        ]]. Action = [[-0.5267702   0.30359268  0.28396606  0.45620227]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 48 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 48 is tensor(0.1012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 48 is 1
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.25055388  0.00262989  0.23277989  1.        ]]. Action = [[ 0.9285207   0.621763   -0.94198656 -0.72476745]]. Reward = [-100.]
Curr episode timestep = 4
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.0694, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 49 is 1
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.24979486  0.00134718  0.23269872  1.        ]]. Action = [[ 0.42687786 -0.06258619 -0.8893925  -0.43841958]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 50 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 50 is tensor(0.0921, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 50 is 1
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.2288384  -0.01475471  0.20958878  1.        ]]. Action = [[ 0.9238782  -0.8124842   0.20765424  0.14486015]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 51 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 51 is tensor(0.0885, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 51 is 1
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.2508482   0.00264623  0.2326435   1.        ]]. Action = [[ 0.9008051  -0.21428043  0.6866586  -0.687032  ]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.0898, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 52 is 1
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.25015458  0.00244404  0.23266669  1.        ]]. Action = [[ 0.02158165 -0.9103095  -0.9892749  -0.9223193 ]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.0778, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 53 is -1
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.25016904  0.00272224  0.23265971  1.        ]]. Action = [[ 0.7660135   0.3385625  -0.242746   -0.07238978]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 54 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 54 is tensor(0.1059, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 54 is 1
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.25146854  0.00211273  0.23287392  1.        ]]. Action = [[-0.6739266  -0.89493215  0.90343964 -0.4944046 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.0690, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 55 is 1
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.25068566  0.0024481   0.23371035  1.        ]]. Action = [[-0.02324259  0.19791865  0.7118256  -0.14071506]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.1102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 56 is 1
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-2.5026199e-01  4.1306231e-04  2.3403417e-01  1.0000000e+00]]. Action = [[-0.9357508  -0.98438734 -0.00398564  0.45265245]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.0807, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 57 is 1
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.23818025  0.01072388  0.24045599  1.        ]]. Action = [[0.7556114  0.62678576 0.3641045  0.6983783 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 58 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 58 is tensor(0.0954, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 58 is -1
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.250728    0.00244618  0.23263906  1.        ]]. Action = [[ 0.41150737  0.11945546 -0.5990878  -0.58992124]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 59 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 59 is tensor(0.1052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 59 is -1
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.24228206  0.01038243  0.23840913  1.        ]]. Action = [[0.6288911  0.41713643 0.3886149  0.8842857 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.0953, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 60 is -1
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.23144399  0.01906118  0.24640942  1.        ]]. Action = [[-0.74762654 -0.78028697  0.71318555 -0.7785214 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 61 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 61 is tensor(0.0798, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 61 is 1
Human Feedback received at timestep 61 of 1
Current timestep = 62. State = [[-0.2212482   0.01756848  0.2404998   1.        ]]. Action = [[ 0.9343778  -0.10324323 -0.89584315  0.5048895 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 62 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.1011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 62 is -1
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.17904226  0.00730819  0.23005325  1.        ]]. Action = [[ 0.7255964  -0.575439    0.6968007   0.30281806]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 63 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 63 is tensor(0.1032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 63 is 1
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.25061414  0.00259992  0.23259796  1.        ]]. Action = [[ 0.82913435 -0.33598042  0.83142686 -0.42645562]]. Reward = [-100.]
Curr episode timestep = 4
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.1045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 64 is 1
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-2.5154793e-01 -3.6860548e-04  2.3279956e-01  1.0000000e+00]]. Action = [[-0.47945738 -0.7148402   0.34801936  0.05224276]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.1053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 65 is 1
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.25051212  0.00240789  0.23250552  1.        ]]. Action = [[-0.14331591 -0.75065154 -0.724451   -0.54224545]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 66 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 66 is tensor(0.1122, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 66 is 1
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.2382704  -0.0111847   0.22885877  1.        ]]. Action = [[ 0.8753476  -0.66162974 -0.47142482  0.9686177 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.0947, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 67 is 1
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.2154424  -0.00782942  0.23087059  1.        ]]. Action = [[0.11156034 0.9220624  0.78040767 0.13874578]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.1068, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 68 is -1
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.21235676  0.00360971  0.25343767  1.        ]]. Action = [[-0.50426453 -0.2580287   0.6211544   0.48524678]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.1088, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 69 is 1
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.22237697 -0.00943908  0.27191284  1.        ]]. Action = [[-0.30942905 -0.54578966 -0.12153518  0.89171004]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.1132, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 70 is -1
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.250673    0.00270143  0.23264052  1.        ]]. Action = [[ 0.471596   -0.47782713  0.3453548  -0.07984483]]. Reward = [-100.]
Curr episode timestep = 4
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.1244, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 71 is 1
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.2510566   0.00278835  0.2332583   1.        ]]. Action = [[ 0.5522673  -0.17057025  0.43939126 -0.05786413]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.1231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 72 is 1
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.25119457  0.0023468   0.23249488  1.        ]]. Action = [[ 0.5921018   0.7789308  -0.75175214 -0.54631615]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.1007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 73 is 1
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.23742409  0.01571512  0.24154323  1.        ]]. Action = [[0.93319297 0.6842828  0.45078003 0.81969273]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.0973, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 74 is -1
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.21572368  0.01564804  0.26509014  1.        ]]. Action = [[ 0.23086357 -0.7938868   0.8844342   0.5664854 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.1006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 75 is -1
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.19426401 -0.01821724  0.28544986  1.        ]]. Action = [[ 0.7066684  -0.9681535  -0.44260615  0.5887357 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.1026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 76 is 1
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.1674071  -0.05279352  0.29233435  1.        ]]. Action = [[ 0.5139419  -0.53301233  0.59975624  0.06175292]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.1187, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 77 is 1
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.14124504 -0.06916278  0.3241909   1.        ]]. Action = [[ 0.5670092  -0.02866042  0.93786836  0.36205912]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.1129, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 78 is -1
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.25057548  0.00255258  0.232726    1.        ]]. Action = [[ 0.80281055  0.104671    0.67482424 -0.03060085]]. Reward = [-100.]
Curr episode timestep = 5
Scene graph at timestep 79 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 79 is tensor(0.1158, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 79 is -1
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.25137424  0.00175696  0.23247041  1.        ]]. Action = [[-0.5387813  -0.7689571  -0.3818195  -0.25797153]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 80 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 80 is tensor(0.1188, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 80 is 1
Human Feedback received at timestep 80 of 1
Current timestep = 81. State = [[-0.25172842  0.00125737  0.23235321  1.        ]]. Action = [[-0.7395843  -0.08658808 -0.72178304  0.25541568]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.1149, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 81 is -1
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-2.5280866e-01  6.1342993e-04  2.3168083e-01  1.0000000e+00]]. Action = [[-0.7818333  -0.8084614  -0.03984463 -0.7322531 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.1041, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 82 is -1
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.25123885 -0.01565348  0.22550662  1.        ]]. Action = [[ 0.30642414 -0.8230852  -0.44765365  0.27341127]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 83 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 83 is tensor(0.1129, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 83 is 1
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.2508648  -0.03277948  0.21411927  1.        ]]. Action = [[-0.47248006  0.22204828  0.45126557 -0.9225001 ]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 84 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 84 is tensor(0.1049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 84 is 1
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.2537635  -0.03252664  0.2055151   1.        ]]. Action = [[-0.25583738  0.25947738 -0.6410064   0.39237094]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 85 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 85 is tensor(0.1143, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 85 is 1
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.25044683  0.00259905  0.23271884  1.        ]]. Action = [[ 0.59998035  0.8833525   0.3477087  -0.29893315]]. Reward = [-100.]
Curr episode timestep = 6
Scene graph at timestep 86 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 86 is tensor(0.1040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 86 is 1
Human Feedback received at timestep 86 of 1
Current timestep = 87. State = [[-0.24803431  0.00292511  0.23392716  1.        ]]. Action = [[-0.41173053 -0.8627025   0.39573705 -0.82082206]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.0929, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 87 is 1
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.24923141 -0.00830665  0.24353991  1.        ]]. Action = [[-0.3397361  -0.59095836  0.665885    0.05484092]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.1013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 88 is 1
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25324956 -0.02080707  0.25962344  1.        ]]. Action = [[-0.87007225  0.65792596  0.5114975   0.03446174]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 89 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 89 is tensor(0.0962, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 89 is 1
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.25438467 -0.02315574  0.26041952  1.        ]]. Action = [[-0.61488205  0.7251419   0.54206777  0.6329181 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0872, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 90 is 1
Human Feedback received at timestep 90 of 1
Current timestep = 91. State = [[-0.25465387 -0.02371187  0.2605668   1.        ]]. Action = [[-0.4502809  -0.14976436 -0.04208577 -0.76111865]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.1039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 91 is 1
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.25467253 -0.02425093  0.26063237  1.        ]]. Action = [[-0.91871697 -0.88054127  0.5619869  -0.43747747]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 92 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 92 is tensor(0.0789, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 92 is 1
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.25469223 -0.02415905  0.2606699   1.        ]]. Action = [[-0.40934944 -0.74829876 -0.91038173  0.7512598 ]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0882, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 93 is 1
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.2500274  -0.02657042  0.2734763   1.        ]]. Action = [[ 0.30344045 -0.13995314  0.7833426   0.27209163]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0942, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 94 is 1
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.24134296 -0.03779047  0.29719222  1.        ]]. Action = [[ 0.32260907 -0.41905153  0.01873994  0.5911727 ]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0989, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 95 is 1
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.23644097 -0.04654216  0.30343652  1.        ]]. Action = [[-0.8735385  -0.8050614  -0.4756133  -0.27202463]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.0902, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 96 is 1
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.2359409  -0.04837033  0.30426005  1.        ]]. Action = [[-0.9100184  -0.285276    0.6957377   0.10011041]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 97 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.0779, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 97 is 1
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.23568392 -0.04899355  0.30470902  1.        ]]. Action = [[-0.6821058  -0.8575096  -0.24932754 -0.27893686]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 98 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 98 is tensor(0.0898, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 98 is 1
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.25089133  0.00263112  0.23267293  1.        ]]. Action = [[ 0.82837987 -0.960449   -0.0227828  -0.5098266 ]]. Reward = [-100.]
Curr episode timestep = 12
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.0745, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 99 is 1
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.25115046  0.001815    0.2322018   1.        ]]. Action = [[-0.6939821  -0.5023008  -0.76673126  0.54304004]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 100 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 100 is tensor(0.0828, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 100 is 1
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.25105488  0.00260375  0.2324848   1.        ]]. Action = [[ 0.34996164 -0.66743743  0.35023868 -0.061041  ]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 101 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.0872, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 101 is 1
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.24128637 -0.01272936  0.23721173  1.        ]]. Action = [[ 0.4137529  -0.65329635  0.20614815  0.45432985]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 102 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 102 is tensor(0.0825, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 102 is 1
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.25086677  0.00272451  0.23262337  1.        ]]. Action = [[ 0.160115  -0.334687  -0.0587281 -0.5143922]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 103 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 103 is tensor(0.0908, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 103 is 1
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-2.5098765e-01  3.9646181e-04  2.3262832e-01  1.0000000e+00]]. Action = [[-0.8374355   0.62410283  0.9148097  -0.38993055]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0685, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 104 is -1
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-2.5105712e-01 -3.7457715e-04  2.3265916e-01  1.0000000e+00]]. Action = [[-0.4757685  -0.5165884  -0.6065688  -0.41717362]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 105 is tensor(0.0864, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 105 is 1
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-2.5111821e-01 -8.2205341e-04  2.3267922e-01  1.0000000e+00]]. Action = [[-0.32640898  0.9258237   0.50596285  0.7924681 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 106 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 106 is tensor(0.0634, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 106 is -1
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-2.5111252e-01 -9.7787427e-04  2.3268370e-01  1.0000000e+00]]. Action = [[-0.7594235  -0.97123396 -0.3830014   0.8001447 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0673, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 107 is 1
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-2.5111252e-01 -9.7787427e-04  2.3268370e-01  1.0000000e+00]]. Action = [[-0.6486269   0.7997372   0.16123784 -0.8244869 ]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 108 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 108 is tensor(0.0719, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 108 is 1
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-2.5111252e-01 -9.7787427e-04  2.3268370e-01  1.0000000e+00]]. Action = [[-0.94981503 -0.15863425 -0.72860557  0.94026375]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 109 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 109 is tensor(0.0654, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 109 is 1
Human Feedback received at timestep 109 of 1
Current timestep = 110. State = [[-0.25349283  0.00667932  0.23702034  1.        ]]. Action = [[-0.30003834  0.39993036  0.47892666  0.8848691 ]]. Reward = [10.]
Curr episode timestep = 6
Scene graph at timestep 110 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 110 is tensor(0.0732, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 110 is 1
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.2582173   0.01518324  0.2408791   1.        ]]. Action = [[-0.3940388  -0.5727508   0.63684773  0.53004146]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 111 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 111 is tensor(0.0804, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 111 is 1
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.25788045  0.00189664  0.25269297  1.        ]]. Action = [[-0.04880035 -0.7705524   0.821216    0.3745122 ]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 112 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 112 is tensor(0.0761, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 112 is 1
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.26366612 -0.01596641  0.27745178  1.        ]]. Action = [[-0.759565   -0.05249351  0.9389541  -0.01417536]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 113 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 113 is tensor(0.0779, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 113 is 1
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.2645136  -0.01846762  0.27941152  1.        ]]. Action = [[-0.29139137 -0.5632255   0.05175161 -0.22876573]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 114 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 114 is tensor(0.0918, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 114 is 1
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.26458392 -0.01908618  0.2795039   1.        ]]. Action = [[-0.36823893 -0.79974854  0.64833903  0.88933957]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 115 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 115 is tensor(0.0768, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 115 is 1
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.26460868 -0.01924508  0.27951345  1.        ]]. Action = [[-0.19676358 -0.17701209 -0.17877126  0.3973559 ]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.0969, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 116 is 1
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.26460868 -0.01924508  0.27951345  1.        ]]. Action = [[-0.48429275  0.07215726 -0.28490484  0.8351116 ]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.0862, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 117 is 1
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.26463947 -0.01931384  0.27949932  1.        ]]. Action = [[-0.9012147  -0.8416172  -0.27496827 -0.7463558 ]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 118 is 1
Human Feedback received at timestep 118 of 1
Current timestep = 119. State = [[-0.2646169  -0.01929769  0.2795166   1.        ]]. Action = [[-0.6309067  -0.7273176   0.06823719  0.30780947]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.0848, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 119 is 1
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.26463947 -0.01931384  0.27949932  1.        ]]. Action = [[-0.68175274 -0.6538713  -0.5298428   0.93986607]]. Reward = [10.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.0781, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 120 is 1
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.26463947 -0.01931384  0.27949932  1.        ]]. Action = [[-0.83326834 -0.16326076  0.87217975  0.71282244]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 121 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 121 is tensor(0.0745, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 121 is 1
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.26463947 -0.01931384  0.27949932  1.        ]]. Action = [[-0.64305615 -0.8497867   0.8913429  -0.54645544]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 122 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 122 is tensor(0.0643, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 122 is 1
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.26463947 -0.01931384  0.27949932  1.        ]]. Action = [[-0.83890194 -0.95127815  0.25862336  0.1094054 ]]. Reward = [10.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0673, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 123 is 1
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.2619343  -0.03175048  0.29042143  1.        ]]. Action = [[ 0.10074818 -0.62797123  0.60852826  0.33581018]]. Reward = [10.]
Curr episode timestep = 20
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0797, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 124 is 1
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.2520684  -0.05359082  0.3171362   1.        ]]. Action = [[ 0.42990184 -0.35206443  0.41053772  0.8967643 ]]. Reward = [10.]
Curr episode timestep = 21
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 125 is 1
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.24098298 -0.06512262  0.334207    1.        ]]. Action = [[-0.6457894  -0.86913157  0.874892    0.13941228]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0591, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 126 is 1
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.25086448  0.002661    0.23262739  1.        ]]. Action = [[-0.17079353 -0.78248    -0.92102015 -0.04689276]]. Reward = [-100.]
Curr episode timestep = 23
Scene graph at timestep 127 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 127 is tensor(0.0699, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 127 is 1
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.24804568  0.00303622  0.23270747  1.        ]]. Action = [[0.24416697 0.14504504 0.29552805 0.67451334]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0691, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 128 is 1
Human Feedback received at timestep 128 of 1
Current timestep = 129. State = [[-0.2468298   0.00359869  0.233299    1.        ]]. Action = [[-0.6366506  -0.71466535  0.53180134  0.7029424 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 129 is 1
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.2467123   0.00361289  0.23334518  1.        ]]. Action = [[-0.5919356   0.6746613   0.6046102  -0.37865114]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0663, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 130 is 1
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.2467034   0.00372344  0.23334105  1.        ]]. Action = [[-0.80439544 -0.22203058  0.48637986  0.2210257 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0579, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 131 is 1
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.24763416 -0.00823275  0.23434521  1.        ]]. Action = [[-0.30960464 -0.6680534   0.09348631  0.98048997]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0522, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 132 is 1
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.25344712 -0.03423429  0.22953178  1.        ]]. Action = [[-0.14922851 -0.5991668  -0.6231692   0.72834575]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0551, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 133 is 1
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.25721946 -0.04864676  0.22091603  1.        ]]. Action = [[-0.6349114  -0.81527615  0.1894871   0.8514557 ]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0461, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 134 is 1
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.2584869  -0.05078841  0.22059745  1.        ]]. Action = [[-0.94042844 -0.03882515  0.77235913  0.9055095 ]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0370, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 135 is 1
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.25893617 -0.05153352  0.22051102  1.        ]]. Action = [[-0.4321531   0.28655434  0.49171495  0.592793  ]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.0580, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 136 is 1
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.25408563 -0.06984925  0.22651213  1.        ]]. Action = [[ 0.36422956 -0.9065004   0.4870093   0.09027624]]. Reward = [10.]
Curr episode timestep = 9
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0565, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 137 is 1
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.25134915 -0.09191814  0.23385279  1.        ]]. Action = [[-0.45236236 -0.8957946  -0.07586026  0.39027297]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 138 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 138 is tensor(0.0612, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 138 is 1
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.25068238 -0.09545048  0.23541191  1.        ]]. Action = [[-0.92103136 -0.5634099  -0.34136313 -0.40916812]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0658, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 139 is -1
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.25068125 -0.0956797   0.23547602  1.        ]]. Action = [[-0.5117561  -0.66767037 -0.88398504  0.32356334]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 140 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.0665, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 140 is 1
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.25288954 -0.11077632  0.23941872  1.        ]]. Action = [[-0.21609014 -0.68000966  0.30982542  0.29614627]]. Reward = [10.]
Curr episode timestep = 13
Scene graph at timestep 141 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0724, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 141 is 1
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.25682792 -0.12888284  0.24430826  1.        ]]. Action = [[-0.32492435 -0.73889625 -0.17329997  0.09998536]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 142 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0766, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 142 is 1
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.25776523 -0.13116215  0.2449773   1.        ]]. Action = [[-0.579511   -0.76549083  0.09446859  0.00506544]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 143 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.0706, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 143 is 1
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.25779638 -0.1312712   0.24500228  1.        ]]. Action = [[-0.8493659  -0.52537274 -0.02426243  0.8488581 ]]. Reward = [10.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 144 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0626, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 144 is 1
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.25916076 -0.11682277  0.25469494  1.        ]]. Action = [[-0.07607472  0.8238872   0.72174513  0.7274978 ]]. Reward = [10.]
Curr episode timestep = 17
Scene graph at timestep 145 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0512, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 145 is -1
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.26331818 -0.09924846  0.2730058   1.        ]]. Action = [[-0.7737814  -0.02222586  0.22167277  0.59178305]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 146 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.0645, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 146 is -1
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.25083083  0.00264609  0.23260525  1.        ]]. Action = [[ 0.126477   -0.5910437   0.7545806  -0.05337518]]. Reward = [-100.]
Curr episode timestep = 19
Scene graph at timestep 147 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 147 is tensor(0.0604, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 147 is 1
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-0.25044698  0.00260117  0.23270322  1.        ]]. Action = [[-0.6984462   0.48993182  0.95345616  0.70189834]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 148 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 148 is tensor(0.0379, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 148 is -1
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.25038394  0.00254285  0.23273674  1.        ]]. Action = [[-0.6017588  -0.9157316   0.68327606  0.9013598 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 149 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 149 is tensor(0.0397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 149 is 1
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.25038394  0.00254285  0.23273674  1.        ]]. Action = [[-0.80704546 -0.46135378  0.2222122   0.6803899 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 150 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 150 is tensor(0.0497, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 150 is 1
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.25038394  0.00254285  0.23273674  1.        ]]. Action = [[-0.4700173  -0.25665236  0.89458025  0.82534504]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 151 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 151 is tensor(0.0396, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 151 is 1
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.25143486 -0.01267361  0.23406047  1.        ]]. Action = [[-0.31643456 -0.7935251   0.24143791  0.6796417 ]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 152 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 152 is tensor(0.0509, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 152 is 1
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.2545522  -0.03091878  0.23594677  1.        ]]. Action = [[-0.74197084 -0.21400797  0.27125537  0.74835193]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 153 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 153 is tensor(0.0474, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 153 is 1
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.25516725 -0.03454878  0.23624742  1.        ]]. Action = [[-0.7171518   0.14191794 -0.6378853   0.20997143]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 154 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 154 is tensor(0.0593, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 154 is 1
Human Feedback received at timestep 154 of 1
Current timestep = 155. State = [[-0.2555467  -0.03621035  0.23638627  1.        ]]. Action = [[-0.50044125 -0.9472554   0.1404401   0.47461116]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.0468, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 155 is 1
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.25573316 -0.03678306  0.2364442   1.        ]]. Action = [[-0.35517436 -0.57151127 -0.56403726  0.55308247]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 156 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0557, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 156 is 1
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.2558587  -0.03713207  0.23648177  1.        ]]. Action = [[-0.9371613 -0.9183301  0.6082499  0.719728 ]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 157 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 157 is tensor(0.0327, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 157 is 1
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.43752122 -0.9606286   0.9000945   0.738014  ]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.0366, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 158 is 1
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.8163397   0.25064754  0.5277908   0.78484356]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.0397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 159 is 1
Human Feedback received at timestep 159 of 1
Current timestep = 160. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.6576641  -0.18053663 -0.71947384  0.5114236 ]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.0532, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 160 is 1
Human Feedback received at timestep 160 of 1
Current timestep = 161. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.8926431  -0.09014255  0.7383466   0.71135914]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.0390, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 161 is 1
Human Feedback received at timestep 161 of 1
Current timestep = 162. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.91132563 -0.16406697 -0.3344962   0.89040565]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.0435, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 162 is 1
Human Feedback received at timestep 162 of 1
Current timestep = 163. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.8074089  -0.43264008  0.11068654 -0.3571731 ]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.0538, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 163 is 1
Human Feedback received at timestep 163 of 1
Current timestep = 164. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.8767524  -0.92038643 -0.03357059  0.3611467 ]]. Reward = [10.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.0419, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 164 is 1
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.82857174  0.04280722  0.40247953  0.84525657]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.0397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 165 is 1
Human Feedback received at timestep 165 of 1
Current timestep = 166. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.641279   -0.42889524 -0.38333106 -0.7398953 ]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.0501, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 166 is 1
Human Feedback received at timestep 166 of 1
Current timestep = 167. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.8968889  0.0724262  0.5545678  0.59552  ]]. Reward = [10.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.0386, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 167 is 1
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.2558885  -0.03729424  0.23649436  1.        ]]. Action = [[-0.70599246 -0.1935327   0.887818    0.8463552 ]]. Reward = [10.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 168 is 1
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.2529009  -0.03891592  0.23787302  1.        ]]. Action = [[ 0.3987559  -0.15452236 -0.03317475  0.6843052 ]]. Reward = [10.]
Curr episode timestep = 21
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.0466, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 169 is 1
Human Feedback received at timestep 169 of 1
Current timestep = 170. State = [[-0.25141677 -0.03999782  0.23879158  1.        ]]. Action = [[-0.86682826 -0.3575486  -0.59341013  0.5894121 ]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.0334, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 170 is 1
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.25144762 -0.0401608   0.23880443  1.        ]]. Action = [[-0.73462987 -0.33349884  0.88315964  0.8443105 ]]. Reward = [10.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 171 is 1
Human Feedback received at timestep 171 of 1
Current timestep = 172. State = [[-0.25144762 -0.0401608   0.23880443  1.        ]]. Action = [[-0.873273   -0.20105416  0.6927265   0.8682196 ]]. Reward = [10.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 172 is 1
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.25125682 -0.04023233  0.23890555  1.        ]]. Action = [[-0.68327093 -0.9874501   0.90198517  0.46379423]]. Reward = [10.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 173 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 173 is tensor(0.0254, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 173 is 1
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.25125682 -0.04023233  0.23890555  1.        ]]. Action = [[-0.85420924 -0.93164706 -0.16209608  0.8380618 ]]. Reward = [10.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 174 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 174 is 1
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.25128844 -0.04039904  0.23891872  1.        ]]. Action = [[-0.8971369  -0.45270616 -0.42021018  0.4677204 ]]. Reward = [10.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 175 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 175 is tensor(0.0328, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 175 is 1
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.2512988  -0.0404535   0.23892303  1.        ]]. Action = [[-0.81662023  0.6935332  -0.71823734  0.87851727]]. Reward = [10.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 176 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 176 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 176 is -1
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.2512988  -0.0404535   0.23892303  1.        ]]. Action = [[-0.8338611  -0.86332005 -0.3116243   0.87067914]]. Reward = [10.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 177 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.0253, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 177 is 1
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.25131944 -0.04056206  0.23893163  1.        ]]. Action = [[-0.342337   -0.58055234  0.12766314  0.8358054 ]]. Reward = [10.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Scene graph at timestep 178 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 178 is 1
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.2513298  -0.04061651  0.23893593  1.        ]]. Action = [[-0.9742686 -0.7277604  0.3269372  0.160424 ]]. Reward = [10.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 179 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 179 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 179 is 1
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.24999739 -0.05893868  0.23713031  1.        ]]. Action = [[ 0.00813866 -0.92256224 -0.212812    0.39038873]]. Reward = [10.]
Curr episode timestep = 32
Scene graph at timestep 180 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0348, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 180 is 1
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.25005242 -0.0796349   0.23374805  1.        ]]. Action = [[-0.8789917   0.02526486 -0.2484929   0.17014134]]. Reward = [10.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 181 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0393, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 181 is 1
Human Feedback received at timestep 181 of 1
Current timestep = 182. State = [[-0.2508138  -0.10449129  0.23473863  1.        ]]. Action = [[-0.24299419 -0.99209034 -0.10477191  0.77620983]]. Reward = [10.]
Curr episode timestep = 34
Scene graph at timestep 182 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 182 is tensor(0.0287, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 182 is 1
Human Feedback received at timestep 182 of 1
Current timestep = 183. State = [[-0.256406   -0.14111285  0.22217816  1.        ]]. Action = [[-0.01967084 -0.48394167 -0.901921    0.12123203]]. Reward = [10.]
Curr episode timestep = 35
Scene graph at timestep 183 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 183 is -1
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.25515163 -0.1577357   0.19322224  1.        ]]. Action = [[-0.24055791 -0.30241543 -0.06757116  0.7087418 ]]. Reward = [10.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 184 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.0419, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 184 is -1
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.25361818 -0.15921123  0.1908416   1.        ]]. Action = [[-0.902135   -0.21270275  0.44064116  0.69829655]]. Reward = [10.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 185 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.0353, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 185 is 1
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.25347212 -0.1595205   0.19093889  1.        ]]. Action = [[-0.5198165  -0.8862254  -0.28562558  0.95416737]]. Reward = [10.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 186 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.0293, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 186 is 1
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.24680793 -0.17224614  0.19691795  1.        ]]. Action = [[ 0.41565752 -0.6142599   0.50439763  0.40010643]]. Reward = [10.]
Curr episode timestep = 39
Scene graph at timestep 187 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.0457, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 187 is 1
Human Feedback received at timestep 187 of 1
Current timestep = 188. State = [[-0.24097466 -0.18470213  0.2022139   1.        ]]. Action = [[-0.8126718  -0.9026339   0.95661354  0.77621126]]. Reward = [10.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 188 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 188 is tensor(0.0262, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 188 is -1
Human Feedback received at timestep 188 of -1
Current timestep = 189. State = [[-0.23976971 -0.18881272  0.20429559  1.        ]]. Action = [[-0.7197974  -0.28538537  0.8892739   0.90633416]]. Reward = [10.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 189 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0371, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 189 is -1
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.24018769 -0.1900967   0.2046952   1.        ]]. Action = [[-0.6947341  -0.82252264  0.09372771  0.24217391]]. Reward = [10.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 190 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0393, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 190 is 1
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.2502419   0.00245937  0.23268133  1.        ]]. Action = [[-0.15942287 -0.7560081  -0.29293406 -0.36642843]]. Reward = [-100.]
Curr episode timestep = 43
Scene graph at timestep 191 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0440, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 191 is -1
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-2.4849123e-01  5.8183691e-04  2.3434608e-01  1.0000000e+00]]. Action = [[-0.51828617 -0.84036714 -0.2105124   0.887475  ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 192 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 192 is 1
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-2.4816549e-01 -4.2973479e-04  2.3479027e-01  1.0000000e+00]]. Action = [[-0.6755134  -0.5003487  -0.41775358  0.664101  ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 193 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 193 is 1
Human Feedback received at timestep 193 of 1
Current timestep = 194. State = [[-2.4803764e-01 -6.4485241e-04  2.3503993e-01  1.0000000e+00]]. Action = [[-0.43130147 -0.35320246  0.5298414   0.89648926]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 194 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0318, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 194 is 1
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.24807307 -0.00103075  0.23505548  1.        ]]. Action = [[-0.5650818   0.20008838 -0.35670793  0.5601225 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 195 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 195 is 1
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.25285885  0.00283752  0.23246391  1.        ]]. Action = [[-0.34644824  0.22976875 -0.40716588  0.4905529 ]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 196 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0382, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 196 is 1
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.25674033  0.00562098  0.23078768  1.        ]]. Action = [[-0.88953483 -0.28056443 -0.1872831   0.5906284 ]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 197 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 197 is 1
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.257103    0.00578588  0.23064971  1.        ]]. Action = [[-0.9454354   0.40264964 -0.94427645  0.6361389 ]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 198 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 198 is -1
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.257103    0.00578588  0.23064971  1.        ]]. Action = [[-0.86989665  0.24996626 -0.6118883   0.87093663]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 199 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 199 is -1
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.2571708   0.00578523  0.23061913  1.        ]]. Action = [[-0.9412164  -0.46647656  0.18420696 -0.5282182 ]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 200 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 200 is 1
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.2571708   0.00578523  0.23061913  1.        ]]. Action = [[-0.7265679  -0.7880583   0.73128176  0.9624816 ]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 201 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0217, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 201 is 1
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.2571708   0.00578523  0.23061913  1.        ]]. Action = [[-0.9383352  -0.60310113 -0.12022269 -0.2548915 ]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 202 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 202 is 1
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.2571708   0.00578523  0.23061913  1.        ]]. Action = [[-0.9403021  -0.43214893  0.20281506  0.6976454 ]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 203 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0235, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 203 is 1
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.25720385  0.00587613  0.23061913  1.        ]]. Action = [[-0.31818676 -0.9376951   0.14290023  0.82011235]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 204 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0215, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 204 is 1
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.25723705  0.00596745  0.23061915  1.        ]]. Action = [[-0.736023   -0.47266465  0.3677721   0.8779383 ]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 205 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0216, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 205 is 1
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.25723705  0.00596745  0.23061915  1.        ]]. Action = [[-0.93041784 -0.91590786 -0.10240054  0.55190504]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 206 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 206 is 1
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.25723705  0.00596745  0.23061915  1.        ]]. Action = [[-0.5505168  -0.8574468  -0.00320715  0.8974688 ]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 207 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0176, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 207 is 1
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.25264147  0.00221564  0.23141424  1.        ]]. Action = [[-1.0622907e-01 -7.7240020e-01 -3.3813715e-04 -2.7895397e-01]]. Reward = [-100.]
Curr episode timestep = 16
Scene graph at timestep 208 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 208 is 1
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.25797945 -0.01896289  0.22981255  1.        ]]. Action = [[-0.4050269  -0.0025481  -0.03834832  0.23949659]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 209 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 209 is 1
Human Feedback received at timestep 209 of 1
Current timestep = 210. State = [[-0.2590898  -0.02566853  0.23022437  1.        ]]. Action = [[-0.71185625  0.24255824  0.08552122  0.9057796 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 210 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0201, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 210 is 1
Human Feedback received at timestep 210 of 1
Current timestep = 211. State = [[-0.25952807 -0.02728019  0.22989196  1.        ]]. Action = [[-0.8681218  -0.1909014   0.0826751  -0.50229365]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 211 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0311, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 211 is 1
Human Feedback received at timestep 211 of 1
Current timestep = 212. State = [[-0.25967422 -0.0277631   0.22989461  1.        ]]. Action = [[-0.5255879  -0.23675907 -0.48980176  0.9685799 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 212 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.0234, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 212 is -1
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.25969148 -0.02787065  0.22990194  1.        ]]. Action = [[-0.706357    0.32433403  0.10418904 -0.07758009]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 213 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.0397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 213 is 1
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.2529843  -0.04154497  0.2363373   1.        ]]. Action = [[ 0.50088024 -0.68750244  0.6355295   0.6833775 ]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 214 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 214 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 214 is 1
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.2502777  -0.05699718  0.24275635  1.        ]]. Action = [[-0.78964615  0.89680433 -0.47454977 -0.0971911 ]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 215 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 215 is tensor(0.0347, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 215 is -1
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.25038353 -0.06870606  0.23998989  1.        ]]. Action = [[-0.10558635 -0.45834243 -0.51439005  0.828578  ]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 216 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 216 is tensor(0.0312, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 216 is 1
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.25074017  0.00250606  0.23250934  1.        ]]. Action = [[-0.00302428  0.9015199   0.87452865 -0.15765613]]. Reward = [-100.]
Curr episode timestep = 8
Scene graph at timestep 217 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 217 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 217 is -1
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.24901056  0.00256868  0.23321597  1.        ]]. Action = [[-0.817934    0.11936092  0.309489    0.6912583 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 218 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 218 is tensor(0.0319, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 218 is 1
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.24823347  0.00256847  0.23356499  1.        ]]. Action = [[-0.9547853  -0.79192936  0.70076585  0.86434114]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 219 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 219 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 219 is 1
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.24817358 -0.0091664   0.23710841  1.        ]]. Action = [[-0.24841273 -0.6197941   0.39082026  0.17296982]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 220 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 220 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 220 is 1
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.24932754 -0.02243648  0.24184798  1.        ]]. Action = [[-0.93528175  0.45229924  0.54306424  0.5224881 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 221 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 221 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 221 is 1
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.24990943 -0.02542441  0.2419487   1.        ]]. Action = [[-0.42948985 -0.22499734 -0.21714813  0.8313832 ]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 222 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 222 is tensor(0.0298, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 222 is 1
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.24998498 -0.02597332  0.24202095  1.        ]]. Action = [[-0.39981043 -0.4604324  -0.1562481   0.8595325 ]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 223 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 223 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 223 is 1
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.25004795 -0.02628653  0.24208513  1.        ]]. Action = [[-0.74943846 -0.52107215  0.8842766   0.7675128 ]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 224 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 224 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 224 is 1
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.25012353 -0.02633733  0.24205561  1.        ]]. Action = [[-0.8925202  -0.20818532 -0.8283232   0.80305886]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 225 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 225 is tensor(0.0250, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 225 is 1
Human Feedback received at timestep 225 of 1
Current timestep = 226. State = [[-0.25012353 -0.02633733  0.24205561  1.        ]]. Action = [[-0.96314186 -0.18015426  0.92303824  0.7964163 ]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 226 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 226 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 226 is -1
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.25012353 -0.02633733  0.24205561  1.        ]]. Action = [[-0.7209286  -0.55536366 -0.11895049  0.9300597 ]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 227 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 227 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 227 is 1
Human Feedback received at timestep 227 of 1
Current timestep = 228. State = [[-0.25012353 -0.02633733  0.24205561  1.        ]]. Action = [[-0.92148745 -0.47627962 -0.11264277  0.9384358 ]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 228 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 228 is tensor(0.0236, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 228 is 1
Human Feedback received at timestep 228 of 1
Current timestep = 229. State = [[-0.25171563 -0.01581149  0.24191539  1.        ]]. Action = [[ 0.10271072  0.57416594 -0.07601273  0.438874  ]]. Reward = [10.]
Curr episode timestep = 11
Scene graph at timestep 229 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 229 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 229 is -1
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.25514635 -0.00452211  0.2420507   1.        ]]. Action = [[-0.48803312 -0.013946    0.20065272 -0.35267705]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 230 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 230 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 230 is 1
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.25534275 -0.00353771  0.2420585   1.        ]]. Action = [[-0.573194   -0.3386944   0.9057642   0.72583723]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 231 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 231 is tensor(0.0168, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 231 is 1
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.2553797  -0.00350105  0.2420618   1.        ]]. Action = [[-0.7192426   0.23846376  0.34779263  0.53559494]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 232 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 232 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 232 is 1
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.2554735  -0.00322642  0.24206181  1.        ]]. Action = [[-0.9327745   0.8385979  -0.7390664   0.83730173]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 233 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 233 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 233 is -1
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[-0.2554735  -0.00322642  0.24206181  1.        ]]. Action = [[-0.83284956 -0.7718244   0.2739141   0.78133655]]. Reward = [10.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 234 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 234 is tensor(0.0143, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 234 is 1
Human Feedback received at timestep 234 of 1
Current timestep = 235. State = [[-0.2554735  -0.00322642  0.24206181  1.        ]]. Action = [[-0.5662121 -0.5430882  0.8113589  0.8964797]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 235 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 235 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 235 is 1
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.2554735  -0.00322642  0.24206181  1.        ]]. Action = [[-0.8003141  -0.2601173   0.89986026  0.82853556]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 236 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 236 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 236 is 1
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.25667435 -0.00821412  0.2373903   1.        ]]. Action = [[-0.19802547 -0.37112272 -0.43419564  0.9350687 ]]. Reward = [10.]
Curr episode timestep = 19
Scene graph at timestep 237 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 237 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 237 is 1
Human Feedback received at timestep 237 of 1
Current timestep = 238. State = [[-0.25868803 -0.01261529  0.23212442  1.        ]]. Action = [[-0.7199422  -0.7603213   0.15043223 -0.07334447]]. Reward = [10.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 238 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 238 is tensor(0.0184, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 238 is 1
Human Feedback received at timestep 238 of 1
Current timestep = 239. State = [[-0.2586917  -0.01337268  0.23222537  1.        ]]. Action = [[-0.9521717   0.6535808  -0.18277049  0.6699065 ]]. Reward = [10.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 239 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 239 is tensor(0.0181, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 239 is -1
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.25871238 -0.01353716  0.23223726  1.        ]]. Action = [[-0.4069687  -0.7444533   0.5601798  -0.15633196]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 240 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 240 is tensor(0.0209, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 240 is 1
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-0.2580281  -0.0169258   0.23831955  1.        ]]. Action = [[-0.11326081 -0.1353761   0.6273565   0.971472  ]]. Reward = [10.]
Curr episode timestep = 23
Scene graph at timestep 241 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 241 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 241 is 1
Human Feedback received at timestep 241 of 1
Current timestep = 242. State = [[-0.25822338 -0.02195913  0.24775872  1.        ]]. Action = [[-0.5907388  -0.5668187   0.21936727  0.9648162 ]]. Reward = [10.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 242 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 242 is tensor(0.0230, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 242 is 1
Human Feedback received at timestep 242 of 1
Current timestep = 243. State = [[-0.25845197 -0.02267622  0.24892077  1.        ]]. Action = [[-0.55779666 -0.6959086  -0.46423388  0.85637426]]. Reward = [10.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 243 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 243 is tensor(0.0252, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 243 is 1
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.25241235 -0.01811997  0.25240895  1.        ]]. Action = [[0.57189345 0.3065529  0.15583038 0.69039273]]. Reward = [10.]
Curr episode timestep = 26
Scene graph at timestep 244 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 244 is tensor(0.0254, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 244 is 1
Human Feedback received at timestep 244 of 1
Current timestep = 245. State = [[-0.24470703 -0.01503937  0.25781584  1.        ]]. Action = [[-0.6595762   0.79005015  0.02862573  0.43351305]]. Reward = [10.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 245 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 245 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 245 is -1
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.24311206 -0.01432093  0.25944075  1.        ]]. Action = [[-0.5854015 -0.6433149 -0.8007222  0.5789497]]. Reward = [10.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 246 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 246 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 246 is -1
Human Feedback received at timestep 246 of -1
Current timestep = 247. State = [[-0.24274132 -0.01402191  0.25974008  1.        ]]. Action = [[-0.5997287  -0.5354655   0.48668778  0.5609783 ]]. Reward = [10.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 247 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 247 is tensor(0.0242, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 247 is 1
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.2457253  -0.01047258  0.26815742  1.        ]]. Action = [[-0.3376854   0.07813525  0.6055461   0.671499  ]]. Reward = [10.]
Curr episode timestep = 30
Scene graph at timestep 248 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 248 is tensor(0.0226, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 248 is 1
Human Feedback received at timestep 248 of 1
Current timestep = 249. State = [[-0.2510008  -0.00309949  0.2954444   1.        ]]. Action = [[-0.08612734  0.14791691  0.80689406  0.8130934 ]]. Reward = [10.]
Curr episode timestep = 31
Scene graph at timestep 249 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 249 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 249 is 1
Human Feedback received at timestep 249 of 1
Current timestep = 250. State = [[-0.25155923 -0.01362784  0.31926832  1.        ]]. Action = [[ 0.34638488 -0.8158301  -0.22000289  0.42850578]]. Reward = [10.]
Curr episode timestep = 32
Scene graph at timestep 250 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 250 is tensor(0.0244, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 250 is 1
Human Feedback received at timestep 250 of 1
Current timestep = 251. State = [[-0.24726208 -0.03016737  0.3217234   1.        ]]. Action = [[-0.7293667 -0.7228123  0.818508   0.924808 ]]. Reward = [10.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 251 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 251 is tensor(0.0213, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 251 is 1
Human Feedback received at timestep 251 of 1
Current timestep = 252. State = [[-0.24966119 -0.04408258  0.32770452  1.        ]]. Action = [[-0.34709477 -0.4968474   0.39420724  0.9380665 ]]. Reward = [10.]
Curr episode timestep = 34
Scene graph at timestep 252 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 252 is tensor(0.0217, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 252 is -1
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.25430626 -0.05757104  0.33468714  1.        ]]. Action = [[-0.8925762  -0.54253733 -0.86594695  0.96655726]]. Reward = [10.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 253 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 253 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 253 is -1
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.254304   -0.05978665  0.33587852  1.        ]]. Action = [[-0.6257313  -0.82193327  0.03106439  0.8533987 ]]. Reward = [10.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 254 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 254 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 254 is 1
Human Feedback received at timestep 254 of 1
Current timestep = 255. State = [[-0.2541599  -0.06025583  0.33622703  1.        ]]. Action = [[-0.8170478  -0.62677526 -0.11391485 -0.28481477]]. Reward = [10.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 255 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 255 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 255 is 1
Human Feedback received at timestep 255 of 1
Current timestep = 256. State = [[-0.25507155 -0.05258679  0.32642516  1.        ]]. Action = [[ 0.06531024  0.458184   -0.8663009   0.7708317 ]]. Reward = [10.]
Curr episode timestep = 38
Scene graph at timestep 256 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 256 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 256 is -1
Human Feedback received at timestep 256 of -1
Current timestep = 257. State = [[-0.25360587 -0.04604216  0.311938    1.        ]]. Action = [[-0.76365685  0.8635268   0.22896326  0.9000566 ]]. Reward = [10.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 257 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 257 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 257 is -1
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.25328773 -0.04512033  0.31005287  1.        ]]. Action = [[-0.6255241  -0.7975163   0.28907228  0.64477587]]. Reward = [10.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 258 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 258 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 258 is 1
Human Feedback received at timestep 258 of 1
Current timestep = 259. State = [[-0.25319862 -0.04504951  0.31003192  1.        ]]. Action = [[-0.92495036  0.16708839  0.81741714  0.8897135 ]]. Reward = [10.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 259 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 259 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 259 is -1
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.25363404 -0.05096701  0.31524542  1.        ]]. Action = [[-0.14654744 -0.41570514  0.5355966   0.74280953]]. Reward = [10.]
Curr episode timestep = 42
Scene graph at timestep 260 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 260 is tensor(0.0241, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 260 is 1
Human Feedback received at timestep 260 of 1
Current timestep = 261. State = [[-0.25566876 -0.05798501  0.3206284   1.        ]]. Action = [[-0.5799366  -0.62278354  0.34309173  0.28893602]]. Reward = [10.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 261 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 261 is tensor(0.0268, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 261 is 1
Human Feedback received at timestep 261 of 1
Current timestep = 262. State = [[-0.25629595 -0.05956407  0.32082504  1.        ]]. Action = [[-0.3175158  -0.50683963  0.5699489   0.43852222]]. Reward = [10.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 262 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 262 is tensor(0.0277, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 262 is 1
Human Feedback received at timestep 262 of 1
Current timestep = 263. State = [[-0.25642172 -0.05995072  0.32086825  1.        ]]. Action = [[-0.5547821  -0.66913664  0.9122865   0.8265401 ]]. Reward = [10.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 263 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 263 is tensor(0.0235, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 263 is 1
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.25646538 -0.06011318  0.3208845   1.        ]]. Action = [[-0.5783511 -0.5203787  0.6086875  0.7075801]]. Reward = [10.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 264 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 264 is tensor(0.0243, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 264 is 1
Human Feedback received at timestep 264 of 1
Current timestep = 265. State = [[-0.25646538 -0.06011318  0.3208845   1.        ]]. Action = [[-0.8482609  -0.8222734   0.45371425  0.9158387 ]]. Reward = [10.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 265 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 265 is tensor(0.0233, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 265 is 1
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.25646538 -0.06011318  0.3208845   1.        ]]. Action = [[-0.76141506  0.4531455   0.48638904  0.8307512 ]]. Reward = [10.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 266 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 266 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 266 is 1
Human Feedback received at timestep 266 of 1
Current timestep = 267. State = [[-0.25646538 -0.06011318  0.3208845   1.        ]]. Action = [[-0.85721624  0.45448673  0.81816983  0.8193313 ]]. Reward = [10.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 267 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 267 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 267 is -1
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.25646538 -0.06011318  0.3208845   1.        ]]. Action = [[-0.8930241   0.11639416  0.6685343   0.38836098]]. Reward = [10.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Scene graph at timestep 268 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 268 is tensor(0.0216, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 268 is 1
Human Feedback received at timestep 268 of 1
Current timestep = 269. State = [[-0.25646538 -0.06011318  0.3208845   1.        ]]. Action = [[-0.59311956  0.3556471  -0.5980216  -0.16733581]]. Reward = [10.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 269 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 269 is tensor(0.0252, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 269 is -1
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.25646538 -0.06011318  0.3208845   1.        ]]. Action = [[-0.9458966  -0.7184278   0.14643526  0.21831048]]. Reward = [10.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 270 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 270 is tensor(0.0193, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 270 is 1
Human Feedback received at timestep 270 of 1
Current timestep = 271. State = [[-0.25639406 -0.07227178  0.33361787  1.        ]]. Action = [[-0.0660181 -0.5982275  0.7131163  0.6679065]]. Reward = [10.]
Curr episode timestep = 53
Scene graph at timestep 271 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 271 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 271 is -1
Human Feedback received at timestep 271 of -1
