Current timestep = 0. State = [[-0.22606033 -0.00850273  0.25644544  1.        ]]. Action = [[ 0.6752727  -0.29342973  0.8605362   0.00105536]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3906, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 0 is 1
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.17088874 -0.05317277  0.30892482  1.        ]]. Action = [[ 0.8012171  -0.98565406 -0.15904337  0.15532458]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3650, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1 is 1
Human Feedback received at timestep 1 of 1
Current timestep = 2. State = [[-0.1266118  -0.10799914  0.3060963   1.        ]]. Action = [[ 0.2328105   0.9852332   0.96431446 -0.25266683]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3186, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 2 is -1
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.13302714 -0.10495489  0.30006096  1.        ]]. Action = [[-0.81515795  0.45365965 -0.20846474  0.16553354]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 3 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 3 is tensor(0.3090, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 3 is 1
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.15670209 -0.11855441  0.3048526   1.        ]]. Action = [[ 0.1444838  -0.9534328   0.20702124  0.96218514]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 4 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 4 is tensor(0.2770, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 4 is 1
Human Feedback received at timestep 4 of 1
Current timestep = 5. State = [[-0.18189245 -0.14693063  0.29334655  1.        ]]. Action = [[-0.5533213   0.528782   -0.87240547  0.00386441]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 5 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2462, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 5 is -1
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.2506687   0.00264554  0.23279503  1.        ]]. Action = [[ 0.29768765  0.7880323  -0.23605454 -0.8917599 ]]. Reward = [-100.]
Curr episode timestep = 6
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 6 is -1
Human Feedback received at timestep 6 of -1
Current timestep = 7. State = [[-2.6139259e-01 -4.7493557e-04  2.1672644e-01  1.0000000e+00]]. Action = [[-0.118218   -0.10333502 -0.87576175  0.23117602]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.2016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 7 is 1
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.27213976 -0.00249543  0.1621744   1.        ]]. Action = [[-0.5828341   0.5449505   0.36707282  0.5159364 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1855, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 8 is -1
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.27428758 -0.00421338  0.15029281  1.        ]]. Action = [[-0.24512404  0.75547385 -0.17229652  0.0781666 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 9 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 9 is tensor(0.1678, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 9 is 1
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.27434763 -0.00506113  0.15033817  1.        ]]. Action = [[-0.8643231  -0.66252863 -0.14588147  0.6480062 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1333, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 10 is 1
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.27445373 -0.00512503  0.15031545  1.        ]]. Action = [[-0.7178455   0.4897883   0.70477986 -0.06461382]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1202, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 11 is 1
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.2744271  -0.00511632  0.15032396  1.        ]]. Action = [[-0.28670436 -0.802062    0.673265   -0.03586453]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 12 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 12 is tensor(0.0830, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 12 is 1
Human Feedback received at timestep 12 of 1
Current timestep = 13. State = [[-0.2744271  -0.00511632  0.15032396  1.        ]]. Action = [[-0.7474091   0.8583014   0.09504271 -0.17060357]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.0939, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 13 is 1
Human Feedback received at timestep 13 of 1
Current timestep = 14. State = [[-0.2744271  -0.00511632  0.15032396  1.        ]]. Action = [[-0.2377578  -0.33005083  0.96674144  0.03993416]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0611, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 14 is 1
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.2507249   0.00285642  0.23218524  1.        ]]. Action = [[ 0.46682513  0.54127073 -0.92465526 -0.00312877]]. Reward = [-100.]
Curr episode timestep = 8
Scene graph at timestep 15 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 15 is tensor(0.0492, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 15 is 1
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.22464421  0.00701051  0.2514088   1.        ]]. Action = [[0.94138634 0.1302836  0.55171895 0.20365655]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0432, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 16 is 1
Human Feedback received at timestep 16 of 1
Current timestep = 17. State = [[-0.25094154  0.00217671  0.23251696  1.        ]]. Action = [[ 0.6884086   0.65090084  0.13570333 -0.09389901]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 17 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0453, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 17 is 1
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.2527136   0.00207905  0.23258305  1.        ]]. Action = [[-0.66919345 -0.64613146  0.21927154 -0.5521149 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 18 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 18 is tensor(0.0184, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 18 is 1
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.24508032 -0.01484776  0.23050286  1.        ]]. Action = [[ 0.37918234 -0.48334527 -0.16372836  0.6689271 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 19 is 1
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.2319421  -0.03835651  0.2162965   1.        ]]. Action = [[ 0.22909534 -0.08179128 -0.79917794  0.00860476]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 20 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 20 is tensor(0.0222, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 20 is 1
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-0.21868959 -0.04481648  0.16239066  1.        ]]. Action = [[-0.3609243  -0.10213327  0.8765526  -0.8583778 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 21 is 1
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.21280175 -0.0456369   0.1504133   1.        ]]. Action = [[ 0.56960416  0.6953603   0.45617115 -0.39414692]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0216, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 22 is 1
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.21244031 -0.04568615  0.15003413  1.        ]]. Action = [[-0.9601088   0.7637862   0.46661747  0.54632676]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 23 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 23 is tensor(0.0080, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 23 is 1
Human Feedback received at timestep 23 of 1
Current timestep = 24. State = [[-0.21244031 -0.04568615  0.15003413  1.        ]]. Action = [[-0.7963659  -0.46450913  0.53591466 -0.24208665]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 24 is 1
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.21377674 -0.0362955   0.16122413  1.        ]]. Action = [[-0.17665273  0.31758773  0.5136168   0.5105357 ]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 25 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 25 is 1
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.21570662 -0.02468384  0.18117009  1.        ]]. Action = [[ 0.7657378  -0.1425739   0.19286287  0.5531498 ]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: No entry zone
Scene graph at timestep 26 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 26 is 1
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.2164768  -0.0238935   0.18364449  1.        ]]. Action = [[-0.83412176  0.37234414 -0.38358897 -0.28399932]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 27 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 27 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 27 is 1
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.2503072   0.00275128  0.23284854  1.        ]]. Action = [[-0.02857339 -0.6637439  -0.59068495 -0.17956805]]. Reward = [-100.]
Curr episode timestep = 10
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 28 is 1
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.22991835 -0.02573735  0.25682956  1.        ]]. Action = [[ 0.5468637  -0.8052732   0.80459404  0.89306486]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 29 is 1
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.20840271 -0.09627178  0.31115925  1.        ]]. Action = [[ 0.03841209 -0.9043726   0.26364708  0.21448505]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 30 is 1
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.25076512  0.00264581  0.23264334  1.        ]]. Action = [[ 0.40072358 -0.27211064 -0.66887385 -0.58963597]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 31 is -1
Human Feedback received at timestep 31 of -1
Current timestep = 32. State = [[-0.25292325  0.00143987  0.23133145  1.        ]]. Action = [[-0.7943689   0.40996194 -0.9927811   0.42921138]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 32 is -1
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.2501371   0.00261728  0.23273808  1.        ]]. Action = [[ 0.408468  -0.7140982 -0.5433065 -0.5872163]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 33 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 33 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 33 is 1
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-2.5018951e-01  7.4037915e-04  2.3307148e-01  1.0000000e+00]]. Action = [[-0.6815797   0.21520114  0.7513851   0.3485936 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 34 is 1
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.25633332  0.01623382  0.22223252  1.        ]]. Action = [[-0.08099532  0.49185753 -0.49154675  0.8988693 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 35 is 1
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.25102684  0.00236148  0.23244096  1.        ]]. Action = [[ 0.5557765   0.4427421   0.44392133 -0.24590302]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 36 is 1
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.2512061   0.00373438  0.23344652  1.        ]]. Action = [[-0.38783872  0.4857068  -0.93955684  0.7931808 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 37 is -1
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.25121197  0.00402874  0.23363243  1.        ]]. Action = [[-0.35367608 -0.8155038   0.901541   -0.9946224 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 38 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 38 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 38 is 1
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.25192612  0.00263219  0.23302081  1.        ]]. Action = [[-0.11770529 -0.3319745   0.35096788 -0.3000815 ]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 39 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 39 is tensor(0.0358, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 39 is 1
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.25203404  0.00269193  0.23297186  1.        ]]. Action = [[-0.25549734  0.82694316  0.33964312  0.9417374 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 40 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 40 is -1
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.2525397  -0.00409536  0.24308333  1.        ]]. Action = [[-0.13767749 -0.16994584  0.44255567  0.82827735]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 41 is 1
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.24456239 -0.00708952  0.25734723  1.        ]]. Action = [[ 0.6276827   0.1716528  -0.37505138  0.16221094]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 42 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 42 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 42 is 1
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.21750987 -0.00261967  0.24233076  1.        ]]. Action = [[ 0.689242   -0.821321   -0.5042209  -0.54318684]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: No entry zone
Scene graph at timestep 43 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 43 is 1
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.21347366 -0.00228739  0.24125358  1.        ]]. Action = [[ 0.893229    0.86284137 -0.32678908 -0.67475796]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: No entry zone
Scene graph at timestep 44 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 44 is tensor(0.0209, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 44 is 1
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.20872961 -0.02983718  0.26061463  1.        ]]. Action = [[-0.187397  -0.9858362  0.8006954  0.7373769]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 45 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 45 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 45 is 1
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.22081086 -0.0822918   0.30097476  1.        ]]. Action = [[-0.93774366 -0.75827986  0.76354027 -0.40457362]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 46 is 1
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.20313159 -0.11018929  0.33311483  1.        ]]. Action = [[ 0.6289276  -0.54507923  0.8082664   0.4052286 ]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 47 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 47 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 47 is 1
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.17508118 -0.13806586  0.3847258   1.        ]]. Action = [[-0.6505932   0.28968358  0.4086795   0.545068  ]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 48 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 48 is tensor(0.0584, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 48 is -1
Human Feedback received at timestep 48 of -1
Current timestep = 49. State = [[-0.25086263  0.00260096  0.23271875  1.        ]]. Action = [[ 0.9049568   0.61212206 -0.9243982  -0.6529947 ]]. Reward = [-100.]
Curr episode timestep = 9
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.0223, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 49 is -1
Human Feedback received at timestep 49 of -1
Current timestep = 50. State = [[-0.2504043   0.00206796  0.23249581  1.        ]]. Action = [[ 0.2807144  -0.06905651 -0.85900104 -0.32364702]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 50 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 50 is tensor(0.0472, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 50 is 1
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.22252329 -0.02476706  0.24837796  1.        ]]. Action = [[ 0.90015864 -0.8153776   0.34220123  0.27059245]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 51 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 51 is tensor(0.0428, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 51 is 1
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.2509397   0.00255525  0.23268105  1.        ]]. Action = [[ 0.86806834 -0.21637976  0.7586179  -0.60399175]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.0442, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 52 is 1
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.2504611   0.00264354  0.23257896  1.        ]]. Action = [[-0.15858912 -0.91238666 -0.9864476  -0.89606065]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 53 is -1
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.22913101  0.01533412  0.23550422  1.        ]]. Action = [[ 0.691869    0.34702742 -0.10837084  0.06528676]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 54 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 54 is tensor(0.0612, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 54 is 1
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.19668935  0.02905608  0.23256278  1.        ]]. Action = [[-0.766601   -0.89730394  0.9277632  -0.3792991 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 55 is 1
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.25098234  0.00203809  0.23228897  1.        ]]. Action = [[-0.19493175  0.20858729  0.7750709  -0.00323212]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.0612, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 56 is 1
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.25103304  0.00199587  0.23222332  1.        ]]. Action = [[-0.95654714 -0.98538506  0.12601197  0.54865956]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.0378, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 57 is 1
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.23228307  0.02427175  0.2451886   1.        ]]. Action = [[0.68422985 0.6418359  0.4704417  0.7560055 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 58 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 58 is tensor(0.0443, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 58 is -1
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.25122958  0.0019769   0.23236805  1.        ]]. Action = [[ 0.27581584  0.13257682 -0.51516384 -0.49549448]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 59 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 59 is tensor(0.0569, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 59 is -1
Human Feedback received at timestep 59 of -1
Current timestep = 60. State = [[-0.23652476  0.02051671  0.2477006   1.        ]]. Action = [[0.53342354 0.43364918 0.48655713 0.9061874 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.0456, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 60 is -1
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.21357256  0.04176855  0.27183953  1.        ]]. Action = [[-0.8119928 -0.7839476  0.7666321 -0.7220538]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 61 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 61 is tensor(0.0350, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 61 is -1
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.21034265  0.04366298  0.27497736  1.        ]]. Action = [[ 0.916703   -0.09433675 -0.87128526  0.5823369 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 62 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.0490, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 62 is -1
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.18723907  0.02580514  0.29982534  1.        ]]. Action = [[ 0.65849733 -0.5762004   0.7499958   0.39696813]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 63 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 63 is tensor(0.0540, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 63 is 1
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.25076386  0.00249592  0.23253086  1.        ]]. Action = [[ 0.786664   -0.33148915  0.8621342  -0.332977  ]]. Reward = [-100.]
Curr episode timestep = 4
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 64 is 1
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.25327072  0.00192295  0.23124063  1.        ]]. Action = [[-0.5776018  -0.72052443  0.43703175  0.15212929]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.0548, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 65 is 1
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.2533251   0.00210723  0.23127863  1.        ]]. Action = [[-0.2642312  -0.7569653  -0.67053187 -0.46447688]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 66 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 66 is tensor(0.0561, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 66 is 1
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.25334126  0.00215307  0.23127863  1.        ]]. Action = [[ 0.8493891  -0.66753554 -0.38846093  0.9724338 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: No entry zone
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.0435, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 67 is 1
Human Feedback received at timestep 67 of 1
Current timestep = 68. State = [[-0.25278372  0.02327118  0.24572913  1.        ]]. Action = [[-0.00135553  0.93058753  0.8165386   0.22307742]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.0508, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 68 is -1
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.26067078  0.08204422  0.29618117  1.        ]]. Action = [[-0.58592516 -0.25153267  0.67824054  0.54290867]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.0606, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 69 is 1
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.26281765  0.09103943  0.30274472  1.        ]]. Action = [[-0.40488052 -0.54519415 -0.03045857  0.90437317]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.0655, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 70 is -1
Human Feedback received at timestep 70 of -1
Current timestep = 71. State = [[-0.2459266   0.07661314  0.32078806  1.        ]]. Action = [[ 0.39310145 -0.4749555   0.4262104   0.00076556]]. Reward = [10.]
Curr episode timestep = 6
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 71 is 1
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.21051723  0.04999746  0.36620322  1.        ]]. Action = [[ 0.48462653 -0.15632802  0.5129473   0.01953244]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.0712, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 72 is -1
Human Feedback received at timestep 72 of -1
Current timestep = 73. State = [[-0.25150806  0.00234685  0.23233052  1.        ]]. Action = [[ 0.5284276   0.7966393  -0.7166122  -0.48486298]]. Reward = [-100.]
Curr episode timestep = 8
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 73 is -1
Human Feedback received at timestep 73 of -1
Current timestep = 74. State = [[-0.23006245  0.02758286  0.2507263   1.        ]]. Action = [[0.92351246 0.7155018  0.5199425  0.83811593]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.0399, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 74 is -1
Human Feedback received at timestep 74 of -1
Current timestep = 75. State = [[-0.17601942  0.03560795  0.3098177   1.        ]]. Action = [[ 0.15238678 -0.79846746  0.903708    0.6089728 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.0480, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 75 is 1
Human Feedback received at timestep 75 of 1
Current timestep = 76. State = [[-0.1481848  -0.0252092   0.35928324  1.        ]]. Action = [[ 0.66663    -0.97098416 -0.37792253  0.6286981 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.0467, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 76 is 1
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.10888027 -0.08257185  0.34487778  1.        ]]. Action = [[ 0.4552977  -0.5235926   0.6523106   0.13244045]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.0638, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 77 is -1
Human Feedback received at timestep 77 of -1
Current timestep = 78. State = [[-0.09819322 -0.09220309  0.3423955   1.        ]]. Action = [[0.51598525 0.01161492 0.9475267  0.41821098]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.0572, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 78 is -1
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.09878971 -0.09470429  0.34254026  1.        ]]. Action = [[0.7778256  0.15734637 0.7184657  0.04112792]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 79 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 79 is tensor(0.0628, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 79 is -1
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.2508048   0.00264567  0.23261222  1.        ]]. Action = [[-0.5863521  -0.7706377  -0.31211424 -0.18346065]]. Reward = [-100.]
Curr episode timestep = 6
Scene graph at timestep 80 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 80 is tensor(0.0536, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 80 is -1
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.25311375  0.00177449  0.23125358  1.        ]]. Action = [[-0.76814914 -0.02948147 -0.68600464  0.31788766]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 81 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.0546, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 81 is -1
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.25313485  0.0017623   0.23125584  1.        ]]. Action = [[-0.8046946  -0.8087692   0.03305387 -0.69046986]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.0374, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 82 is -1
Human Feedback received at timestep 82 of -1
Current timestep = 83. State = [[-0.24759048 -0.02507454  0.22293617  1.        ]]. Action = [[ 0.26006532 -0.8231587  -0.3919314   0.33273005]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 83 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 83 is tensor(0.0483, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 83 is 1
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.24536537 -0.0636932   0.2011837   1.        ]]. Action = [[-0.51154286  0.30928624  0.5075233  -0.90764874]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 84 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 84 is tensor(0.0420, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 84 is 1
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.2445497  -0.07065396  0.19909146  1.        ]]. Action = [[-0.29833508  0.35263824 -0.6017174   0.44127607]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 85 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 85 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 85 is 1
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.25055367  0.00272428  0.23260897  1.        ]]. Action = [[ 0.5783088   0.9180634   0.40865767 -0.23619932]]. Reward = [-100.]
Curr episode timestep = 5
Scene graph at timestep 86 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 86 is tensor(0.0400, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 86 is -1
Human Feedback received at timestep 86 of -1
Current timestep = 87. State = [[-0.2505523   0.00268318  0.2332018   1.        ]]. Action = [[-0.44685304 -0.86010337  0.45379972 -0.79385275]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.0305, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 87 is 1
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.2505008   0.00268322  0.23339443  1.        ]]. Action = [[-0.37433696 -0.5516575   0.70575285  0.112903  ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.0427, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 88 is 1
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.25049067  0.00268323  0.23343231  1.        ]]. Action = [[-0.8815808   0.74318826  0.562554    0.09054351]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 89 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 89 is tensor(0.0356, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 89 is 1
Human Feedback received at timestep 89 of 1
Current timestep = 90. State = [[-0.2507968   0.00248856  0.23323603  1.        ]]. Action = [[-0.6369479   0.80259514  0.5908686   0.65936315]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0288, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 90 is -1
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.2517995   0.00176435  0.23273745  1.        ]]. Action = [[-0.47287667 -0.03339887  0.01842225 -0.73461425]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.0432, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 91 is 1
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.2518675   0.00176433  0.23270614  1.        ]]. Action = [[-0.9258508 -0.8728269  0.6090119 -0.3956794]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 92 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 92 is tensor(0.0248, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 92 is 1
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.2518675   0.00176433  0.23270614  1.        ]]. Action = [[-0.4254874  -0.71604675 -0.9052954   0.76548135]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 93 is 1
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.23778923  0.00179845  0.2542113   1.        ]]. Action = [[ 0.30723035 -0.00222969  0.81202555  0.30434906]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0404, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 94 is 1
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.21843162 -0.0092623   0.3042267   1.        ]]. Action = [[ 0.33223748 -0.3140216   0.07543123  0.6092285 ]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 95 is 1
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.20224857 -0.02150523  0.3214711   1.        ]]. Action = [[-0.88070935 -0.7769224  -0.43927693 -0.24319333]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 96 is 1
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.19958924 -0.0235089   0.32470042  1.        ]]. Action = [[-0.9150749  -0.15140432  0.73338175  0.12326539]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 97 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.0335, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 97 is 1
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.19930905 -0.02436394  0.32502767  1.        ]]. Action = [[-0.6846798  -0.83896    -0.20249611 -0.26145524]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 98 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 98 is tensor(0.0389, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 98 is 1
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.25069806  0.00266012  0.23262714  1.        ]]. Action = [[ 0.84706044 -0.9592361   0.02987075 -0.5009872 ]]. Reward = [-100.]
Curr episode timestep = 12
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 99 is 1
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.25336152  0.00185495  0.23127855  1.        ]]. Action = [[-0.69027644 -0.4084344  -0.7556197   0.5447402 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 100 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 100 is tensor(0.0374, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 100 is 1
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.25073108  0.00291609  0.233773    1.        ]]. Action = [[ 0.3965509  -0.6089535   0.39767027 -0.06633842]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 101 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.0448, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 101 is 1
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.23559527 -0.01821224  0.24205914  1.        ]]. Action = [[ 0.46585298 -0.5919533   0.25400853  0.4472208 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 102 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 102 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 102 is 1
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.2503323   0.00279669  0.23281115  1.        ]]. Action = [[ 0.22133398 -0.21249288 -0.01410973 -0.53265023]]. Reward = [-100.]
Curr episode timestep = 1
Scene graph at timestep 103 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 103 is tensor(0.0474, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 103 is 1
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.25182748  0.00222168  0.2319122   1.        ]]. Action = [[-0.83053035  0.73943615  0.9254836  -0.4174165 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0287, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 104 is -1
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.25263754  0.00174994  0.23140582  1.        ]]. Action = [[-0.43080807 -0.4258939  -0.5881565  -0.4510975 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 105 is tensor(0.0431, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 105 is 1
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.25263754  0.00174994  0.23140582  1.        ]]. Action = [[-0.26005387  0.95800054  0.539974    0.78400993]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 106 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 106 is tensor(0.0251, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 106 is -1
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.25265366  0.00179585  0.23140582  1.        ]]. Action = [[-0.7365423  -0.9713952  -0.35995066  0.79034114]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0292, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 107 is 1
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.25265366  0.00179585  0.23140582  1.        ]]. Action = [[-0.6059586   0.87806165  0.19290149 -0.84715766]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 108 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 108 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 108 is 1
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.2526699   0.0018422   0.23140582  1.        ]]. Action = [[-0.94787765  0.01257646 -0.7262703   0.9377129 ]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 109 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 109 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 109 is -1
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.2526699   0.0018422   0.23140582  1.        ]]. Action = [[-0.19139808  0.57965374  0.5042108   0.87767005]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 110 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 110 is tensor(0.0335, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 110 is 1
Human Feedback received at timestep 110 of 1
Current timestep = 111. State = [[-0.2526699   0.0018422   0.23140582  1.        ]]. Action = [[-0.28856993 -0.46527833  0.65774727  0.48828447]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 111 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 111 is tensor(0.0468, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 111 is 1
Human Feedback received at timestep 111 of 1
Current timestep = 112. State = [[-0.24323732 -0.02177914  0.2507134   1.        ]]. Action = [[ 0.10887289 -0.71758705  0.8354285   0.3148669 ]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 112 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 112 is tensor(0.0413, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 112 is 1
Human Feedback received at timestep 112 of 1
Current timestep = 113. State = [[-0.24706078 -0.05728612  0.29708502  1.        ]]. Action = [[-0.7170886   0.18015039  0.9460244  -0.09880984]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 113 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 113 is tensor(0.0431, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 113 is 1
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.2504072   0.00302471  0.23397398  1.        ]]. Action = [[-0.14186066 -0.42631042  0.05653238 -0.31703943]]. Reward = [-100.]
Curr episode timestep = 10
Scene graph at timestep 114 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 114 is tensor(0.0550, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 114 is 1
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.25055453  0.00256857  0.23392992  1.        ]]. Action = [[-0.20900804 -0.74276435  0.6635574   0.87778354]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 115 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 115 is tensor(0.0399, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 115 is 1
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.25181922  0.00338801  0.22746389  1.        ]]. Action = [[ 0.005072    0.09424722 -0.19397736  0.31437778]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.0561, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 116 is 1
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.2515322   0.00406553  0.218386    1.        ]]. Action = [[-0.33406764  0.37528956 -0.30800658  0.81333756]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.0426, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 117 is 1
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.25140283  0.00462704  0.21838033  1.        ]]. Action = [[-0.88345003 -0.79212624 -0.30132782 -0.80984807]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.0312, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 118 is -1
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.25140283  0.00462704  0.21838033  1.        ]]. Action = [[-0.51533586 -0.6195047   0.05376446  0.20490074]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.0499, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 119 is 1
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.25140283  0.00462704  0.21838033  1.        ]]. Action = [[-0.5813034 -0.5068669 -0.5632293  0.9338788]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 120 is 1
Human Feedback received at timestep 120 of 1
Current timestep = 121. State = [[-0.25140283  0.00462704  0.21838033  1.        ]]. Action = [[-0.7886025   0.15124273  0.88265693  0.66682744]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 121 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 121 is tensor(0.0378, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 121 is -1
Human Feedback received at timestep 121 of -1
Current timestep = 122. State = [[-0.25140283  0.00462704  0.21838033  1.        ]]. Action = [[-0.5216441 -0.7998893  0.901191  -0.6494625]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 122 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 122 is tensor(0.0341, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 122 is 1
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.25140283  0.00462704  0.21838033  1.        ]]. Action = [[-0.79323065 -0.94402725  0.2538619  -0.01976943]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 123 is 1
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.23767073 -0.01118728  0.23308925  1.        ]]. Action = [[ 0.3770572  -0.46824205  0.6188128   0.22295511]]. Reward = [10.]
Curr episode timestep = 9
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0467, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 124 is 1
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.2020241  -0.03276647  0.27061135  1.        ]]. Action = [[ 0.6671021  -0.08620203  0.41532528  0.88280225]]. Reward = [10.]
Curr episode timestep = 10
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 125 is 1
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.2505635   0.00279424  0.23258372  1.        ]]. Action = [[-0.5065913  -0.82951087  0.88648736 -0.00763142]]. Reward = [-100.]
Curr episode timestep = 11
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0341, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 126 is 1
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.25036868  0.0027371   0.23261704  1.        ]]. Action = [[ 0.12239254 -0.70080775 -0.9330961  -0.21997696]]. Reward = [-100.]
Curr episode timestep = 0
Scene graph at timestep 127 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 127 is tensor(0.0307, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 127 is 1
Human Feedback received at timestep 127 of 1
Current timestep = 128. State = [[-0.23343514  0.01859533  0.24377708  1.        ]]. Action = [[0.50891376 0.43964732 0.30238092 0.6126337 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 128 is -1
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.21057673  0.0355798   0.26250124  1.        ]]. Action = [[-0.49523056 -0.608583    0.5457783   0.6442611 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0355, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 129 is 1
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.25113344  0.00231536  0.23233837  1.        ]]. Action = [[-0.4261918   0.8424618   0.61991096 -0.54270905]]. Reward = [-100.]
Curr episode timestep = 2
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 130 is -1
Human Feedback received at timestep 130 of -1
Current timestep = 131. State = [[-0.25120634  0.00221829  0.23231259  1.        ]]. Action = [[-0.72983295  0.04997826  0.498482    0.05210567]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0387, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 131 is 1
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.25163773 -0.01809493  0.23337853  1.        ]]. Action = [[-0.03843957 -0.5425827   0.08939028  0.98054624]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 132 is 1
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.25245968 -0.06098424  0.22497225  1.        ]]. Action = [[ 0.15565944 -0.44367206 -0.6497688   0.66575646]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0358, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 133 is 1
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.2480066  -0.08417916  0.18522067  1.        ]]. Action = [[-0.46758074 -0.75760984  0.18552756  0.8230777 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0318, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 134 is 1
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.2478962  -0.08844595  0.1791354   1.        ]]. Action = [[-0.9242686   0.26974308  0.7850914   0.8909786 ]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0209, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 135 is -1
Human Feedback received at timestep 135 of -1
Current timestep = 136. State = [[-0.24924392 -0.0723636   0.18874443  1.        ]]. Action = [[-0.16112077  0.58390284  0.49498963  0.47917986]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 136 is 1
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.25081968  0.0025236   0.23258209  1.        ]]. Action = [[ 0.66956973 -0.88890666  0.48642337 -0.1300137 ]]. Reward = [-100.]
Curr episode timestep = 6
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0314, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 137 is 1
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.2508301   0.00249368  0.2324692   1.        ]]. Action = [[-0.17930913 -0.87273026 -0.11450356  0.23026931]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 138 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 138 is tensor(0.0377, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 138 is 1
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.2508301   0.00249368  0.2324692   1.        ]]. Action = [[-0.8946658  -0.37482274 -0.39432943 -0.6019177 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 139 is -1
Human Feedback received at timestep 139 of -1
Current timestep = 140. State = [[-0.2508301   0.00249368  0.2324692   1.        ]]. Action = [[-0.25287318 -0.53014123 -0.90914977  0.16160178]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 140 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 140 is 1
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.2465726  -0.01800175  0.23861825  1.        ]]. Action = [[ 0.13027418 -0.55145687  0.28537095  0.1444993 ]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 141 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0486, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 141 is 1
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.24980716  0.0029119   0.23279397  1.        ]]. Action = [[-0.01743615 -0.64504397 -0.22531128 -0.05897969]]. Reward = [-100.]
Curr episode timestep = 4
Scene graph at timestep 142 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0488, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 142 is 1
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.24989149  0.00299251  0.23268196  1.        ]]. Action = [[-0.38063627 -0.6890446   0.05744982 -0.14385527]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 143 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.0455, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 143 is 1
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.24989149  0.00299251  0.23268196  1.        ]]. Action = [[-0.79520667 -0.342026   -0.06569934  0.8407347 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 144 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0410, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 144 is 1
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.24301283  0.02741247  0.24935675  1.        ]]. Action = [[0.24110925 0.93343043 0.72573256 0.7048485 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 145 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 145 is -1
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.23498031  0.0805717   0.28490603  1.        ]]. Action = [[-0.68237376  0.28073025  0.190364    0.54983807]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 146 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.0491, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 146 is -1
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.25110525  0.00234638  0.23235457  1.        ]]. Action = [[ 0.45330667 -0.4445222   0.7593237  -0.17231822]]. Reward = [-100.]
Curr episode timestep = 4
Scene graph at timestep 147 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 147 is tensor(0.0411, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 147 is 1
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-2.5126165e-01  7.9263485e-04  2.3239878e-01  1.0000000e+00]]. Action = [[-0.5659659   0.72731864  0.95902574  0.6793579 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 148 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 148 is tensor(0.0268, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 148 is -1
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-2.5134531e-01 -1.4279748e-04  2.3243594e-01  1.0000000e+00]]. Action = [[-0.40987277 -0.90865433  0.6822705   0.9023398 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 149 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 149 is tensor(0.0380, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 149 is 1
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-2.5138021e-01 -5.2609970e-04  2.3245144e-01  1.0000000e+00]]. Action = [[-0.7206172  -0.27763057  0.1761564   0.6533735 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 150 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 150 is tensor(0.0455, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 150 is 1
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-2.5138021e-01 -5.2609970e-04  2.3245144e-01  1.0000000e+00]]. Action = [[-0.18704838 -0.01359993  0.90344334  0.8202323 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 151 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 151 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 151 is 1
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.24909836 -0.02606498  0.23633243  1.        ]]. Action = [[ 0.03409278 -0.74432003  0.19814098  0.6584097 ]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 152 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 152 is tensor(0.0467, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 152 is 1
Human Feedback received at timestep 152 of 1
Current timestep = 153. State = [[-0.2539767  -0.06416702  0.24103592  1.        ]]. Action = [[-0.5997297   0.02924538  0.2352066   0.74164224]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 153 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 153 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 153 is 1
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.25479573 -0.06994461  0.24193747  1.        ]]. Action = [[-0.5588101   0.4084803  -0.6996266   0.15332055]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 154 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 154 is tensor(0.0426, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 154 is -1
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.25497326 -0.07093316  0.2420916   1.        ]]. Action = [[-0.22112    -0.9458299   0.10608196  0.46028674]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.0450, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 155 is 1
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.25658655 -0.08813258  0.23234357  1.        ]]. Action = [[-0.02675837 -0.44651163 -0.6172551   0.55792713]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 156 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0481, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 156 is 1
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.25475627 -0.1086764   0.19955084  1.        ]]. Action = [[-0.91804755 -0.9110029   0.6209774   0.7386477 ]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 157 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 157 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 157 is 1
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.2538494  -0.11336904  0.1941663   1.        ]]. Action = [[-0.14733058 -0.9611878   0.91312265  0.75977373]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 158 is 1
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.25405845 -0.11431691  0.19441062  1.        ]]. Action = [[-0.7284804   0.4967569   0.5424793   0.80917907]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.0309, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 159 is -1
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.48016602  0.05164051 -0.75098723  0.5451026 ]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 160 is -1
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.8511848   0.15825403  0.75995255  0.7462919 ]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 161 is -1
Human Feedback received at timestep 161 of -1
Current timestep = 162. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.8804268   0.07752085 -0.35855913  0.91196394]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 162 is -1
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.72446966 -0.25613016  0.11400914 -0.3594789 ]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.0474, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 163 is -1
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.8326325  -0.9137366  -0.03581929  0.40393567]]. Reward = [10.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 164 is 1
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.76409787  0.29670572  0.4222437   0.8747355 ]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 165 is -1
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.49159503 -0.26084316 -0.3985631  -0.7616694 ]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.0377, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 166 is -1
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.86725146  0.3253057   0.5768453   0.63931155]]. Reward = [10.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 167 is 1
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.25411445 -0.11448075  0.1944411   1.        ]]. Action = [[-0.5903868   0.03048551  0.9012258   0.8757086 ]]. Reward = [10.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.0248, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 168 is -1
Human Feedback received at timestep 168 of -1
Current timestep = 169. State = [[-0.2327553  -0.11193537  0.19449975  1.        ]]. Action = [[ 0.6565002   0.07514977 -0.03083515  0.72461605]]. Reward = [10.]
Curr episode timestep = 21
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.0365, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 169 is -1
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.20610909 -0.11235089  0.18710561  1.        ]]. Action = [[-0.8271078  -0.17191148 -0.60913324  0.62969565]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 170 is -1
Human Feedback received at timestep 170 of -1
Current timestep = 171. State = [[-0.20192645 -0.11176014  0.18717645  1.        ]]. Action = [[-0.631349   -0.14302158  0.8954364   0.8727906 ]]. Reward = [10.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 171 is -1
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.20096235 -0.11124835  0.18740834  1.        ]]. Action = [[-0.8375238  0.0143255  0.7117543  0.894704 ]]. Reward = [10.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.0249, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 172 is -1
Human Feedback received at timestep 172 of -1
Current timestep = 173. State = [[-0.205929   -0.14148864  0.21070762  1.        ]]. Action = [[-0.5600274  -0.9907351   0.91192555  0.50828254]]. Reward = [10.]
Curr episode timestep = 25
Scene graph at timestep 173 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 173 is tensor(0.0307, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 173 is 1
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.2380959  -0.19608945  0.2602232   1.        ]]. Action = [[-0.81533235 -0.93590736 -0.14254093  0.87154007]]. Reward = [10.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 174 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 174 is 1
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.24354959 -0.206319    0.2662363   1.        ]]. Action = [[-0.8789764  -0.35049224 -0.39431292  0.54604316]]. Reward = [10.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 175 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 175 is tensor(0.0401, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 175 is -1
Human Feedback received at timestep 175 of -1
Current timestep = 176. State = [[-0.24485882 -0.20887452  0.26729476  1.        ]]. Action = [[-0.7726294   0.8162209  -0.69753146  0.9105605 ]]. Reward = [10.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 176 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 176 is tensor(0.0215, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 176 is -1
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.24577975 -0.20960492  0.26755303  1.        ]]. Action = [[-0.79743445 -0.8690541  -0.26157463  0.90557885]]. Reward = [10.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 177 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.0351, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 177 is -1
Human Feedback received at timestep 177 of -1
Current timestep = 178. State = [[-0.25191143 -0.2304444   0.27781895  1.        ]]. Action = [[-0.14162719 -0.5446672   0.18911457  0.8798909 ]]. Reward = [10.]
Curr episode timestep = 30
Scene graph at timestep 178 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.0471, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 178 is -1
Human Feedback received at timestep 178 of -1
Current timestep = 179. State = [[-0.2616298  -0.25396743  0.29220673  1.        ]]. Action = [[-0.9751452 -0.7247109  0.3880589  0.2886052]]. Reward = [10.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 179 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 179 is tensor(0.0458, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 179 is 1
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.2639765 -0.2576886  0.2937899  1.       ]]. Action = [[ 0.25562286 -0.93563336 -0.13749498  0.5138302 ]]. Reward = [10.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Scene graph at timestep 180 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0488, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 180 is -1
Human Feedback received at timestep 180 of -1
Current timestep = 181. State = [[-0.26392505 -0.25789535  0.29396224  1.        ]]. Action = [[-0.8588005   0.13964748 -0.16828918  0.30877733]]. Reward = [10.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 181 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0549, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 181 is -1
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.26397327 -0.25782794  0.2939566   1.        ]]. Action = [[-0.02475369 -0.9952645  -0.01636541  0.83961105]]. Reward = [10.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Scene graph at timestep 182 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 182 is tensor(0.0424, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 182 is 1
Human Feedback received at timestep 182 of 1
Current timestep = 183. State = [[-0.26398942 -0.25780538  0.29395473  1.        ]]. Action = [[ 0.23002648 -0.4526086  -0.88416606  0.2594223 ]]. Reward = [10.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 183 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 183 is -1
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-0.2657214  -0.26721734  0.29609916  1.        ]]. Action = [[-0.01788282 -0.23612833  0.02493358  0.7854929 ]]. Reward = [10.]
Curr episode timestep = 36
Scene graph at timestep 184 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.0533, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 184 is -1
Human Feedback received at timestep 184 of -1
Current timestep = 185. State = [[-0.2665013  -0.27725074  0.29802647  1.        ]]. Action = [[-0.88857085 -0.13107419  0.51603055  0.7778411 ]]. Reward = [10.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 185 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.0441, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 185 is -1
Human Feedback received at timestep 185 of -1
Current timestep = 186. State = [[-0.2665555  -0.27998558  0.2986582   1.        ]]. Action = [[-0.38258046 -0.90029395 -0.20198935  0.9709077 ]]. Reward = [10.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 186 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.0407, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 186 is -1
Human Feedback received at timestep 186 of -1
Current timestep = 187. State = [[-0.26707304 -0.28053537  0.29849705  1.        ]]. Action = [[ 0.6232202  -0.5994844   0.57500434  0.51478744]]. Reward = [10.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 187 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.0490, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 187 is -1
Human Feedback received at timestep 187 of -1
Current timestep = 188. State = [[-0.2672841  -0.28068945  0.29853866  1.        ]]. Action = [[-0.7744903  -0.9160117   0.9658406   0.83839285]]. Reward = [10.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 188 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 188 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 188 is 1
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.2672841  -0.28068945  0.29853866  1.        ]]. Action = [[-0.6531215  -0.19772077  0.9104389   0.9375918 ]]. Reward = [10.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 189 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 189 is -1
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.2672841  -0.28068945  0.29853866  1.        ]]. Action = [[-0.6230649  -0.83153135  0.17469895  0.36107087]]. Reward = [10.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 190 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0438, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 190 is 1
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.2672841  -0.28068945  0.29853866  1.        ]]. Action = [[ 0.03684521 -0.75666195 -0.22699809 -0.28270197]]. Reward = [10.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 191 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0509, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 191 is -1
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.2673235  -0.28073466  0.29855934  1.        ]]. Action = [[-0.40597653 -0.8556481  -0.13939732  0.9242122 ]]. Reward = [10.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 192 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0356, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 192 is 1
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.2675597  -0.28100562  0.2986835   1.        ]]. Action = [[-0.6115002  -0.45653063 -0.36018652  0.7525172 ]]. Reward = [10.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 193 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0393, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 193 is -1
Human Feedback received at timestep 193 of -1
Current timestep = 194. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.30519408 -0.27449733  0.5774472   0.93184566]]. Reward = [10.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 194 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 194 is -1
Human Feedback received at timestep 194 of -1
Current timestep = 195. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.47821462  0.3702098  -0.3005165   0.674783  ]]. Reward = [10.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 195 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0400, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 195 is -1
Human Feedback received at timestep 195 of -1
Current timestep = 196. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.20958447  0.40057683 -0.3539964   0.6226126 ]]. Reward = [10.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 196 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0413, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 196 is -1
Human Feedback received at timestep 196 of -1
Current timestep = 197. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.8860868  -0.19102436 -0.12871194  0.7083411 ]]. Reward = [10.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 197 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 197 is 1
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.9489618   0.5670973  -0.93434453  0.74663997]]. Reward = [10.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Scene graph at timestep 198 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0243, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 198 is 1
Human Feedback received at timestep 198 of 1
Current timestep = 199. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.86424863  0.4069543  -0.5667364   0.9170852 ]]. Reward = [10.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 199 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0272, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 199 is -1
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.94508255 -0.44440937  0.23417318 -0.3907677 ]]. Reward = [10.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 200 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0395, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 200 is -1
Human Feedback received at timestep 200 of -1
Current timestep = 201. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.68585426 -0.81853336  0.7463784   0.9769039 ]]. Reward = [10.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 201 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 201 is 1
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.94228876 -0.6238472  -0.05744487 -0.0744822 ]]. Reward = [10.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 202 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 202 is 1
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.9442448  -0.43658996  0.25584888  0.7900171 ]]. Reward = [10.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 203 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0277, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 203 is 1
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.14932185 -0.9604965   0.20210099  0.8783648 ]]. Reward = [10.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 204 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0252, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 204 is 1
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.69065195 -0.50108343  0.41567814  0.9184849 ]]. Reward = [10.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 205 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0270, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 205 is 1
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.9320378  -0.94541854 -0.027031    0.6710422 ]]. Reward = [10.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Scene graph at timestep 206 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 206 is -1
Human Feedback received at timestep 206 of -1
Current timestep = 207. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.4359424  -0.89880866  0.07279146  0.9318471 ]]. Reward = [10.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 207 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0244, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 207 is -1
Human Feedback received at timestep 207 of -1
Current timestep = 208. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[ 0.12316942 -0.82411814  0.07840002 -0.11860371]]. Reward = [10.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Scene graph at timestep 208 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 208 is -1
Human Feedback received at timestep 208 of -1
Current timestep = 209. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.24153912  0.03025723  0.04231346  0.4054942 ]]. Reward = [10.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 209 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 209 is -1
Human Feedback received at timestep 209 of -1
Current timestep = 210. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.6490897   0.3152126   0.16255653  0.9389343 ]]. Reward = [10.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 210 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0248, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 210 is 1
Human Feedback received at timestep 210 of 1
Current timestep = 211. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.8536574  -0.18713176  0.15806818 -0.3646884 ]]. Reward = [10.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Scene graph at timestep 211 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0386, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 211 is -1
Human Feedback received at timestep 211 of -1
Current timestep = 212. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.39908898 -0.23925793 -0.43293643  0.98081946]]. Reward = [10.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Scene graph at timestep 212 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.0270, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 212 is -1
Human Feedback received at timestep 212 of -1
Current timestep = 213. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.642341    0.41333628  0.16994202  0.10842681]]. Reward = [10.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Scene graph at timestep 213 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 213 is -1
Human Feedback received at timestep 213 of -1
Current timestep = 214. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[ 0.6995599  -0.7406739   0.67363286  0.7817632 ]]. Reward = [10.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Scene graph at timestep 214 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 214 is tensor(0.0224, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 214 is 1
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[-0.7534989   0.9436121  -0.4393556   0.08209169]]. Reward = [10.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 215 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 215 is tensor(0.0290, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 215 is -1
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.26759917 -0.28105083  0.29870424  1.        ]]. Action = [[ 0.12245774 -0.48922187 -0.4887116   0.88696504]]. Reward = [10.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 216 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 216 is tensor(0.0266, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 216 is -1
Human Feedback received at timestep 216 of -1
Current timestep = 217. State = [[-0.2536441  -0.25786325  0.3156238   1.        ]]. Action = [[0.24488807 0.9486885  0.8914063  0.00718355]]. Reward = [10.]
Curr episode timestep = 69
Scene graph at timestep 217 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 217 is tensor(0.0229, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 217 is -1
Human Feedback received at timestep 217 of -1
Current timestep = 218. State = [[-0.23970515 -0.21084037  0.3688149   1.        ]]. Action = [[-0.7915163   0.19861126  0.34691226  0.7870624 ]]. Reward = [10.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 218 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 218 is tensor(0.0219, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 218 is -1
Human Feedback received at timestep 218 of -1
Current timestep = 219. State = [[-0.23754871 -0.2020897   0.37560132  1.        ]]. Action = [[-0.95805    -0.83953977  0.72845876  0.9136591 ]]. Reward = [10.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 219 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 219 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 219 is -1
Human Feedback received at timestep 219 of -1
Current timestep = 220. State = [[-0.23562035 -0.20182836  0.37830555  1.        ]]. Action = [[-0.03204036 -0.6647291   0.4222046   0.3568046 ]]. Reward = [10.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Scene graph at timestep 220 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 220 is tensor(0.0319, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 220 is -1
Human Feedback received at timestep 220 of -1
Current timestep = 221. State = [[-0.23531055 -0.20193376  0.37862602  1.        ]]. Action = [[-0.93666756  0.56782293  0.5731021   0.66134405]]. Reward = [10.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Scene graph at timestep 221 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 221 is tensor(0.0212, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 221 is -1
Human Feedback received at timestep 221 of -1
Current timestep = 222. State = [[-0.24349193 -0.20935936  0.37706205  1.        ]]. Action = [[-0.24593383 -0.21028781 -0.2083047   0.8925221 ]]. Reward = [10.]
Curr episode timestep = 74
Scene graph at timestep 222 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 222 is tensor(0.0265, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 222 is -1
Human Feedback received at timestep 222 of -1
Current timestep = 223. State = [[-0.26542968 -0.23403963  0.37388602  1.        ]]. Action = [[-0.2052936  -0.49505824 -0.14225233  0.9123912 ]]. Reward = [10.]
Curr episode timestep = 75
Scene graph at timestep 223 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 223 is tensor(0.0267, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 223 is -1
Human Feedback received at timestep 223 of -1
Current timestep = 224. State = [[-0.2766745  -0.25666404  0.36955258  1.        ]]. Action = [[-0.6944467 -0.5703617  0.8986037  0.8507744]]. Reward = [10.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Scene graph at timestep 224 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 224 is tensor(0.0247, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 224 is -1
Human Feedback received at timestep 224 of -1
Current timestep = 225. State = [[-0.27792564 -0.2596058   0.36945596  1.        ]]. Action = [[-0.88565075 -0.22101319 -0.83201987  0.8755555 ]]. Reward = [10.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Scene graph at timestep 225 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 225 is tensor(0.0262, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 225 is -1
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.2784655  -0.2603945   0.36974695  1.        ]]. Action = [[-0.9677347  -0.19445008  0.9333128   0.871104  ]]. Reward = [10.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Scene graph at timestep 226 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 226 is tensor(0.0227, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 226 is -1
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.27869973 -0.260549    0.3697615   1.        ]]. Action = [[-0.64425516 -0.6189087  -0.09639382  0.95790327]]. Reward = [10.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Scene graph at timestep 227 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 227 is tensor(0.0264, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 227 is -1
Human Feedback received at timestep 227 of -1
Current timestep = 228. State = [[-0.27873918 -0.26059276  0.36978218  1.        ]]. Action = [[-0.9194142  -0.536676   -0.09094012  0.9630424 ]]. Reward = [10.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Scene graph at timestep 228 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 228 is tensor(0.0253, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 228 is -1
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.26624846 -0.24140505  0.36420816  1.        ]]. Action = [[ 0.4436332   0.6532868  -0.05375719  0.6037878 ]]. Reward = [10.]
Curr episode timestep = 81
Scene graph at timestep 229 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 229 is tensor(0.0325, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 229 is -1
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.24795046 -0.21344624  0.35587683  1.        ]]. Action = [[-0.27043176 -0.00260246  0.23095417 -0.16973692]]. Reward = [10.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Scene graph at timestep 230 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 230 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 230 is -1
Human Feedback received at timestep 230 of -1
Current timestep = 231. State = [[-0.24381147 -0.20879367  0.35376334  1.        ]]. Action = [[-0.3823408  -0.37846494  0.92111635  0.8190727 ]]. Reward = [10.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Scene graph at timestep 231 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 231 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 231 is -1
Human Feedback received at timestep 231 of -1
Current timestep = 232. State = [[-0.24231489 -0.20723657  0.35301527  1.        ]]. Action = [[-0.6063825   0.3057754   0.38358748  0.67441654]]. Reward = [10.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Scene graph at timestep 232 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 232 is tensor(0.0175, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 232 is -1
Human Feedback received at timestep 232 of -1
Current timestep = 233. State = [[-0.24150385 -0.20647621  0.35263222  1.        ]]. Action = [[-0.9290782   0.9026272  -0.7518639   0.89782405]]. Reward = [10.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Scene graph at timestep 233 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 233 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 233 is -1
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[-0.2414047  -0.20632233  0.35257065  1.        ]]. Action = [[-0.7808722 -0.8343028  0.3113047  0.8601563]]. Reward = [10.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Scene graph at timestep 234 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 234 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 234 is -1
Human Feedback received at timestep 234 of -1
Current timestep = 235. State = [[-0.24127492 -0.2061207   0.35249007  1.        ]]. Action = [[-0.3404013  -0.60546494  0.84089184  0.9379554 ]]. Reward = [10.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Scene graph at timestep 235 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 235 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 235 is -1
Human Feedback received at timestep 235 of -1
Current timestep = 236. State = [[-0.24111308 -0.20586887  0.35238954  1.        ]]. Action = [[-0.7265517  -0.28040534  0.9199941   0.89405644]]. Reward = [10.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Scene graph at timestep 236 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 236 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 236 is -1
Human Feedback received at timestep 236 of -1
Current timestep = 237. State = [[-0.23711665 -0.21928187  0.34275487  1.        ]]. Action = [[ 0.1842506  -0.4104514  -0.43136668  0.9627093 ]]. Reward = [10.]
Curr episode timestep = 89
Scene graph at timestep 237 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 237 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 237 is -1
Human Feedback received at timestep 237 of -1
Current timestep = 238. State = [[-0.2251953 -0.2331327  0.3195362  1.       ]]. Action = [[-0.59288466 -0.82539916  0.19231606  0.12532234]]. Reward = [10.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Scene graph at timestep 238 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 238 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 238 is -1
Human Feedback received at timestep 238 of -1
Current timestep = 239. State = [[-0.22168246 -0.23534931  0.31601205  1.        ]]. Action = [[-0.95040953  0.76233983 -0.15887302  0.7797893 ]]. Reward = [10.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Scene graph at timestep 239 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 239 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 239 is -1
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.22194085 -0.23563881  0.31600213  1.        ]]. Action = [[-0.10105026 -0.81043816  0.6116812   0.02056193]]. Reward = [10.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Scene graph at timestep 240 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 240 is tensor(0.0184, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 240 is -1
Human Feedback received at timestep 240 of -1
Current timestep = 241. State = [[-0.21131517 -0.24101911  0.33352152  1.        ]]. Action = [[ 0.27833855 -0.1148755   0.67684007  0.9842967 ]]. Reward = [10.]
Curr episode timestep = 93
Scene graph at timestep 241 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 241 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 241 is -1
Human Feedback received at timestep 241 of -1
Current timestep = 242. State = [[-0.19748214 -0.2493438   0.36086932  1.        ]]. Action = [[-0.3803116  -0.6283837   0.27212548  0.98039937]]. Reward = [10.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Scene graph at timestep 242 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 242 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 242 is -1
Human Feedback received at timestep 242 of -1
Current timestep = 243. State = [[-0.19477007 -0.2517043   0.36728248  1.        ]]. Action = [[-0.33005238 -0.76448    -0.45217043  0.9110261 ]]. Reward = [10.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Scene graph at timestep 243 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 243 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 243 is -1
Human Feedback received at timestep 243 of -1
Current timestep = 244. State = [[-0.16895649 -0.23703207  0.3758902   1.        ]]. Action = [[0.82999206 0.40951717 0.20903039 0.7916157 ]]. Reward = [10.]
Curr episode timestep = 96
Scene graph at timestep 244 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 244 is tensor(0.0244, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 244 is -1
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.13420011 -0.19761023  0.39100194  1.        ]]. Action = [[-0.49485672  0.876035    0.07993031  0.58368516]]. Reward = [10.]
Curr episode timestep = 97
Scene graph at timestep 245 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 245 is tensor(0.0200, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 245 is -1
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.1581703  -0.17088026  0.38908225  1.        ]]. Action = [[-0.3774333 -0.7117193 -0.802631   0.7061746]]. Reward = [10.]
Curr episode timestep = 98
Scene graph at timestep 246 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 246 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 246 is -1
Human Feedback received at timestep 246 of -1
Current timestep = 247. State = [[-0.1853547  -0.18593304  0.34701875  1.        ]]. Action = [[-0.39794743 -0.59401995  0.55145895  0.6927016 ]]. Reward = [10.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Scene graph at timestep 247 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 247 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 247 is -1
Human Feedback received at timestep 247 of -1
Current timestep = 248. State = [[-0.19059269 -0.18854052  0.34184262  1.        ]]. Action = [[-0.00320315  0.14577687  0.6673782   0.7773644 ]]. Reward = [10.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Scene graph at timestep 248 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 248 is tensor(0.0136, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 248 is -1
Human Feedback received at timestep 248 of -1
Current timestep = 249. State = [[-0.19099614 -0.18964654  0.3422726   1.        ]]. Action = [[0.3123845  0.23208582 0.84858346 0.8825362 ]]. Reward = [10.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Scene graph at timestep 249 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 249 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 249 is -1
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.18117972 -0.20662439  0.34399846  1.        ]]. Action = [[ 0.70783424 -0.8719595  -0.16410077  0.5715151 ]]. Reward = [10.]
Curr episode timestep = 102
Scene graph at timestep 250 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 250 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 250 is -1
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.14695017 -0.2627667   0.31598943  1.        ]]. Action = [[-0.5926639  -0.7837281   0.85987306  0.9571643 ]]. Reward = [10.]
Curr episode timestep = 103
Action ignored: Workspace boundary
Scene graph at timestep 251 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 251 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 251 is -1
Human Feedback received at timestep 251 of -1
Current timestep = 252. State = [[-0.13933113 -0.27177814  0.3108233   1.        ]]. Action = [[ 0.01944971 -0.53495044  0.47078955  0.9651027 ]]. Reward = [10.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Scene graph at timestep 252 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 252 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 252 is -1
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.13915327 -0.2722766   0.31103846  1.        ]]. Action = [[-0.8651224  -0.58359575 -0.8688051   0.9823805 ]]. Reward = [10.]
Curr episode timestep = 105
Action ignored: Workspace boundary
Scene graph at timestep 253 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 253 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 253 is -1
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.13915327 -0.2722766   0.31103846  1.        ]]. Action = [[-0.4144224  -0.87330097  0.09529829  0.90716934]]. Reward = [10.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Scene graph at timestep 254 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 254 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 254 is -1
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.13915327 -0.2722766   0.31103846  1.        ]]. Action = [[-0.7454482  -0.67098427 -0.06223541 -0.2134229 ]]. Reward = [10.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Scene graph at timestep 255 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 255 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 255 is -1
Human Feedback received at timestep 255 of -1
Current timestep = 256. State = [[-0.13216187 -0.25306708  0.2979371   1.        ]]. Action = [[ 0.5080137  0.616323  -0.8711398  0.8458855]]. Reward = [10.]
Curr episode timestep = 108
Scene graph at timestep 256 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 256 is tensor(0.0148, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 256 is 1
Human Feedback received at timestep 256 of 1
Current timestep = 257. State = [[-0.10453279 -0.20312633  0.25070706  1.        ]]. Action = [[-0.6576016  0.939646   0.2885958  0.9405608]]. Reward = [10.]
Curr episode timestep = 109
Scene graph at timestep 257 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 257 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 257 is 1
Human Feedback received at timestep 257 of 1
Current timestep = 258. State = [[-0.14274046 -0.17932628  0.26926053  1.        ]]. Action = [[-0.414007   -0.84355086  0.34700716  0.7455442 ]]. Reward = [10.]
Curr episode timestep = 110
Scene graph at timestep 258 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 258 is tensor(0.0162, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 258 is -1
Human Feedback received at timestep 258 of -1
Current timestep = 259. State = [[-0.18861644 -0.20534632  0.30649891  1.        ]]. Action = [[-0.91616553  0.33237898  0.8510549   0.93476737]]. Reward = [10.]
Curr episode timestep = 111
Scene graph at timestep 259 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 259 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 259 is -1
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.24145006 -0.20213015  0.3800692   1.        ]]. Action = [[ 0.31046677 -0.38825524  0.5856706   0.83203185]]. Reward = [10.]
Curr episode timestep = 112
Scene graph at timestep 260 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 260 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 260 is -1
Human Feedback received at timestep 260 of -1
Current timestep = 261. State = [[-0.24416976 -0.21216492  0.41321167  1.        ]]. Action = [[-0.31923252 -0.64279777  0.3875587   0.45553505]]. Reward = [10.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Scene graph at timestep 261 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 261 is tensor(0.0207, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 261 is -1
Human Feedback received at timestep 261 of -1
Current timestep = 262. State = [[-0.2455377  -0.21385239  0.41756094  1.        ]]. Action = [[ 0.11123443 -0.49270153  0.6107037   0.5983448 ]]. Reward = [10.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Scene graph at timestep 262 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 262 is tensor(0.0194, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 262 is -1
Human Feedback received at timestep 262 of -1
Current timestep = 263. State = [[-0.24579489 -0.21402335  0.41759765  1.        ]]. Action = [[-0.25239897 -0.6916431   0.92977846  0.9000368 ]]. Reward = [10.]
Curr episode timestep = 115
Action ignored: Workspace boundary
Scene graph at timestep 263 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 263 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 263 is 1
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.28448242 -0.5007618   0.63899136  0.82003534]]. Reward = [10.]
Curr episode timestep = 116
Action ignored: Workspace boundary
Scene graph at timestep 264 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 264 is tensor(0.0164, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 264 is -1
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.78321475 -0.8607462   0.47186422  0.9565774 ]]. Reward = [10.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Scene graph at timestep 265 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 265 is tensor(0.0154, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 265 is -1
Human Feedback received at timestep 265 of -1
Current timestep = 266. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.6228165   0.6703718   0.4962871   0.90606785]]. Reward = [10.]
Curr episode timestep = 118
Action ignored: Workspace boundary
Scene graph at timestep 266 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 266 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 266 is -1
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.80000025  0.66917527  0.8309579   0.9000678 ]]. Reward = [10.]
Curr episode timestep = 119
Action ignored: Workspace boundary
Scene graph at timestep 267 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 267 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 267 is -1
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.8613528   0.31832755  0.6695616   0.58650565]]. Reward = [10.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Scene graph at timestep 268 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 268 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 268 is -1
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.328031    0.5726156  -0.66329366  0.04946399]]. Reward = [10.]
Curr episode timestep = 121
Action ignored: Workspace boundary
Scene graph at timestep 269 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 269 is tensor(0.0170, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 269 is -1
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.94102544 -0.749022    0.08075213  0.44154978]]. Reward = [10.]
Curr episode timestep = 122
Action ignored: Workspace boundary
Scene graph at timestep 270 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 270 is tensor(0.0152, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 270 is -1
Human Feedback received at timestep 270 of -1
Current timestep = 271. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[ 0.40941727 -0.6069236   0.698915    0.8036065 ]]. Reward = [10.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Scene graph at timestep 271 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 271 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 271 is -1
Human Feedback received at timestep 271 of -1
Current timestep = 272. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.97110826 -0.881886   -0.02640915  0.31591082]]. Reward = [10.]
Curr episode timestep = 124
Action ignored: Workspace boundary
Scene graph at timestep 272 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 272 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 272 is -1
Human Feedback received at timestep 272 of -1
Current timestep = 273. State = [[-0.24584799 -0.21392623  0.4175914   1.        ]]. Action = [[-0.45664185 -0.9589484  -0.39294887  0.92102516]]. Reward = [10.]
Curr episode timestep = 125
Action ignored: Workspace boundary
Scene graph at timestep 273 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 273 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 273 is -1
Human Feedback received at timestep 273 of -1
Current timestep = 274. State = [[-0.25087282  0.00260095  0.23268092  1.        ]]. Action = [[-0.3260973   0.02668619  0.48106384  0.647974  ]]. Reward = [10.]
Curr episode timestep = 126
Action ignored: Workspace boundary
Scene graph at timestep 274 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 274 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 274 is -1
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.25351617  0.00216675  0.23121949  1.        ]]. Action = [[-0.49964285 -0.5741204  -0.23143685  0.577839  ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 275 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 275 is tensor(0.0178, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 275 is 1
Human Feedback received at timestep 275 of 1
Current timestep = 276. State = [[-0.25349978  0.00233751  0.23123021  1.        ]]. Action = [[-0.8915403  0.6288538 -0.7585106  0.6014776]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 276 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 276 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 276 is -1
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.25348586  0.00250159  0.23122388  1.        ]]. Action = [[-0.70772815 -0.4553622  -0.12695956  0.48863065]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 277 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 277 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 277 is 1
Human Feedback received at timestep 277 of 1
Current timestep = 278. State = [[-0.25348586  0.00250159  0.23122388  1.        ]]. Action = [[-0.8818759   0.43764496  0.8941643   0.84982777]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 278 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 278 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 278 is -1
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.2361343  -0.01098835  0.24213411  1.        ]]. Action = [[ 0.587217   -0.39585435  0.4465754   0.7236495 ]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 279 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 279 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 279 is 1
Human Feedback received at timestep 279 of 1
Current timestep = 280. State = [[-0.21420623 -0.02755531  0.25893965  1.        ]]. Action = [[-0.6043405  -0.55028296 -0.4681456   0.9215262 ]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 280 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 280 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 280 is 1
Human Feedback received at timestep 280 of 1
Current timestep = 281. State = [[-0.18250003 -0.03739675  0.27408418  1.        ]]. Action = [[ 0.9113134  -0.2111603   0.26808023  0.9730668 ]]. Reward = [10.]
Curr episode timestep = 6
Scene graph at timestep 281 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 281 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 281 is -1
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.11900078 -0.02550145  0.3097754   1.        ]]. Action = [[0.64205885 0.65396357 0.5791609  0.24742639]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 282 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 282 is tensor(0.0254, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 282 is -1
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.08977778 -0.00470934  0.32369187  1.        ]]. Action = [[-0.7343884 -0.0908947 -0.6455176  0.5602685]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 283 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 283 is tensor(0.0320, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 283 is -1
Human Feedback received at timestep 283 of -1
Current timestep = 284. State = [[-0.10564338 -0.00188594  0.30512738  1.        ]]. Action = [[-0.45128107  0.39199722 -0.89667416  0.9352238 ]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: No entry zone
Scene graph at timestep 284 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 284 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 284 is -1
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.12397368 -0.02092288  0.290082    1.        ]]. Action = [[-0.8242491  -0.48012173 -0.46780396  0.9099072 ]]. Reward = [10.]
Curr episode timestep = 10
Scene graph at timestep 285 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 285 is tensor(0.0412, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 285 is 1
Human Feedback received at timestep 285 of 1
Current timestep = 286. State = [[-0.17559889 -0.07066954  0.24741583  1.        ]]. Action = [[-0.03254062 -0.81106836 -0.2740258   0.8750243 ]]. Reward = [10.]
Curr episode timestep = 11
Scene graph at timestep 286 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 286 is tensor(0.0293, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 286 is -1
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.19374435 -0.11359763  0.22937188  1.        ]]. Action = [[ 0.68092656 -0.6796813  -0.735986    0.93197584]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: No entry zone
Scene graph at timestep 287 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 287 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 287 is -1
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.19712624 -0.12204562  0.22747873  1.        ]]. Action = [[ 0.7878187  -0.37017465 -0.58298886  0.45814002]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: No entry zone
Scene graph at timestep 288 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 288 is tensor(0.0145, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 288 is -1
Human Feedback received at timestep 288 of -1
Current timestep = 289. State = [[-0.19794302 -0.12351152  0.22619866  1.        ]]. Action = [[-0.6136476  -0.6836215  -0.36807185  0.89220107]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 289 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 289 is tensor(0.0179, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 289 is 1
Human Feedback received at timestep 289 of 1
Current timestep = 290. State = [[-0.19800836 -0.12380019  0.22632937  1.        ]]. Action = [[-0.8587227   0.14604688  0.05153775  0.44743323]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 290 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 290 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 290 is 1
Human Feedback received at timestep 290 of 1
Current timestep = 291. State = [[-0.19812942 -0.12402352  0.22633651  1.        ]]. Action = [[-0.7065402   0.94941497  0.9418063   0.54106593]]. Reward = [10.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 291 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 291 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 291 is -1
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.2004361  -0.14946423  0.21659426  1.        ]]. Action = [[ 0.08785188 -0.76491004 -0.73497045  0.8220248 ]]. Reward = [10.]
Curr episode timestep = 17
Scene graph at timestep 292 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 292 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 292 is 1
Human Feedback received at timestep 292 of 1
Current timestep = 293. State = [[-0.20051694 -0.19332293  0.1648508   1.        ]]. Action = [[-0.6720611  -0.14878684 -0.6480399   0.3978274 ]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 293 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 293 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 293 is -1
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.19805266 -0.1993668   0.15542082  1.        ]]. Action = [[-0.8711586  0.2767576 -0.973521   0.697152 ]]. Reward = [10.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 294 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 294 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 294 is -1
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.1959877  -0.20065026  0.15351526  1.        ]]. Action = [[ 0.25503767  0.38141716 -0.0995487  -0.58160925]]. Reward = [10.]
Curr episode timestep = 20
Action ignored: No entry zone
Scene graph at timestep 295 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 295 is tensor(0.0189, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 295 is 1
Human Feedback received at timestep 295 of 1
Current timestep = 296. State = [[-0.19530252 -0.20100752  0.15380996  1.        ]]. Action = [[-0.7279427  -0.39168411 -0.6688945   0.6366775 ]]. Reward = [10.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 296 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 296 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 296 is -1
Human Feedback received at timestep 296 of -1
Current timestep = 297. State = [[-0.20958239 -0.19533393  0.15807462  1.        ]]. Action = [[-0.56200814  0.2798971   0.25029898  0.5754974 ]]. Reward = [10.]
Curr episode timestep = 22
Scene graph at timestep 297 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 297 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 297 is -1
Human Feedback received at timestep 297 of -1
Current timestep = 298. State = [[-0.23491655 -0.18740295  0.16679409  1.        ]]. Action = [[-0.8253198  -0.96479887 -0.05608451  0.14689863]]. Reward = [10.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 298 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 298 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 298 is -1
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.2306349  -0.19029176  0.17226578  1.        ]]. Action = [[ 0.4012804  -0.23628837  0.144032    0.8787966 ]]. Reward = [10.]
Curr episode timestep = 24
Scene graph at timestep 299 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 299 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 299 is -1
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.20724958 -0.22012356  0.1743512   1.        ]]. Action = [[ 0.4571221  -0.77438253 -0.48230594  0.9366672 ]]. Reward = [10.]
Curr episode timestep = 25
Scene graph at timestep 300 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 300 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 300 is -1
Human Feedback received at timestep 300 of -1
Current timestep = 301. State = [[-0.18386574 -0.2630897   0.1465977   1.        ]]. Action = [[-0.970253   0.3783989  0.4178307  0.8876691]]. Reward = [10.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 301 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 301 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 301 is -1
Human Feedback received at timestep 301 of -1
Current timestep = 302. State = [[-0.17799574 -0.26794803  0.14186096  1.        ]]. Action = [[-0.43470812 -0.6834674   0.32586098  0.85171247]]. Reward = [10.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 302 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 302 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 302 is 1
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.15584941 -0.25951335  0.16297847  1.        ]]. Action = [[0.6443944  0.20743346 0.885983   0.95832884]]. Reward = [10.]
Curr episode timestep = 28
Scene graph at timestep 303 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 303 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 303 is -1
Human Feedback received at timestep 303 of -1
Current timestep = 304. State = [[-0.14473516 -0.23233762  0.20497978  1.        ]]. Action = [[-0.8606134   0.8235543   0.01408207  0.8887285 ]]. Reward = [10.]
Curr episode timestep = 29
Scene graph at timestep 304 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 304 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 304 is -1
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.16465236 -0.17920023  0.22936079  1.        ]]. Action = [[0.35355806 0.23973417 0.35451043 0.91711354]]. Reward = [10.]
Curr episode timestep = 30
Scene graph at timestep 305 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 305 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 305 is 1
Human Feedback received at timestep 305 of 1
Current timestep = 306. State = [[-0.1545243  -0.15334499  0.26933873  1.        ]]. Action = [[0.07505429 0.10198128 0.75559163 0.74684596]]. Reward = [10.]
Curr episode timestep = 31
Scene graph at timestep 306 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 306 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 306 is -1
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.16160382 -0.16661826  0.32379484  1.        ]]. Action = [[-0.20020396 -0.51227045  0.20869327  0.8286433 ]]. Reward = [10.]
Curr episode timestep = 32
Scene graph at timestep 307 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 307 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 307 is -1
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-0.16820635 -0.18866828  0.34515926  1.        ]]. Action = [[ 0.7047372 -0.9333573  0.7687439  0.6929779]]. Reward = [10.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 308 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 308 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 308 is -1
Human Feedback received at timestep 308 of -1
Current timestep = 309. State = [[-0.19448602 -0.22409897  0.35745552  1.        ]]. Action = [[-0.8035188  -0.89768225  0.10877037  0.9189863 ]]. Reward = [10.]
Curr episode timestep = 34
Scene graph at timestep 309 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 309 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 309 is -1
Human Feedback received at timestep 309 of -1
Current timestep = 310. State = [[-0.2370508  -0.26646292  0.37308306  1.        ]]. Action = [[-0.28217268  0.7335932   0.7847655   0.68741274]]. Reward = [10.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 310 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 310 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 310 is -1
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.2217117  -0.27626023  0.38322952  1.        ]]. Action = [[ 0.78418326 -0.16658807  0.28158963  0.33589017]]. Reward = [10.]
Curr episode timestep = 36
Scene graph at timestep 311 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 311 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 311 is -1
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.18702602 -0.28662544  0.3921244   1.        ]]. Action = [[-0.61504054 -0.8012785   0.9733808   0.97882116]]. Reward = [10.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 312 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 312 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 312 is -1
Human Feedback received at timestep 312 of -1
Current timestep = 313. State = [[-0.18013752 -0.2883336   0.3939255   1.        ]]. Action = [[-0.6455475 -0.3718816  0.8451803  0.5533469]]. Reward = [10.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 313 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 313 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 313 is -1
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.17975177 -0.28853682  0.39407694  1.        ]]. Action = [[-0.50267744 -0.65703464  0.42465985  0.40134072]]. Reward = [10.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 314 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 314 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 314 is -1
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.17982674 -0.28862745  0.39414304  1.        ]]. Action = [[ 0.1018188  -0.03877652  0.7491827   0.78913283]]. Reward = [10.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 315 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 315 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 315 is -1
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.17982674 -0.28862745  0.39414304  1.        ]]. Action = [[-0.9725337  -0.2179982   0.4129728   0.93556714]]. Reward = [10.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 316 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 316 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 316 is -1
Human Feedback received at timestep 316 of -1
Current timestep = 317. State = [[-0.17982674 -0.28862745  0.39414304  1.        ]]. Action = [[-0.29277402  0.46831012  0.5904596   0.4770558 ]]. Reward = [10.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 317 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 317 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 317 is -1
Human Feedback received at timestep 317 of -1
Current timestep = 318. State = [[-0.18888706 -0.2669075   0.3787792   1.        ]]. Action = [[-0.5262744  0.879125  -0.5218775  0.9474906]]. Reward = [10.]
Curr episode timestep = 43
Scene graph at timestep 318 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 318 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 318 is -1
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.20813303 -0.22701189  0.36034486  1.        ]]. Action = [[-0.38468403 -0.8964824  -0.66229236  0.8873507 ]]. Reward = [10.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 319 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 319 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 319 is -1
Human Feedback received at timestep 319 of -1
Current timestep = 320. State = [[-0.2199463  -0.19964428  0.34208742  1.        ]]. Action = [[-0.43985355  0.95246816 -0.45443344  0.9794837 ]]. Reward = [10.]
Curr episode timestep = 45
Scene graph at timestep 320 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 320 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 320 is -1
Human Feedback received at timestep 320 of -1
Current timestep = 321. State = [[-0.2497399  -0.11839612  0.30034465  1.        ]]. Action = [[ 0.06043208  0.5926864  -0.6780096   0.86169004]]. Reward = [10.]
Curr episode timestep = 46
Scene graph at timestep 321 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 321 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 321 is -1
Human Feedback received at timestep 321 of -1
Current timestep = 322. State = [[-0.24268638 -0.09398293  0.26113227  1.        ]]. Action = [[ 0.3262862  -0.5240816  -0.01862735  0.7424753 ]]. Reward = [10.]
Curr episode timestep = 47
Scene graph at timestep 322 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 322 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 322 is -1
Human Feedback received at timestep 322 of -1
Current timestep = 323. State = [[-0.20495452 -0.09017683  0.2661214   1.        ]]. Action = [[0.9011923  0.31891525 0.7505851  0.74513245]]. Reward = [10.]
Curr episode timestep = 48
Scene graph at timestep 323 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 323 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 323 is -1
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.1634165  -0.10666215  0.2916125   1.        ]]. Action = [[ 0.11022091 -0.8042802  -0.27451527  0.6937034 ]]. Reward = [10.]
Curr episode timestep = 49
Scene graph at timestep 324 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 324 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 324 is -1
Human Feedback received at timestep 324 of -1
Current timestep = 325. State = [[-0.16160731 -0.17589091  0.274716    1.        ]]. Action = [[-0.88437617 -0.78536147 -0.66913486  0.9283216 ]]. Reward = [10.]
Curr episode timestep = 50
Scene graph at timestep 325 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 325 is tensor(0.0167, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 325 is -1
Human Feedback received at timestep 325 of -1
Current timestep = 326. State = [[-0.22272056 -0.2526814   0.23728032  1.        ]]. Action = [[-0.40324962 -0.76138973 -0.25604826  0.78695786]]. Reward = [10.]
Curr episode timestep = 51
Scene graph at timestep 326 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 326 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 326 is -1
Human Feedback received at timestep 326 of -1
Current timestep = 327. State = [[-0.23465323 -0.27476034  0.21508153  1.        ]]. Action = [[-0.33480543 -0.8074168  -0.7000355   0.9503275 ]]. Reward = [10.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 327 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 327 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 327 is -1
Human Feedback received at timestep 327 of -1
Current timestep = 328. State = [[-0.23582578 -0.27305043  0.21420446  1.        ]]. Action = [[-0.963752   -0.8283535   0.82678914  0.98302054]]. Reward = [10.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 328 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 328 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 328 is -1
Human Feedback received at timestep 328 of -1
Current timestep = 329. State = [[-0.23536247 -0.27361658  0.21445261  1.        ]]. Action = [[-0.97256523  0.2686404  -0.35498023  0.81316364]]. Reward = [10.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 329 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 329 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 329 is -1
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.23587152 -0.27347684  0.21406965  1.        ]]. Action = [[-0.559242   -0.5478005   0.25571728  0.85474086]]. Reward = [10.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 330 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 330 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 330 is 1
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.23561667 -0.27403706  0.21427223  1.        ]]. Action = [[-0.01522237 -0.59006834 -0.8020452   0.8357518 ]]. Reward = [10.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 331 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 331 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 331 is 1
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[-0.23551747 -0.27386227  0.21408549  1.        ]]. Action = [[ 0.80455935 -0.95197123  0.05136037  0.7181239 ]]. Reward = [10.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 332 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 332 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 332 is 1
Human Feedback received at timestep 332 of 1
Current timestep = 333. State = [[-0.23225476 -0.27157938  0.20371054  1.        ]]. Action = [[ 0.30522168  0.18037641 -0.5084151   0.4033922 ]]. Reward = [10.]
Curr episode timestep = 58
Scene graph at timestep 333 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 333 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 333 is -1
Human Feedback received at timestep 333 of -1
Current timestep = 334. State = [[-0.21141404 -0.2638997   0.1755767   1.        ]]. Action = [[-0.48310626  0.7097229  -0.24009883  0.574978  ]]. Reward = [10.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 334 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 334 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 334 is -1
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.20614755 -0.26365936  0.16776326  1.        ]]. Action = [[-0.57257295  0.5399654  -0.95438725  0.17689466]]. Reward = [10.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Scene graph at timestep 335 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 335 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 335 is -1
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.2046371  -0.263354    0.16774872  1.        ]]. Action = [[-0.09218925 -0.87413186  0.85517526  0.7418339 ]]. Reward = [10.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 336 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 336 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 336 is 1
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.20455214 -0.2633044   0.16771433  1.        ]]. Action = [[ 0.46582067 -0.5060198   0.9701855   0.6352092 ]]. Reward = [10.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 337 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 337 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 337 is -1
Human Feedback received at timestep 337 of -1
Current timestep = 338. State = [[-0.20426607 -0.26329586  0.16778773  1.        ]]. Action = [[-0.03810912 -0.60700715 -0.6480453   0.94222105]]. Reward = [10.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Scene graph at timestep 338 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 338 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 338 is -1
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.20156758 -0.2515505   0.16406836  1.        ]]. Action = [[ 0.01129675  0.50452733 -0.07407659  0.6135137 ]]. Reward = [10.]
Curr episode timestep = 64
Scene graph at timestep 339 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 339 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 339 is 1
Human Feedback received at timestep 339 of 1
Current timestep = 340. State = [[-0.1859076  -0.22800405  0.16572712  1.        ]]. Action = [[ 0.39848518 -0.29095113  0.16391754  0.83977294]]. Reward = [10.]
Curr episode timestep = 65
Scene graph at timestep 340 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 340 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 340 is -1
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.2508204   0.00249739  0.23260239  1.        ]]. Action = [[-0.12650234 -0.08245933  0.5464754  -0.11284459]]. Reward = [-100.]
Curr episode timestep = 66
Scene graph at timestep 341 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 341 is tensor(0.0174, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 341 is 1
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[-0.23786744 -0.03481267  0.24245304  1.        ]]. Action = [[ 0.52383804 -0.60617006  0.7430607   0.94530547]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 342 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 342 is tensor(0.0248, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 342 is 1
Human Feedback received at timestep 342 of 1
Current timestep = 343. State = [[-0.19617198 -0.07401129  0.29928565  1.        ]]. Action = [[ 0.79628253 -0.16850114  0.9005873   0.8922781 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 343 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 343 is tensor(0.0142, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 343 is 1
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[-0.15714955 -0.10845198  0.3791396   1.        ]]. Action = [[-0.30003893 -0.4272604   0.6503186   0.52712953]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 344 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 344 is tensor(0.0232, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 344 is -1
Human Feedback received at timestep 344 of -1
Current timestep = 345. State = [[-0.15904045 -0.1338777   0.42566663  1.        ]]. Action = [[-0.45834243 -0.97795063  0.49292397  0.79592323]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 345 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 345 is tensor(0.0186, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 345 is -1
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.15743627 -0.13861728  0.4346207   1.        ]]. Action = [[ 0.31528807 -0.7794893   0.30748117  0.7066474 ]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 346 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 346 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 346 is -1
Human Feedback received at timestep 346 of -1
Current timestep = 347. State = [[-0.17512843 -0.15852726  0.41762337  1.        ]]. Action = [[-0.6700249  -0.40013033 -0.8321227   0.5947094 ]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 347 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 347 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 347 is -1
Human Feedback received at timestep 347 of -1
Current timestep = 348. State = [[-0.20996885 -0.17751323  0.38091648  1.        ]]. Action = [[-0.05400616 -0.9019882   0.6526427   0.55793905]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 348 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 348 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 348 is -1
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.21705417 -0.18072283  0.37494016  1.        ]]. Action = [[-0.92110115 -0.69784206 -0.7555504   0.8238363 ]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 349 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 349 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 349 is -1
Human Feedback received at timestep 349 of -1
Current timestep = 350. State = [[-0.22382013 -0.16276981  0.35528666  1.        ]]. Action = [[-0.34810364  0.8286928  -0.85953045  0.7453716 ]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 350 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 350 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 350 is -1
Human Feedback received at timestep 350 of -1
Current timestep = 351. State = [[-0.24719681 -0.12478127  0.28674266  1.        ]]. Action = [[ 0.14949656 -0.29381073 -0.57649195  0.8386352 ]]. Reward = [10.]
Curr episode timestep = 9
Scene graph at timestep 351 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 351 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 351 is -1
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.24462506 -0.12707745  0.23672199  1.        ]]. Action = [[-0.39336628  0.21111965  0.28396726  0.71867645]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 352 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 352 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 352 is -1
Human Feedback received at timestep 352 of -1
Current timestep = 353. State = [[-0.24323972 -0.12747072  0.2276353   1.        ]]. Action = [[ 0.8425908 -0.0093528 -0.5342661  0.8528683]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: No entry zone
Scene graph at timestep 353 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 353 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 353 is 1
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.24300851 -0.12723836  0.22724505  1.        ]]. Action = [[-0.34052086  0.6486243  -0.77608246  0.9866836 ]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 354 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 354 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 354 is -1
Human Feedback received at timestep 354 of -1
Current timestep = 355. State = [[-0.2437248  -0.12705839  0.22658317  1.        ]]. Action = [[-0.66711825  0.7348585  -0.5285345   0.500664  ]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 355 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 355 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 355 is -1
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.24753048 -0.14119552  0.21267098  1.        ]]. Action = [[ 0.01847553 -0.39016426 -0.9526971   0.67765546]]. Reward = [10.]
Curr episode timestep = 14
Scene graph at timestep 356 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 356 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 356 is -1
Human Feedback received at timestep 356 of -1
Current timestep = 357. State = [[-0.2454971  -0.17792103  0.16968934  1.        ]]. Action = [[-0.03160644 -0.38041013  0.86208975  0.9835191 ]]. Reward = [10.]
Curr episode timestep = 15
Scene graph at timestep 357 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 357 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 357 is -1
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.23058319 -0.22749339  0.20017038  1.        ]]. Action = [[ 0.693177   -0.94297117 -0.22483456  0.8154061 ]]. Reward = [10.]
Curr episode timestep = 16
Scene graph at timestep 358 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 358 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 358 is -1
Human Feedback received at timestep 358 of -1
Current timestep = 359. State = [[-0.1768444  -0.28751907  0.18070936  1.        ]]. Action = [[ 0.82961917 -0.26483047 -0.564685    0.8858206 ]]. Reward = [10.]
Curr episode timestep = 17
Scene graph at timestep 359 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 359 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 359 is -1
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.12763229 -0.30960873  0.13928826  1.        ]]. Action = [[-0.41992754 -0.5636132   0.07500148 -0.1215359 ]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 360 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 360 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 360 is 1
Human Feedback received at timestep 360 of 1
Current timestep = 361. State = [[-0.13068312 -0.30332005  0.1475995   1.        ]]. Action = [[-0.6515657   0.47638297  0.52017     0.960577  ]]. Reward = [10.]
Curr episode timestep = 19
Scene graph at timestep 361 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 361 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 361 is -1
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.16598798 -0.2787401   0.16745259  1.        ]]. Action = [[-0.3086434  0.2215848 -0.1437208  0.9197619]]. Reward = [10.]
Curr episode timestep = 20
Scene graph at timestep 362 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 362 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 362 is 1
Human Feedback received at timestep 362 of 1
Current timestep = 363. State = [[-0.18336071 -0.2670129   0.16736704  1.        ]]. Action = [[ 0.658445   -0.93761826  0.5632901   0.9608178 ]]. Reward = [10.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 363 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 363 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 363 is -1
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.18429236 -0.26664037  0.16700186  1.        ]]. Action = [[ 0.0975492  -0.5048414   0.26954436  0.9017577 ]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 364 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 364 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 364 is -1
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.18466888 -0.266352    0.16684537  1.        ]]. Action = [[-0.23860323 -0.48544753  0.9292325   0.9317179 ]]. Reward = [10.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 365 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 365 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 365 is -1
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.20148507 -0.26125595  0.1600128   1.        ]]. Action = [[-0.5450788   0.24064708 -0.20742536  0.41349626]]. Reward = [10.]
Curr episode timestep = 24
Scene graph at timestep 366 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 366 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 366 is -1
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.23264585 -0.24843056  0.15170452  1.        ]]. Action = [[-0.04207712 -0.4800865  -0.10274649  0.9029739 ]]. Reward = [10.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 367 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 367 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 367 is -1
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.21201244 -0.23240261  0.16430524  1.        ]]. Action = [[0.7790499 0.2897532 0.4882977 0.9576125]]. Reward = [10.]
Curr episode timestep = 26
Scene graph at timestep 368 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 368 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 368 is 1
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.19375147 -0.22090112  0.18856929  1.        ]]. Action = [[-0.30995703 -0.20415962  0.13687027  0.97325397]]. Reward = [10.]
Curr episode timestep = 27
Scene graph at timestep 369 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 369 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 369 is -1
Human Feedback received at timestep 369 of -1
Current timestep = 370. State = [[-0.20314494 -0.20240258  0.1999881   1.        ]]. Action = [[-0.18483847  0.7139995   0.06789887  0.6657262 ]]. Reward = [10.]
Curr episode timestep = 28
Scene graph at timestep 370 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 370 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 370 is -1
Human Feedback received at timestep 370 of -1
Current timestep = 371. State = [[-0.20983979 -0.19670251  0.20543219  1.        ]]. Action = [[ 0.20975077 -0.87019867 -0.5046869   0.9314406 ]]. Reward = [10.]
Curr episode timestep = 29
Scene graph at timestep 371 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 371 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 371 is -1
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.20874088 -0.23491001  0.1826363   1.        ]]. Action = [[-0.1114046  -0.10950047  0.13397276  0.9659436 ]]. Reward = [10.]
Curr episode timestep = 30
Scene graph at timestep 372 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 372 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 372 is -1
Human Feedback received at timestep 372 of -1
Current timestep = 373. State = [[-0.21065499 -0.24547721  0.18293259  1.        ]]. Action = [[-0.65792316 -0.87039685  0.7001045   0.5726111 ]]. Reward = [10.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 373 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 373 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 373 is -1
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.211239   -0.24684018  0.18320568  1.        ]]. Action = [[ 0.03700876 -0.92556286 -0.03411651  0.9437072 ]]. Reward = [10.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Scene graph at timestep 374 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 374 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 374 is -1
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.19001679 -0.23431791  0.20124505  1.        ]]. Action = [[0.692121   0.2768681  0.6741028  0.97481453]]. Reward = [10.]
Curr episode timestep = 33
Scene graph at timestep 375 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 375 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 375 is -1
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.15831825 -0.22406635  0.231265    1.        ]]. Action = [[-0.8686296   0.89569426  0.25994253  0.6153357 ]]. Reward = [10.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Scene graph at timestep 376 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 376 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 376 is 1
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.13720685 -0.23201688  0.22780922  1.        ]]. Action = [[ 0.7171154  -0.39061713 -0.6955776   0.98443484]]. Reward = [10.]
Curr episode timestep = 35
Scene graph at timestep 377 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 377 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 377 is -1
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.09675698 -0.25130484  0.18620409  1.        ]]. Action = [[-0.29938185 -0.7226406  -0.1891051   0.67956734]]. Reward = [10.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 378 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 378 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 378 is -1
Human Feedback received at timestep 378 of -1
Current timestep = 379. State = [[-0.08692174 -0.25551435  0.17763384  1.        ]]. Action = [[ 0.9379847 -0.4987434  0.8578496  0.7104391]]. Reward = [10.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 379 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 379 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 379 is 1
Human Feedback received at timestep 379 of 1
Current timestep = 380. State = [[-0.08682673 -0.25561672  0.17699315  1.        ]]. Action = [[ 0.91129994 -0.9245914   0.65122366  0.53470826]]. Reward = [10.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 380 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 380 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 380 is 1
Human Feedback received at timestep 380 of 1
Current timestep = 381. State = [[-0.06748605 -0.2682431   0.20431796  1.        ]]. Action = [[ 0.66292286 -0.4499172   0.9059856   0.7201419 ]]. Reward = [10.]
Curr episode timestep = 39
Scene graph at timestep 381 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 381 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 381 is -1
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.03106041 -0.29254806  0.25369644  1.        ]]. Action = [[-0.92102486 -0.95882136  0.8027899   0.9117638 ]]. Reward = [10.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 382 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 382 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 382 is -1
Human Feedback received at timestep 382 of -1
Current timestep = 383. State = [[-0.01975581 -0.29937083  0.26276702  1.        ]]. Action = [[ 0.6246264  -0.7291329   0.43267274  0.98056614]]. Reward = [10.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 383 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 383 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 383 is -1
Human Feedback received at timestep 383 of -1
Current timestep = 384. State = [[-0.0353753  -0.29121342  0.26033917  1.        ]]. Action = [[-0.6951403   0.3971455  -0.11464256  0.65642524]]. Reward = [10.]
Curr episode timestep = 42
Scene graph at timestep 384 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 384 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 384 is -1
Human Feedback received at timestep 384 of -1
Current timestep = 385. State = [[-0.07452854 -0.2601334   0.27148274  1.        ]]. Action = [[-0.6748032   0.5938189   0.36664355  0.9757639 ]]. Reward = [10.]
Curr episode timestep = 43
Scene graph at timestep 385 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 385 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 385 is -1
Human Feedback received at timestep 385 of -1
Current timestep = 386. State = [[-0.1312559  -0.19882117  0.2979018   1.        ]]. Action = [[-0.45529008  0.67000055  0.13238442  0.77996767]]. Reward = [10.]
Curr episode timestep = 44
Scene graph at timestep 386 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 386 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 386 is -1
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.17644829 -0.18774635  0.32184145  1.        ]]. Action = [[-0.16473001 -0.9066937   0.22967613  0.8725753 ]]. Reward = [10.]
Curr episode timestep = 45
Scene graph at timestep 387 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 387 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 387 is -1
Human Feedback received at timestep 387 of -1
Current timestep = 388. State = [[-0.19579417 -0.21938758  0.33609194  1.        ]]. Action = [[ 0.2840134  -0.9451984   0.8433342   0.84652674]]. Reward = [10.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 388 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 388 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 388 is -1
Human Feedback received at timestep 388 of -1
Current timestep = 389. State = [[-0.25084746  0.00252373  0.23258214  1.        ]]. Action = [[-0.43233776  0.74338627  0.30933857 -0.02685177]]. Reward = [-100.]
Curr episode timestep = 47
Scene graph at timestep 389 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 389 is tensor(0.0162, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 389 is -1
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.25226417  0.00171824  0.22778383  1.        ]]. Action = [[-0.8681876   0.1243453   0.3789606   0.44989645]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 390 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 390 is tensor(0.0218, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 390 is 1
Human Feedback received at timestep 390 of 1
Current timestep = 391. State = [[-0.23025057  0.02096333  0.24519986  1.        ]]. Action = [[0.81985664 0.49513197 0.6312847  0.8118515 ]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 391 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 391 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 391 is 1
Human Feedback received at timestep 391 of 1
Current timestep = 392. State = [[-0.19244681  0.06532603  0.26749575  1.        ]]. Action = [[ 0.38195252  0.65875006 -0.52259237  0.92811584]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 392 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 392 is tensor(0.0303, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 392 is -1
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.17306457  0.12244612  0.22230616  1.        ]]. Action = [[-0.13614905  0.53158975 -0.5774085   0.74590945]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 393 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 393 is tensor(0.0430, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 393 is -1
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.16496569  0.15718165  0.18738987  1.        ]]. Action = [[-0.85805726 -0.18898118  0.02111614  0.5263499 ]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 394 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 394 is tensor(0.0610, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 394 is 1
Human Feedback received at timestep 394 of 1
Current timestep = 395. State = [[-0.16462305  0.18761769  0.20342661  1.        ]]. Action = [[-0.20232034  0.6818241   0.7920234   0.31055188]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 395 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 395 is tensor(0.0534, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 395 is -1
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.17416722  0.20114467  0.2582659   1.        ]]. Action = [[-0.28892308 -0.77493924  0.49130952  0.81651974]]. Reward = [10.]
Curr episode timestep = 6
Scene graph at timestep 396 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 396 is tensor(0.0897, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 396 is 1
Human Feedback received at timestep 396 of 1
Current timestep = 397. State = [[-0.19188413  0.15555036  0.2817297   1.        ]]. Action = [[-0.27115542 -0.45324618 -0.35968572  0.948313  ]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 397 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 397 is tensor(0.0985, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 397 is -1
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.19033132  0.1477909   0.26885188  1.        ]]. Action = [[ 0.75537646  0.5748962  -0.12042797  0.7760639 ]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 398 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 398 is tensor(0.0543, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 398 is -1
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.16279893  0.17088696  0.25122628  1.        ]]. Action = [[ 0.1683439   0.20899212 -0.30414748  0.9433166 ]]. Reward = [10.]
Curr episode timestep = 9
Scene graph at timestep 399 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 399 is tensor(0.0647, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 399 is -1
Human Feedback received at timestep 399 of -1
Current timestep = 400. State = [[-0.14994252  0.16435273  0.22958347  1.        ]]. Action = [[-0.29138517 -0.6110227  -0.10127896  0.39764583]]. Reward = [10.]
Curr episode timestep = 10
Scene graph at timestep 400 is [True, False, False, False, False, True, False, True, True, False]
State prediction error at timestep 400 is tensor(0.0688, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 400 is 1
Human Feedback received at timestep 400 of 1
Current timestep = 401. State = [[-0.14584404  0.12212088  0.22635421  1.        ]]. Action = [[ 0.21837294 -0.6076992   0.07586372  0.8431902 ]]. Reward = [10.]
Curr episode timestep = 11
Scene graph at timestep 401 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 401 is tensor(0.0810, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 401 is -1
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.13538173  0.06160987  0.24830793  1.        ]]. Action = [[-0.13040918 -0.88755846  0.53378963  0.85254455]]. Reward = [10.]
Curr episode timestep = 12
Scene graph at timestep 402 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 402 is tensor(0.0674, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 402 is 1
Human Feedback received at timestep 402 of 1
Current timestep = 403. State = [[-0.14460821  0.00317039  0.2740933   1.        ]]. Action = [[ 0.75274944 -0.24281633 -0.97083837  0.9208567 ]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: No entry zone
Scene graph at timestep 403 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 403 is tensor(0.0337, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 403 is -1
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.15588424 -0.02556743  0.29988793  1.        ]]. Action = [[-0.7843983 -0.3918848  0.9083774  0.8451195]]. Reward = [10.]
Curr episode timestep = 14
Scene graph at timestep 404 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 404 is tensor(0.0341, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 404 is 1
Human Feedback received at timestep 404 of 1
Current timestep = 405. State = [[-0.22744644 -0.05915555  0.34413663  1.        ]]. Action = [[-0.92055076 -0.23684835 -0.70309466  0.93742275]]. Reward = [10.]
Curr episode timestep = 15
Scene graph at timestep 405 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 405 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 405 is -1
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.29428378 -0.07442151  0.3148196   1.        ]]. Action = [[-0.99034774 -0.46775424  0.3489982  -0.20753789]]. Reward = [10.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 406 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 406 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 406 is -1
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.30157802 -0.07669518  0.31107506  1.        ]]. Action = [[0.17638445 0.29581118 0.89335465 0.90644693]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 407 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 407 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 407 is -1
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.30197644 -0.07699239  0.3109627   1.        ]]. Action = [[-0.05878091  0.8084985   0.20946884  0.8739284 ]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 408 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 408 is tensor(0.0084, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 408 is 1
Human Feedback received at timestep 408 of 1
Current timestep = 409. State = [[-0.29284847 -0.09880278  0.3231466   1.        ]]. Action = [[ 0.37710083 -0.6610464   0.43231213  0.45795906]]. Reward = [10.]
Curr episode timestep = 19
Scene graph at timestep 409 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 409 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 409 is 1
Human Feedback received at timestep 409 of 1
Current timestep = 410. State = [[-0.2854858  -0.13072787  0.33431795  1.        ]]. Action = [[-0.73501205  0.26297665  0.4248551   0.8684368 ]]. Reward = [10.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 410 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 410 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 410 is 1
Human Feedback received at timestep 410 of 1
Current timestep = 411. State = [[-0.28472525 -0.13723448  0.3379553   1.        ]]. Action = [[0.13675392 0.9478401  0.8473728  0.7692659 ]]. Reward = [10.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 411 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 411 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 411 is -1
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.28433314 -0.13871755  0.33951777  1.        ]]. Action = [[-0.17196923  0.29316044 -0.24753708  0.9290185 ]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 412 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 412 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 412 is -1
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.2659454  -0.12239181  0.35081223  1.        ]]. Action = [[0.67258286 0.49442554 0.35789263 0.8460214 ]]. Reward = [10.]
Curr episode timestep = 23
Scene graph at timestep 413 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 413 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 413 is -1
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.22292517 -0.12638749  0.3718291   1.        ]]. Action = [[ 0.66479206 -0.75371295 -0.23611122  0.81653357]]. Reward = [10.]
Curr episode timestep = 24
Scene graph at timestep 414 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 414 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 414 is -1
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.17663191 -0.15878722  0.35638577  1.        ]]. Action = [[0.56945276 0.35359597 0.7155411  0.83478177]]. Reward = [10.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 415 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 415 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 415 is -1
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.16740437 -0.16279101  0.3538993   1.        ]]. Action = [[0.670341  0.8904079 0.6881554 0.5246866]]. Reward = [10.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 416 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 416 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 416 is -1
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.15797755 -0.15132643  0.34540334  1.        ]]. Action = [[ 0.33417237  0.34976053 -0.33029383  0.89829195]]. Reward = [10.]
Curr episode timestep = 27
Scene graph at timestep 417 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 417 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 417 is -1
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[-0.13534823 -0.14121507  0.32819727  1.        ]]. Action = [[0.86502814 0.6794572  0.9181442  0.89485455]]. Reward = [10.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 418 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 418 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 418 is -1
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.1532248  -0.16826382  0.32979342  1.        ]]. Action = [[-0.79830575 -0.76728815 -0.1006822   0.5910977 ]]. Reward = [10.]
Curr episode timestep = 29
Scene graph at timestep 419 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 419 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 419 is 1
Human Feedback received at timestep 419 of 1
Current timestep = 420. State = [[-0.20575039 -0.19252326  0.31356314  1.        ]]. Action = [[-0.6159723   0.3335806  -0.35977376  0.95995164]]. Reward = [10.]
Curr episode timestep = 30
Scene graph at timestep 420 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 420 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 420 is -1
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.24404186 -0.18742517  0.29303175  1.        ]]. Action = [[-0.68487525 -0.6636955  -0.67219275  0.90849006]]. Reward = [10.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 421 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 421 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 421 is -1
Human Feedback received at timestep 421 of -1
Current timestep = 422. State = [[-0.2248375  -0.16413479  0.29536238  1.        ]]. Action = [[0.90874994 0.86040425 0.11859977 0.88646245]]. Reward = [10.]
Curr episode timestep = 32
Scene graph at timestep 422 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 422 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 422 is -1
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.18773717 -0.11148791  0.2971108   1.        ]]. Action = [[-0.66169834 -0.27979076 -0.16836798  0.6654674 ]]. Reward = [10.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 423 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 423 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 423 is 1
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.1799907  -0.10854153  0.30827343  1.        ]]. Action = [[-0.01320815 -0.14757848  0.43496943  0.97144103]]. Reward = [10.]
Curr episode timestep = 34
Scene graph at timestep 424 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 424 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 424 is -1
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.18082424 -0.1358651   0.34097862  1.        ]]. Action = [[-0.07394242 -0.7220406   0.54477215  0.93078756]]. Reward = [10.]
Curr episode timestep = 35
Scene graph at timestep 425 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 425 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 425 is 1
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.20587385 -0.19683644  0.3916594   1.        ]]. Action = [[-0.3586297  -0.9300634   0.42236006  0.56784487]]. Reward = [10.]
Curr episode timestep = 36
Scene graph at timestep 426 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 426 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 426 is -1
Human Feedback received at timestep 426 of -1
Current timestep = 427. State = [[-0.22277796 -0.2521029   0.4189931   1.        ]]. Action = [[0.05773807 0.5751091  0.7924231  0.98102283]]. Reward = [10.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 427 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 427 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 427 is -1
Human Feedback received at timestep 427 of -1
Current timestep = 428. State = [[-0.22423951 -0.26057482  0.4218489   1.        ]]. Action = [[ 0.75103724 -0.8827464   0.744975    0.9081825 ]]. Reward = [10.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 428 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 428 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 428 is -1
Human Feedback received at timestep 428 of -1
Current timestep = 429. State = [[-0.22239245 -0.25954747  0.41679487  1.        ]]. Action = [[ 0.10417306  0.10291779 -0.20898843  0.768666  ]]. Reward = [10.]
Curr episode timestep = 39
Scene graph at timestep 429 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 429 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 429 is -1
Human Feedback received at timestep 429 of -1
Current timestep = 430. State = [[-0.21935892 -0.25890177  0.41010743  1.        ]]. Action = [[ 0.30822265 -0.9428892   0.31132782  0.587229  ]]. Reward = [10.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 430 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 430 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 430 is -1
Human Feedback received at timestep 430 of -1
Current timestep = 431. State = [[-0.21937397 -0.25894538  0.41020933  1.        ]]. Action = [[-0.36293846 -0.82366943  0.4918623   0.648756  ]]. Reward = [10.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 431 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 431 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 431 is -1
Human Feedback received at timestep 431 of -1
Current timestep = 432. State = [[-0.21937397 -0.25894538  0.41020933  1.        ]]. Action = [[ 0.06881177 -0.9645855  -0.20358002  0.95906615]]. Reward = [10.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 432 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 432 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 432 is -1
Human Feedback received at timestep 432 of -1
Current timestep = 433. State = [[-0.23085926 -0.26847103  0.38987833  1.        ]]. Action = [[-0.35325396 -0.2491399  -0.8815888   0.922137  ]]. Reward = [10.]
Curr episode timestep = 43
Scene graph at timestep 433 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 433 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 433 is -1
Human Feedback received at timestep 433 of -1
Current timestep = 434. State = [[-0.24511136 -0.28372014  0.32900727  1.        ]]. Action = [[-0.0733664  -0.02592772 -0.60425884  0.9028163 ]]. Reward = [10.]
Curr episode timestep = 44
Scene graph at timestep 434 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 434 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 434 is -1
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.24817754 -0.2892451   0.28894648  1.        ]]. Action = [[0.02019083 0.33467913 0.9931458  0.975646  ]]. Reward = [10.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 435 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 435 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 435 is -1
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.24724135 -0.29205137  0.28538695  1.        ]]. Action = [[-0.6239934  -0.96303844 -0.18163115  0.9218178 ]]. Reward = [10.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 436 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 436 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 436 is 1
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.24708933 -0.2926475   0.2845403   1.        ]]. Action = [[ 0.49652004 -0.9731373  -0.6268176   0.60743654]]. Reward = [10.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 437 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 437 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 437 is 1
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.23727307 -0.26854524  0.297593    1.        ]]. Action = [[0.09420085 0.903613   0.7526159  0.635476  ]]. Reward = [10.]
Curr episode timestep = 48
Scene graph at timestep 438 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 438 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 438 is -1
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.23708123 -0.21844126  0.32854402  1.        ]]. Action = [[-0.849767   -0.98445624 -0.7501449   0.7027221 ]]. Reward = [10.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 439 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 439 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 439 is -1
Human Feedback received at timestep 439 of -1
Current timestep = 440. State = [[-0.23856549 -0.20850031  0.33304942  1.        ]]. Action = [[-0.91288686  0.40475953  0.7285619   0.8358419 ]]. Reward = [10.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Scene graph at timestep 440 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 440 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 440 is -1
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.23909152 -0.20754588  0.33403406  1.        ]]. Action = [[-0.16072273 -0.9285145   0.68476653  0.8875748 ]]. Reward = [10.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 441 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 441 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 441 is -1
Human Feedback received at timestep 441 of -1
Current timestep = 442. State = [[-0.24629924 -0.21043988  0.32266438  1.        ]]. Action = [[-0.10077107 -0.11786544 -0.5006815   0.84979415]]. Reward = [10.]
Curr episode timestep = 52
Scene graph at timestep 442 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 442 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 442 is -1
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.23213235 -0.20684823  0.30067435  1.        ]]. Action = [[ 0.6524384   0.1568867  -0.36586177  0.7475736 ]]. Reward = [10.]
Curr episode timestep = 53
Scene graph at timestep 443 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 443 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 443 is -1
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.2025934  -0.19948378  0.26690403  1.        ]]. Action = [[ 0.4568348 -0.9778819  0.6524743  0.8461218]]. Reward = [10.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 444 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 444 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 444 is -1
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.19623333 -0.19977075  0.2605398   1.        ]]. Action = [[-0.88428104  0.4247899  -0.24322718  0.82776666]]. Reward = [10.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 445 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 445 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 445 is -1
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.1821581  -0.21297504  0.25235167  1.        ]]. Action = [[ 0.58968663 -0.59977376 -0.46816802  0.95300865]]. Reward = [10.]
Curr episode timestep = 56
Scene graph at timestep 446 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 446 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 446 is 1
Human Feedback received at timestep 446 of 1
Current timestep = 447. State = [[-0.25079015  0.00250419  0.23261943  1.        ]]. Action = [[ 0.45848024 -0.04749721  0.37200105 -0.12476802]]. Reward = [-100.]
Curr episode timestep = 57
Scene graph at timestep 447 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 447 is tensor(0.0172, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 447 is -1
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.25103283  0.00391128  0.21936671  1.        ]]. Action = [[-0.24350458 -0.8847354   0.28281093  0.80482316]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 448 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 448 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 448 is 1
Human Feedback received at timestep 448 of 1
Current timestep = 449. State = [[-0.25075263  0.00412329  0.21378322  1.        ]]. Action = [[-0.595885   -0.84870726  0.8341453   0.86172915]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 449 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 449 is tensor(0.0159, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 449 is 1
Human Feedback received at timestep 449 of 1
Current timestep = 450. State = [[-0.24919324 -0.02535247  0.22350813  1.        ]]. Action = [[-0.12878394 -0.8720887   0.42923236  0.8018458 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 450 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 450 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 450 is 1
Human Feedback received at timestep 450 of 1
Current timestep = 451. State = [[-0.2508091   0.00269249  0.2326434   1.        ]]. Action = [[ 0.6367569  -0.13278472  0.17240334 -0.04027295]]. Reward = [-100.]
Curr episode timestep = 3
Scene graph at timestep 451 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 451 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 451 is 1
Human Feedback received at timestep 451 of 1
Current timestep = 452. State = [[-0.23191833 -0.01862155  0.23827012  1.        ]]. Action = [[ 0.5135696  -0.51595414  0.20047617  0.3490206 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 452 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 452 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 452 is 1
Human Feedback received at timestep 452 of 1
Current timestep = 453. State = [[-0.21258488 -0.04253485  0.24354583  1.        ]]. Action = [[-0.72115564 -0.8477397  -0.33989465  0.667349  ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 453 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 453 is tensor(0.0171, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 453 is 1
Human Feedback received at timestep 453 of 1
Current timestep = 454. State = [[-0.20897    -0.04675461  0.24597906  1.        ]]. Action = [[-0.74372077 -0.70793235  0.5940943   0.7493646 ]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 454 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 454 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 454 is 1
Human Feedback received at timestep 454 of 1
Current timestep = 455. State = [[-0.20786333 -0.04754336  0.2468173   1.        ]]. Action = [[ 0.5680138  -0.7365445  -0.36400878  0.6997204 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: No entry zone
Scene graph at timestep 455 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 455 is tensor(0.0131, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 455 is 1
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.20788182 -0.04776431  0.24684331  1.        ]]. Action = [[-0.55812365 -0.9983885   0.76621246  0.82516813]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 456 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 456 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 456 is 1
Human Feedback received at timestep 456 of 1
Current timestep = 457. State = [[-0.19856316 -0.06725182  0.27186185  1.        ]]. Action = [[ 0.06033146 -0.48439103  0.93364096  0.69403386]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 457 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 457 is tensor(0.0151, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 457 is 1
Human Feedback received at timestep 457 of 1
Current timestep = 458. State = [[-0.19335236 -0.09338453  0.32197928  1.        ]]. Action = [[ 0.45904374 -0.41250598 -0.49473834  0.8205347 ]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 458 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 458 is tensor(0.0157, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 458 is -1
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.20840085 -0.08686166  0.30748975  1.        ]]. Action = [[-0.6362081   0.33537745 -0.9136124   0.5589571 ]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 459 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 459 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 459 is -1
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.23956585 -0.07408924  0.26767114  1.        ]]. Action = [[-0.6640165  0.5076065 -0.7297102  0.7744738]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 460 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 460 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 460 is -1
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.24743801 -0.07239984  0.2608695   1.        ]]. Action = [[ 0.9424434   0.11606824 -0.33956963  0.9850085 ]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: No entry zone
Scene graph at timestep 461 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 461 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 461 is -1
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[-0.24802199 -0.07185695  0.26026502  1.        ]]. Action = [[-0.943242    0.58162665 -0.2701748   0.63410187]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 462 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 462 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 462 is -1
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.22457828 -0.09807135  0.26371405  1.        ]]. Action = [[ 0.8591212  -0.8004836  -0.093804    0.96782804]]. Reward = [10.]
Curr episode timestep = 11
Scene graph at timestep 463 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 463 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 463 is -1
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.16903472 -0.11652641  0.24614353  1.        ]]. Action = [[ 0.5515709   0.49468148 -0.01885521  0.9173069 ]]. Reward = [10.]
Curr episode timestep = 12
Scene graph at timestep 464 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 464 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 464 is -1
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.11942805 -0.13021928  0.23798442  1.        ]]. Action = [[ 0.6486912  -0.86961585 -0.44497538  0.9228704 ]]. Reward = [10.]
Curr episode timestep = 13
Scene graph at timestep 465 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 465 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 465 is -1
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.07642592 -0.1700868   0.2046578   1.        ]]. Action = [[ 0.62194526  0.33782387 -0.10917491  0.95775104]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: No entry zone
Scene graph at timestep 466 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 466 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 466 is 1
Human Feedback received at timestep 466 of 1
Current timestep = 467. State = [[-0.25083584  0.00256368  0.23267886  1.        ]]. Action = [[-0.96271735 -0.86141115  0.8102728  -0.16035903]]. Reward = [-100.]
Curr episode timestep = 15
Scene graph at timestep 467 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 467 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 467 is -1
Human Feedback received at timestep 467 of -1
Current timestep = 468. State = [[-0.23727015 -0.01371337  0.23432662  1.        ]]. Action = [[ 0.53758883 -0.41509867  0.05180538  0.8426634 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 468 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 468 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 468 is 1
Human Feedback received at timestep 468 of 1
Current timestep = 469. State = [[-0.19351459 -0.05624544  0.2519315   1.        ]]. Action = [[ 0.779335   -0.6782483   0.47532082  0.83689594]]. Reward = [10.]
Curr episode timestep = 1
Scene graph at timestep 469 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 469 is tensor(0.0106, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 469 is 1
Human Feedback received at timestep 469 of 1
Current timestep = 470. State = [[-0.13617769 -0.12470684  0.28477094  1.        ]]. Action = [[ 0.5041697  -0.8934942   0.20816708  0.80668736]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 470 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 470 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 470 is -1
Human Feedback received at timestep 470 of -1
Current timestep = 471. State = [[-0.11119355 -0.20783418  0.30317563  1.        ]]. Action = [[-0.2627294  -0.9390873   0.15108395  0.8156719 ]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 471 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 471 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 471 is -1
Human Feedback received at timestep 471 of -1
Current timestep = 472. State = [[-0.11069186 -0.26856938  0.30408144  1.        ]]. Action = [[ 0.19652271 -0.00192297 -0.2967931   0.71360564]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 472 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 472 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 472 is -1
Human Feedback received at timestep 472 of -1
Current timestep = 473. State = [[-0.09740151 -0.2804833   0.29096246  1.        ]]. Action = [[-0.90800613 -0.68963265 -0.35822618  0.79997206]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 473 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 473 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 473 is 1
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[-0.07420921 -0.27738598  0.29172802  1.        ]]. Action = [[ 0.9483663  -0.02638626 -0.23427945  0.604452  ]]. Reward = [10.]
Curr episode timestep = 6
Scene graph at timestep 474 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 474 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 474 is -1
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.02276775 -0.28375438  0.26792106  1.        ]]. Action = [[-0.8610525  -0.70255667  0.4485948   0.35562062]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 475 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 475 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 475 is -1
Human Feedback received at timestep 475 of -1
Current timestep = 476. State = [[-0.00589781 -0.28704616  0.26529187  1.        ]]. Action = [[ 0.9531207  -0.8378606  -0.30729365  0.8902042 ]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 476 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 476 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 476 is -1
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[-0.01194167 -0.27855605  0.28801176  1.        ]]. Action = [[-0.8994693  0.6721494  0.9293494  0.8748114]]. Reward = [10.]
Curr episode timestep = 9
Scene graph at timestep 477 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 477 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 477 is -1
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.0546627  -0.2405979   0.35134923  1.        ]]. Action = [[-0.01534373 -0.78027254  0.44791603  0.50714123]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 478 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 478 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 478 is -1
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.06516003 -0.2327404   0.36084914  1.        ]]. Action = [[0.8436277  0.66183066 0.8449042  0.8777611 ]]. Reward = [10.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 479 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 479 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 479 is -1
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.06550454 -0.2321868   0.36110353  1.        ]]. Action = [[ 0.6173656  -0.8726681   0.22356915  0.65481114]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 480 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 480 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 480 is -1
Human Feedback received at timestep 480 of -1
Current timestep = 481. State = [[-0.06556116 -0.23207203  0.36109957  1.        ]]. Action = [[ 0.71585274 -0.8815359   0.6161287   0.6623868 ]]. Reward = [10.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 481 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 481 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 481 is -1
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.06556116 -0.23207203  0.36109957  1.        ]]. Action = [[-0.2149694  -0.8761061   0.09080803  0.66194177]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 482 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 482 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 482 is -1
Human Feedback received at timestep 482 of -1
Current timestep = 483. State = [[-0.06556116 -0.23207203  0.36109957  1.        ]]. Action = [[ 0.7669283  -0.23015952  0.78546846  0.7704904 ]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 483 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 483 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 483 is -1
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[-0.08921024 -0.23731843  0.36108088  1.        ]]. Action = [[-0.9944032   0.03048849 -0.21507555  0.3185885 ]]. Reward = [10.]
Curr episode timestep = 16
Scene graph at timestep 484 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 484 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 484 is -1
Human Feedback received at timestep 484 of -1
Current timestep = 485. State = [[-0.14897415 -0.2300823   0.35231656  1.        ]]. Action = [[-0.8183517  -0.94745654 -0.07081485  0.84066606]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 485 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 485 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 485 is -1
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.16951936 -0.21888499  0.34503487  1.        ]]. Action = [[-0.478765    0.36656344 -0.21320468  0.7310872 ]]. Reward = [10.]
Curr episode timestep = 18
Scene graph at timestep 486 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 486 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 486 is -1
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.20322071 -0.19770771  0.33084658  1.        ]]. Action = [[-0.10503328 -0.5829684   0.6564636   0.906445  ]]. Reward = [10.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 487 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 487 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 487 is -1
Human Feedback received at timestep 487 of -1
Current timestep = 488. State = [[-0.20490472 -0.17340726  0.3238396   1.        ]]. Action = [[0.13528109 0.64943933 0.06727707 0.10942221]]. Reward = [10.]
Curr episode timestep = 20
Scene graph at timestep 488 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 488 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 488 is -1
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.1927711  -0.16443856  0.32773823  1.        ]]. Action = [[ 0.7051871  -0.9769635  -0.49769902  0.8610829 ]]. Reward = [10.]
Curr episode timestep = 21
Scene graph at timestep 489 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 489 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 489 is -1
Human Feedback received at timestep 489 of -1
Current timestep = 490. State = [[-0.2507917   0.00251701  0.23267882  1.        ]]. Action = [[ 0.87408686 -0.83752245  0.6965139  -0.28842068]]. Reward = [-100.]
Curr episode timestep = 22
Scene graph at timestep 490 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 490 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 490 is 1
Human Feedback received at timestep 490 of 1
Current timestep = 491. State = [[-0.24984977 -0.01559844  0.23405217  1.        ]]. Action = [[ 0.01082659 -0.50922096  0.34926176  0.9812019 ]]. Reward = [10.]
Curr episode timestep = 0
Scene graph at timestep 491 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 491 is tensor(0.0184, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 491 is 1
Human Feedback received at timestep 491 of 1
Current timestep = 492. State = [[-0.25206453 -0.03868432  0.24184689  1.        ]]. Action = [[-0.50969046  0.5734601   0.6046622   0.8942025 ]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 492 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 492 is tensor(0.0139, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 492 is 1
Human Feedback received at timestep 492 of 1
Current timestep = 493. State = [[-0.2494254  -0.01945845  0.2590218   1.        ]]. Action = [[-0.00474477  0.7392051   0.8689866   0.9216776 ]]. Reward = [10.]
Curr episode timestep = 2
Scene graph at timestep 493 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 493 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 493 is -1
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.24677587 -0.00807645  0.3391148   1.        ]]. Action = [[-0.04507941 -0.96770704  0.6522434   0.94367456]]. Reward = [10.]
Curr episode timestep = 3
Scene graph at timestep 494 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 494 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 494 is 1
Human Feedback received at timestep 494 of 1
Current timestep = 495. State = [[-0.25389522 -0.05369304  0.3828649   1.        ]]. Action = [[-0.50057465 -0.56274134  0.89291036  0.7161988 ]]. Reward = [10.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 495 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 495 is tensor(0.0189, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 495 is 1
Human Feedback received at timestep 495 of 1
Current timestep = 496. State = [[-0.24981675 -0.06238156  0.39437157  1.        ]]. Action = [[-0.30884707 -0.08738065  0.36080432  0.8255911 ]]. Reward = [10.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 496 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 496 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 496 is -1
Human Feedback received at timestep 496 of -1
Current timestep = 497. State = [[-0.25004753 -0.0647015   0.39616677  1.        ]]. Action = [[-0.46629864  0.5051224  -0.54843193  0.8075588 ]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 497 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 497 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 497 is -1
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.24944419 -0.06534464  0.39766112  1.        ]]. Action = [[-0.7503265   0.10235286 -0.8444624   0.9433737 ]]. Reward = [10.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 498 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 498 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 498 is -1
Human Feedback received at timestep 498 of -1
Current timestep = 499. State = [[-0.24997602 -0.06566144  0.39764526  1.        ]]. Action = [[-0.31548572  0.41413975 -0.41045666  0.9534452 ]]. Reward = [10.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 499 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 499 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 499 is -1
Human Feedback received at timestep 499 of -1
Current timestep = 500. State = [[-0.2501461  -0.06560142  0.39765844  1.        ]]. Action = [[-0.18962699 -0.957069    0.33356977  0.57697415]]. Reward = [10.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 500 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 500 is tensor(0.0128, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 500 is 1
Human Feedback received at timestep 500 of 1
Current timestep = 501. State = [[-0.25178364 -0.0931653   0.38236696  1.        ]]. Action = [[ 0.08846807 -0.80563194 -0.9861292   0.87203217]]. Reward = [10.]
Curr episode timestep = 10
Scene graph at timestep 501 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 501 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 501 is 1
Human Feedback received at timestep 501 of 1
Current timestep = 502. State = [[-0.2525564  -0.16626737  0.31905812  1.        ]]. Action = [[ 0.00788629 -0.7983115  -0.48638904  0.81004786]]. Reward = [10.]
Curr episode timestep = 11
Scene graph at timestep 502 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 502 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 502 is -1
Human Feedback received at timestep 502 of -1
Current timestep = 503. State = [[-0.22960685 -0.20627895  0.28720698  1.        ]]. Action = [[0.55897117 0.19383311 0.31779528 0.9355922 ]]. Reward = [10.]
Curr episode timestep = 12
Scene graph at timestep 503 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 503 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 503 is -1
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.17986809 -0.21003672  0.31030566  1.        ]]. Action = [[ 0.92273796 -0.18648142  0.638695    0.7055342 ]]. Reward = [10.]
Curr episode timestep = 13
Scene graph at timestep 504 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 504 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 504 is -1
Human Feedback received at timestep 504 of -1
Current timestep = 505. State = [[-0.12691733 -0.2188793   0.34329396  1.        ]]. Action = [[-0.01835477 -0.9237109  -0.9183576   0.9027877 ]]. Reward = [10.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 505 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 505 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 505 is -1
Human Feedback received at timestep 505 of -1
Current timestep = 506. State = [[-0.11407712 -0.22254793  0.3511973   1.        ]]. Action = [[ 0.816002   -0.8101157   0.7735485   0.89254904]]. Reward = [10.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 506 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 506 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 506 is -1
Human Feedback received at timestep 506 of -1
Current timestep = 507. State = [[-0.13490155 -0.21860154  0.355514    1.        ]]. Action = [[-0.8740476   0.24888802  0.04354918  0.4814366 ]]. Reward = [10.]
Curr episode timestep = 16
Scene graph at timestep 507 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 507 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 507 is -1
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[-0.17026296 -0.20949656  0.3621185   1.        ]]. Action = [[-0.90271634 -0.91104233  0.4827249   0.79223144]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 508 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 508 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 508 is -1
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.17646231 -0.2078684   0.362768    1.        ]]. Action = [[-0.7021271 -0.9027731 -0.889984   0.9298885]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 509 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 509 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 509 is -1
Human Feedback received at timestep 509 of -1
Current timestep = 510. State = [[-0.18462487 -0.20987551  0.36075857  1.        ]]. Action = [[-0.19258875 -0.01739019 -0.05602622  0.7542833 ]]. Reward = [10.]
Curr episode timestep = 19
Scene graph at timestep 510 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 510 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 510 is -1
Human Feedback received at timestep 510 of -1
Current timestep = 511. State = [[-0.19243595 -0.210243    0.35758916  1.        ]]. Action = [[-0.02818483 -0.6381139   0.82178545 -0.5120142 ]]. Reward = [10.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 511 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 511 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 511 is -1
Human Feedback received at timestep 511 of -1
Current timestep = 512. State = [[-0.19339938 -0.21073376  0.35759246  1.        ]]. Action = [[-0.89427245 -0.21243167  0.86578083  0.9350424 ]]. Reward = [10.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 512 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 512 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 512 is -1
Human Feedback received at timestep 512 of -1
Current timestep = 513. State = [[-0.19357392 -0.21088701  0.35755438  1.        ]]. Action = [[-0.40267456  0.12978327  0.6111374   0.92169094]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 513 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 513 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 513 is -1
Human Feedback received at timestep 513 of -1
Current timestep = 514. State = [[-0.19360599 -0.2109407   0.35757315  1.        ]]. Action = [[-0.8185349  -0.948021   -0.7689973   0.84464645]]. Reward = [10.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 514 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 514 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 514 is -1
Human Feedback received at timestep 514 of -1
Current timestep = 515. State = [[-0.19360599 -0.2109407   0.35757315  1.        ]]. Action = [[-0.9028606  -0.80644906 -0.7806201   0.82296884]]. Reward = [10.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 515 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 515 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 515 is -1
Human Feedback received at timestep 515 of -1
Current timestep = 516. State = [[-0.21583392 -0.23697336  0.36079374  1.        ]]. Action = [[-0.5088267  -0.70843047  0.00717902  0.6235497 ]]. Reward = [10.]
Curr episode timestep = 25
Scene graph at timestep 516 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 516 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 516 is -1
Human Feedback received at timestep 516 of -1
Current timestep = 517. State = [[-0.24673179 -0.26761165  0.3670228   1.        ]]. Action = [[ 0.54609036 -0.95937645  0.81556034  0.9892924 ]]. Reward = [10.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 517 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 517 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 517 is -1
Human Feedback received at timestep 517 of -1
Current timestep = 518. State = [[-0.25219744 -0.2738795   0.37033692  1.        ]]. Action = [[-0.42090356 -0.22683758  0.38627934  0.6268308 ]]. Reward = [10.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 518 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 518 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 518 is -1
Human Feedback received at timestep 518 of -1
Current timestep = 519. State = [[-0.25326136 -0.27469954  0.3706553   1.        ]]. Action = [[-0.08199739 -0.9431543   0.50564003  0.87863255]]. Reward = [10.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 519 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 519 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 519 is -1
Human Feedback received at timestep 519 of -1
Current timestep = 520. State = [[-0.25339732 -0.27470583  0.3705828   1.        ]]. Action = [[ 0.1232717 -0.626408  -0.9719465  0.947438 ]]. Reward = [10.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 520 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 520 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 520 is 1
Human Feedback received at timestep 520 of 1
Current timestep = 521. State = [[-0.25339732 -0.27470583  0.3705828   1.        ]]. Action = [[-0.49094826 -0.44270265  0.8438902   0.9227617 ]]. Reward = [10.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Scene graph at timestep 521 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 521 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 521 is -1
Human Feedback received at timestep 521 of -1
Current timestep = 522. State = [[-0.25339732 -0.27470583  0.3705828   1.        ]]. Action = [[-0.07602519 -0.8760337   0.37390292  0.9723437 ]]. Reward = [10.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 522 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 522 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 522 is -1
Human Feedback received at timestep 522 of -1
Current timestep = 523. State = [[-0.25339732 -0.27470583  0.3705828   1.        ]]. Action = [[-0.08103526  0.9465616   0.73603463  0.51240087]]. Reward = [10.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Scene graph at timestep 523 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 523 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 523 is -1
Human Feedback received at timestep 523 of -1
Current timestep = 524. State = [[-0.25339732 -0.27470583  0.3705828   1.        ]]. Action = [[-0.82524097 -0.10257596  0.02965736  0.8648515 ]]. Reward = [10.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 524 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 524 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 524 is -1
Human Feedback received at timestep 524 of -1
Current timestep = 525. State = [[-0.25120524 -0.2561475   0.35808256  1.        ]]. Action = [[ 0.01103079  0.5867677  -0.3869418   0.87910044]]. Reward = [10.]
Curr episode timestep = 34
Scene graph at timestep 525 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 525 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 525 is -1
Human Feedback received at timestep 525 of -1
Current timestep = 526. State = [[-0.25438303 -0.2310401   0.33899105  1.        ]]. Action = [[-0.7593406   0.5616214  -0.6487975   0.82948816]]. Reward = [10.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 526 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 526 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 526 is 1
Human Feedback received at timestep 526 of 1
Current timestep = 527. State = [[-0.2524401  -0.22707337  0.33701092  1.        ]]. Action = [[-0.98880607  0.43932927  0.1373502   0.72503734]]. Reward = [10.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 527 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 527 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 527 is -1
Human Feedback received at timestep 527 of -1
Current timestep = 528. State = [[-0.25198182 -0.22633037  0.3366406   1.        ]]. Action = [[-0.8801804  -0.6212833   0.92127633  0.8445728 ]]. Reward = [10.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 528 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 528 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 528 is -1
Human Feedback received at timestep 528 of -1
Current timestep = 529. State = [[-0.2519137  -0.22623338  0.33659706  1.        ]]. Action = [[ 0.51269746 -0.9275449  -0.12051392  0.4339453 ]]. Reward = [10.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 529 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 529 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 529 is -1
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.22904332 -0.21209157  0.33328107  1.        ]]. Action = [[ 0.9381063   0.26002383 -0.16215587  0.75399446]]. Reward = [10.]
Curr episode timestep = 39
Scene graph at timestep 530 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 530 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 530 is -1
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.18327583 -0.1975871   0.31045055  1.        ]]. Action = [[ 0.7732154  -0.997994   -0.69665587  0.8810797 ]]. Reward = [10.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 531 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 531 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 531 is -1
Human Feedback received at timestep 531 of -1
Current timestep = 532. State = [[-0.18558948 -0.21930276  0.29380247  1.        ]]. Action = [[-0.17952359 -0.67579997 -0.6552149   0.9406676 ]]. Reward = [10.]
Curr episode timestep = 41
Scene graph at timestep 532 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 532 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 532 is -1
Human Feedback received at timestep 532 of -1
Current timestep = 533. State = [[-0.18437259 -0.2515731   0.2571962   1.        ]]. Action = [[-0.29269248 -0.946506   -0.64664274  0.8069434 ]]. Reward = [10.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 533 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 533 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 533 is -1
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.18205412 -0.2565276   0.25076494  1.        ]]. Action = [[-0.70271635 -0.71333647 -0.43573493  0.68976545]]. Reward = [10.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 534 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 534 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 534 is 1
Human Feedback received at timestep 534 of 1
Current timestep = 535. State = [[-0.18275984 -0.25722817  0.25006294  1.        ]]. Action = [[-0.85204    -0.09377766  0.69166756  0.88402605]]. Reward = [10.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 535 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 535 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 535 is -1
Human Feedback received at timestep 535 of -1
Current timestep = 536. State = [[-0.20089692 -0.24163522  0.2588111   1.        ]]. Action = [[-0.71128964  0.5713875   0.39907205  0.6970296 ]]. Reward = [10.]
Curr episode timestep = 45
Scene graph at timestep 536 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 536 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 536 is -1
Human Feedback received at timestep 536 of -1
Current timestep = 537. State = [[-0.23680781 -0.21481016  0.27800775  1.        ]]. Action = [[-0.13707632 -0.79708797 -0.79170376  0.99328315]]. Reward = [10.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 537 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 537 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 537 is -1
Human Feedback received at timestep 537 of -1
Current timestep = 538. State = [[-0.22047108 -0.23002169  0.28221905  1.        ]]. Action = [[ 0.9016299  -0.6701366  -0.05438757  0.6459007 ]]. Reward = [10.]
Curr episode timestep = 47
Scene graph at timestep 538 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 538 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 538 is -1
Human Feedback received at timestep 538 of -1
Current timestep = 539. State = [[-0.18456519 -0.25764936  0.2740035   1.        ]]. Action = [[-0.517402   -0.47081345 -0.31565338  0.8755777 ]]. Reward = [10.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 539 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 539 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 539 is -1
Human Feedback received at timestep 539 of -1
Current timestep = 540. State = [[-0.17959316 -0.26087695  0.27417314  1.        ]]. Action = [[-0.9509348  -0.3041361  -0.07487839  0.9461467 ]]. Reward = [10.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 540 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 540 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 540 is -1
Human Feedback received at timestep 540 of -1
Current timestep = 541. State = [[-0.15170525 -0.26205423  0.2785429   1.        ]]. Action = [[ 0.9132478  -0.12991256  0.11537695  0.19147992]]. Reward = [10.]
Curr episode timestep = 50
Scene graph at timestep 541 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 541 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 541 is -1
Human Feedback received at timestep 541 of -1
Current timestep = 542. State = [[-0.09970009 -0.27231875  0.28070387  1.        ]]. Action = [[-0.6671987  -0.79527843  0.98393226  0.7340261 ]]. Reward = [10.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 542 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 542 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 542 is -1
Human Feedback received at timestep 542 of -1
Current timestep = 543. State = [[-0.09071187 -0.27387667  0.28304312  1.        ]]. Action = [[-0.3809088  -0.90907687  0.11774242  0.8859639 ]]. Reward = [10.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 543 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 543 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 543 is -1
Human Feedback received at timestep 543 of -1
Current timestep = 544. State = [[-0.09990402 -0.26858276  0.27019945  1.        ]]. Action = [[-0.371464    0.24919319 -0.456477    0.5805969 ]]. Reward = [10.]
Curr episode timestep = 53
Scene graph at timestep 544 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 544 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 544 is -1
Human Feedback received at timestep 544 of -1
Current timestep = 545. State = [[-0.11401693 -0.24746148  0.2674962   1.        ]]. Action = [[-0.42128122  0.6137979   0.43501306  0.98557377]]. Reward = [10.]
Curr episode timestep = 54
Scene graph at timestep 545 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 545 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 545 is -1
Human Feedback received at timestep 545 of -1
Current timestep = 546. State = [[-0.1539658  -0.19854318  0.2968985   1.        ]]. Action = [[-0.56754476  0.38886476  0.4425552   0.9063699 ]]. Reward = [10.]
Curr episode timestep = 55
Scene graph at timestep 546 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 546 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 546 is -1
Human Feedback received at timestep 546 of -1
Current timestep = 547. State = [[-0.21675095 -0.18918575  0.32156542  1.        ]]. Action = [[-0.8136892  -0.4204421  -0.23042941  0.76625943]]. Reward = [10.]
Curr episode timestep = 56
Scene graph at timestep 547 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 547 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 547 is -1
Human Feedback received at timestep 547 of -1
Current timestep = 548. State = [[-0.26798767 -0.20196049  0.31438524  1.        ]]. Action = [[ 0.08230448 -0.22810256  0.8323145   0.8116317 ]]. Reward = [10.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 548 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 548 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 548 is -1
Human Feedback received at timestep 548 of -1
Current timestep = 549. State = [[-0.27868852 -0.20319371  0.31209692  1.        ]]. Action = [[-0.1728195  -0.34337765 -0.6311667   0.7356961 ]]. Reward = [10.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Scene graph at timestep 549 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 549 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 549 is -1
Human Feedback received at timestep 549 of -1
Current timestep = 550. State = [[-0.28007796 -0.20359145  0.31186274  1.        ]]. Action = [[-0.5041623   0.34871912 -0.47268963  0.995559  ]]. Reward = [10.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 550 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 550 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 550 is -1
Human Feedback received at timestep 550 of -1
Current timestep = 551. State = [[-0.2803961  -0.20372786  0.31181264  1.        ]]. Action = [[-0.95655817 -0.8780951  -0.20057642 -0.5840739 ]]. Reward = [10.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Scene graph at timestep 551 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 551 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 551 is -1
Human Feedback received at timestep 551 of -1
Current timestep = 552. State = [[-0.2803961  -0.20372786  0.31181264  1.        ]]. Action = [[ 0.48208654 -0.9947669   0.7985245   0.96787477]]. Reward = [10.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 552 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 552 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 552 is 1
Human Feedback received at timestep 552 of 1
Current timestep = 553. State = [[-0.2803961  -0.20372786  0.31181264  1.        ]]. Action = [[-0.579644   -0.21773094  0.31460857  0.96090245]]. Reward = [10.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 553 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 553 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 553 is -1
Human Feedback received at timestep 553 of -1
Current timestep = 554. State = [[-0.2803961  -0.20372786  0.31181264  1.        ]]. Action = [[-0.11937839 -0.47794825 -0.69296753  0.85277164]]. Reward = [10.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Scene graph at timestep 554 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 554 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 554 is -1
Human Feedback received at timestep 554 of -1
Current timestep = 555. State = [[-0.28042412 -0.20377438  0.3118459   1.        ]]. Action = [[-0.9875034  -0.7218214  -0.90913314  0.7437563 ]]. Reward = [10.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Scene graph at timestep 555 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 555 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 555 is -1
Human Feedback received at timestep 555 of -1
Current timestep = 556. State = [[-0.26901227 -0.22421508  0.31229788  1.        ]]. Action = [[ 0.5792687  -0.72667426 -0.13655412  0.92951274]]. Reward = [10.]
Curr episode timestep = 65
Scene graph at timestep 556 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 556 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 556 is -1
Human Feedback received at timestep 556 of -1
Current timestep = 557. State = [[-0.24926259 -0.26043808  0.29928622  1.        ]]. Action = [[-0.81464577 -0.52829504  0.13426733  0.6869273 ]]. Reward = [10.]
Curr episode timestep = 66
Action ignored: Workspace boundary
Scene graph at timestep 557 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 557 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 557 is -1
Human Feedback received at timestep 557 of -1
Current timestep = 558. State = [[-0.2462244  -0.26498887  0.29738018  1.        ]]. Action = [[-0.8349095   0.4014318  -0.9022841   0.68883467]]. Reward = [10.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 558 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 558 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 558 is -1
Human Feedback received at timestep 558 of -1
Current timestep = 559. State = [[-0.24598157 -0.26530623  0.29715675  1.        ]]. Action = [[-0.48946482  0.45976448 -0.8679441   0.92566013]]. Reward = [10.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 559 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 559 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 559 is 1
Human Feedback received at timestep 559 of 1
Current timestep = 560. State = [[-0.24598157 -0.26530623  0.29715675  1.        ]]. Action = [[ 0.13823164 -0.9880777   0.6985959   0.97990465]]. Reward = [10.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Scene graph at timestep 560 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 560 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 560 is -1
Human Feedback received at timestep 560 of -1
Current timestep = 561. State = [[-0.22264662 -0.24032451  0.30776042  1.        ]]. Action = [[0.8336706  0.78568995 0.8241842  0.64997435]]. Reward = [10.]
Curr episode timestep = 70
Scene graph at timestep 561 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 561 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 561 is -1
Human Feedback received at timestep 561 of -1
Current timestep = 562. State = [[-0.16948532 -0.2028357   0.35446632  1.        ]]. Action = [[ 0.8442724 -0.7350087  0.8221408  0.9157139]]. Reward = [10.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 562 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 562 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 562 is -1
Human Feedback received at timestep 562 of -1
Current timestep = 563. State = [[-0.160819   -0.21389194  0.37298164  1.        ]]. Action = [[ 0.10020208 -0.5241189   0.25738764  0.85046554]]. Reward = [10.]
Curr episode timestep = 72
Scene graph at timestep 563 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 563 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 563 is -1
Human Feedback received at timestep 563 of -1
Current timestep = 564. State = [[-0.16588522 -0.25070274  0.39825976  1.        ]]. Action = [[-0.31473863 -0.40123612  0.25866663  0.97686076]]. Reward = [10.]
Curr episode timestep = 73
Scene graph at timestep 564 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 564 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 564 is -1
Human Feedback received at timestep 564 of -1
Current timestep = 565. State = [[-0.1771735  -0.272448    0.41426215  1.        ]]. Action = [[-0.29881346 -0.58980304 -0.4747283   0.7204453 ]]. Reward = [10.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Scene graph at timestep 565 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 565 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 565 is -1
Human Feedback received at timestep 565 of -1
Current timestep = 566. State = [[-0.1793934  -0.27505308  0.41639635  1.        ]]. Action = [[-0.9640748  -0.29081297 -0.5905514   0.90487194]]. Reward = [10.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 566 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 566 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 566 is -1
Human Feedback received at timestep 566 of -1
Current timestep = 567. State = [[-0.17969263 -0.2755203   0.4166863   1.        ]]. Action = [[-0.8240593 -0.7852274 -0.7740681  0.9548118]]. Reward = [10.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Scene graph at timestep 567 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 567 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 567 is -1
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.17970496 -0.27557367  0.41668633  1.        ]]. Action = [[ 0.0301702  -0.7076291   0.86670256  0.7681416 ]]. Reward = [10.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Scene graph at timestep 568 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 568 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 568 is -1
Human Feedback received at timestep 568 of -1
Current timestep = 569. State = [[-0.16707203 -0.27161777  0.40626132  1.        ]]. Action = [[ 0.5829624   0.00780988 -0.5429481   0.89033604]]. Reward = [10.]
Curr episode timestep = 78
Scene graph at timestep 569 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 569 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 569 is -1
Human Feedback received at timestep 569 of -1
Current timestep = 570. State = [[-0.13259248 -0.27744618  0.37866557  1.        ]]. Action = [[-0.41989934 -0.07893151  0.5687368   0.92933154]]. Reward = [10.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Scene graph at timestep 570 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 570 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 570 is -1
Human Feedback received at timestep 570 of -1
Current timestep = 571. State = [[-0.1266712  -0.2774771   0.37291223  1.        ]]. Action = [[ 0.9887593  -0.43813527  0.95039666  0.90496457]]. Reward = [10.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Scene graph at timestep 571 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 571 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 571 is -1
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.14259984 -0.27467     0.35915485  1.        ]]. Action = [[-0.7276178   0.2637974  -0.6357258   0.38173008]]. Reward = [10.]
Curr episode timestep = 81
Scene graph at timestep 572 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 572 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 572 is -1
Human Feedback received at timestep 572 of -1
Current timestep = 573. State = [[-0.17613697 -0.2646386   0.33050063  1.        ]]. Action = [[ 0.19547439 -0.7286187   0.69282293  0.854584  ]]. Reward = [10.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Scene graph at timestep 573 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 573 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 573 is -1
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.1766139 -0.2752471  0.3129003  1.       ]]. Action = [[ 0.5727172  -0.37392747 -0.92877203  0.8630793 ]]. Reward = [10.]
Curr episode timestep = 83
Scene graph at timestep 574 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 574 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 574 is -1
Human Feedback received at timestep 574 of -1
Current timestep = 575. State = [[-0.14531152 -0.28937915  0.24342552  1.        ]]. Action = [[ 0.22977078 -0.40293884  0.4759915   0.8051243 ]]. Reward = [10.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Scene graph at timestep 575 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 575 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 575 is -1
Human Feedback received at timestep 575 of -1
Current timestep = 576. State = [[-0.11411009 -0.2796604   0.24220279  1.        ]]. Action = [[0.71365845 0.2543702  0.4726088  0.91348124]]. Reward = [10.]
Curr episode timestep = 85
Scene graph at timestep 576 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 576 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 576 is -1
Human Feedback received at timestep 576 of -1
Current timestep = 577. State = [[-0.07973982 -0.27097788  0.26027858  1.        ]]. Action = [[-0.09034157 -0.92610157  0.6766789   0.96528697]]. Reward = [10.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Scene graph at timestep 577 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 577 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 577 is 1
Human Feedback received at timestep 577 of 1
Current timestep = 578. State = [[-0.05172007 -0.2534058   0.27639955  1.        ]]. Action = [[0.67523813 0.29678726 0.48260975 0.08534455]]. Reward = [10.]
Curr episode timestep = 87
Scene graph at timestep 578 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 578 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 578 is -1
Human Feedback received at timestep 578 of -1
Current timestep = 579. State = [[-0.01589487 -0.23771839  0.29937983  1.        ]]. Action = [[ 0.348351   -0.9165181   0.9285711   0.90018415]]. Reward = [10.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Scene graph at timestep 579 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 579 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 579 is -1
Human Feedback received at timestep 579 of -1
Current timestep = 580. State = [[ 2.4617769e-04 -2.2242376e-01  2.9687840e-01  1.0000000e+00]]. Action = [[ 0.55731654  0.2204858  -0.56127506  0.80217576]]. Reward = [10.]
Curr episode timestep = 89
Scene graph at timestep 580 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 580 is tensor(0.0088, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 580 is -1
Human Feedback received at timestep 580 of -1
Current timestep = 581. State = [[ 0.01968065 -0.22428314  0.24973425  1.        ]]. Action = [[-0.5573839  -0.12632668 -0.37103677  0.5241883 ]]. Reward = [10.]
Curr episode timestep = 90
Scene graph at timestep 581 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 581 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 581 is -1
Human Feedback received at timestep 581 of -1
Current timestep = 582. State = [[ 0.02516182 -0.21233165  0.23891263  1.        ]]. Action = [[0.16035187 0.60256994 0.39735687 0.93600464]]. Reward = [10.]
Curr episode timestep = 91
Scene graph at timestep 582 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 582 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 582 is -1
Human Feedback received at timestep 582 of -1
Current timestep = 583. State = [[ 0.02784154 -0.18026277  0.2508784   1.        ]]. Action = [[ 0.8377738  -0.75783885  0.0991081   0.9443145 ]]. Reward = [10.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Scene graph at timestep 583 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 583 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 583 is -1
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[ 0.02765845 -0.17581207  0.251806    1.        ]]. Action = [[0.9014423  0.45332968 0.12418497 0.9596181 ]]. Reward = [10.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Scene graph at timestep 584 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 584 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 584 is -1
Human Feedback received at timestep 584 of -1
Current timestep = 585. State = [[ 0.03307269 -0.20021743  0.2644764   1.        ]]. Action = [[ 0.4894191  -0.8596096   0.27415168  0.83500195]]. Reward = [10.]
Curr episode timestep = 94
Scene graph at timestep 585 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 585 is tensor(0.0159, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 585 is -1
Human Feedback received at timestep 585 of -1
Current timestep = 586. State = [[ 0.0658857  -0.22770642  0.2793332   1.        ]]. Action = [[0.33968782 0.2748692  0.15899408 0.7316823 ]]. Reward = [10.]
Curr episode timestep = 95
Scene graph at timestep 586 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 586 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 586 is -1
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[ 0.08820967 -0.22408302  0.2890524   1.        ]]. Action = [[-0.64829123  0.8302059  -0.79471385  0.9749379 ]]. Reward = [10.]
Curr episode timestep = 96
Action ignored: No entry zone
Scene graph at timestep 587 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 587 is tensor(0.0153, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 587 is -1
Human Feedback received at timestep 587 of -1
Current timestep = 588. State = [[ 0.07493095 -0.24248157  0.31210998  1.        ]]. Action = [[-0.7448753  -0.35032225  0.44234443  0.8234788 ]]. Reward = [10.]
Curr episode timestep = 97
Scene graph at timestep 588 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 588 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 588 is -1
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[ 0.03328454 -0.27417177  0.3262294   1.        ]]. Action = [[-0.70882165 -0.22427553 -0.44030255  0.34975243]]. Reward = [10.]
Curr episode timestep = 98
Scene graph at timestep 589 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 589 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 589 is -1
Human Feedback received at timestep 589 of -1
Current timestep = 590. State = [[-0.00285639 -0.2698112   0.31399578  1.        ]]. Action = [[-0.01793188  0.7552936   0.08583474  0.6614213 ]]. Reward = [10.]
Curr episode timestep = 99
Scene graph at timestep 590 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 590 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 590 is -1
Human Feedback received at timestep 590 of -1
Current timestep = 591. State = [[-0.0411438  -0.23069341  0.33136398  1.        ]]. Action = [[-0.7912843   0.00428677  0.34863603  0.926116  ]]. Reward = [10.]
Curr episode timestep = 100
Scene graph at timestep 591 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 591 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 591 is -1
Human Feedback received at timestep 591 of -1
Current timestep = 592. State = [[-0.09521533 -0.21410847  0.34983197  1.        ]]. Action = [[-0.32505906 -0.92787224 -0.8514778   0.86032224]]. Reward = [10.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Scene graph at timestep 592 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 592 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 592 is -1
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.1228629  -0.2002444   0.33173138  1.        ]]. Action = [[-0.83076304  0.60009956 -0.7592503   0.95205104]]. Reward = [10.]
Curr episode timestep = 102
Scene graph at timestep 593 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 593 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 593 is -1
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.1579537  -0.17696536  0.29291236  1.        ]]. Action = [[ 0.9562094  -0.82386637 -0.36471343  0.35665488]]. Reward = [10.]
Curr episode timestep = 103
Scene graph at timestep 594 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 594 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 594 is -1
Human Feedback received at timestep 594 of -1
Current timestep = 595. State = [[-0.140499   -0.20721518  0.2635214   1.        ]]. Action = [[-0.7380835   0.5237397   0.39663565  0.9161985 ]]. Reward = [10.]
Curr episode timestep = 104
Scene graph at timestep 595 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 595 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 595 is -1
Human Feedback received at timestep 595 of -1
Current timestep = 596. State = [[-0.1649986  -0.18741888  0.27423468  1.        ]]. Action = [[-0.16755027  0.7793486  -0.90484464  0.84183455]]. Reward = [10.]
Curr episode timestep = 105
Action ignored: No entry zone
Scene graph at timestep 596 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 596 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 596 is -1
Human Feedback received at timestep 596 of -1
Current timestep = 597. State = [[-0.1827793  -0.2148046   0.28584862  1.        ]]. Action = [[-0.7420663  -0.6883663  -0.24480861  0.76731443]]. Reward = [10.]
Curr episode timestep = 106
Scene graph at timestep 597 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 597 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 597 is -1
Human Feedback received at timestep 597 of -1
Current timestep = 598. State = [[-0.240698   -0.24306267  0.26884198  1.        ]]. Action = [[-0.82211775  0.39671028  0.63084435  0.9507555 ]]. Reward = [10.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Scene graph at timestep 598 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 598 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 598 is -1
Human Feedback received at timestep 598 of -1
Current timestep = 599. State = [[-0.23816864 -0.22778518  0.2554628   1.        ]]. Action = [[ 0.5091729   0.99148345 -0.882525    0.92438185]]. Reward = [10.]
Curr episode timestep = 108
Scene graph at timestep 599 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 599 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 599 is -1
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.19001536 -0.17486586  0.21376364  1.        ]]. Action = [[ 0.5686796  -0.18741441  0.5239694   0.9730259 ]]. Reward = [10.]
Curr episode timestep = 109
Scene graph at timestep 600 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 600 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 600 is -1
Human Feedback received at timestep 600 of -1
Current timestep = 601. State = [[-0.14408906 -0.15094692  0.2377327   1.        ]]. Action = [[0.74723876 0.31409383 0.8454473  0.7561804 ]]. Reward = [10.]
Curr episode timestep = 110
Scene graph at timestep 601 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 601 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 601 is -1
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.11668438 -0.15203547  0.2741824   1.        ]]. Action = [[-0.9651454  -0.23581535 -0.47768438  0.7357334 ]]. Reward = [10.]
Curr episode timestep = 111
Scene graph at timestep 602 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 602 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 602 is -1
Human Feedback received at timestep 602 of -1
Current timestep = 603. State = [[-0.14855732 -0.18003774  0.27255556  1.        ]]. Action = [[ 0.4103017  -0.6312022   0.38122773  0.8341104 ]]. Reward = [10.]
Curr episode timestep = 112
Scene graph at timestep 603 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 603 is tensor(0.0081, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 603 is -1
Human Feedback received at timestep 603 of -1
Current timestep = 604. State = [[-0.14506744 -0.21584943  0.28144342  1.        ]]. Action = [[-0.24894112 -0.0757345   0.02686036  0.9412885 ]]. Reward = [10.]
Curr episode timestep = 113
Scene graph at timestep 604 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 604 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 604 is -1
Human Feedback received at timestep 604 of -1
Current timestep = 605. State = [[-0.15729862 -0.20371714  0.28316328  1.        ]]. Action = [[-0.48671365  0.96642756 -0.05466211  0.9562154 ]]. Reward = [10.]
Curr episode timestep = 114
Scene graph at timestep 605 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 605 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 605 is -1
Human Feedback received at timestep 605 of -1
Current timestep = 606. State = [[-0.20668456 -0.18372086  0.31379467  1.        ]]. Action = [[-0.5079462 -0.8447964  0.7125994  0.9401059]]. Reward = [10.]
Curr episode timestep = 115
Scene graph at timestep 606 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 606 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 606 is -1
Human Feedback received at timestep 606 of -1
Current timestep = 607. State = [[-0.24246815 -0.18935949  0.33070117  1.        ]]. Action = [[-0.02679479  0.5978756  -0.6027524   0.7301538 ]]. Reward = [10.]
Curr episode timestep = 116
Scene graph at timestep 607 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 607 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 607 is -1
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.25962204 -0.17476174  0.3019759   1.        ]]. Action = [[-0.18342716 -0.08608961 -0.66098803  0.86827254]]. Reward = [10.]
Curr episode timestep = 117
Scene graph at timestep 608 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 608 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 608 is -1
Human Feedback received at timestep 608 of -1
Current timestep = 609. State = [[-0.2518756  -0.18690726  0.28193498  1.        ]]. Action = [[ 0.42688322 -0.92362726  0.6473932   0.9762192 ]]. Reward = [10.]
Curr episode timestep = 118
Scene graph at timestep 609 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 609 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 609 is 1
Human Feedback received at timestep 609 of 1
Current timestep = 610. State = [[-0.24514502 -0.23476534  0.29454997  1.        ]]. Action = [[-0.79543006 -0.9804966   0.24051309  0.90532684]]. Reward = [10.]
Curr episode timestep = 119
Action ignored: Workspace boundary
Scene graph at timestep 610 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 610 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 610 is 1
Human Feedback received at timestep 610 of 1
Current timestep = 611. State = [[-0.2403467  -0.25160676  0.29431328  1.        ]]. Action = [[ 0.6795294  -0.7911626  -0.75713545  0.8504033 ]]. Reward = [10.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Scene graph at timestep 611 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 611 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 611 is -1
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.23836437 -0.2534862   0.29463926  1.        ]]. Action = [[-0.9630449   0.14258718 -0.18540299  0.67976546]]. Reward = [10.]
Curr episode timestep = 121
Action ignored: Workspace boundary
Scene graph at timestep 612 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 612 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 612 is -1
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.2176537  -0.25029272  0.28601143  1.        ]]. Action = [[ 0.7187748  -0.04128695 -0.39857835  0.8984723 ]]. Reward = [10.]
Curr episode timestep = 122
Scene graph at timestep 613 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 613 is tensor(1.2547e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 613 is 1
Human Feedback received at timestep 613 of 1
Current timestep = 614. State = [[-0.17864165 -0.25926903  0.2655157   1.        ]]. Action = [[-0.33249933 -0.9376811   0.9439473   0.92898226]]. Reward = [10.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Scene graph at timestep 614 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 614 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 614 is -1
Human Feedback received at timestep 614 of -1
Current timestep = 615. State = [[-0.17220797 -0.25970796  0.2613115   1.        ]]. Action = [[-0.9708837  -0.16727132 -0.31982577  0.86094546]]. Reward = [10.]
Curr episode timestep = 124
Action ignored: Workspace boundary
Scene graph at timestep 615 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 615 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 615 is -1
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.17120396 -0.26029652  0.26165402  1.        ]]. Action = [[-0.89752424  0.7207619  -0.4741552   0.9656416 ]]. Reward = [10.]
Curr episode timestep = 125
Action ignored: Workspace boundary
Scene graph at timestep 616 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 616 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 616 is -1
Human Feedback received at timestep 616 of -1
Current timestep = 617. State = [[-0.25077382  0.00248937  0.23263551  1.        ]]. Action = [[-0.4308951   0.17666161  0.9387574   0.8379152 ]]. Reward = [10.]
Curr episode timestep = 126
Scene graph at timestep 617 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 617 is tensor(0.0208, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 617 is -1
Human Feedback received at timestep 617 of -1
Current timestep = 618. State = [[-0.2506751   0.00226562  0.21928124  1.        ]]. Action = [[-0.9423019  -0.95911485  0.06009674  0.8379402 ]]. Reward = [10.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 618 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 618 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 618 is 1
Human Feedback received at timestep 618 of 1
Current timestep = 619. State = [[-0.2503322   0.00226202  0.21287581  1.        ]]. Action = [[-0.27943897 -0.8612703   0.8102455   0.93501234]]. Reward = [10.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 619 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 619 is tensor(0.0136, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 619 is 1
Human Feedback received at timestep 619 of 1
Current timestep = 620. State = [[-0.25017062  0.00227577  0.21295618  1.        ]]. Action = [[-0.87359685 -0.5943494   0.6068704   0.92161655]]. Reward = [10.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 620 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 620 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 620 is 1
Human Feedback received at timestep 620 of 1
Current timestep = 621. State = [[-0.25017062  0.00227577  0.21295618  1.        ]]. Action = [[-0.9182577   0.91378534  0.76724195  0.9252243 ]]. Reward = [10.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 621 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 621 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 621 is -1
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.23173183 -0.02596401  0.21244496  1.        ]]. Action = [[ 0.6032399  -0.9684184  -0.07026935  0.94228077]]. Reward = [10.]
Curr episode timestep = 4
Scene graph at timestep 622 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 622 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 622 is 1
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.20363747 -0.10043281  0.2261922   1.        ]]. Action = [[-0.07689875 -0.5350003   0.8392832   0.98377573]]. Reward = [10.]
Curr episode timestep = 5
Scene graph at timestep 623 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 623 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 623 is 1
Human Feedback received at timestep 623 of 1
Current timestep = 624. State = [[-0.20478638 -0.13656218  0.26795617  1.        ]]. Action = [[-0.68835515 -0.688202   -0.04773456  0.85554194]]. Reward = [10.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 624 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 624 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 624 is 1
Human Feedback received at timestep 624 of 1
Current timestep = 625. State = [[-0.209642   -0.14052911  0.28957516  1.        ]]. Action = [[-0.3551119   0.10558164  0.54728496  0.8840201 ]]. Reward = [10.]
Curr episode timestep = 7
Scene graph at timestep 625 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 625 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 625 is 1
Human Feedback received at timestep 625 of 1
Current timestep = 626. State = [[-0.23758803 -0.13733709  0.30240825  1.        ]]. Action = [[-0.25798875  0.08972812 -0.95184183  0.54818904]]. Reward = [10.]
Curr episode timestep = 8
Scene graph at timestep 626 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 626 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 626 is -1
Human Feedback received at timestep 626 of -1
Current timestep = 627. State = [[-0.25046724 -0.15034749  0.2751568   1.        ]]. Action = [[ 0.04835021 -0.43881702  0.37569296  0.7496433 ]]. Reward = [10.]
Curr episode timestep = 9
Scene graph at timestep 627 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 627 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 627 is -1
Human Feedback received at timestep 627 of -1
Current timestep = 628. State = [[-0.25284213 -0.1675792   0.28234935  1.        ]]. Action = [[-0.554928  -0.6513959  0.8198221  0.8709868]]. Reward = [10.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 628 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 628 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 628 is 1
Human Feedback received at timestep 628 of 1
Current timestep = 629. State = [[-0.22590616 -0.15491465  0.28375283  1.        ]]. Action = [[ 0.9911586   0.39714038 -0.11017156  0.89476264]]. Reward = [10.]
Curr episode timestep = 11
Scene graph at timestep 629 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 629 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 629 is -1
Human Feedback received at timestep 629 of -1
Current timestep = 630. State = [[-0.17759514 -0.13909593  0.27141955  1.        ]]. Action = [[-0.7137418   0.4002614   0.69866276  0.5725038 ]]. Reward = [10.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 630 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 630 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 630 is -1
Human Feedback received at timestep 630 of -1
Current timestep = 631. State = [[-0.15516011 -0.160302    0.2872449   1.        ]]. Action = [[ 0.398777   -0.66811985  0.43962717  0.9837551 ]]. Reward = [10.]
Curr episode timestep = 13
Scene graph at timestep 631 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 631 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 631 is -1
Human Feedback received at timestep 631 of -1
Current timestep = 632. State = [[-0.15710783 -0.20761228  0.32744366  1.        ]]. Action = [[-0.9428615  -0.36162448  0.63776135  0.88443017]]. Reward = [10.]
Curr episode timestep = 14
Scene graph at timestep 632 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 632 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 632 is -1
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.22163957 -0.25420475  0.36272886  1.        ]]. Action = [[-0.8314244  -0.45308316 -0.41841674  0.7629738 ]]. Reward = [10.]
Curr episode timestep = 15
Scene graph at timestep 633 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 633 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 633 is -1
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.25955275 -0.2681395   0.33772588  1.        ]]. Action = [[ 0.6772007   0.17445493 -0.6590992   0.98124206]]. Reward = [10.]
Curr episode timestep = 16
Scene graph at timestep 634 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 634 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 634 is -1
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.23194212 -0.26651964  0.29696172  1.        ]]. Action = [[-0.9370089  -0.52815837 -0.38346863  0.87732947]]. Reward = [10.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 635 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 635 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 635 is 1
Human Feedback received at timestep 635 of 1
Current timestep = 636. State = [[-0.22781628 -0.26796144  0.2883671   1.        ]]. Action = [[-0.6053659   0.2225759   0.07972908  0.7544644 ]]. Reward = [10.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 636 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 636 is tensor(1.6729e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 636 is 1
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[-0.22754292 -0.2681635   0.28823116  1.        ]]. Action = [[-0.48668194  0.87334263  0.9040041   0.5886574 ]]. Reward = [10.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 637 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 637 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 637 is -1
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[-0.22754292 -0.2681635   0.28823116  1.        ]]. Action = [[-0.82735586 -0.3761257  -0.62699825  0.77846134]]. Reward = [10.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 638 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 638 is tensor(7.9233e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 638 is -1
Human Feedback received at timestep 638 of -1
Current timestep = 639. State = [[-0.22758949 -0.26815858  0.28805     1.        ]]. Action = [[ 0.5424019  -0.51235425 -0.8246823  -0.14656287]]. Reward = [10.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 639 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 639 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 639 is -1
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-0.22756104 -0.26816282  0.2880677   1.        ]]. Action = [[-0.5851342 -0.9021118  0.8133732  0.9707029]]. Reward = [10.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 640 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 640 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 640 is -1
Human Feedback received at timestep 640 of -1
Current timestep = 641. State = [[-0.22756104 -0.26816282  0.2880677   1.        ]]. Action = [[-0.92187685  0.01759768  0.17018056  0.9387629 ]]. Reward = [10.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 641 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 641 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 641 is 1
Human Feedback received at timestep 641 of 1
Current timestep = 642. State = [[-0.22756104 -0.26816282  0.2880677   1.        ]]. Action = [[-0.6449556   0.8141489  -0.6355331   0.68470144]]. Reward = [10.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 642 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 642 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 642 is -1
Human Feedback received at timestep 642 of -1
Current timestep = 643. State = [[-0.22756104 -0.26816282  0.2880677   1.        ]]. Action = [[-0.69428474 -0.9172482   0.6247361   0.6233239 ]]. Reward = [10.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 643 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 643 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 643 is -1
Human Feedback received at timestep 643 of -1
Current timestep = 644. State = [[-0.22023831 -0.24787632  0.27249762  1.        ]]. Action = [[ 0.43700635  0.5598266  -0.74859786  0.9885752 ]]. Reward = [10.]
Curr episode timestep = 26
Scene graph at timestep 644 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 644 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 644 is -1
Human Feedback received at timestep 644 of -1
Current timestep = 645. State = [[-0.18844311 -0.22083376  0.22133987  1.        ]]. Action = [[-0.8656731   0.920472    0.76219606  0.7174233 ]]. Reward = [10.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 645 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 645 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 645 is -1
Human Feedback received at timestep 645 of -1
Current timestep = 646. State = [[-0.18552493 -0.2158959   0.2136488   1.        ]]. Action = [[ 0.10590863 -0.9578434  -0.04764032  0.9396924 ]]. Reward = [10.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 646 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 646 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 646 is -1
Human Feedback received at timestep 646 of -1
Current timestep = 647. State = [[-0.1567113  -0.22883545  0.22362114  1.        ]]. Action = [[ 0.95596504 -0.532       0.3654325   0.70047987]]. Reward = [10.]
Curr episode timestep = 29
Scene graph at timestep 647 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 647 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 647 is -1
Human Feedback received at timestep 647 of -1
Current timestep = 648. State = [[-0.11626923 -0.23640473  0.24326785  1.        ]]. Action = [[-0.24699008  0.43653405  0.39427137  0.6067934 ]]. Reward = [10.]
Curr episode timestep = 30
Scene graph at timestep 648 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 648 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 648 is -1
Human Feedback received at timestep 648 of -1
Current timestep = 649. State = [[-0.13350664 -0.20097189  0.24415453  1.        ]]. Action = [[-0.5898002   0.7440605  -0.66159075  0.84586   ]]. Reward = [10.]
Curr episode timestep = 31
Scene graph at timestep 649 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 649 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 649 is -1
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.1345658  -0.18281056  0.22996612  1.        ]]. Action = [[ 0.94786274 -0.73942035  0.11452842  0.9816241 ]]. Reward = [10.]
Curr episode timestep = 32
Scene graph at timestep 650 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 650 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 650 is -1
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.0987464  -0.20732585  0.22154625  1.        ]]. Action = [[-0.14946395  0.39689136 -0.34571016  0.9323547 ]]. Reward = [10.]
Curr episode timestep = 33
Action ignored: No entry zone
Scene graph at timestep 651 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 651 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 651 is -1
Human Feedback received at timestep 651 of -1
Current timestep = 652. State = [[-0.10670061 -0.1998305   0.21561635  1.        ]]. Action = [[-0.6652415   0.40068662 -0.12471008  0.67281747]]. Reward = [10.]
Curr episode timestep = 34
Scene graph at timestep 652 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 652 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 652 is -1
Human Feedback received at timestep 652 of -1
Current timestep = 653. State = [[-0.11883149 -0.20320992  0.19925623  1.        ]]. Action = [[ 0.7190988  -0.68583494 -0.9373249   0.9013308 ]]. Reward = [10.]
Curr episode timestep = 35
Scene graph at timestep 653 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 653 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 653 is -1
Human Feedback received at timestep 653 of -1
Current timestep = 654. State = [[-0.11726147 -0.26604497  0.14183784  1.        ]]. Action = [[-0.72381216 -0.84272146  0.1021198   0.5395504 ]]. Reward = [10.]
Curr episode timestep = 36
Scene graph at timestep 654 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 654 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 654 is 1
Human Feedback received at timestep 654 of 1
Current timestep = 655. State = [[-0.12092663 -0.29648408  0.15559983  1.        ]]. Action = [[0.5414641  0.53745866 0.87135947 0.9209385 ]]. Reward = [10.]
Curr episode timestep = 37
Scene graph at timestep 655 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 655 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 655 is -1
Human Feedback received at timestep 655 of -1
Current timestep = 656. State = [[-0.10190981 -0.2816141   0.19784507  1.        ]]. Action = [[-0.51303506 -0.87645537  0.22289467  0.80064964]]. Reward = [10.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 656 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 656 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 656 is 1
Human Feedback received at timestep 656 of 1
Current timestep = 657. State = [[-0.09842713 -0.27857798  0.20427963  1.        ]]. Action = [[-0.5211507  -0.8200872   0.5462786   0.86710644]]. Reward = [10.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 657 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 657 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 657 is 1
Human Feedback received at timestep 657 of 1
Current timestep = 658. State = [[-0.09704436 -0.25801325  0.20736372  1.        ]]. Action = [[-0.03519934  0.6266682   0.07764113  0.86270726]]. Reward = [10.]
Curr episode timestep = 40
Scene graph at timestep 658 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 658 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 658 is 1
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.11664426 -0.20045228  0.19341429  1.        ]]. Action = [[-0.8227612   0.82290053 -0.75369287  0.83670735]]. Reward = [10.]
Curr episode timestep = 41
Scene graph at timestep 659 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 659 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 659 is -1
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.15874855 -0.15311004  0.16453256  1.        ]]. Action = [[-0.12206864  0.8754678  -0.9028062   0.87702227]]. Reward = [10.]
Curr episode timestep = 42
Action ignored: No entry zone
Scene graph at timestep 660 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 660 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 660 is -1
Human Feedback received at timestep 660 of -1
Current timestep = 661. State = [[-0.15265973 -0.16016823  0.15392318  1.        ]]. Action = [[ 0.54055667 -0.4866321  -0.43274927  0.85948646]]. Reward = [10.]
Curr episode timestep = 43
Scene graph at timestep 661 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 661 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 661 is 1
Human Feedback received at timestep 661 of 1
Current timestep = 662. State = [[-0.15567276 -0.1878569   0.10273942  1.        ]]. Action = [[-0.78181064 -0.01257628 -0.7664254   0.8724941 ]]. Reward = [10.]
Curr episode timestep = 44
Scene graph at timestep 662 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 662 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 662 is -1
Human Feedback received at timestep 662 of -1
Current timestep = 663. State = [[-0.17884867 -0.15954198  0.0607252   1.        ]]. Action = [[-0.20776916  0.08872855  0.02768862  0.70928   ]]. Reward = [10.]
Curr episode timestep = 45
Scene graph at timestep 663 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 663 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 663 is 1
Human Feedback received at timestep 663 of 1
Current timestep = 664. State = [[-0.19142836 -0.1401041   0.07538907  1.        ]]. Action = [[-0.3073339   0.10746288  0.6229191   0.8563733 ]]. Reward = [10.]
Curr episode timestep = 46
Scene graph at timestep 664 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 664 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 664 is 1
Human Feedback received at timestep 664 of 1
Current timestep = 665. State = [[-0.2151692  -0.12817585  0.10567589  1.        ]]. Action = [[-0.95733255 -0.5660729   0.31151128  0.89428294]]. Reward = [10.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 665 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 665 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 665 is 1
Human Feedback received at timestep 665 of 1
Current timestep = 666. State = [[-0.21388489 -0.1153816   0.12700826  1.        ]]. Action = [[0.28580904 0.79329526 0.7084403  0.809916  ]]. Reward = [10.]
Curr episode timestep = 48
Scene graph at timestep 666 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 666 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 666 is -1
Human Feedback received at timestep 666 of -1
Current timestep = 667. State = [[-0.21088971 -0.08245258  0.1592881   1.        ]]. Action = [[0.8617444  0.68677104 0.09797299 0.71265817]]. Reward = [10.]
Curr episode timestep = 49
Action ignored: No entry zone
Scene graph at timestep 667 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 667 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 667 is -1
Human Feedback received at timestep 667 of -1
Current timestep = 668. State = [[-0.21430357 -0.07505546  0.16537812  1.        ]]. Action = [[-0.5706758  -0.96132135  0.8269417   0.9416963 ]]. Reward = [10.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Scene graph at timestep 668 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 668 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 668 is 1
Human Feedback received at timestep 668 of 1
Current timestep = 669. State = [[-0.21701172 -0.09918128  0.17766665  1.        ]]. Action = [[ 0.00476432 -0.97322774  0.12264836  0.6909981 ]]. Reward = [10.]
Curr episode timestep = 51
Scene graph at timestep 669 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 669 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 669 is 1
Human Feedback received at timestep 669 of 1
Current timestep = 670. State = [[-0.2216142  -0.1252598   0.19673418  1.        ]]. Action = [[-0.46306527 -0.45146132 -0.5553641   0.8929596 ]]. Reward = [10.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 670 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 670 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 670 is 1
Human Feedback received at timestep 670 of 1
Current timestep = 671. State = [[-0.22224036 -0.12926115  0.20060635  1.        ]]. Action = [[-0.9270718  -0.01028562  0.5865563   0.94443417]]. Reward = [10.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 671 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 671 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 671 is -1
Human Feedback received at timestep 671 of -1
Current timestep = 672. State = [[-0.22207633 -0.12992461  0.20127511  1.        ]]. Action = [[-0.7159877 -0.9285904 -0.5282687  0.8379042]]. Reward = [10.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 672 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 672 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 672 is 1
Human Feedback received at timestep 672 of 1
Current timestep = 673. State = [[-0.22186348 -0.12981252  0.20138018  1.        ]]. Action = [[-0.64706093 -0.96818674  0.2190212   0.91802   ]]. Reward = [10.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 673 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 673 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 673 is 1
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.22192651 -0.1297238   0.20137182  1.        ]]. Action = [[-0.5018062 -0.8198155  0.7126365  0.9734371]]. Reward = [10.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 674 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 674 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 674 is 1
Human Feedback received at timestep 674 of 1
Current timestep = 675. State = [[-0.22179249 -0.12962936  0.20143665  1.        ]]. Action = [[-0.9872799   0.8704984  -0.02935261  0.9349985 ]]. Reward = [10.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 675 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 675 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 675 is -1
Human Feedback received at timestep 675 of -1
Current timestep = 676. State = [[-0.20839109 -0.14944518  0.22517915  1.        ]]. Action = [[ 0.4144485  -0.06074739  0.93116593  0.5164263 ]]. Reward = [10.]
Curr episode timestep = 58
Scene graph at timestep 676 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 676 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 676 is -1
Human Feedback received at timestep 676 of -1
Current timestep = 677. State = [[-0.19454879 -0.1748351   0.2576881   1.        ]]. Action = [[-0.7894745   0.45162272  0.04591572  0.8917997 ]]. Reward = [10.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 677 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 677 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 677 is -1
Human Feedback received at timestep 677 of -1
Current timestep = 678. State = [[-0.19260079 -0.17950995  0.2627845   1.        ]]. Action = [[-0.63648665  0.8338195   0.6376159   0.8321762 ]]. Reward = [10.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Scene graph at timestep 678 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 678 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 678 is -1
Human Feedback received at timestep 678 of -1
Current timestep = 679. State = [[-0.18539944 -0.15117462  0.28451166  1.        ]]. Action = [[-0.04829162  0.9256995   0.9118118   0.8689585 ]]. Reward = [10.]
Curr episode timestep = 61
Scene graph at timestep 679 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 679 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 679 is -1
Human Feedback received at timestep 679 of -1
Current timestep = 680. State = [[-0.18386182 -0.11681566  0.33778235  1.        ]]. Action = [[ 0.20177531 -0.2265659  -0.04999208  0.9217236 ]]. Reward = [10.]
Curr episode timestep = 62
Scene graph at timestep 680 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 680 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 680 is -1
Human Feedback received at timestep 680 of -1
Current timestep = 681. State = [[-0.15420268 -0.13999522  0.36894944  1.        ]]. Action = [[ 0.9961829  -0.91390604  0.49197793  0.7811706 ]]. Reward = [10.]
Curr episode timestep = 63
Scene graph at timestep 681 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 681 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 681 is -1
Human Feedback received at timestep 681 of -1
Current timestep = 682. State = [[-0.10191496 -0.17925853  0.3664467   1.        ]]. Action = [[ 0.00863993  0.25954914 -0.80429316  0.9110429 ]]. Reward = [10.]
Curr episode timestep = 64
Scene graph at timestep 682 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 682 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 682 is -1
Human Feedback received at timestep 682 of -1
Current timestep = 683. State = [[-0.06150616 -0.17492859  0.34476185  1.        ]]. Action = [[ 0.74033666 -0.03269064  0.43066907  0.6773753 ]]. Reward = [10.]
Curr episode timestep = 65
Scene graph at timestep 683 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 683 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 683 is -1
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[-0.01265279 -0.15304703  0.35014793  1.        ]]. Action = [[ 0.3764391   0.9650278  -0.87383354  0.9611449 ]]. Reward = [10.]
Curr episode timestep = 66
Scene graph at timestep 684 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 684 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 684 is -1
Human Feedback received at timestep 684 of -1
Current timestep = 685. State = [[ 0.0212085  -0.10012145  0.29719356  1.        ]]. Action = [[-0.25015277 -0.934818    0.7594018   0.81696165]]. Reward = [10.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 685 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 685 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 685 is -1
Human Feedback received at timestep 685 of -1
Current timestep = 686. State = [[ 0.03761469 -0.07537849  0.28717256  1.        ]]. Action = [[-0.03164047  0.4386158   0.11756825  0.91285586]]. Reward = [10.]
Curr episode timestep = 68
Scene graph at timestep 686 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 686 is tensor(0.0145, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 686 is -1
Human Feedback received at timestep 686 of -1
Current timestep = 687. State = [[ 0.03118504 -0.07723974  0.3160362   1.        ]]. Action = [[-0.34715235 -0.61255187  0.7866905   0.9391241 ]]. Reward = [10.]
Curr episode timestep = 69
Scene graph at timestep 687 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 687 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 687 is -1
Human Feedback received at timestep 687 of -1
Current timestep = 688. State = [[ 0.00179752 -0.09559741  0.34237394  1.        ]]. Action = [[-0.59577656  0.11303675 -0.57512593  0.8612611 ]]. Reward = [10.]
Curr episode timestep = 70
Scene graph at timestep 688 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 688 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 688 is -1
Human Feedback received at timestep 688 of -1
Current timestep = 689. State = [[-0.04293755 -0.08857924  0.32041627  1.        ]]. Action = [[-0.3783766   0.15548241 -0.05027497  0.7944784 ]]. Reward = [10.]
Curr episode timestep = 71
Scene graph at timestep 689 is [False, True, False, False, True, False, False, True, True, False]
State prediction error at timestep 689 is tensor(0.0161, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 689 is -1
Human Feedback received at timestep 689 of -1
Current timestep = 690. State = [[-0.05542278 -0.10176799  0.34289637  1.        ]]. Action = [[ 0.917598   -0.97617483  0.68667376  0.7282448 ]]. Reward = [10.]
Curr episode timestep = 72
Scene graph at timestep 690 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 690 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 690 is -1
Human Feedback received at timestep 690 of -1
Current timestep = 691. State = [[-0.0337154  -0.17858121  0.3610351   1.        ]]. Action = [[-0.30337775 -0.5716689  -0.21817029  0.6982372 ]]. Reward = [10.]
Curr episode timestep = 73
Scene graph at timestep 691 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 691 is tensor(0.0157, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 691 is -1
Human Feedback received at timestep 691 of -1
Current timestep = 692. State = [[-0.04531174 -0.24114452  0.34880045  1.        ]]. Action = [[ 0.04896569 -0.87423354 -0.32445097  0.9212785 ]]. Reward = [10.]
Curr episode timestep = 74
Scene graph at timestep 692 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 692 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 692 is -1
Human Feedback received at timestep 692 of -1
Current timestep = 693. State = [[-0.03681374 -0.30052316  0.3291462   1.        ]]. Action = [[ 0.6758497  -0.58407444 -0.43497515  0.9715035 ]]. Reward = [10.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 693 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 693 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 693 is -1
Human Feedback received at timestep 693 of -1
Current timestep = 694. State = [[-0.05223    -0.313238    0.33129323  1.        ]]. Action = [[-0.70550233  0.09863174  0.08060765  0.25184608]]. Reward = [10.]
Curr episode timestep = 76
Current timestep = 695. State = [[-0.07939165 -0.31443635  0.33335102  1.        ]]. Action = [[0.1739595  0.44740605 0.8928597  0.2728684 ]]. Reward = [10.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Current timestep = 696. State = [[-0.104804   -0.30764776  0.35651428  1.        ]]. Action = [[-0.9847924   0.81334424  0.61869717  0.92047477]]. Reward = [10.]
Curr episode timestep = 78
Current timestep = 697. State = [[-0.18678246 -0.23561306  0.39523903  1.        ]]. Action = [[-0.7968998   0.44652724 -0.13312465  0.7701746 ]]. Reward = [10.]
Curr episode timestep = 79
Current timestep = 698. State = [[-0.23472475 -0.17081335  0.3816012   1.        ]]. Action = [[ 0.6075592  0.7088723 -0.6102532  0.1714313]]. Reward = [10.]
Curr episode timestep = 80
Current timestep = 699. State = [[-0.21470986 -0.12814322  0.34653148  1.        ]]. Action = [[-0.6527285   0.58737445  0.84288716  0.6660359 ]]. Reward = [10.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Current timestep = 700. State = [[-0.21250808 -0.12174959  0.34062764  1.        ]]. Action = [[-0.23188615  0.8387048   0.71416676  0.33170593]]. Reward = [10.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Scene graph at timestep 700 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 700 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 700 is -1
Human Feedback received at timestep 700 of -1
Current timestep = 701. State = [[-0.21129942 -0.12003516  0.3369433   1.        ]]. Action = [[-0.7244685   0.94137657 -0.20100683  0.05289638]]. Reward = [10.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Scene graph at timestep 701 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 701 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 701 is -1
Human Feedback received at timestep 701 of -1
Current timestep = 702. State = [[-0.20489421 -0.09791847  0.34534198  1.        ]]. Action = [[-0.18173134  0.86166215  0.48398244  0.9288087 ]]. Reward = [10.]
Curr episode timestep = 84
Scene graph at timestep 702 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 702 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 702 is -1
Human Feedback received at timestep 702 of -1
Current timestep = 703. State = [[-0.2032017  -0.04999501  0.35955977  1.        ]]. Action = [[ 0.98351645 -0.4233843  -0.3327086   0.8270842 ]]. Reward = [10.]
Curr episode timestep = 85
Scene graph at timestep 703 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 703 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 703 is -1
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[-0.1571535  -0.06637941  0.34035516  1.        ]]. Action = [[-0.8950107   0.41512656  0.2175281   0.94320846]]. Reward = [10.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Scene graph at timestep 704 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 704 is tensor(0.0113, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 704 is -1
Human Feedback received at timestep 704 of -1
Current timestep = 705. State = [[-0.14857478 -0.08710126  0.32490656  1.        ]]. Action = [[ 0.496261   -0.79203415 -0.7359594   0.9276103 ]]. Reward = [10.]
Curr episode timestep = 87
Scene graph at timestep 705 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 705 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 705 is -1
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[-0.08755139 -0.1453716   0.27786765  1.        ]]. Action = [[ 0.85188556 -0.5382332  -0.00365698  0.9575077 ]]. Reward = [10.]
Curr episode timestep = 88
Current timestep = 707. State = [[-0.0087581  -0.19866674  0.2536453   1.        ]]. Action = [[ 0.8849809  -0.46845222 -0.7633182   0.79333854]]. Reward = [10.]
Curr episode timestep = 89
Scene graph at timestep 707 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 707 is tensor(0.0097, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 707 is -1
Human Feedback received at timestep 707 of -1
Current timestep = 708. State = [[ 0.04350194 -0.26161212  0.18119591  1.        ]]. Action = [[-0.09488362 -0.8955862  -0.6145691   0.6727333 ]]. Reward = [10.]
Curr episode timestep = 90
Scene graph at timestep 708 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 708 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 708 is -1
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[ 0.05208254 -0.32305434  0.1200579   1.        ]]. Action = [[-3.8359582e-01 -4.9757957e-04 -2.8025901e-01  6.7719817e-01]]. Reward = [10.]
Curr episode timestep = 91
Scene graph at timestep 709 is [False, False, True, True, False, False, False, True, True, False]
State prediction error at timestep 709 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 709 is -1
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[ 0.04691542 -0.33339378  0.10128787  1.        ]]. Action = [[-0.9022772 -0.5070851 -0.1833018  0.8548484]]. Reward = [10.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Scene graph at timestep 710 is [False, True, False, True, False, False, False, True, True, False]
State prediction error at timestep 710 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 710 is -1
Human Feedback received at timestep 710 of -1
