Current timestep = 0. State = [[-0.23641266 -0.00292291  0.24885353  1.        ]]. Action = [[ 0.6752353  -0.29340184  0.8605319   0.00104368]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 0 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 0 is tensor(0.3913, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 0 is 1
Human Feedback received at timestep 0 of 1
Current timestep = 1. State = [[-0.20913634 -0.02857254  0.26988336  1.        ]]. Action = [[ 0.8003268  -0.9856809  -0.15738809  0.15516508]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1 is tensor(0.3653, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1 is 1
Human Feedback received at timestep 1 of 1
Current timestep = 2. State = [[-0.25060335  0.00279409  0.23255262  1.        ]]. Action = [[ 0.23388565  0.98544025  0.9644829  -0.25232208]]. Reward = [-1.]
Curr episode timestep = 2
Scene graph at timestep 2 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 2 is tensor(0.3223, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 2 is -1
Human Feedback received at timestep 2 of -1
Current timestep = 3. State = [[-0.25234476  0.00236603  0.23145574  1.        ]]. Action = [[-0.81093615  0.4581313  -0.20424551  0.16478753]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 3 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 3 is tensor(0.3189, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 3 is 1
Human Feedback received at timestep 3 of 1
Current timestep = 4. State = [[-0.25064397 -0.01322449  0.23307554  1.        ]]. Action = [[ 0.15027606 -0.9525731   0.21341836  0.9623885 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 4 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 4 is tensor(0.2891, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 4 is 1
Human Feedback received at timestep 4 of 1
Current timestep = 5. State = [[-0.25172836 -0.03183572  0.2351222   1.        ]]. Action = [[-0.5441782   0.53101945 -0.8730576   0.00842929]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 5 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 5 is tensor(0.2516, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 5 is -1
Human Feedback received at timestep 5 of -1
Current timestep = 6. State = [[-0.25106245  0.00288916  0.23246615  1.        ]]. Action = [[ 0.30583394  0.78826463 -0.23373973 -0.8905513 ]]. Reward = [-1.]
Curr episode timestep = 3
Scene graph at timestep 6 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 6 is tensor(0.2088, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 6 is 1
Human Feedback received at timestep 6 of 1
Current timestep = 7. State = [[-0.24597198  0.00274782  0.21866086  1.        ]]. Action = [[-0.10821617 -0.10164636 -0.8758533   0.23593187]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 7 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 7 is tensor(0.2074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 7 is 1
Human Feedback received at timestep 7 of 1
Current timestep = 8. State = [[-0.24481586  0.00224706  0.19156167  1.        ]]. Action = [[-0.57403344  0.5442302   0.369573    0.5210879 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 8 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 8 is tensor(0.1928, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 8 is -1
Human Feedback received at timestep 8 of -1
Current timestep = 9. State = [[-0.24771808  0.01473476  0.18634847  1.        ]]. Action = [[-0.23351431  0.7536578  -0.17142451  0.08398736]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 9 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 9 is tensor(0.1737, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 9 is 1
Human Feedback received at timestep 9 of 1
Current timestep = 10. State = [[-0.25263843  0.02662833  0.18024062  1.        ]]. Action = [[-0.8602408  -0.6640612  -0.14504182  0.65310097]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 10 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 10 is tensor(0.1412, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 10 is 1
Human Feedback received at timestep 10 of 1
Current timestep = 11. State = [[-0.25275457  0.0276526   0.179992    1.        ]]. Action = [[-0.7102606   0.48506618  0.7074199  -0.05706453]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 11 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 11 is tensor(0.1300, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 11 is 1
Human Feedback received at timestep 11 of 1
Current timestep = 12. State = [[-0.2505762   0.00249997  0.23232074  1.        ]]. Action = [[-0.2749287  -0.8044911   0.6751424  -0.02768135]]. Reward = [-1.]
Curr episode timestep = 5
Scene graph at timestep 12 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 12 is tensor(0.0913, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 12 is -1
Human Feedback received at timestep 12 of -1
Current timestep = 13. State = [[-2.5051430e-01  9.5122855e-04  2.3318197e-01  1.0000000e+00]]. Action = [[-0.7431594   0.85539675  0.09008372 -0.16525424]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 13 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 13 is tensor(0.1022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 13 is 1
Human Feedback received at timestep 13 of 1
Current timestep = 14. State = [[-0.2513264  -0.00688524  0.2466293   1.        ]]. Action = [[-0.23241198 -0.3363363   0.9672197   0.04603207]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 14 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 14 is tensor(0.0703, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 14 is 1
Human Feedback received at timestep 14 of 1
Current timestep = 15. State = [[-0.25575218 -0.00674629  0.26464236  1.        ]]. Action = [[ 0.46806383  0.5340351  -0.9287612   0.0020268 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 15 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 15 is tensor(0.0620, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 15 is 1
Human Feedback received at timestep 15 of 1
Current timestep = 16. State = [[-0.23427144  0.00468554  0.2573348   1.        ]]. Action = [[0.94203985 0.12303686 0.54034257 0.21156394]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 16 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 16 is tensor(0.0566, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 16 is -1
Human Feedback received at timestep 16 of -1
Current timestep = 17. State = [[-0.25128418  0.00288926  0.23246594  1.        ]]. Action = [[ 0.6911156   0.64899147  0.11120284 -0.08782393]]. Reward = [-1.]
Curr episode timestep = 4
Scene graph at timestep 17 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 17 is tensor(0.0616, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 17 is 1
Human Feedback received at timestep 17 of 1
Current timestep = 18. State = [[-0.25147578  0.00195357  0.23253325  1.        ]]. Action = [[-0.6717569  -0.6524429   0.19047642 -0.5519711 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 18 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 18 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 18 is 1
Human Feedback received at timestep 18 of 1
Current timestep = 19. State = [[-0.24911697 -0.00728582  0.2305351   1.        ]]. Action = [[ 0.37727797 -0.48960435 -0.20362705  0.67756414]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 19 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 19 is tensor(0.0373, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 19 is 1
Human Feedback received at timestep 19 of 1
Current timestep = 20. State = [[-0.24731891 -0.0174872   0.21865158  1.        ]]. Action = [[ 0.22221613 -0.08432341 -0.8200183   0.01362944]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 20 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 20 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 20 is 1
Human Feedback received at timestep 20 of 1
Current timestep = 21. State = [[-0.25057134  0.00245526  0.23230155  1.        ]]. Action = [[-0.3753512  -0.10249501  0.8680713  -0.86321616]]. Reward = [-1.]
Curr episode timestep = 3
Scene graph at timestep 21 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 21 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 21 is 1
Human Feedback received at timestep 21 of 1
Current timestep = 22. State = [[-0.25092795  0.00199149  0.23232713  1.        ]]. Action = [[ 0.55896354  0.6991397   0.41272926 -0.40152788]]. Reward = [-1.]
Curr episode timestep = 0
Scene graph at timestep 22 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 22 is tensor(0.0330, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 22 is 1
Human Feedback received at timestep 22 of 1
Current timestep = 23. State = [[-0.25101948  0.00168368  0.23223163  1.        ]]. Action = [[-0.9630952   0.7685509   0.41777706  0.55525005]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 23 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 23 is tensor(0.0188, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 23 is -1
Human Feedback received at timestep 23 of -1
Current timestep = 24. State = [[-0.2510242   0.00162878  0.23223372  1.        ]]. Action = [[-0.8102198  -0.45402157  0.48695934 -0.2498756 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 24 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 24 is tensor(0.0177, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 24 is 1
Human Feedback received at timestep 24 of 1
Current timestep = 25. State = [[-0.25298387  0.00763844  0.23689154  1.        ]]. Action = [[-0.21038479  0.33557558  0.456429    0.5209122 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 25 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 25 is tensor(0.0331, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 25 is 1
Human Feedback received at timestep 25 of 1
Current timestep = 26. State = [[-0.24594201  0.01259395  0.24629922  1.        ]]. Action = [[ 0.755569   -0.1183989   0.10206103  0.56522036]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 26 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 26 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 26 is 1
Human Feedback received at timestep 26 of 1
Current timestep = 27. State = [[-0.23651947  0.01219088  0.25235164  1.        ]]. Action = [[-0.8513325   0.39982486 -0.4800182  -0.29388845]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 27 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 27 is tensor(0.0342, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 27 is 1
Human Feedback received at timestep 27 of 1
Current timestep = 28. State = [[-0.2513826   0.00238127  0.23246616  1.        ]]. Action = [[-0.07435745 -0.6453402  -0.67264754 -0.18632358]]. Reward = [-1.]
Curr episode timestep = 5
Scene graph at timestep 28 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 28 is tensor(0.0349, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 28 is 1
Human Feedback received at timestep 28 of 1
Current timestep = 29. State = [[-0.2418667  -0.01318892  0.2417792   1.        ]]. Action = [[ 0.5228251  -0.79562837  0.7688886   0.9036548 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 29 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 29 is tensor(0.0193, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 29 is 1
Human Feedback received at timestep 29 of 1
Current timestep = 30. State = [[-0.23178078 -0.04944614  0.25667453  1.        ]]. Action = [[-0.01114053 -0.89973915  0.14206266  0.22461355]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 30 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 30 is tensor(0.0405, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 30 is 1
Human Feedback received at timestep 30 of 1
Current timestep = 31. State = [[-0.25055715  0.00285477  0.23282334  1.        ]]. Action = [[ 0.36446226 -0.23007798 -0.752977   -0.6048531 ]]. Reward = [-1.]
Curr episode timestep = 2
Scene graph at timestep 31 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 31 is tensor(0.0363, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 31 is 1
Human Feedback received at timestep 31 of 1
Current timestep = 32. State = [[-0.25058246  0.00279647  0.23274942  1.        ]]. Action = [[-0.8232834   0.45748138 -0.9955326   0.44660103]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 32 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 32 is tensor(0.0405, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 32 is -1
Human Feedback received at timestep 32 of -1
Current timestep = 33. State = [[-0.24949811  0.00103192  0.23281807  1.        ]]. Action = [[ 0.36487508 -0.69274884 -0.661115   -0.6039386 ]]. Reward = [-1.]
Curr episode timestep = 1
Scene graph at timestep 33 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 33 is tensor(0.0368, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 33 is 1
Human Feedback received at timestep 33 of 1
Current timestep = 34. State = [[-0.246454   -0.01912547  0.21118617  1.        ]]. Action = [[-0.7243218   0.27487707  0.67692506  0.36652207]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 34 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 34 is tensor(0.0531, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 34 is 1
Human Feedback received at timestep 34 of 1
Current timestep = 35. State = [[-0.24691975 -0.01700515  0.1911135   1.        ]]. Action = [[-0.14989984  0.54583454 -0.631561    0.9122784 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 35 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 35 is tensor(0.0512, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 35 is 1
Human Feedback received at timestep 35 of 1
Current timestep = 36. State = [[-0.250347    0.00336577  0.23290925  1.        ]]. Action = [[ 0.5161519   0.50163054  0.27625024 -0.24823064]]. Reward = [-1.]
Curr episode timestep = 2
Scene graph at timestep 36 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 36 is tensor(0.0738, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 36 is 1
Human Feedback received at timestep 36 of 1
Current timestep = 37. State = [[-0.25030172  0.00336915  0.23309627  1.        ]]. Action = [[-0.45923102  0.54283977 -0.96380424  0.81618667]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 37 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 37 is tensor(0.0529, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 37 is -1
Human Feedback received at timestep 37 of -1
Current timestep = 38. State = [[-0.25030172  0.00336915  0.23309627  1.        ]]. Action = [[-0.43035215 -0.8017263   0.8575082  -0.99588645]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 38 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 38 is tensor(0.0394, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 38 is 1
Human Feedback received at timestep 38 of 1
Current timestep = 39. State = [[-0.25067475  0.00300803  0.23370452  1.        ]]. Action = [[-0.20503819 -0.27970612  0.12847352 -0.29666793]]. Reward = [-1.]
Curr episode timestep = 2
Scene graph at timestep 39 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 39 is tensor(0.0872, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 39 is 1
Human Feedback received at timestep 39 of 1
Current timestep = 40. State = [[-0.25126174 -0.00628116  0.23767455  1.        ]]. Action = [[-0.34504008  0.85563946  0.10169303  0.9536171 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 40 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 40 is tensor(0.0660, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 40 is -1
Human Feedback received at timestep 40 of -1
Current timestep = 41. State = [[-0.25457165 -0.01283129  0.24080442  1.        ]]. Action = [[-0.23625994 -0.11182427  0.20833862  0.8551661 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 41 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 41 is tensor(0.0873, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 41 is 1
Human Feedback received at timestep 41 of 1
Current timestep = 42. State = [[-0.25586322 -0.01340836  0.23969424  1.        ]]. Action = [[ 0.57142687  0.23625863 -0.5966491   0.20567238]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 42 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 42 is tensor(0.0904, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 42 is 1
Human Feedback received at timestep 42 of 1
Current timestep = 43. State = [[-0.25018972  0.00282825  0.23275785  1.        ]]. Action = [[ 0.6381173  -0.815884   -0.69703346 -0.5350547 ]]. Reward = [-1.]
Curr episode timestep = 3
Scene graph at timestep 43 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 43 is tensor(0.0677, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 43 is 1
Human Feedback received at timestep 43 of 1
Current timestep = 44. State = [[-0.25074837  0.00290122  0.23242696  1.        ]]. Action = [[ 0.8744688   0.8864     -0.57330525 -0.6704846 ]]. Reward = [-1.]
Curr episode timestep = 0
Scene graph at timestep 44 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 44 is tensor(0.0640, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 44 is 1
Human Feedback received at timestep 44 of 1
Current timestep = 45. State = [[-0.25258592 -0.01521835  0.24188061  1.        ]]. Action = [[-0.30799228 -0.9870686   0.66049194  0.7829106 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 45 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 45 is tensor(0.0770, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 45 is 1
Human Feedback received at timestep 45 of 1
Current timestep = 46. State = [[-0.2598697  -0.03999633  0.2592821   1.        ]]. Action = [[-0.95513546 -0.7573902   0.5938823  -0.36356854]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 46 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 46 is tensor(0.0687, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 46 is 1
Human Feedback received at timestep 46 of 1
Current timestep = 47. State = [[-0.2545619  -0.05353825  0.27199027  1.        ]]. Action = [[ 0.5475782 -0.5386212  0.6584146  0.485798 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 47 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 47 is tensor(0.0926, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 47 is 1
Human Feedback received at timestep 47 of 1
Current timestep = 48. State = [[-0.24799275 -0.06658687  0.28833193  1.        ]]. Action = [[-0.73532885  0.32494378  0.08428693  0.6233208 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 48 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 48 is tensor(0.0976, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 48 is 1
Human Feedback received at timestep 48 of 1
Current timestep = 49. State = [[-0.2510707   0.00221666  0.23242846  1.        ]]. Action = [[ 0.8810234   0.6425115  -0.96377677 -0.6102315 ]]. Reward = [-1.]
Curr episode timestep = 4
Scene graph at timestep 49 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 49 is tensor(0.0674, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 49 is 1
Human Feedback received at timestep 49 of 1
Current timestep = 50. State = [[-0.2510631   0.00239228  0.2322369   1.        ]]. Action = [[ 0.13185883 -0.06638402 -0.9302308  -0.23166853]]. Reward = [-1.]
Curr episode timestep = 0
Scene graph at timestep 50 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 50 is tensor(0.0960, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 50 is 1
Human Feedback received at timestep 50 of 1
Current timestep = 51. State = [[-0.23895739 -0.01329485  0.23474364  1.        ]]. Action = [[ 0.8695104  -0.8301816  -0.01341063  0.38844216]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 51 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 51 is tensor(0.0880, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 51 is 1
Human Feedback received at timestep 51 of 1
Current timestep = 52. State = [[-0.25069812  0.00249565  0.23256904  1.        ]]. Action = [[ 0.8266566  -0.23906553  0.5535431  -0.525928  ]]. Reward = [-1.]
Curr episode timestep = 1
Scene graph at timestep 52 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 52 is tensor(0.0897, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 52 is 1
Human Feedback received at timestep 52 of 1
Current timestep = 53. State = [[-0.25075883  0.00251849  0.23255609  1.        ]]. Action = [[-0.3270713 -0.9244497 -0.9939067 -0.8717161]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 53 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 53 is tensor(0.0762, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 53 is -1
Human Feedback received at timestep 53 of -1
Current timestep = 54. State = [[-0.24523467  0.0071706   0.22676751  1.        ]]. Action = [[ 0.5952991   0.3192712  -0.4549058   0.22125268]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 54 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 54 is tensor(0.1069, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 54 is 1
Human Feedback received at timestep 54 of 1
Current timestep = 55. State = [[-0.23419428  0.01118281  0.21732263  1.        ]]. Action = [[-0.83836275 -0.91470945  0.857036   -0.23235017]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 55 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 55 is tensor(0.0670, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 55 is 1
Human Feedback received at timestep 55 of 1
Current timestep = 56. State = [[-0.23563209  0.01600446  0.22301471  1.        ]]. Action = [[-0.3707465   0.14800966  0.58469534  0.1734767 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 56 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 56 is tensor(0.1069, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 56 is 1
Human Feedback received at timestep 56 of 1
Current timestep = 57. State = [[-0.23731868  0.02074159  0.23001523  1.        ]]. Action = [[-0.97205853 -0.98895985 -0.2418164   0.6708932 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 57 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 57 is tensor(0.0851, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 57 is 1
Human Feedback received at timestep 57 of 1
Current timestep = 58. State = [[-0.23128259  0.0326781   0.23401012  1.        ]]. Action = [[0.5745213  0.59137106 0.14790905 0.8316138 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 58 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 58 is tensor(0.0953, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 58 is -1
Human Feedback received at timestep 58 of -1
Current timestep = 59. State = [[-0.25101852  0.00193371  0.23251209  1.        ]]. Action = [[ 0.08684671  0.02621627 -0.74065334 -0.32172465]]. Reward = [-1.]
Curr episode timestep = 6
Scene graph at timestep 59 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 59 is tensor(0.1012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 59 is 1
Human Feedback received at timestep 59 of 1
Current timestep = 60. State = [[-0.24703887  0.00795687  0.23515894  1.        ]]. Action = [[0.3785286  0.33585286 0.17874694 0.9389968 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 60 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 60 is tensor(0.0949, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 60 is -1
Human Feedback received at timestep 60 of -1
Current timestep = 61. State = [[-0.24252738  0.0125259   0.23774797  1.        ]]. Action = [[-0.87705666 -0.83411527  0.593402   -0.5904063 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 61 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 61 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 61 is -1
Human Feedback received at timestep 61 of -1
Current timestep = 62. State = [[-0.23221034  0.01125408  0.23061422  1.        ]]. Action = [[ 0.8815839  -0.23226279 -0.9395352   0.7199397 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 62 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 62 is tensor(0.0880, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 62 is -1
Human Feedback received at timestep 62 of -1
Current timestep = 63. State = [[-0.19635932 -0.00239105  0.21597765  1.        ]]. Action = [[ 0.5271354  -0.6709908   0.57889247  0.5870807 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 63 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 63 is tensor(0.0885, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 63 is 1
Human Feedback received at timestep 63 of 1
Current timestep = 64. State = [[-0.2510324   0.00234544  0.23231904  1.        ]]. Action = [[ 0.69669986 -0.46512902  0.766556   -0.07760769]]. Reward = [-1.]
Curr episode timestep = 4
Scene graph at timestep 64 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 64 is tensor(0.0855, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 64 is 1
Human Feedback received at timestep 64 of 1
Current timestep = 65. State = [[-0.25318637  0.00161012  0.23129001  1.        ]]. Action = [[-0.7166782  -0.7889482   0.15243053  0.40909135]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 65 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 65 is tensor(0.0799, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 65 is 1
Human Feedback received at timestep 65 of 1
Current timestep = 66. State = [[-0.25325453  0.00161006  0.23125902  1.        ]]. Action = [[-0.47286892 -0.8174986  -0.82826513 -0.19986254]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 66 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 66 is tensor(0.0886, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 66 is 1
Human Feedback received at timestep 66 of 1
Current timestep = 67. State = [[-0.24636613 -0.01098153  0.22428088  1.        ]]. Action = [[ 0.76982856 -0.74789274 -0.6392776   0.98458326]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 67 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 67 is tensor(0.0700, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 67 is -1
Human Feedback received at timestep 67 of -1
Current timestep = 68. State = [[-0.23891065 -0.00991764  0.2182519   1.        ]]. Action = [[-0.25248885  0.899688    0.71041787  0.50326467]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 68 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 68 is tensor(0.0747, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 68 is -1
Human Feedback received at timestep 68 of -1
Current timestep = 69. State = [[-0.24068725  0.00372902  0.22753642  1.        ]]. Action = [[-0.73571986 -0.40439463  0.50616145  0.7367127 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 69 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 69 is tensor(0.0722, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 69 is 1
Human Feedback received at timestep 69 of 1
Current timestep = 70. State = [[-0.24104103  0.00414109  0.22791192  1.        ]]. Action = [[-0.608079   -0.6544138  -0.32251704  0.9493151 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 70 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 70 is tensor(0.0767, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 70 is 1
Human Feedback received at timestep 70 of 1
Current timestep = 71. State = [[-0.23881404 -0.00419124  0.2307798   1.        ]]. Action = [[ 0.13147247 -0.5963244   0.17956722  0.3619907 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 71 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 71 is tensor(0.0898, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 71 is 1
Human Feedback received at timestep 71 of 1
Current timestep = 72. State = [[-0.23106255 -0.02027001  0.23963112  1.        ]]. Action = [[ 0.22933722 -0.31589854  0.29564047  0.3922577 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 72 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 72 is tensor(0.0898, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 72 is 1
Human Feedback received at timestep 72 of 1
Current timestep = 73. State = [[-0.25034446  0.00219697  0.23281     1.        ]]. Action = [[ 0.27546287  0.72442865 -0.84152913 -0.09882903]]. Reward = [-1.]
Curr episode timestep = 8
Scene graph at timestep 73 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 73 is tensor(0.0737, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 73 is 1
Human Feedback received at timestep 73 of 1
Current timestep = 74. State = [[-0.23928943  0.0135689   0.23837918  1.        ]]. Action = [[0.85601044 0.6152041  0.31844056 0.9204037 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 74 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 74 is tensor(0.0640, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 74 is 1
Human Feedback received at timestep 74 of 1
Current timestep = 75. State = [[-0.22621936  0.00908451  0.2577033   1.        ]]. Action = [[-0.17706507 -0.84517056  0.86169434  0.8046856 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 75 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 75 is tensor(0.0577, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 75 is -1
Human Feedback received at timestep 75 of -1
Current timestep = 76. State = [[-0.22196002 -0.02299421  0.2708221   1.        ]]. Action = [[ 0.42592812 -0.9777675  -0.59145886  0.8176639 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 76 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 76 is tensor(0.0590, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 76 is 1
Human Feedback received at timestep 76 of 1
Current timestep = 77. State = [[-0.20824537 -0.05950648  0.2732335   1.        ]]. Action = [[ 0.13502407 -0.62027407  0.5097321   0.5250479 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 77 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 77 is tensor(0.0761, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 77 is 1
Human Feedback received at timestep 77 of 1
Current timestep = 78. State = [[-0.19823954 -0.08208407  0.29701865  1.        ]]. Action = [[ 0.20121884 -0.12644333  0.9290582   0.70874095]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 78 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 78 is tensor(0.0636, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 78 is -1
Human Feedback received at timestep 78 of -1
Current timestep = 79. State = [[-0.17777127 -0.08915048  0.33237857  1.        ]]. Action = [[0.57440376 0.02563477 0.6055671  0.47009504]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 79 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 79 is tensor(0.0775, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 79 is -1
Human Feedback received at timestep 79 of -1
Current timestep = 80. State = [[-0.17195903 -0.10503837  0.35059318  1.        ]]. Action = [[-0.7750739  -0.81506765 -0.5328948   0.2952677 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 80 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 80 is tensor(0.0719, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 80 is -1
Human Feedback received at timestep 80 of -1
Current timestep = 81. State = [[-0.19939692 -0.13275695  0.33569914  1.        ]]. Action = [[-0.8780957  -0.15325016 -0.8123047   0.6555579 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 81 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 81 is tensor(0.0709, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 81 is -1
Human Feedback received at timestep 81 of -1
Current timestep = 82. State = [[-0.24991165  0.0029835   0.23279266  1.        ]]. Action = [[-0.8964503  -0.8438999  -0.21144354 -0.30267024]]. Reward = [-1.]
Curr episode timestep = 8
Scene graph at timestep 82 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 82 is tensor(0.0599, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 82 is 1
Human Feedback received at timestep 82 of 1
Current timestep = 83. State = [[-0.25253072 -0.01253433  0.21999611  1.        ]]. Action = [[-0.07986259 -0.85114026 -0.59328216  0.64750266]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 83 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 83 is tensor(0.0573, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 83 is 1
Human Feedback received at timestep 83 of 1
Current timestep = 84. State = [[-0.2548883  -0.0313751   0.20295228  1.        ]]. Action = [[-0.7026324   0.24182189  0.3312621  -0.74968225]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 84 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 84 is tensor(0.0617, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 84 is 1
Human Feedback received at timestep 84 of 1
Current timestep = 85. State = [[-0.2555162  -0.03364329  0.20109762  1.        ]]. Action = [[-0.54235953  0.2980615  -0.7556453   0.70196795]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 85 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 85 is tensor(0.0572, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 85 is 1
Human Feedback received at timestep 85 of 1
Current timestep = 86. State = [[-0.25307593 -0.01952318  0.20120293  1.        ]]. Action = [[0.3342904  0.9115     0.19986069 0.195997  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 86 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 86 is tensor(0.0615, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 86 is 1
Human Feedback received at timestep 86 of 1
Current timestep = 87. State = [[-0.25148576 -0.0048821   0.20119354  1.        ]]. Action = [[-0.631067   -0.87723744  0.2509973  -0.5299836 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 87 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 87 is tensor(0.0488, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 87 is 1
Human Feedback received at timestep 87 of 1
Current timestep = 88. State = [[-0.25157246 -0.00390065  0.20115794  1.        ]]. Action = [[-0.56487364 -0.588042    0.57942927  0.47399735]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 88 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 88 is tensor(0.0481, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 88 is 1
Human Feedback received at timestep 88 of 1
Current timestep = 89. State = [[-0.2515878  -0.00363871  0.20114733  1.        ]]. Action = [[-0.919712    0.7328217   0.3795575   0.45247126]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 89 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 89 is tensor(0.0431, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 89 is -1
Human Feedback received at timestep 89 of -1
Current timestep = 90. State = [[-0.25161907 -0.00354623  0.20114733  1.        ]]. Action = [[-0.7413356   0.79691935  0.41137886  0.8119699 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 90 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 90 is tensor(0.0350, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 90 is -1
Human Feedback received at timestep 90 of -1
Current timestep = 91. State = [[-0.25161907 -0.00354623  0.20114733  1.        ]]. Action = [[-0.60465217 -0.06657034 -0.26317012 -0.44480562]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 91 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 91 is tensor(0.0621, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 91 is 1
Human Feedback received at timestep 91 of 1
Current timestep = 92. State = [[-0.25163478 -0.00349977  0.20114733  1.        ]]. Action = [[-9.4414634e-01 -8.8440228e-01  4.2374778e-01 -5.0592422e-04]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 92 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 92 is tensor(0.0351, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 92 is 1
Human Feedback received at timestep 92 of 1
Current timestep = 93. State = [[-0.25163478 -0.00349977  0.20114733  1.        ]]. Action = [[-0.54025036 -0.7347547  -0.955833    0.866498  ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 93 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 93 is tensor(0.0368, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 93 is 1
Human Feedback received at timestep 93 of 1
Current timestep = 94. State = [[-0.24642101 -0.00340114  0.21017557  1.        ]]. Action = [[ 0.17509747 -0.02588981  0.7117789   0.5842869 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 94 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 94 is tensor(0.0481, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 94 is 1
Human Feedback received at timestep 94 of 1
Current timestep = 95. State = [[-0.23937146 -0.00628387  0.22151533  1.        ]]. Action = [[ 0.22370088 -0.3349359  -0.23310077  0.77313197]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 95 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 95 is tensor(0.0523, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 95 is 1
Human Feedback received at timestep 95 of 1
Current timestep = 96. State = [[-0.23587367 -0.00845531  0.22288011  1.        ]]. Action = [[-0.89937586 -0.7865473  -0.67746973  0.13323355]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 96 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 96 is tensor(0.0419, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 96 is 1
Human Feedback received at timestep 96 of 1
Current timestep = 97. State = [[-0.2350808  -0.00875624  0.22327976  1.        ]]. Action = [[-0.926692   -0.16183382  0.5806047   0.43795037]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 97 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 97 is tensor(0.0406, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 97 is 1
Human Feedback received at timestep 97 of 1
Current timestep = 98. State = [[-0.23468325 -0.00888898  0.22356221  1.        ]]. Action = [[-0.7130297  -0.84181005 -0.4997654   0.09630942]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 98 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 98 is tensor(0.0458, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 98 is 1
Human Feedback received at timestep 98 of 1
Current timestep = 99. State = [[-0.25036874  0.00260788  0.23271006  1.        ]]. Action = [[ 0.84567857 -0.959416   -0.29377377 -0.17361271]]. Reward = [-1.]
Curr episode timestep = 16
Scene graph at timestep 99 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 99 is tensor(0.0388, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 99 is 1
Human Feedback received at timestep 99 of 1
Current timestep = 100. State = [[-0.25026646  0.0024858   0.23321071  1.        ]]. Action = [[-0.7081047  -0.40338355 -0.8814692   0.71857476]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 100 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 100 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 100 is 1
Human Feedback received at timestep 100 of 1
Current timestep = 101. State = [[-0.24575499 -0.00833561  0.235913    1.        ]]. Action = [[ 0.40676987 -0.5998084   0.10515225  0.2603507 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 101 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 101 is tensor(0.0565, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 101 is 1
Human Feedback received at timestep 101 of 1
Current timestep = 102. State = [[-0.23432085 -0.0321911   0.24039477  1.        ]]. Action = [[ 0.49415052 -0.5789523  -0.07057315  0.6459644 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 102 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 102 is tensor(0.0500, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 102 is 1
Human Feedback received at timestep 102 of 1
Current timestep = 103. State = [[-0.2504303   0.00262878  0.23263496  1.        ]]. Action = [[ 0.2664597  -0.19282061 -0.35095578 -0.23904276]]. Reward = [-1.]
Curr episode timestep = 3
Scene graph at timestep 103 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 103 is tensor(0.0598, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 103 is 1
Human Feedback received at timestep 103 of 1
Current timestep = 104. State = [[-0.2505286   0.00250348  0.23265602  1.        ]]. Action = [[-0.8283814   0.744249    0.87863064 -0.11506975]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 104 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 104 is tensor(0.0413, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 104 is -1
Human Feedback received at timestep 104 of -1
Current timestep = 105. State = [[-0.25046575  0.0024485   0.23268923  1.        ]]. Action = [[-0.38570857 -0.39963818 -0.79246086 -0.16386431]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 105 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 105 is tensor(0.0507, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 105 is 1
Human Feedback received at timestep 105 of 1
Current timestep = 106. State = [[-0.25472617  0.01847241  0.23577662  1.        ]]. Action = [[-0.183689    0.9583528   0.27105045  0.8532746 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 106 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 106 is tensor(0.0376, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 106 is -1
Human Feedback received at timestep 106 of -1
Current timestep = 107. State = [[-0.2620918   0.0409852   0.23861079  1.        ]]. Action = [[-0.7089825 -0.967852  -0.6449233  0.8547398]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 107 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 107 is tensor(0.0312, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 107 is 1
Human Feedback received at timestep 107 of 1
Current timestep = 108. State = [[-0.26310042  0.04288562  0.23856391  1.        ]]. Action = [[-0.54889536  0.88101697 -0.15761232 -0.72953176]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 108 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 108 is tensor(0.0364, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 108 is 1
Human Feedback received at timestep 108 of 1
Current timestep = 109. State = [[-0.2631616   0.04302056  0.23856394  1.        ]]. Action = [[-0.94444746  0.04396307 -0.87463343  0.95325124]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 109 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 109 is tensor(0.0281, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 109 is -1
Human Feedback received at timestep 109 of -1
Current timestep = 110. State = [[-0.26479697  0.05594762  0.2423049   1.        ]]. Action = [[-0.05046135  0.5938585   0.21402001  0.9090631 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 110 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 110 is tensor(0.0417, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 110 is -1
Human Feedback received at timestep 110 of -1
Current timestep = 111. State = [[-0.26862466  0.06946391  0.2459772   1.        ]]. Action = [[-0.14681268 -0.4378959   0.43322754  0.6179764 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 111 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 111 is tensor(0.0513, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 111 is -1
Human Feedback received at timestep 111 of -1
Current timestep = 112. State = [[-0.26207122  0.06046823  0.258709    1.        ]]. Action = [[ 0.2846768  -0.6992937   0.7202344   0.47347808]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 112 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 112 is tensor(0.0431, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 112 is -1
Human Feedback received at timestep 112 of -1
Current timestep = 113. State = [[-0.25508818  0.04814268  0.28106013  1.        ]]. Action = [[-0.64425033  0.20050764  0.9108932   0.09797621]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 113 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 113 is tensor(0.0464, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 113 is 1
Human Feedback received at timestep 113 of 1
Current timestep = 114. State = [[-0.25115114  0.00236161  0.23228903  1.        ]]. Action = [[ 0.05681109 -0.40343446 -0.31715977 -0.13463002]]. Reward = [-1.]
Curr episode timestep = 10
Scene graph at timestep 114 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 114 is tensor(0.0490, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 114 is 1
Human Feedback received at timestep 114 of 1
Current timestep = 115. State = [[-0.24933837 -0.01371415  0.23552462  1.        ]]. Action = [[-0.01617527 -0.7319234   0.43600726  0.90165186]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 115 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 115 is tensor(0.0415, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 115 is 1
Human Feedback received at timestep 115 of 1
Current timestep = 116. State = [[-0.24934413 -0.02916362  0.23510045  1.        ]]. Action = [[ 0.22005427  0.1051141  -0.5335147   0.43991995]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 116 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 116 is tensor(0.0491, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 116 is 1
Human Feedback received at timestep 116 of 1
Current timestep = 117. State = [[-0.24792275 -0.02513465  0.22044505  1.        ]]. Action = [[-0.14253926  0.3811717  -0.6205637   0.846019  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 117 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 117 is tensor(0.0385, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 117 is 1
Human Feedback received at timestep 117 of 1
Current timestep = 118. State = [[-0.24643123 -0.02055952  0.20152684  1.        ]]. Action = [[-0.85008734 -0.78409016 -0.61512357 -0.7467871 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 118 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 118 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 118 is -1
Human Feedback received at timestep 118 of -1
Current timestep = 119. State = [[-0.251142   -0.02824163  0.19378522  1.        ]]. Action = [[-0.35041308 -0.60803765 -0.31632495  0.30158162]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 119 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 119 is tensor(0.0499, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 119 is 1
Human Feedback received at timestep 119 of 1
Current timestep = 120. State = [[-0.25590274 -0.03790788  0.18590404  1.        ]]. Action = [[-0.43097192 -0.49352765 -0.7869804   0.9409392 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 120 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 120 is tensor(0.0381, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 120 is -1
Human Feedback received at timestep 120 of -1
Current timestep = 121. State = [[-0.25667676 -0.04074939  0.18608487  1.        ]]. Action = [[-0.7141297   0.16411364  0.7967973   0.7007983 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 121 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 121 is tensor(0.0475, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 121 is 1
Human Feedback received at timestep 121 of 1
Current timestep = 122. State = [[-0.25693712 -0.04141307  0.18610345  1.        ]]. Action = [[-0.34269106 -0.79040045  0.82909954 -0.60581696]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 122 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 122 is tensor(0.0389, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 122 is 1
Human Feedback received at timestep 122 of 1
Current timestep = 123. State = [[-0.25705773 -0.04168682  0.18610705  1.        ]]. Action = [[-0.7185142  -0.93999463 -0.11025828  0.03387916]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 123 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 123 is tensor(0.0402, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 123 is 1
Human Feedback received at timestep 123 of 1
Current timestep = 124. State = [[-0.24860156 -0.04961319  0.190222    1.        ]]. Action = [[ 0.6244855  -0.4438367   0.36197925  0.262141  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 124 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 124 is tensor(0.0521, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 124 is 1
Human Feedback received at timestep 124 of 1
Current timestep = 125. State = [[-0.22670741 -0.05760895  0.19832502  1.        ]]. Action = [[ 0.83327055 -0.05126619  0.07671022  0.889446  ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 125 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 125 is tensor(0.0444, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 125 is 1
Human Feedback received at timestep 125 of 1
Current timestep = 126. State = [[-0.21057396 -0.07798516  0.20915882  1.        ]]. Action = [[-0.29514515 -0.8132401   0.7988688   0.01029253]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 126 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 126 is tensor(0.0413, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 126 is 1
Human Feedback received at timestep 126 of 1
Current timestep = 127. State = [[-0.25014493  0.00253954  0.23271498  1.        ]]. Action = [[ 0.42830086 -0.6728868  -0.9746515  -0.20787781]]. Reward = [-1.]
Curr episode timestep = 12
Scene graph at timestep 127 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 127 is tensor(0.0316, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 127 is -1
Human Feedback received at timestep 127 of -1
Current timestep = 128. State = [[-0.24135397  0.00899795  0.23517518  1.        ]]. Action = [[ 0.7407918   0.4826089  -0.06848347  0.6193669 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 128 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 128 is tensor(0.0386, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 128 is -1
Human Feedback received at timestep 128 of -1
Current timestep = 129. State = [[-0.2321531   0.00799322  0.23742971  1.        ]]. Action = [[-0.27417207 -0.5610701   0.24275553  0.6445894 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 129 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 129 is tensor(0.0401, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 129 is 1
Human Feedback received at timestep 129 of 1
Current timestep = 130. State = [[-0.25089544  0.00197537  0.23224337  1.        ]]. Action = [[-0.17446238  0.861804    0.34845507 -0.5657749 ]]. Reward = [-1.]
Curr episode timestep = 2
Scene graph at timestep 130 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 130 is tensor(0.0363, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 130 is 1
Human Feedback received at timestep 130 of 1
Current timestep = 131. State = [[-0.25097594  0.00181725  0.23220256  1.        ]]. Action = [[-0.6139715   0.1330266   0.17154896  0.01798141]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 131 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 131 is tensor(0.0459, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 131 is 1
Human Feedback received at timestep 131 of 1
Current timestep = 132. State = [[-0.24849688 -0.00705798  0.22770979  1.        ]]. Action = [[ 0.29448414 -0.47316796 -0.30621064  0.9807563 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 132 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 132 is tensor(0.0314, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 132 is 1
Human Feedback received at timestep 132 of 1
Current timestep = 133. State = [[-0.2426891  -0.02291183  0.2115091   1.        ]]. Action = [[ 0.4878683  -0.35948414 -0.8444776   0.64041257]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 133 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 133 is tensor(0.0309, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 133 is 1
Human Feedback received at timestep 133 of 1
Current timestep = 134. State = [[-0.23243238 -0.04661481  0.17916629  1.        ]]. Action = [[-0.22106838 -0.706935   -0.20698595  0.8094586 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 134 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 134 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 134 is 1
Human Feedback received at timestep 134 of 1
Current timestep = 135. State = [[-0.23360223 -0.06478361  0.17054372  1.        ]]. Action = [[-0.90631455  0.36882555  0.61806536  0.8837426 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 135 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 135 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 135 is 1
Human Feedback received at timestep 135 of 1
Current timestep = 136. State = [[-0.23098065 -0.05815287  0.17102998  1.        ]]. Action = [[0.17751992 0.6543007  0.17443156 0.41715217]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 136 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 136 is tensor(0.0435, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 136 is 1
Human Feedback received at timestep 136 of 1
Current timestep = 137. State = [[-0.25007296  0.00264464  0.23283255  1.        ]]. Action = [[ 0.8679751  -0.86030424  0.1657126  -0.24404794]]. Reward = [-1.]
Curr episode timestep = 6
Scene graph at timestep 137 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 137 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 137 is 1
Human Feedback received at timestep 137 of 1
Current timestep = 138. State = [[-0.24997456 -0.01369301  0.22897032  1.        ]]. Action = [[ 0.17962825 -0.8389218  -0.4747094   0.12289858]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 138 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 138 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 138 is 1
Human Feedback received at timestep 138 of 1
Current timestep = 139. State = [[-0.25139213 -0.03173771  0.22302264  1.        ]]. Action = [[-0.8605818  -0.25891125 -0.6827174  -0.6984665 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 139 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 139 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 139 is 1
Human Feedback received at timestep 139 of 1
Current timestep = 140. State = [[-0.25062764 -0.04343351  0.21277647  1.        ]]. Action = [[ 0.10977054 -0.43078762 -0.9640979   0.02591276]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 140 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 140 is tensor(0.0380, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 140 is 1
Human Feedback received at timestep 140 of 1
Current timestep = 141. State = [[-0.25041243  0.00283994  0.23270495  1.        ]]. Action = [[ 0.5184872  -0.4569595  -0.08028477 -0.00469971]]. Reward = [-1.]
Curr episode timestep = 3
Scene graph at timestep 141 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 141 is tensor(0.0463, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 141 is 1
Human Feedback received at timestep 141 of 1
Current timestep = 142. State = [[-0.24945995  0.00175081  0.23286258  1.        ]]. Action = [[ 0.37690675 -0.56485385 -0.5584076  -0.22713876]]. Reward = [-1.]
Curr episode timestep = 0
Scene graph at timestep 142 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 142 is tensor(0.0392, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 142 is 1
Human Feedback received at timestep 142 of 1
Current timestep = 143. State = [[-0.25025678  0.00263733  0.2326628   1.        ]]. Action = [[-0.0664593  -0.6210186  -0.31165946 -0.3166436 ]]. Reward = [-1.]
Curr episode timestep = 0
Scene graph at timestep 143 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 143 is tensor(0.0452, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 143 is 1
Human Feedback received at timestep 143 of 1
Current timestep = 144. State = [[-0.24741785  0.00263962  0.23433626  1.        ]]. Action = [[-0.70788413 -0.24226022 -0.41833723  0.80755675]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 144 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 144 is tensor(0.0378, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 144 is 1
Human Feedback received at timestep 144 of 1
Current timestep = 145. State = [[-0.2372964   0.02014891  0.24378103  1.        ]]. Action = [[0.58996594 0.94289625 0.52669907 0.6319834 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 145 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 145 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 145 is -1
Human Feedback received at timestep 145 of -1
Current timestep = 146. State = [[-0.2336693   0.0487937   0.25457275  1.        ]]. Action = [[-0.5351606   0.35256493 -0.17218381  0.4246974 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 146 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 146 is tensor(0.0496, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 146 is -1
Human Feedback received at timestep 146 of -1
Current timestep = 147. State = [[-0.25110316  0.00230039  0.23227394  1.        ]]. Action = [[ 0.7461666  -0.37910926  0.57714033 -0.3969534 ]]. Reward = [-1.]
Curr episode timestep = 3
Scene graph at timestep 147 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 147 is tensor(0.0385, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 147 is 1
Human Feedback received at timestep 147 of 1
Current timestep = 148. State = [[-0.25110596  0.00211725  0.23218967  1.        ]]. Action = [[-0.36304522  0.74690974  0.92658734  0.5718459 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 148 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 148 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 148 is -1
Human Feedback received at timestep 148 of -1
Current timestep = 149. State = [[-0.25125596 -0.01498411  0.23569329  1.        ]]. Action = [[-0.14057928 -0.8921293   0.45549858  0.8732567 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 149 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 149 is tensor(0.0418, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 149 is 1
Human Feedback received at timestep 149 of 1
Current timestep = 150. State = [[-0.2540182  -0.03758487  0.24117298  1.        ]]. Action = [[-0.6051138  -0.22854376 -0.18541127  0.5166886 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 150 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 150 is tensor(0.0518, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 150 is 1
Human Feedback received at timestep 150 of 1
Current timestep = 151. State = [[-0.25158897 -0.0414045   0.2533483   1.        ]]. Action = [[0.1469295  0.02619958 0.82588124 0.747897  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 151 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 151 is tensor(0.0407, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 151 is 1
Human Feedback received at timestep 151 of 1
Current timestep = 152. State = [[-0.24728425 -0.05468023  0.2748909   1.        ]]. Action = [[ 0.38901424 -0.7170996  -0.15442425  0.5077182 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 152 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 152 is tensor(0.0554, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 152 is -1
Human Feedback received at timestep 152 of -1
Current timestep = 153. State = [[-0.24105729 -0.06978988  0.2811105   1.        ]]. Action = [[-0.42198658  0.06275856 -0.10959661  0.6248847 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 153 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 153 is tensor(0.0570, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 153 is 1
Human Feedback received at timestep 153 of 1
Current timestep = 154. State = [[-0.25007766  0.00310518  0.23272046  1.        ]]. Action = [[-0.35605997  0.43115103 -0.84800255 -0.1446681 ]]. Reward = [-1.]
Curr episode timestep = 6
Scene graph at timestep 154 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 154 is tensor(0.0518, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 154 is -1
Human Feedback received at timestep 154 of -1
Current timestep = 155. State = [[-0.2502679 -0.0150094  0.2269928  1.       ]]. Action = [[ 0.12664473 -0.9365437  -0.23157573  0.21071053]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 155 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 155 is tensor(0.0498, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 155 is 1
Human Feedback received at timestep 155 of 1
Current timestep = 156. State = [[-0.2483148  -0.04446889  0.21076368  1.        ]]. Action = [[ 0.35941696 -0.39773142 -0.7977617   0.33904505]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 156 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 156 is tensor(0.0546, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 156 is 1
Human Feedback received at timestep 156 of 1
Current timestep = 157. State = [[-0.24146795 -0.05547269  0.18318017  1.        ]]. Action = [[-0.89956254 -0.89631015  0.398592    0.6024716 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 157 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 157 is tensor(0.0390, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 157 is 1
Human Feedback received at timestep 157 of 1
Current timestep = 158. State = [[-0.2317907  -0.07786655  0.19187583  1.        ]]. Action = [[ 0.25574625 -0.95330095  0.8575479   0.633026  ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 158 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 158 is tensor(0.0423, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 158 is 1
Human Feedback received at timestep 158 of 1
Current timestep = 159. State = [[-0.22975464 -0.09433088  0.21018912  1.        ]]. Action = [[-0.5792852   0.5471244   0.29360783  0.70763445]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 159 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 159 is tensor(0.0499, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 159 is -1
Human Feedback received at timestep 159 of -1
Current timestep = 160. State = [[-0.24362527 -0.08805396  0.20990118  1.        ]]. Action = [[-0.155478    0.13489771 -0.8772019   0.29590213]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 160 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 160 is tensor(0.0575, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 160 is -1
Human Feedback received at timestep 160 of -1
Current timestep = 161. State = [[-0.24813059 -0.08670444  0.1977131   1.        ]]. Action = [[-0.7807481   0.24297297  0.61124563  0.5975003 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 161 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 161 is tensor(0.0489, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 161 is 1
Human Feedback received at timestep 161 of 1
Current timestep = 162. State = [[-0.24883305 -0.08608685  0.19575106  1.        ]]. Action = [[-0.8277287   0.1712693  -0.62455237  0.86814284]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 162 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 162 is tensor(0.0469, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 162 is -1
Human Feedback received at timestep 162 of -1
Current timestep = 163. State = [[-0.2493552  -0.08550247  0.19536722  1.        ]]. Action = [[-0.5453181  -0.15375352 -0.20731276 -0.6646719 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 163 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 163 is tensor(0.0547, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 163 is -1
Human Feedback received at timestep 163 of -1
Current timestep = 164. State = [[-0.24932511 -0.0851035   0.19523624  1.        ]]. Action = [[-0.7423924  -0.8853569  -0.34436965  0.10708249]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 164 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 164 is tensor(0.0476, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 164 is 1
Human Feedback received at timestep 164 of 1
Current timestep = 165. State = [[-0.24927977 -0.08493576  0.1952098   1.        ]]. Action = [[-0.62160575  0.37774253  0.16141331  0.81602836]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 165 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 165 is tensor(0.0442, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 165 is -1
Human Feedback received at timestep 165 of -1
Current timestep = 166. State = [[-0.25036108  0.00248992  0.23271108  1.        ]]. Action = [[-0.1678071  -0.15302908 -0.6288184  -0.89488095]]. Reward = [-1.]
Curr episode timestep = 11
Scene graph at timestep 166 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 166 is tensor(0.0416, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 166 is -1
Human Feedback received at timestep 166 of -1
Current timestep = 167. State = [[-0.25020757  0.00250676  0.23304771  1.        ]]. Action = [[-0.8059512   0.40056813  0.37785208  0.4655373 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 167 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 167 is tensor(0.0387, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 167 is 1
Human Feedback received at timestep 167 of 1
Current timestep = 168. State = [[-0.25020757  0.00250676  0.23304771  1.        ]]. Action = [[-0.33809853  0.1296562   0.8515456   0.82503855]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 168 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 168 is tensor(0.0280, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 168 is 1
Human Feedback received at timestep 168 of 1
Current timestep = 169. State = [[-0.23831545  0.00477747  0.2316935   1.        ]]. Action = [[ 0.8817998   0.16972864 -0.2944764   0.59889007]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 169 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 169 is tensor(0.0354, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 169 is -1
Human Feedback received at timestep 169 of -1
Current timestep = 170. State = [[-0.2198043   0.00712315  0.22628105  1.        ]]. Action = [[-0.7360683  -0.06755382 -0.7578584   0.464633  ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 170 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 170 is tensor(0.0332, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 170 is 1
Human Feedback received at timestep 170 of 1
Current timestep = 171. State = [[-0.21889283  0.00753898  0.23794903  1.        ]]. Action = [[-0.39918387 -0.04174107  0.8469198   0.8240751 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 171 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 171 is tensor(0.0271, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 171 is -1
Human Feedback received at timestep 171 of -1
Current timestep = 172. State = [[-0.2320709   0.01051874  0.2642503   1.        ]]. Action = [[-0.75255376  0.10458076  0.5908592   0.8563732 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 172 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 172 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 172 is 1
Human Feedback received at timestep 172 of 1
Current timestep = 173. State = [[-0.24891797 -0.00534219  0.30009317  1.        ]]. Action = [[-0.28074074 -0.98612106  0.87395704  0.31628227]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 173 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 173 is tensor(0.0273, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 173 is 1
Human Feedback received at timestep 173 of 1
Current timestep = 174. State = [[-0.2623328  -0.02795025  0.3285841   1.        ]]. Action = [[-0.711984   -0.9139248  -0.34487373  0.8233341 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 174 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 174 is tensor(0.0305, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 174 is 1
Human Feedback received at timestep 174 of 1
Current timestep = 175. State = [[-0.26397172 -0.03181826  0.33074233  1.        ]]. Action = [[-0.81880754 -0.26699072 -0.55998826  0.35006714]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 175 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 175 is tensor(0.0398, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 175 is 1
Human Feedback received at timestep 175 of 1
Current timestep = 176. State = [[-0.26416492 -0.03271699  0.330939    1.        ]]. Action = [[-0.61994     0.82546496 -0.7931074   0.8740294 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 176 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 176 is tensor(0.0256, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 176 is -1
Human Feedback received at timestep 176 of -1
Current timestep = 177. State = [[-0.26432654 -0.03338732  0.33103693  1.        ]]. Action = [[-0.6619433  -0.8390399  -0.44005418  0.8643049 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 177 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 177 is tensor(0.0353, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 177 is 1
Human Feedback received at timestep 177 of 1
Current timestep = 178. State = [[-0.26238453 -0.0424463   0.33378437  1.        ]]. Action = [[ 0.3892095  -0.4894495  -0.00334257  0.82195354]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 178 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 178 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 178 is 1
Human Feedback received at timestep 178 of 1
Current timestep = 179. State = [[-0.2626671  -0.05238973  0.335496    1.        ]]. Action = [[-0.9743027  -0.6847451   0.21408641 -0.01791018]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 179 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 179 is tensor(0.0360, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 179 is 1
Human Feedback received at timestep 179 of 1
Current timestep = 180. State = [[-0.25230968 -0.07177641  0.33375502  1.        ]]. Action = [[ 0.7615144  -0.92110944 -0.34006578  0.25708556]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 180 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 180 is tensor(0.0391, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 180 is 1
Human Feedback received at timestep 180 of 1
Current timestep = 181. State = [[-0.23065197 -0.09420489  0.32602474  1.        ]]. Action = [[-0.76419705  0.15308034 -0.37460333 -0.00854629]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 181 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 181 is tensor(0.0485, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 181 is -1
Human Feedback received at timestep 181 of -1
Current timestep = 182. State = [[-0.21926394 -0.11724256  0.32302138  1.        ]]. Action = [[ 0.57405996 -0.9935432  -0.23524904  0.7483959 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 182 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 182 is tensor(0.0400, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 182 is -1
Human Feedback received at timestep 182 of -1
Current timestep = 183. State = [[-0.2507682   0.00278673  0.23262411  1.        ]]. Action = [[ 0.7601619  -0.4427166  -0.93456155 -0.05834562]]. Reward = [-1.]
Curr episode timestep = 16
Scene graph at timestep 183 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 183 is tensor(0.0357, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 183 is -1
Human Feedback received at timestep 183 of -1
Current timestep = 184. State = [[-2.4662361e-01 -6.8474544e-04  2.3000227e-01  1.0000000e+00]]. Action = [[ 0.5791278  -0.24080831 -0.20551646  0.66513944]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 184 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 184 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 184 is 1
Human Feedback received at timestep 184 of 1
Current timestep = 185. State = [[-0.23948541 -0.00411273  0.22611947  1.        ]]. Action = [[-0.8264708  -0.15499991  0.35587025  0.66188014]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 185 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 185 is tensor(0.0327, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 185 is 1
Human Feedback received at timestep 185 of 1
Current timestep = 186. State = [[-0.23749629 -0.02367319  0.2193647   1.        ]]. Action = [[ 0.12828481 -0.8876304  -0.42941236  0.96103215]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 186 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 186 is tensor(0.0298, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 186 is 1
Human Feedback received at timestep 186 of 1
Current timestep = 187. State = [[-0.21714646 -0.05508719  0.21571428  1.        ]]. Action = [[ 0.91799307 -0.5978321   0.43536532  0.31320238]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 187 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 187 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 187 is 1
Human Feedback received at timestep 187 of 1
Current timestep = 188. State = [[-0.20190819 -0.09098629  0.23465797  1.        ]]. Action = [[-0.6052097  -0.9047793   0.9606292   0.77109694]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 188 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 188 is tensor(0.0255, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 188 is 1
Human Feedback received at timestep 188 of 1
Current timestep = 189. State = [[-0.21533419 -0.12465612  0.2708632   1.        ]]. Action = [[-0.36833936 -0.23177266  0.8905854   0.9164275 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 189 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 189 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 189 is -1
Human Feedback received at timestep 189 of -1
Current timestep = 190. State = [[-0.2324908  -0.15220714  0.3010743   1.        ]]. Action = [[-0.31914943 -0.81866854 -0.04179561  0.14097238]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 190 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 190 is tensor(0.0476, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 190 is 1
Human Feedback received at timestep 190 of 1
Current timestep = 191. State = [[-0.24970417  0.00281926  0.2329393   1.        ]]. Action = [[ 0.5669999  -0.74391854 -0.44816172 -0.5157445 ]]. Reward = [-1.]
Curr episode timestep = 7
Scene graph at timestep 191 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 191 is tensor(0.0329, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 191 is -1
Human Feedback received at timestep 191 of -1
Current timestep = 192. State = [[-0.25014678 -0.01207689  0.22456336  1.        ]]. Action = [[ 0.05138898 -0.8359269  -0.36457562  0.8982849 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 192 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 192 is tensor(0.0276, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 192 is 1
Human Feedback received at timestep 192 of 1
Current timestep = 193. State = [[-0.25492164 -0.03853848  0.20479067  1.        ]]. Action = [[-0.29199755 -0.43962622 -0.5662103   0.6424947 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 193 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 193 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 193 is 1
Human Feedback received at timestep 193 of 1
Current timestep = 194. State = [[-0.25728577 -0.0572008   0.19422667  1.        ]]. Action = [[ 0.2066865  -0.25762993  0.48359585  0.90728116]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 194 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 194 is tensor(0.0300, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 194 is 1
Human Feedback received at timestep 194 of 1
Current timestep = 195. State = [[-0.25837207 -0.06046213  0.19096212  1.        ]]. Action = [[-0.04191101  0.361542   -0.50259423  0.5108665 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 195 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 195 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 195 is 1
Human Feedback received at timestep 195 of 1
Current timestep = 196. State = [[-0.25554696 -0.04830176  0.17549428  1.        ]]. Action = [[ 0.34831345  0.40446794 -0.54678726  0.4213879 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 196 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 196 is tensor(0.0344, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 196 is 1
Human Feedback received at timestep 196 of 1
Current timestep = 197. State = [[-0.25214174 -0.04009529  0.1573215   1.        ]]. Action = [[-0.82056516 -0.1393627  -0.31989753  0.5463364 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 197 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 197 is tensor(0.0349, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 197 is 1
Human Feedback received at timestep 197 of 1
Current timestep = 198. State = [[-0.25176254 -0.03902086  0.15562138  1.        ]]. Action = [[-0.9344474   0.5843245  -0.9695828   0.60329676]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 198 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 198 is tensor(0.0229, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 198 is -1
Human Feedback received at timestep 198 of -1
Current timestep = 199. State = [[-0.25157422 -0.03797694  0.15551046  1.        ]]. Action = [[-0.7764453   0.44773173 -0.70919704  0.88079286]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 199 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 199 is tensor(0.0251, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 199 is -1
Human Feedback received at timestep 199 of -1
Current timestep = 200. State = [[-0.25137195 -0.0366387   0.1553708   1.        ]]. Action = [[-0.92898023 -0.3558395   0.11890459 -0.67653185]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 200 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 200 is tensor(0.0313, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 200 is 1
Human Feedback received at timestep 200 of 1
Current timestep = 201. State = [[-0.25115576 -0.03517993  0.15522157  1.        ]]. Action = [[-0.42652267 -0.7633162   0.7350099   0.9718554 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 201 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 201 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 201 is 1
Human Feedback received at timestep 201 of 1
Current timestep = 202. State = [[-0.25101104 -0.0341864   0.15512173  1.        ]]. Action = [[-0.92679626 -0.53526706 -0.18342805 -0.37521297]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 202 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 202 is tensor(0.0313, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 202 is 1
Human Feedback received at timestep 202 of 1
Current timestep = 203. State = [[-0.250969  -0.0338948  0.1550927  1.       ]]. Action = [[-0.93145305 -0.33045423  0.17480624  0.7078301 ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 203 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 203 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 203 is 1
Human Feedback received at timestep 203 of 1
Current timestep = 204. State = [[-0.244686   -0.0499515   0.15761434  1.        ]]. Action = [[ 0.3300184  -0.94038856  0.12179744  0.8387842 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 204 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 204 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 204 is 1
Human Feedback received at timestep 204 of 1
Current timestep = 205. State = [[-0.23813374 -0.06844806  0.16095293  1.        ]]. Action = [[-0.49506354 -0.39566928  0.36704218  0.8959763 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 205 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 205 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 205 is 1
Human Feedback received at timestep 205 of 1
Current timestep = 206. State = [[-0.23555712 -0.07160171  0.16240874  1.        ]]. Action = [[-0.91781634 -0.9188221  -0.11243397  0.55345714]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 206 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 206 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 206 is 1
Human Feedback received at timestep 206 of 1
Current timestep = 207. State = [[-0.23634315 -0.08982557  0.16325995  1.        ]]. Action = [[-0.08678478 -0.85603106 -0.0031442   0.91312337]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 207 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 207 is tensor(0.0332, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 207 is 1
Human Feedback received at timestep 207 of 1
Current timestep = 208. State = [[-0.24985686  0.00293692  0.23295178  1.        ]]. Action = [[ 0.5683303  -0.76186705  0.00375271 -0.38404912]]. Reward = [-1.]
Curr episode timestep = 16
Scene graph at timestep 208 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 208 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 208 is 1
Human Feedback received at timestep 208 of 1
Current timestep = 209. State = [[-0.24925439  0.00216843  0.23362969  1.        ]]. Action = [[ 0.17773294  0.11998355 -0.0392741   0.17868423]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 209 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 209 is tensor(0.0353, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 209 is 1
Human Feedback received at timestep 209 of 1
Current timestep = 210. State = [[-0.24861674  0.00218246  0.23393144  1.        ]]. Action = [[-0.43959582  0.37534845  0.08842087  0.9187119 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 210 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 210 is tensor(0.0266, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 210 is 1
Human Feedback received at timestep 210 of 1
Current timestep = 211. State = [[-0.2485494   0.00218238  0.23396292  1.        ]]. Action = [[-0.7974578  -0.08663142  0.08285582 -0.6297203 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 211 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 211 is tensor(0.0265, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 211 is 1
Human Feedback received at timestep 211 of 1
Current timestep = 212. State = [[-0.24897479  0.00161969  0.22761692  1.        ]]. Action = [[-0.01726556 -0.13989991 -0.517429    0.97626185]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 212 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 212 is tensor(0.0275, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 212 is 1
Human Feedback received at timestep 212 of 1
Current timestep = 213. State = [[-2.4805558e-01  7.5370440e-04  2.1752726e-01  1.0000000e+00]]. Action = [[-0.42115188  0.45245564  0.09542584 -0.2158634 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 213 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 213 is tensor(0.0333, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 213 is 1
Human Feedback received at timestep 213 of 1
Current timestep = 214. State = [[-0.23237854 -0.01359924  0.22412054  1.        ]]. Action = [[ 0.9232788  -0.6686443   0.6493037   0.67314816]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 214 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 214 is tensor(0.0223, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 214 is 1
Human Feedback received at timestep 214 of 1
Current timestep = 215. State = [[-0.24997222  0.00222702  0.23275079  1.        ]]. Action = [[-0.6288373   0.9353584  -0.52020884 -0.23351574]]. Reward = [-1.]
Curr episode timestep = 6
Scene graph at timestep 215 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 215 is tensor(0.0227, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 215 is -1
Human Feedback received at timestep 215 of -1
Current timestep = 216. State = [[-0.24540791 -0.00358798  0.22509658  1.        ]]. Action = [[ 0.56518793 -0.41069925 -0.56811136  0.84131336]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 216 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 216 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 216 is 1
Human Feedback received at timestep 216 of 1
Current timestep = 217. State = [[-0.250573    0.0022473   0.23268943  1.        ]]. Action = [[ 0.6597519   0.93781424  0.8846302  -0.29254007]]. Reward = [-1.]
Curr episode timestep = 1
Scene graph at timestep 217 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 217 is tensor(0.0186, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 217 is 1
Human Feedback received at timestep 217 of 1
Current timestep = 218. State = [[-0.2501562   0.00224078  0.2343448   1.        ]]. Action = [[-0.70292294  0.22413588  0.27393317  0.6978402 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 218 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 218 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 218 is 1
Human Feedback received at timestep 218 of 1
Current timestep = 219. State = [[-0.2501281   0.00222675  0.23452073  1.        ]]. Action = [[-0.95878977 -0.79287696  0.69579625  0.8833598 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 219 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 219 is tensor(0.0215, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 219 is 1
Human Feedback received at timestep 219 of 1
Current timestep = 220. State = [[-0.24506466 -0.00926859  0.23947693  1.        ]]. Action = [[ 0.4043616  -0.59993523  0.34435022  0.1032933 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 220 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 220 is tensor(0.0321, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 220 is 1
Human Feedback received at timestep 220 of 1
Current timestep = 221. State = [[-0.24196981 -0.02197232  0.24432859  1.        ]]. Action = [[-0.93099093  0.5656421   0.5095829   0.5138519 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 221 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 221 is tensor(0.0298, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 221 is 1
Human Feedback received at timestep 221 of 1
Current timestep = 222. State = [[-0.24058384 -0.02659803  0.24294229  1.        ]]. Action = [[ 0.14693058 -0.14625001 -0.32783037  0.85505414]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 222 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 222 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 222 is 1
Human Feedback received at timestep 222 of 1
Current timestep = 223. State = [[-0.23605993 -0.03787105  0.23749349  1.        ]]. Action = [[ 0.18942165 -0.42063844 -0.26453787  0.88540435]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 223 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 223 is tensor(0.0328, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 223 is 1
Human Feedback received at timestep 223 of 1
Current timestep = 224. State = [[-0.23326021 -0.05970104  0.24317198  1.        ]]. Action = [[-0.5495246  -0.49463958  0.88549495  0.8003819 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 224 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 224 is tensor(0.0313, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 224 is 1
Human Feedback received at timestep 224 of 1
Current timestep = 225. State = [[-0.23984024 -0.07573268  0.2611679   1.        ]]. Action = [[-0.8596008  -0.14686924 -0.8748412   0.83872104]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 225 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 225 is tensor(0.0283, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 225 is -1
Human Feedback received at timestep 225 of -1
Current timestep = 226. State = [[-0.24116759 -0.07890504  0.26265314  1.        ]]. Action = [[-0.9703008  -0.11901879  0.925843    0.83517265]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 226 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 226 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 226 is -1
Human Feedback received at timestep 226 of -1
Current timestep = 227. State = [[-0.24850899 -0.09037849  0.2620431   1.        ]]. Action = [[-0.48033    -0.53997165 -0.20286554  0.95116806]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 227 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 227 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 227 is 1
Human Feedback received at timestep 227 of 1
Current timestep = 228. State = [[-0.25994813 -0.10420398  0.26074386  1.        ]]. Action = [[-0.91004324 -0.45473373 -0.18900388  0.9577594 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 228 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 228 is tensor(0.0334, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 228 is -1
Human Feedback received at timestep 228 of -1
Current timestep = 229. State = [[-0.25423416 -0.09373324  0.2593012   1.        ]]. Action = [[ 0.75112915  0.6515837  -0.14382696  0.49552   ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 229 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 229 is tensor(0.0382, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 229 is -1
Human Feedback received at timestep 229 of -1
Current timestep = 230. State = [[-0.25078475  0.00255457  0.23252201  1.        ]]. Action = [[ 0.05647421  0.05211449  0.15631056 -0.35191453]]. Reward = [-1.]
Curr episode timestep = 12
Scene graph at timestep 230 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 230 is tensor(0.0424, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 230 is 1
Human Feedback received at timestep 230 of 1
Current timestep = 231. State = [[-0.24966575 -0.00274342  0.24203022  1.        ]]. Action = [[-0.09271646 -0.3027107   0.91475844  0.773371  ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 231 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 231 is tensor(0.0245, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 231 is 1
Human Feedback received at timestep 231 of 1
Current timestep = 232. State = [[-0.2516075  -0.00905085  0.2583332   1.        ]]. Action = [[-0.4252041   0.32465744  0.32572508  0.5889585 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 232 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 232 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 232 is 1
Human Feedback received at timestep 232 of 1
Current timestep = 233. State = [[-0.25197768 -0.01087958  0.25888804  1.        ]]. Action = [[-0.92235386  0.8850179  -0.78869784  0.8724034 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 233 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 233 is tensor(0.0144, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 233 is -1
Human Feedback received at timestep 233 of -1
Current timestep = 234. State = [[-0.252143   -0.01168474  0.2591477   1.        ]]. Action = [[-0.69981045 -0.7867764   0.2572298   0.8220031 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 234 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 234 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 234 is 1
Human Feedback received at timestep 234 of 1
Current timestep = 235. State = [[-0.25236732 -0.02229448  0.27258393  1.        ]]. Action = [[-0.01796705 -0.5477928   0.82980967  0.9207308 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 235 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 235 is tensor(0.0222, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 235 is 1
Human Feedback received at timestep 235 of 1
Current timestep = 236. State = [[-0.2597583  -0.03388292  0.2955816   1.        ]]. Action = [[-0.60996616 -0.24543649  0.9159615   0.8608875 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 236 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 236 is tensor(0.0242, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 236 is 1
Human Feedback received at timestep 236 of 1
Current timestep = 237. State = [[-0.25106812 -0.04335493  0.2953875   1.        ]]. Action = [[ 0.5686897  -0.37607104 -0.47190714  0.9510598 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 237 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 237 is tensor(0.0307, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 237 is 1
Human Feedback received at timestep 237 of 1
Current timestep = 238. State = [[-0.23875256 -0.05133927  0.2908034   1.        ]]. Action = [[-0.3938657  -0.78791046  0.1596129  -0.0715223 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 238 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 238 is tensor(0.0306, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 238 is 1
Human Feedback received at timestep 238 of 1
Current timestep = 239. State = [[-0.2378861  -0.05258818  0.28928226  1.        ]]. Action = [[-0.9496126   0.70011234 -0.18801403  0.7013829 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 239 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 239 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 239 is -1
Human Feedback received at timestep 239 of -1
Current timestep = 240. State = [[-0.25074428  0.00267669  0.23255879  1.        ]]. Action = [[ 0.27399695 -0.7784398   0.60425174 -0.17837256]]. Reward = [-1.]
Curr episode timestep = 9
Scene graph at timestep 240 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 240 is tensor(0.0310, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 240 is 1
Human Feedback received at timestep 240 of 1
Current timestep = 241. State = [[-2.4090464e-01  7.8265928e-04  2.3796694e-01  1.0000000e+00]]. Action = [[ 0.6339617  -0.14603853  0.67518294  0.97909725]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 241 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 241 is tensor(0.0223, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 241 is 1
Human Feedback received at timestep 241 of 1
Current timestep = 242. State = [[-0.22928044 -0.01467712  0.2506092   1.        ]]. Action = [[-0.08924609 -0.6079862   0.26911378  0.9737065 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 242 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 242 is tensor(0.0260, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 242 is 1
Human Feedback received at timestep 242 of 1
Current timestep = 243. State = [[-0.23079059 -0.04310501  0.25418648  1.        ]]. Action = [[-0.02440107 -0.7379385  -0.4564286   0.8776357 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 243 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 243 is tensor(0.0295, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 243 is 1
Human Feedback received at timestep 243 of 1
Current timestep = 244. State = [[-0.21865857 -0.05676109  0.2551231   1.        ]]. Action = [[0.948766   0.30499816 0.21729124 0.71769714]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 244 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 244 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 244 is -1
Human Feedback received at timestep 244 of -1
Current timestep = 245. State = [[-0.2007412  -0.04076729  0.25780046  1.        ]]. Action = [[-0.2763266   0.8079282   0.08964217  0.45557618]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 245 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 245 is tensor(0.0344, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 245 is -1
Human Feedback received at timestep 245 of -1
Current timestep = 246. State = [[-0.20395386 -0.03626764  0.24880369  1.        ]]. Action = [[-0.11383486 -0.6902495  -0.80352145  0.60868645]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 246 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 246 is tensor(0.0328, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 246 is 1
Human Feedback received at timestep 246 of 1
Current timestep = 247. State = [[-0.20602196 -0.05706162  0.2398507   1.        ]]. Action = [[-0.14503014 -0.5804519   0.55978894  0.58785987]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 247 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 247 is tensor(0.0358, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 247 is 1
Human Feedback received at timestep 247 of 1
Current timestep = 248. State = [[-0.20120938 -0.07161338  0.25600132  1.        ]]. Action = [[0.34237206 0.07355344 0.6736176  0.69964564]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 248 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 248 is tensor(0.0332, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 248 is -1
Human Feedback received at timestep 248 of -1
Current timestep = 249. State = [[-0.18281142 -0.07133753  0.28912225  1.        ]]. Action = [[0.63502634 0.15800416 0.8528738  0.840541  ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 249 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 249 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 249 is -1
Human Feedback received at timestep 249 of -1
Current timestep = 250. State = [[-0.15107483 -0.08511041  0.31577757  1.        ]]. Action = [[ 0.8895198  -0.8428255  -0.17605233  0.44212246]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 250 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 250 is tensor(0.0364, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 250 is -1
Human Feedback received at timestep 250 of -1
Current timestep = 251. State = [[-0.13497366 -0.11608569  0.3316038   1.        ]]. Action = [[-0.4478606 -0.7508787  0.8636501  0.9435077]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 251 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 251 is tensor(0.0407, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 251 is 1
Human Feedback received at timestep 251 of 1
Current timestep = 252. State = [[-0.13307677 -0.149244    0.36357608  1.        ]]. Action = [[ 0.32871747 -0.51495945  0.46924973  0.95523596]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 252 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 252 is tensor(0.0479, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 252 is -1
Human Feedback received at timestep 252 of -1
Current timestep = 253. State = [[-0.13926825 -0.17782979  0.37164855  1.        ]]. Action = [[-0.85024434 -0.5614507  -0.8768804   0.9778154 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 253 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 253 is tensor(0.0465, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 253 is -1
Human Feedback received at timestep 253 of -1
Current timestep = 254. State = [[-0.15856205 -0.21199323  0.36432076  1.        ]]. Action = [[-0.23661596 -0.8435086   0.0849694   0.8883281 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 254 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 254 is tensor(0.0580, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 254 is -1
Human Feedback received at timestep 254 of -1
Current timestep = 255. State = [[-0.24985409  0.00278842  0.23277038  1.        ]]. Action = [[-0.6964805  -0.6473459  -0.07380748 -0.25390232]]. Reward = [-1.]
Curr episode timestep = 14
Scene graph at timestep 255 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 255 is tensor(0.0541, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 255 is -1
Human Feedback received at timestep 255 of -1
Current timestep = 256. State = [[-0.24800327  0.01287881  0.21961236  1.        ]]. Action = [[ 0.7160872  0.5181943 -0.8829266  0.8203237]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 256 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 256 is tensor(0.0179, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 256 is 1
Human Feedback received at timestep 256 of 1
Current timestep = 257. State = [[-0.23485424  0.023202    0.18793207  1.        ]]. Action = [[-0.5821859   0.89485335  0.2714466   0.92934847]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 257 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 257 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 257 is -1
Human Feedback received at timestep 257 of -1
Current timestep = 258. State = [[-0.23132569  0.01094804  0.18609385  1.        ]]. Action = [[-0.2673254  -0.80944204  0.32657015  0.7117629 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 258 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 258 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 258 is 1
Human Feedback received at timestep 258 of 1
Current timestep = 259. State = [[-0.23227549 -0.00473087  0.18816249  1.        ]]. Action = [[-0.91534126  0.23264575  0.8481914   0.9209943 ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 259 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 259 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 259 is -1
Human Feedback received at timestep 259 of -1
Current timestep = 260. State = [[-0.22430876 -0.01529636  0.19762267  1.        ]]. Action = [[ 0.54933107 -0.39173394  0.5674045   0.7950449 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 260 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 260 is tensor(0.0284, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 260 is 1
Human Feedback received at timestep 260 of 1
Current timestep = 261. State = [[-0.22041136 -0.03653252  0.21472292  1.        ]]. Action = [[-0.11529332 -0.6159649   0.35450244  0.3351226 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 261 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 261 is tensor(0.0368, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 261 is 1
Human Feedback received at timestep 261 of 1
Current timestep = 262. State = [[-0.21272244 -0.0614482   0.23845063  1.        ]]. Action = [[ 0.3873378  -0.4788167   0.58795476  0.476964  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 262 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 262 is tensor(0.0347, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 262 is 1
Human Feedback received at timestep 262 of 1
Current timestep = 263. State = [[-0.20069532 -0.08955678  0.27073243  1.        ]]. Action = [[-0.0163753  -0.66091734  0.92795646  0.8601928 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 263 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 263 is tensor(0.0324, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 263 is 1
Human Feedback received at timestep 263 of 1
Current timestep = 264. State = [[-0.19992025 -0.11988959  0.31283036  1.        ]]. Action = [[-0.05642116 -0.48687017  0.6119249   0.7436359 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 264 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 264 is tensor(0.0397, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 264 is -1
Human Feedback received at timestep 264 of -1
Current timestep = 265. State = [[-0.21004327 -0.15109803  0.34269682  1.        ]]. Action = [[-0.7371537  -0.83026236  0.4278779   0.93758583]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 265 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 265 is tensor(0.0407, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 265 is 1
Human Feedback received at timestep 265 of 1
Current timestep = 266. State = [[-0.23340142 -0.16530375  0.3690513   1.        ]]. Action = [[-0.52202183  0.569453    0.45372963  0.8630508 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 266 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 266 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 266 is -1
Human Feedback received at timestep 266 of -1
Current timestep = 267. State = [[-0.24720597 -0.15798183  0.38604856  1.        ]]. Action = [[-0.7658218   0.56104016  0.81908226  0.853531  ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 267 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 267 is tensor(0.0382, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 267 is -1
Human Feedback received at timestep 267 of -1
Current timestep = 268. State = [[-0.24926025 -0.15716843  0.38626274  1.        ]]. Action = [[-0.84674567  0.20971131  0.64665246  0.42619836]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 268 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 268 is tensor(0.0468, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 268 is -1
Human Feedback received at timestep 268 of -1
Current timestep = 269. State = [[-0.2508406   0.00264851  0.23258612  1.        ]]. Action = [[-0.17216462  0.4499035  -0.6984256  -0.17466217]]. Reward = [-1.]
Curr episode timestep = 13
Scene graph at timestep 269 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 269 is tensor(0.0337, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 269 is -1
Human Feedback received at timestep 269 of -1
Current timestep = 270. State = [[-0.25084946  0.00248027  0.23155712  1.        ]]. Action = [[-0.9413271  -0.71539825  0.04380405  0.21633244]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 270 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 270 is tensor(0.0219, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 270 is 1
Human Feedback received at timestep 270 of 1
Current timestep = 271. State = [[-0.24189742 -0.00823823  0.23858786  1.        ]]. Action = [[ 0.5906582  -0.5836271   0.69816995  0.6995189 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 271 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 271 is tensor(0.0184, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 271 is 1
Human Feedback received at timestep 271 of 1
Current timestep = 272. State = [[-0.23351063 -0.02155473  0.247909    1.        ]]. Action = [[-0.97378117 -0.8480431  -0.03993106  0.07355356]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 272 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 272 is tensor(0.0221, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 272 is 1
Human Feedback received at timestep 272 of 1
Current timestep = 273. State = [[-0.2354388  -0.0428319   0.24710931  1.        ]]. Action = [[-0.35482872 -0.9373103  -0.40145183  0.8762466 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 273 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 273 is tensor(0.0197, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 273 is 1
Human Feedback received at timestep 273 of 1
Current timestep = 274. State = [[-0.24205197 -0.06778882  0.24975644  1.        ]]. Action = [[-0.20490521 -0.04613185  0.49966323  0.48577154]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 274 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 274 is tensor(0.0303, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 274 is -1
Human Feedback received at timestep 274 of -1
Current timestep = 275. State = [[-0.2534623  -0.08510276  0.25362876  1.        ]]. Action = [[-0.43760246 -0.56110054 -0.22242117  0.42712808]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 275 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 275 is tensor(0.0308, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 275 is -1
Human Feedback received at timestep 275 of -1
Current timestep = 276. State = [[-0.26443213 -0.10013872  0.25284407  1.        ]]. Action = [[-0.8929958   0.50347686 -0.75209147  0.46327543]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 276 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 276 is tensor(0.0253, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 276 is -1
Human Feedback received at timestep 276 of -1
Current timestep = 277. State = [[-0.2672065  -0.10221324  0.2522032   1.        ]]. Action = [[-0.6899989  -0.464338   -0.09118074  0.3362738 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 277 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 277 is tensor(0.0333, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 277 is -1
Human Feedback received at timestep 277 of -1
Current timestep = 278. State = [[-0.26776037 -0.10246299  0.25206625  1.        ]]. Action = [[-0.88501275  0.2988919   0.90419257  0.78446054]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 278 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 278 is tensor(0.0281, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 278 is -1
Human Feedback received at timestep 278 of -1
Current timestep = 279. State = [[-0.25816324 -0.10954472  0.259561    1.        ]]. Action = [[ 0.6753192  -0.42544985  0.49214017  0.62029386]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 279 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 279 is tensor(0.0277, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 279 is -1
Human Feedback received at timestep 279 of -1
Current timestep = 280. State = [[-0.24945697 -0.11898436  0.26895937  1.        ]]. Action = [[-0.5862923  -0.55900025 -0.41066813  0.8829042 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 280 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 280 is tensor(0.0278, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 280 is -1
Human Feedback received at timestep 280 of -1
Current timestep = 281. State = [[-0.23174603 -0.12497285  0.2793593   1.        ]]. Action = [[ 0.9405557  -0.29036796  0.33902538  0.9577323 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 281 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 281 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 281 is -1
Human Feedback received at timestep 281 of -1
Current timestep = 282. State = [[-0.19622068 -0.12364183  0.3021016   1.        ]]. Action = [[0.71796536 0.48676956 0.6316029  0.05412757]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 282 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 282 is tensor(0.0311, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 282 is -1
Human Feedback received at timestep 282 of -1
Current timestep = 283. State = [[-0.18392979 -0.12295398  0.31461695  1.        ]]. Action = [[-0.73905265 -0.20565599 -0.5815899   0.3863175 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 283 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 283 is tensor(0.0349, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 283 is 1
Human Feedback received at timestep 283 of 1
Current timestep = 284. State = [[-0.19870341 -0.12334594  0.2961799   1.        ]]. Action = [[-0.43073618  0.21131551 -0.87256056  0.8955767 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 284 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 284 is tensor(0.0299, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 284 is -1
Human Feedback received at timestep 284 of -1
Current timestep = 285. State = [[-0.22331443 -0.13134822  0.26459873  1.        ]]. Action = [[-0.8385014  -0.5197741  -0.38376164  0.8533981 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 285 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 285 is tensor(0.0304, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 285 is 1
Human Feedback received at timestep 285 of 1
Current timestep = 286. State = [[-0.24985568 -0.15411653  0.24797751  1.        ]]. Action = [[ 0.03100228 -0.79672986 -0.17998898  0.794997  ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 286 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 286 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 286 is -1
Human Feedback received at timestep 286 of -1
Current timestep = 287. State = [[-0.2427418  -0.1865622   0.23340331  1.        ]]. Action = [[ 0.75296926 -0.679876   -0.6881448   0.88680124]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 287 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 287 is tensor(0.0345, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 287 is -1
Human Feedback received at timestep 287 of -1
Current timestep = 288. State = [[-0.21648133 -0.21488476  0.1994548   1.        ]]. Action = [[ 0.8451073  -0.4201857  -0.5178035   0.19906187]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 288 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 288 is tensor(0.0433, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 288 is -1
Human Feedback received at timestep 288 of -1
Current timestep = 289. State = [[-0.20353128 -0.24422406  0.17605703  1.        ]]. Action = [[-0.62463737 -0.6792248  -0.28797102  0.81806993]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 289 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 289 is tensor(0.0491, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 289 is -1
Human Feedback received at timestep 289 of -1
Current timestep = 290. State = [[-0.22167356 -0.2670179   0.1683849   1.        ]]. Action = [[-0.8797198   0.0311681   0.13598204  0.16178775]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 290 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 290 is tensor(0.0578, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 290 is -1
Human Feedback received at timestep 290 of -1
Current timestep = 291. State = [[-0.24943833 -0.25917637  0.1789709   1.        ]]. Action = [[-0.7284291   0.9065249   0.9497045   0.28018188]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 291 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 291 is tensor(0.0408, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 291 is -1
Human Feedback received at timestep 291 of -1
Current timestep = 292. State = [[-0.27005598 -0.23399593  0.20114248  1.        ]]. Action = [[ 0.14899051 -0.7441279  -0.7000208   0.70049524]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 292 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 292 is tensor(0.0548, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 292 is -1
Human Feedback received at timestep 292 of -1
Current timestep = 293. State = [[-0.27226612 -0.2318588   0.20036969  1.        ]]. Action = [[-0.69596004 -0.19741607 -0.61074287  0.1157341 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 293 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 293 is tensor(0.0486, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 293 is -1
Human Feedback received at timestep 293 of -1
Current timestep = 294. State = [[-0.27226612 -0.2318588   0.20036969  1.        ]]. Action = [[-0.8942836   0.18827093 -0.970923    0.5324732 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 294 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 294 is tensor(0.0411, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 294 is -1
Human Feedback received at timestep 294 of -1
Current timestep = 295. State = [[-0.25240204  0.002708    0.23165515  1.        ]]. Action = [[ 0.3138566   0.2945001  -0.05589533 -0.7354653 ]]. Reward = [-1.]
Curr episode timestep = 25
Scene graph at timestep 295 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 295 is tensor(0.0371, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 295 is 1
Human Feedback received at timestep 295 of 1
Current timestep = 296. State = [[-0.25828114  0.03835338  0.25332376  1.        ]]. Action = [[-0.7575382 -0.3745901 -0.662659   0.4748305]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 296 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 296 is tensor(0.0169, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 296 is 1
Human Feedback received at timestep 296 of 1
Current timestep = 297. State = [[-0.26062274  0.05158115  0.2600393   1.        ]]. Action = [[-0.589637    0.21575475  0.27046442  0.40647638]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 297 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 297 is tensor(0.0166, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 297 is 1
Human Feedback received at timestep 297 of 1
Current timestep = 298. State = [[-0.25982475  0.05251076  0.2609107   1.        ]]. Action = [[-0.8538545  -0.9426626  -0.04770464 -0.07378298]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 298 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 298 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 298 is -1
Human Feedback received at timestep 298 of -1
Current timestep = 299. State = [[-0.25166243  0.05054661  0.26624689  1.        ]]. Action = [[ 0.44555676 -0.22350019  0.15120757  0.82070446]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 299 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 299 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 299 is -1
Human Feedback received at timestep 299 of -1
Current timestep = 300. State = [[-0.23618668  0.03360223  0.2681339   1.        ]]. Action = [[ 0.50038505 -0.71878165 -0.4923526   0.905437  ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 300 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 300 is tensor(0.0138, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 300 is 1
Human Feedback received at timestep 300 of 1
Current timestep = 301. State = [[-0.22209516  0.01745037  0.26085824  1.        ]]. Action = [[-0.9795404   0.31729698  0.423797    0.8340957 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 301 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 301 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 301 is 1
Human Feedback received at timestep 301 of 1
Current timestep = 302. State = [[-2.2097166e-01  7.7691884e-04  2.6304585e-01  1.0000000e+00]]. Action = [[-0.46862763 -0.62380993  0.32833505  0.7854326 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 302 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 302 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 302 is 1
Human Feedback received at timestep 302 of 1
Current timestep = 303. State = [[-0.21586247 -0.01143262  0.28151548  1.        ]]. Action = [[0.67443156 0.15906966 0.8910637  0.93759036]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 303 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 303 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 303 is 1
Human Feedback received at timestep 303 of 1
Current timestep = 304. State = [[-0.2151031   0.00345018  0.30559987  1.        ]]. Action = [[-0.89071643  0.7475947   0.00870383  0.83668935]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 304 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 304 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 304 is -1
Human Feedback received at timestep 304 of -1
Current timestep = 305. State = [[-0.22540997  0.02603663  0.31879154  1.        ]]. Action = [[0.34651875 0.17792988 0.35492396 0.875412  ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 305 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 305 is tensor(0.0204, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 305 is 1
Human Feedback received at timestep 305 of 1
Current timestep = 306. State = [[-0.22334808  0.03325508  0.3370467   1.        ]]. Action = [[0.02788377 0.05701721 0.76025116 0.6336365 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 306 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 306 is tensor(0.0202, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 306 is -1
Human Feedback received at timestep 306 of -1
Current timestep = 307. State = [[-0.22505882  0.0271845   0.36648288  1.        ]]. Action = [[-0.27886474 -0.47001112  0.19911301  0.7421694 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 307 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 307 is tensor(0.0217, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 307 is -1
Human Feedback received at timestep 307 of -1
Current timestep = 308. State = [[-2.2093010e-01 -6.8721554e-04  3.9247259e-01  1.0000000e+00]]. Action = [[ 0.69623685 -0.89538336  0.7706208   0.554752  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 308 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 308 is tensor(0.0180, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 308 is 1
Human Feedback received at timestep 308 of 1
Current timestep = 309. State = [[-0.21084242 -0.02196978  0.41709098  1.        ]]. Action = [[-0.8569139  -0.8529011   0.09599757  0.8718152 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 309 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 309 is tensor(0.0257, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 309 is 1
Human Feedback received at timestep 309 of 1
Current timestep = 310. State = [[-0.20977838 -0.02646997  0.42190143  1.        ]]. Action = [[-0.39471066  0.6072999   0.7841244   0.5472386 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 310 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 310 is tensor(0.0363, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 310 is -1
Human Feedback received at timestep 310 of -1
Current timestep = 311. State = [[-0.20998335 -0.02764494  0.4220183   1.        ]]. Action = [[ 0.76302814 -0.21157044  0.27106452  0.12815535]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 311 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 311 is tensor(0.0322, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 311 is -1
Human Feedback received at timestep 311 of -1
Current timestep = 312. State = [[-0.21011056 -0.02826637  0.42204005  1.        ]]. Action = [[-0.71098256 -0.75607973  0.97338545  0.9637723 ]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 312 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 312 is tensor(0.0311, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 312 is 1
Human Feedback received at timestep 312 of 1
Current timestep = 313. State = [[-0.20978999 -0.02841842  0.4227092   1.        ]]. Action = [[-0.73950994 -0.3880825   0.8429651   0.36536157]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 313 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 313 is tensor(0.0338, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 313 is -1
Human Feedback received at timestep 313 of -1
Current timestep = 314. State = [[-0.20988333 -0.02861893  0.42264536  1.        ]]. Action = [[-0.62150306 -0.62788886  0.41417933  0.17477953]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 314 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 314 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 314 is -1
Human Feedback received at timestep 314 of -1
Current timestep = 315. State = [[-0.20988333 -0.02861893  0.42264536  1.        ]]. Action = [[-0.03947598 -0.12663096  0.7457149   0.6617949 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 315 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 315 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 315 is -1
Human Feedback received at timestep 315 of -1
Current timestep = 316. State = [[-0.20988333 -0.02861893  0.42264536  1.        ]]. Action = [[-0.98392177 -0.27013588  0.40343893  0.8878114 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 316 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 316 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 316 is 1
Human Feedback received at timestep 316 of 1
Current timestep = 317. State = [[-0.2098946  -0.02867886  0.42265     1.        ]]. Action = [[-0.43991995  0.3130393   0.5861993   0.23218822]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 317 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 317 is tensor(0.0348, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 317 is -1
Human Feedback received at timestep 317 of -1
Current timestep = 318. State = [[-0.21516427 -0.0138447   0.4167388   1.        ]]. Action = [[-0.64867926  0.7781292  -0.5359349   0.90625024]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 318 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 318 is tensor(0.0267, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 318 is -1
Human Feedback received at timestep 318 of -1
Current timestep = 319. State = [[-0.2336716  -0.0125272   0.40144637  1.        ]]. Action = [[-0.525714   -0.8583238  -0.6756641   0.80455136]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 319 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 319 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 319 is 1
Human Feedback received at timestep 319 of 1
Current timestep = 320. State = [[-0.25927496 -0.0115612   0.3728754   1.        ]]. Action = [[-0.57480246  0.89995337 -0.47008753  0.96248055]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 320 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 320 is tensor(0.0210, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 320 is -1
Human Feedback received at timestep 320 of -1
Current timestep = 321. State = [[-0.27896994  0.01425032  0.34556723  1.        ]]. Action = [[-0.09107065  0.46649885 -0.69290435  0.76073253]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 321 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 321 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 321 is 1
Human Feedback received at timestep 321 of 1
Current timestep = 322. State = [[-0.28229207  0.01849466  0.32492548  1.        ]]. Action = [[ 0.2013408  -0.49198353 -0.03369427  0.57124996]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 322 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 322 is tensor(0.0182, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 322 is 1
Human Feedback received at timestep 322 of 1
Current timestep = 323. State = [[-0.26690146  0.01755056  0.33076042  1.        ]]. Action = [[0.8886442  0.23494828 0.7569808  0.57844186]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 323 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 323 is tensor(0.0151, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 323 is -1
Human Feedback received at timestep 323 of -1
Current timestep = 324. State = [[-0.25414106  0.00605394  0.3372182   1.        ]]. Action = [[-0.03331989 -0.7497112  -0.29811788  0.5073632 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 324 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 324 is tensor(0.0161, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 324 is -1
Human Feedback received at timestep 324 of -1
Current timestep = 325. State = [[-0.25198746 -0.00983791  0.33193484  1.        ]]. Action = [[-0.9262762  -0.7294086  -0.6916481   0.87387395]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 325 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 325 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 325 is 1
Human Feedback received at timestep 325 of 1
Current timestep = 326. State = [[-0.25212047 -0.01250315  0.33186397  1.        ]]. Action = [[-0.5344937  -0.70448124 -0.27909994  0.65402365]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Scene graph at timestep 326 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 326 is tensor(0.0175, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 326 is 1
Human Feedback received at timestep 326 of 1
Current timestep = 327. State = [[-0.2520571  -0.01295382  0.3320474   1.        ]]. Action = [[-0.4670968  -0.7508127  -0.72317696  0.9135413 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 327 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 327 is tensor(0.0158, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 327 is 1
Human Feedback received at timestep 327 of 1
Current timestep = 328. State = [[-0.25213984 -0.01313028  0.33203006  1.        ]]. Action = [[-0.97825384 -0.7723635   0.83027244  0.9693463 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Scene graph at timestep 328 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 328 is tensor(0.0165, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 328 is 1
Human Feedback received at timestep 328 of 1
Current timestep = 329. State = [[-0.25213984 -0.01313028  0.33203006  1.        ]]. Action = [[-0.9839623   0.17930746 -0.3952163   0.7007537 ]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 329 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 329 is tensor(0.0161, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 329 is -1
Human Feedback received at timestep 329 of -1
Current timestep = 330. State = [[-0.25214878 -0.01318899  0.3320393   1.        ]]. Action = [[-0.663518  -0.507548   0.2239294  0.7632388]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Scene graph at timestep 330 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 330 is tensor(0.0149, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 330 is 1
Human Feedback received at timestep 330 of 1
Current timestep = 331. State = [[-0.25323915 -0.02480545  0.32161662  1.        ]]. Action = [[-0.13577849 -0.54874945 -0.83177817  0.73678565]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 331 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 331 is tensor(0.0143, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 331 is 1
Human Feedback received at timestep 331 of 1
Current timestep = 332. State = [[-0.23980467 -0.05467794  0.3046962   1.        ]]. Action = [[ 0.79157686 -0.91954076 -0.00526786  0.5770385 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 332 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 332 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 332 is 1
Human Feedback received at timestep 332 of 1
Current timestep = 333. State = [[-0.22035961 -0.07821687  0.28570804  1.        ]]. Action = [[ 0.221295    0.06580257 -0.5647918   0.20795655]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 333 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 333 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 333 is 1
Human Feedback received at timestep 333 of 1
Current timestep = 334. State = [[-0.21698277 -0.07351255  0.26430437  1.        ]]. Action = [[-0.5890382   0.55378175 -0.3079356   0.415282  ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 334 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 334 is tensor(0.0213, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 334 is -1
Human Feedback received at timestep 334 of -1
Current timestep = 335. State = [[-0.23378387 -0.06166054  0.24380153  1.        ]]. Action = [[-0.66851467  0.3606931  -0.9641446   0.0130477 ]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 335 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 335 is tensor(0.0171, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 335 is -1
Human Feedback received at timestep 335 of -1
Current timestep = 336. State = [[-0.2554469  -0.06524076  0.22718285  1.        ]]. Action = [[-0.20502245 -0.8387201   0.841082    0.6360619 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 336 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 336 is tensor(0.0183, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 336 is 1
Human Feedback received at timestep 336 of 1
Current timestep = 337. State = [[-0.25651294 -0.09020697  0.25160834  1.        ]]. Action = [[ 0.40814507 -0.5280017   0.9681971   0.50900316]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 337 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 337 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 337 is 1
Human Feedback received at timestep 337 of 1
Current timestep = 338. State = [[-0.2608614  -0.11670329  0.26832378  1.        ]]. Action = [[-0.1434083  -0.61486864 -0.7027882   0.9048753 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 338 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 338 is tensor(0.0238, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 338 is -1
Human Feedback received at timestep 338 of -1
Current timestep = 339. State = [[-0.26014486 -0.13056788  0.26070592  1.        ]]. Action = [[-0.08898097  0.28015578 -0.15225053  0.48549426]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 339 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 339 is tensor(0.0296, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 339 is -1
Human Feedback received at timestep 339 of -1
Current timestep = 340. State = [[-0.25917178 -0.1354275   0.260432    1.        ]]. Action = [[ 0.33703065 -0.3827458   0.09396374  0.7588979 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 340 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 340 is tensor(0.0318, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 340 is -1
Human Feedback received at timestep 340 of -1
Current timestep = 341. State = [[-0.25497025 -0.14142132  0.260084    1.        ]]. Action = [[-0.22890812 -0.22479635  0.5057095  -0.24699277]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 341 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 341 is tensor(0.0311, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 341 is 1
Human Feedback received at timestep 341 of 1
Current timestep = 342. State = [[-0.24548602 -0.15367523  0.26764745  1.        ]]. Action = [[ 0.48727465 -0.6175619   0.7239907   0.9019902 ]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 342 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 342 is tensor(0.0282, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 342 is 1
Human Feedback received at timestep 342 of 1
Current timestep = 343. State = [[-0.22255318 -0.16988946  0.29635543  1.        ]]. Action = [[ 0.79206204 -0.27645826  0.8982723   0.81648827]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 343 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 343 is tensor(0.0273, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 343 is 1
Human Feedback received at timestep 343 of 1
Current timestep = 344. State = [[-0.20983508 -0.18951164  0.3318771   1.        ]]. Action = [[-0.40053827 -0.482419    0.6273439   0.32383394]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 344 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 344 is tensor(0.0383, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 344 is -1
Human Feedback received at timestep 344 of -1
Current timestep = 345. State = [[-0.22471805 -0.22220163  0.3609103   1.        ]]. Action = [[-0.5510494  -0.96507573  0.45911026  0.6644162 ]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 345 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 345 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 345 is -1
Human Feedback received at timestep 345 of -1
Current timestep = 346. State = [[-0.23918484 -0.25980803  0.38501388  1.        ]]. Action = [[ 0.25775135 -0.7695898   0.26205575  0.5316317 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 346 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 346 is tensor(0.0491, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 346 is -1
Human Feedback received at timestep 346 of -1
Current timestep = 347. State = [[-0.23049743 -0.28659403  0.393403    1.        ]]. Action = [[-0.74057615 -0.46323037 -0.8686087   0.380854  ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 347 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 347 is tensor(0.0432, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 347 is -1
Human Feedback received at timestep 347 of -1
Current timestep = 348. State = [[-0.2289861  -0.28954136  0.39466625  1.        ]]. Action = [[-0.13893288 -0.88209414  0.6433375   0.32994032]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 348 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 348 is tensor(0.0499, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 348 is -1
Human Feedback received at timestep 348 of -1
Current timestep = 349. State = [[-0.22891678 -0.28965676  0.39487264  1.        ]]. Action = [[-0.9452716 -0.6951534 -0.8076245  0.6934483]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 349 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 349 is tensor(0.0403, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 349 is 1
Human Feedback received at timestep 349 of 1
Current timestep = 350. State = [[-0.23463815 -0.2819003   0.38398176  1.        ]]. Action = [[-0.43949902  0.710804   -0.89659286  0.57402325]]. Reward = [0.]
Curr episode timestep = 54
Scene graph at timestep 350 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 350 is tensor(0.0390, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 350 is 1
Human Feedback received at timestep 350 of 1
Current timestep = 351. State = [[-0.24157752 -0.27804017  0.36224765  1.        ]]. Action = [[ 0.072649   -0.3421749  -0.65542066  0.7126758 ]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 351 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 351 is tensor(0.0434, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 351 is -1
Human Feedback received at timestep 351 of -1
Current timestep = 352. State = [[-0.23767506 -0.2805625   0.34612855  1.        ]]. Action = [[-0.49063408  0.1076442   0.22672546  0.5393698 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 352 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 352 is tensor(0.0412, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 352 is -1
Human Feedback received at timestep 352 of -1
Current timestep = 353. State = [[-0.22785635 -0.27916163  0.33296803  1.        ]]. Action = [[ 0.83602405 -0.07299185 -0.626412    0.7458074 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 353 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 353 is tensor(0.0410, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 353 is 1
Human Feedback received at timestep 353 of 1
Current timestep = 354. State = [[-0.21397603 -0.27485028  0.29399323  1.        ]]. Action = [[-0.4499508   0.54695797 -0.83684283  0.97250247]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 354 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 354 is tensor(0.0356, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 354 is -1
Human Feedback received at timestep 354 of -1
Current timestep = 355. State = [[-0.22954923 -0.2566182   0.25829345  1.        ]]. Action = [[-0.74869204  0.6444802  -0.6166631   0.30933857]]. Reward = [0.]
Curr episode timestep = 59
Scene graph at timestep 355 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 355 is tensor(0.0406, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 355 is -1
Human Feedback received at timestep 355 of -1
Current timestep = 356. State = [[-0.24977449 -0.24897397  0.22386107  1.        ]]. Action = [[-0.10371679 -0.38225162 -0.9697886   0.5262971 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 356 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 356 is tensor(0.0424, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 356 is -1
Human Feedback received at timestep 356 of -1
Current timestep = 357. State = [[-0.2531749 -0.2534461  0.2096011  1.       ]]. Action = [[-0.16416323 -0.3703462   0.86739373  0.96714866]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 357 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 357 is tensor(0.0343, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 357 is -1
Human Feedback received at timestep 357 of -1
Current timestep = 358. State = [[-0.24768387 -0.2618606   0.22200078  1.        ]]. Action = [[ 0.6432537  -0.9173866  -0.30522168  0.71747804]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 358 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 358 is tensor(0.0362, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 358 is 1
Human Feedback received at timestep 358 of 1
Current timestep = 359. State = [[-0.22841702 -0.28044853  0.21230881  1.        ]]. Action = [[ 0.80360246 -0.2816519  -0.62800443  0.8143884 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 359 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 359 is tensor(0.0426, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 359 is -1
Human Feedback received at timestep 359 of -1
Current timestep = 360. State = [[-0.20355803 -0.2941476   0.18958601  1.        ]]. Action = [[-0.5552746  -0.5483592   0.03911257 -0.23527789]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Scene graph at timestep 360 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 360 is tensor(0.0469, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 360 is 1
Human Feedback received at timestep 360 of 1
Current timestep = 361. State = [[-0.20577733 -0.29706496  0.1927638   1.        ]]. Action = [[-0.75350225  0.34359455  0.5238974   0.92815375]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 361 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 361 is tensor(0.0429, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 361 is -1
Human Feedback received at timestep 361 of -1
Current timestep = 362. State = [[-0.22587523 -0.29441568  0.19844463  1.        ]]. Action = [[-0.46341765  0.10051894 -0.170771    0.8615676 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 362 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 362 is tensor(0.0430, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 362 is 1
Human Feedback received at timestep 362 of 1
Current timestep = 363. State = [[-0.23268624 -0.28410393  0.19836172  1.        ]]. Action = [[ 0.5932746  -0.9131427   0.57981205  0.9247844 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 363 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 363 is tensor(0.0336, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 363 is -1
Human Feedback received at timestep 363 of -1
Current timestep = 364. State = [[-0.23314984 -0.28034186  0.2004982   1.        ]]. Action = [[-0.06172436 -0.51587456  0.27750182  0.822119  ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 364 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 364 is tensor(0.0400, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 364 is -1
Human Feedback received at timestep 364 of -1
Current timestep = 365. State = [[-0.23354761 -0.28067574  0.20106463  1.        ]]. Action = [[-0.40928507 -0.5057928   0.94235516  0.8679683 ]]. Reward = [0.]
Curr episode timestep = 69
Action ignored: Workspace boundary
Scene graph at timestep 365 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 365 is tensor(0.0347, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 365 is -1
Human Feedback received at timestep 365 of -1
Current timestep = 366. State = [[-0.23354761 -0.28067574  0.20106463  1.        ]]. Action = [[-0.6863592   0.07922304 -0.21408701  0.14570343]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 366 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 366 is tensor(0.0435, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 366 is -1
Human Feedback received at timestep 366 of -1
Current timestep = 367. State = [[-0.23354761 -0.28067574  0.20106463  1.        ]]. Action = [[-0.21670419 -0.5176725  -0.09360695  0.8102777 ]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 367 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 367 is tensor(0.0368, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 367 is -1
Human Feedback received at timestep 367 of -1
Current timestep = 368. State = [[-0.22731502 -0.2834039   0.20699853  1.        ]]. Action = [[0.75084066 0.09792519 0.53357697 0.90977645]]. Reward = [0.]
Curr episode timestep = 72
Scene graph at timestep 368 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 368 is tensor(0.0348, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 368 is 1
Human Feedback received at timestep 368 of 1
Current timestep = 369. State = [[-0.22870758 -0.29385576  0.2203775   1.        ]]. Action = [[-0.4877826  -0.32320786  0.17838085  0.94071543]]. Reward = [0.]
Curr episode timestep = 73
Scene graph at timestep 369 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 369 is tensor(0.0341, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 369 is 1
Human Feedback received at timestep 369 of 1
Current timestep = 370. State = [[-0.2392537  -0.29231665  0.22768788  1.        ]]. Action = [[-0.37068748  0.5059087   0.11396849  0.43301642]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 370 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 370 is tensor(0.0398, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 370 is 1
Human Feedback received at timestep 370 of 1
Current timestep = 371. State = [[-0.24409476 -0.27574533  0.235136    1.        ]]. Action = [[ 0.05050838 -0.8583188  -0.5025048   0.85877466]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 371 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 371 is tensor(0.0305, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 371 is -1
Human Feedback received at timestep 371 of -1
Current timestep = 372. State = [[-0.24939695 -0.2722685   0.2440054   1.        ]]. Action = [[-0.30557728 -0.28138936  0.18555617  0.92630064]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 372 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 372 is tensor(0.0307, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 372 is 1
Human Feedback received at timestep 372 of 1
Current timestep = 373. State = [[-0.26168296 -0.27673343  0.24944858  1.        ]]. Action = [[-0.7886816  -0.86065507  0.75080144  0.35830736]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Scene graph at timestep 373 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 373 is tensor(0.0277, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 373 is -1
Human Feedback received at timestep 373 of -1
Current timestep = 374. State = [[-0.26551467 -0.27777046  0.250788    1.        ]]. Action = [[-1.5920252e-01 -9.1166079e-01  2.5331974e-04  8.8978601e-01]]. Reward = [0.]
Curr episode timestep = 78
Action ignored: Workspace boundary
Scene graph at timestep 374 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 374 is tensor(0.0281, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 374 is -1
Human Feedback received at timestep 374 of -1
Current timestep = 375. State = [[-0.2612787  -0.2846469   0.25916582  1.        ]]. Action = [[0.6222534  0.01841617 0.72391415 0.9473444 ]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 375 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 375 is tensor(0.0252, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 375 is -1
Human Feedback received at timestep 375 of -1
Current timestep = 376. State = [[-0.25738078 -0.29239267  0.2681407   1.        ]]. Action = [[-0.93053705  0.75107455  0.30319357  0.4500481 ]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Scene graph at timestep 376 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 376 is tensor(0.0302, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 376 is 1
Human Feedback received at timestep 376 of 1
Current timestep = 377. State = [[-0.25674403 -0.2933548   0.26937804  1.        ]]. Action = [[ 0.645609  -0.4959572 -0.7232461  0.9666549]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Scene graph at timestep 377 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 377 is tensor(0.0276, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 377 is -1
Human Feedback received at timestep 377 of -1
Current timestep = 378. State = [[-0.2567763  -0.29338324  0.2693535   1.        ]]. Action = [[-0.50352615 -0.7406717  -0.19542766  0.53273857]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Scene graph at timestep 378 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 378 is tensor(0.0304, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 378 is 1
Human Feedback received at timestep 378 of 1
Current timestep = 379. State = [[-0.2567763  -0.29338324  0.2693535   1.        ]]. Action = [[ 0.9267683  -0.56729764  0.88633645  0.57863593]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Scene graph at timestep 379 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 379 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 379 is -1
Human Feedback received at timestep 379 of -1
Current timestep = 380. State = [[-0.2567763  -0.29338324  0.2693535   1.        ]]. Action = [[ 0.89078975 -0.9098129   0.6872871   0.37458873]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Scene graph at timestep 380 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 380 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 380 is -1
Human Feedback received at timestep 380 of -1
Current timestep = 381. State = [[-0.2567763  -0.29338324  0.2693535   1.        ]]. Action = [[ 0.55771124 -0.52044034  0.9271133   0.5831218 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Scene graph at timestep 381 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 381 is tensor(0.0264, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 381 is -1
Human Feedback received at timestep 381 of -1
Current timestep = 382. State = [[-0.2567763  -0.29338324  0.2693535   1.        ]]. Action = [[-0.959055  -0.9449088  0.8318205  0.8442762]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Scene graph at timestep 382 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 382 is tensor(0.0214, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 382 is 1
Human Feedback received at timestep 382 of 1
Current timestep = 383. State = [[-0.2567763  -0.29338324  0.2693535   1.        ]]. Action = [[ 0.5003214  -0.7365431   0.44808948  0.9594203 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Scene graph at timestep 383 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 383 is tensor(0.0253, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 383 is 1
Human Feedback received at timestep 383 of 1
Current timestep = 384. State = [[-0.25678182 -0.29330054  0.2693479   1.        ]]. Action = [[-0.8160079   0.1881671  -0.15574682  0.47249055]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Scene graph at timestep 384 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 384 is tensor(0.0367, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 384 is 1
Human Feedback received at timestep 384 of 1
Current timestep = 385. State = [[-0.25678366 -0.29327288  0.26934606  1.        ]]. Action = [[-0.800748    0.39735532  0.3692298   0.949692  ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Scene graph at timestep 385 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 385 is tensor(0.0324, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 385 is 1
Human Feedback received at timestep 385 of 1
Current timestep = 386. State = [[-0.25678366 -0.29327288  0.26934606  1.        ]]. Action = [[-0.6369218   0.49136174  0.10925508  0.6337569 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Scene graph at timestep 386 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 386 is tensor(0.0366, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 386 is -1
Human Feedback received at timestep 386 of -1
Current timestep = 387. State = [[-0.25678366 -0.29327288  0.26934606  1.        ]]. Action = [[-0.38880312 -0.88797295  0.21574771  0.7691339 ]]. Reward = [0.]
Curr episode timestep = 91
Action ignored: Workspace boundary
Scene graph at timestep 387 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 387 is tensor(0.0317, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 387 is 1
Human Feedback received at timestep 387 of 1
Current timestep = 388. State = [[-0.25681207 -0.2933147   0.26939282  1.        ]]. Action = [[ 0.07201064 -0.9284326   0.8634052   0.72487247]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Scene graph at timestep 388 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 388 is tensor(0.0282, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 388 is 1
Human Feedback received at timestep 388 of 1
Current timestep = 389. State = [[-0.25681207 -0.2933147   0.26939282  1.        ]]. Action = [[-0.6147623   0.5894611   0.30302405 -0.26304722]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Scene graph at timestep 389 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 389 is tensor(0.0361, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 389 is -1
Human Feedback received at timestep 389 of -1
Current timestep = 390. State = [[-0.25681207 -0.2933147   0.26939282  1.        ]]. Action = [[-0.9249279   0.02060771  0.3676083   0.22507143]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Scene graph at timestep 390 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 390 is tensor(0.0326, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 390 is -1
Human Feedback received at timestep 390 of -1
Current timestep = 391. State = [[-0.23969907 -0.2873401   0.27852407  1.        ]]. Action = [[0.7464584  0.3597499  0.63951945 0.6875863 ]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 391 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 391 is tensor(0.0282, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 391 is 1
Human Feedback received at timestep 391 of 1
Current timestep = 392. State = [[-0.21992126 -0.27550408  0.286532    1.        ]]. Action = [[ 0.17511058  0.5256078  -0.587853    0.8664597 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 392 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 392 is tensor(0.0291, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 392 is -1
Human Feedback received at timestep 392 of -1
Current timestep = 393. State = [[-0.22043647 -0.25779366  0.2727492   1.        ]]. Action = [[-0.36764902  0.38795066 -0.6396996   0.60183823]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 393 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 393 is tensor(0.0293, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 393 is -1
Human Feedback received at timestep 393 of -1
Current timestep = 394. State = [[-0.22031726 -0.2517419   0.2645118   1.        ]]. Action = [[-0.91832453 -0.2532667  -0.03967476  0.3473482 ]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Scene graph at timestep 394 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 394 is tensor(0.0231, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 394 is -1
Human Feedback received at timestep 394 of -1
Current timestep = 395. State = [[-0.2229987  -0.24120645  0.2706872   1.        ]]. Action = [[-0.42900294  0.55248904  0.7917676   0.1354953 ]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 395 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 395 is tensor(0.0229, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 395 is -1
Human Feedback received at timestep 395 of -1
Current timestep = 396. State = [[-0.24896416 -0.23659727  0.29060414  1.        ]]. Action = [[-0.50193685 -0.75353676  0.45884395  0.70625246]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 396 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 396 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 396 is -1
Human Feedback received at timestep 396 of -1
Current timestep = 397. State = [[-0.26203662 -0.24784644  0.3039397   1.        ]]. Action = [[-0.48575222 -0.47623956 -0.43852276  0.89956355]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Scene graph at timestep 397 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 397 is tensor(0.0147, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 397 is -1
Human Feedback received at timestep 397 of -1
Current timestep = 398. State = [[-0.25558245 -0.24180672  0.30309242  1.        ]]. Action = [[ 0.64950323  0.4211787  -0.19749624  0.64026284]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 398 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 398 is tensor(0.0199, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 398 is -1
Human Feedback received at timestep 398 of -1
Current timestep = 399. State = [[-0.24746153 -0.23251371  0.29826906  1.        ]]. Action = [[-0.05922955  0.07522428 -0.3829242   0.88775253]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 399 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 399 is tensor(0.0163, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 399 is 1
Human Feedback received at timestep 399 of 1
Current timestep = 400. State = [[-0.24522355 -0.22837204  0.2933063   1.        ]]. Action = [[-0.49108016 -0.60816586 -0.18242621  0.1783905 ]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Scene graph at timestep 400 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 400 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 400 is -1
Human Feedback received at timestep 400 of -1
Current timestep = 401. State = [[-0.24641857 -0.23696555  0.2929432   1.        ]]. Action = [[ 0.00347173 -0.60282606  0.00195289  0.7192482 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 401 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 401 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 401 is -1
Human Feedback received at timestep 401 of -1
Current timestep = 402. State = [[-0.2537335 -0.2642188  0.2990679  1.       ]]. Action = [[-0.3402784  -0.8626393   0.50723016  0.725888  ]]. Reward = [0.]
Curr episode timestep = 106
Scene graph at timestep 402 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 402 is tensor(0.0150, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 402 is -1
Human Feedback received at timestep 402 of -1
Current timestep = 403. State = [[-0.2525644  -0.29145595  0.2914011   1.        ]]. Action = [[ 0.65725636 -0.30132663 -0.9796711   0.83689165]]. Reward = [0.]
Curr episode timestep = 107
Scene graph at timestep 403 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 403 is tensor(0.0158, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 403 is -1
Human Feedback received at timestep 403 of -1
Current timestep = 404. State = [[-0.23792674 -0.305163    0.2706326   1.        ]]. Action = [[-0.8579927  -0.43509227  0.91561234  0.7030699 ]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: Workspace boundary
Scene graph at timestep 404 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 404 is tensor(0.0161, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 404 is -1
Human Feedback received at timestep 404 of -1
Current timestep = 405. State = [[-0.23736148 -0.30557722  0.2697586   1.        ]]. Action = [[-0.9487626  -0.31279802 -0.7396552   0.8634182 ]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: Workspace boundary
Scene graph at timestep 405 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 405 is tensor(0.0227, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 405 is -1
Human Feedback received at timestep 405 of -1
Current timestep = 406. State = [[-0.23736148 -0.30557722  0.2697586   1.        ]]. Action = [[-0.99396116 -0.5068914   0.34528947 -0.44431764]]. Reward = [0.]
Curr episode timestep = 110
Action ignored: Workspace boundary
Scene graph at timestep 406 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 406 is tensor(0.0204, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 406 is -1
Human Feedback received at timestep 406 of -1
Current timestep = 407. State = [[-0.23266453 -0.30574048  0.27936503  1.        ]]. Action = [[-0.02020967  0.13617527  0.90422523  0.80633175]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 407 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 407 is tensor(0.0205, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 407 is -1
Human Feedback received at timestep 407 of -1
Current timestep = 408. State = [[-0.22978237 -0.2963469   0.29052976  1.        ]]. Action = [[-0.25131333  0.68025327  0.20555997  0.7582526 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 408 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 408 is tensor(0.0228, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 408 is 1
Human Feedback received at timestep 408 of 1
Current timestep = 409. State = [[-0.23359023 -0.27910987  0.30539677  1.        ]]. Action = [[ 0.19238317 -0.6540668   0.4426471   0.2422005 ]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Scene graph at timestep 409 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 409 is tensor(0.0259, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 409 is -1
Human Feedback received at timestep 409 of -1
Current timestep = 410. State = [[-0.23364548 -0.27796185  0.30591452  1.        ]]. Action = [[-0.808954    0.11022234  0.43566358  0.7571521 ]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Scene graph at timestep 410 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 410 is tensor(0.0165, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 410 is 1
Human Feedback received at timestep 410 of 1
Current timestep = 411. State = [[-0.23208791 -0.26350608  0.31750563  1.        ]]. Action = [[-0.0637238   0.8813747   0.8643923   0.61604476]]. Reward = [0.]
Curr episode timestep = 115
Scene graph at timestep 411 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 411 is tensor(0.0146, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 411 is -1
Human Feedback received at timestep 411 of -1
Current timestep = 412. State = [[-0.24489571 -0.23501238  0.33858836  1.        ]]. Action = [[-0.35094035  0.13518846 -0.27482426  0.8554225 ]]. Reward = [0.]
Curr episode timestep = 116
Scene graph at timestep 412 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 412 is tensor(0.0171, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 412 is -1
Human Feedback received at timestep 412 of -1
Current timestep = 413. State = [[-0.24248455 -0.22006783  0.34374556  1.        ]]. Action = [[0.52665937 0.3250066  0.3585899  0.72342205]]. Reward = [0.]
Curr episode timestep = 117
Scene graph at timestep 413 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 413 is tensor(0.0184, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 413 is -1
Human Feedback received at timestep 413 of -1
Current timestep = 414. State = [[-0.2338821  -0.2198895   0.34509563  1.        ]]. Action = [[ 0.51462555 -0.71782094 -0.28121936  0.68155026]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 414 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 414 is tensor(0.0142, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 414 is -1
Human Feedback received at timestep 414 of -1
Current timestep = 415. State = [[-0.21124528 -0.22666335  0.3546895   1.        ]]. Action = [[0.39745724 0.20250344 0.7281327  0.7056651 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 415 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 415 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 415 is -1
Human Feedback received at timestep 415 of -1
Current timestep = 416. State = [[-0.1853392  -0.2102796   0.37945384  1.        ]]. Action = [[0.521034   0.78810644 0.6979666  0.3314407 ]]. Reward = [0.]
Curr episode timestep = 120
Scene graph at timestep 416 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 416 is tensor(0.0151, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 416 is -1
Human Feedback received at timestep 416 of -1
Current timestep = 417. State = [[-0.16910625 -0.19264166  0.39550057  1.        ]]. Action = [[ 0.13565326  0.19275844 -0.39884573  0.8039236 ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 417 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 417 is tensor(0.0155, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 417 is -1
Human Feedback received at timestep 417 of -1
Current timestep = 418. State = [[-0.16249524 -0.18658775  0.39300245  1.        ]]. Action = [[0.7842479  0.5256102  0.92540455 0.79720974]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: Workspace boundary
Scene graph at timestep 418 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 418 is tensor(0.0131, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 418 is -1
Human Feedback received at timestep 418 of -1
Current timestep = 419. State = [[-0.17488645 -0.19780733  0.39318797  1.        ]]. Action = [[-0.84393054 -0.74097615 -0.19521922  0.3867128 ]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 419 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 419 is tensor(0.0124, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 419 is -1
Human Feedback received at timestep 419 of -1
Current timestep = 420. State = [[-0.20446545 -0.20854026  0.38025013  1.        ]]. Action = [[-0.70293516  0.17671251 -0.46352452  0.90810585]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 420 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 420 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 420 is -1
Human Feedback received at timestep 420 of -1
Current timestep = 421. State = [[-0.23599003 -0.22368619  0.3630196   1.        ]]. Action = [[-0.75594586 -0.66746444 -0.753018    0.80907273]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 421 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 421 is tensor(0.0130, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 421 is -1
Human Feedback received at timestep 421 of -1
Current timestep = 422. State = [[-0.25076362  0.00255657  0.23266768  1.        ]]. Action = [[0.8609538  0.759253   0.01566672 0.76889396]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 422 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 422 is tensor(0.0240, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 422 is -1
Human Feedback received at timestep 422 of -1
Current timestep = 423. State = [[-0.2522039   0.00374235  0.21951842  1.        ]]. Action = [[-0.75210565 -0.35295832 -0.36079472  0.42010212]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 423 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 423 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 423 is 1
Human Feedback received at timestep 423 of 1
Current timestep = 424. State = [[-0.252628    0.00115613  0.21673621  1.        ]]. Action = [[-0.21618986 -0.23675561  0.28451562  0.9286108 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 424 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 424 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 424 is -1
Human Feedback received at timestep 424 of -1
Current timestep = 425. State = [[-0.2565751  -0.01701782  0.22336876  1.        ]]. Action = [[-0.26955652 -0.71491873  0.41427886  0.8459786 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 425 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 425 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 425 is 1
Human Feedback received at timestep 425 of 1
Current timestep = 426. State = [[-0.2627558  -0.03583903  0.23485345  1.        ]]. Action = [[-0.5070129  -0.91274     0.26994538  0.32003367]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 426 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 426 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 426 is 1
Human Feedback received at timestep 426 of 1
Current timestep = 427. State = [[-0.2642279  -0.03864769  0.23527656  1.        ]]. Action = [[-0.14168221  0.38669956  0.7250197   0.94963205]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 427 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 427 is tensor(0.0090, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 427 is 1
Human Feedback received at timestep 427 of 1
Current timestep = 428. State = [[-0.25635156 -0.05628126  0.24617821  1.        ]]. Action = [[ 0.61194515 -0.86893016  0.6605668   0.8059175 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 428 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 428 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 428 is 1
Human Feedback received at timestep 428 of 1
Current timestep = 429. State = [[-0.2543325  -0.07651959  0.25795156  1.        ]]. Action = [[-0.09678519 -0.06778932 -0.3467325   0.5960071 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 429 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 429 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 429 is 1
Human Feedback received at timestep 429 of 1
Current timestep = 430. State = [[-0.25454143 -0.09974439  0.26217005  1.        ]]. Action = [[ 0.10219586 -0.9257014   0.17358458  0.37844872]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 430 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 430 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 430 is 1
Human Feedback received at timestep 430 of 1
Current timestep = 431. State = [[-0.25465468 -0.12181802  0.26686147  1.        ]]. Action = [[-0.48537058 -0.80642575  0.3785801   0.45436573]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 431 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 431 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 431 is 1
Human Feedback received at timestep 431 of 1
Current timestep = 432. State = [[-0.25735843 -0.14559351  0.26449627  1.        ]]. Action = [[-0.1142171  -0.9479688  -0.30427408  0.90571594]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 432 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 432 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 432 is 1
Human Feedback received at timestep 432 of 1
Current timestep = 433. State = [[-0.25876036 -0.169051    0.25767922  1.        ]]. Action = [[-0.46425116 -0.32296586 -0.8939756   0.8372967 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 433 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 433 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 433 is -1
Human Feedback received at timestep 433 of -1
Current timestep = 434. State = [[-0.25953296 -0.1725138   0.25766817  1.        ]]. Action = [[-0.22541755 -0.13669783 -0.6431685   0.80428076]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 434 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 434 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 434 is -1
Human Feedback received at timestep 434 of -1
Current timestep = 435. State = [[-0.259103   -0.17152455  0.26924172  1.        ]]. Action = [[-0.14222527  0.17849874  0.9907143   0.9397268 ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 435 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 435 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 435 is -1
Human Feedback received at timestep 435 of -1
Current timestep = 436. State = [[-0.2613592  -0.16986378  0.2867329   1.        ]]. Action = [[-0.67118394 -0.94020027 -0.25146788  0.8359176 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 436 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 436 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 436 is 1
Human Feedback received at timestep 436 of 1
Current timestep = 437. State = [[-0.259998   -0.18713099  0.2833451   1.        ]]. Action = [[ 0.31334424 -0.95309913 -0.66387945  0.38398468]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 437 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 437 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 437 is 1
Human Feedback received at timestep 437 of 1
Current timestep = 438. State = [[-0.2505791  -0.19581869  0.28163198  1.        ]]. Action = [[-0.07607818  0.8065585   0.7195717   0.4037373 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 438 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 438 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 438 is -1
Human Feedback received at timestep 438 of -1
Current timestep = 439. State = [[-0.2505618  -0.1825261   0.29205757  1.        ]]. Action = [[-0.8587061  -0.96984535 -0.7733263   0.48158967]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 439 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 439 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 439 is -1
Human Feedback received at timestep 439 of -1
Current timestep = 440. State = [[-0.25066692 -0.18128148  0.2938629   1.        ]]. Action = [[-0.9152666   0.24412549  0.70306873  0.6694381 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 440 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 440 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 440 is -1
Human Feedback received at timestep 440 of -1
Current timestep = 441. State = [[-0.2570229  -0.19777244  0.30590802  1.        ]]. Action = [[-0.31581616 -0.8958194   0.66071796  0.7570945 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 441 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 441 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 441 is 1
Human Feedback received at timestep 441 of 1
Current timestep = 442. State = [[-0.26352626 -0.21811326  0.32438537  1.        ]]. Action = [[-0.27161825 -0.21411514 -0.54091585  0.69225276]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 442 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 442 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 442 is -1
Human Feedback received at timestep 442 of -1
Current timestep = 443. State = [[-0.2595073  -0.22190213  0.3245158   1.        ]]. Action = [[ 0.46544003 -0.00498557 -0.40020752  0.5396651 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 443 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 443 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 443 is -1
Human Feedback received at timestep 443 of -1
Current timestep = 444. State = [[-0.24758588 -0.24085307  0.33204064  1.        ]]. Action = [[ 0.23598039 -0.96155626  0.6529169   0.6939597 ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 444 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 444 is tensor(0.0080, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 444 is -1
Human Feedback received at timestep 444 of -1
Current timestep = 445. State = [[-0.24022067 -0.25869915  0.34823465  1.        ]]. Action = [[-0.8973211   0.22434926 -0.25871795  0.67239726]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 445 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 445 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 445 is -1
Human Feedback received at timestep 445 of -1
Current timestep = 446. State = [[-0.23667102 -0.27248627  0.3449321   1.        ]]. Action = [[ 0.38427806 -0.615892   -0.47579968  0.88797414]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 446 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 446 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 446 is 1
Human Feedback received at timestep 446 of 1
Current timestep = 447. State = [[-0.25091526  0.00270239  0.23256502  1.        ]]. Action = [[ 0.23854983 -0.19348717  0.406029   -0.29379475]]. Reward = [-1.]
Curr episode timestep = 24
Scene graph at timestep 447 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 447 is tensor(0.0312, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 447 is -1
Human Feedback received at timestep 447 of -1
Current timestep = 448. State = [[-0.25095788  0.0029583   0.22771871  1.        ]]. Action = [[-0.4507643  -0.84583443  0.20415413  0.62850237]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 448 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 448 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 448 is 1
Human Feedback received at timestep 448 of 1
Current timestep = 449. State = [[-0.25089556  0.00297291  0.2277715   1.        ]]. Action = [[-0.7076361  -0.8089581   0.8306793   0.71782446]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 449 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 449 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 449 is 1
Human Feedback received at timestep 449 of 1
Current timestep = 450. State = [[-0.25089556  0.00297291  0.2277715   1.        ]]. Action = [[-0.35588872 -0.83387804  0.37913358  0.62336683]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 450 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 450 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 450 is 1
Human Feedback received at timestep 450 of 1
Current timestep = 451. State = [[-0.24973084  0.00189062  0.23288265  1.        ]]. Action = [[ 0.4065578  -0.19797593  0.09706807 -0.26739752]]. Reward = [-1.]
Curr episode timestep = 3
Scene graph at timestep 451 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 451 is tensor(0.0135, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 451 is 1
Human Feedback received at timestep 451 of 1
Current timestep = 452. State = [[-0.23918179 -0.01209038  0.23141618  1.        ]]. Action = [[ 0.26106238 -0.5044949   0.12841654  0.07138026]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 452 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 452 is tensor(0.0142, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 452 is 1
Human Feedback received at timestep 452 of 1
Current timestep = 453. State = [[-0.23207387 -0.02431032  0.23202978  1.        ]]. Action = [[-0.79008836 -0.8142389  -0.43160713  0.42897177]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 453 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 453 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 453 is 1
Human Feedback received at timestep 453 of 1
Current timestep = 454. State = [[-0.23135155 -0.02624742  0.23255955  1.        ]]. Action = [[-0.80334944 -0.6843103   0.57641864  0.5428562 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 454 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 454 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 454 is 1
Human Feedback received at timestep 454 of 1
Current timestep = 455. State = [[-0.2266523  -0.04106541  0.2271025   1.        ]]. Action = [[ 0.32700634 -0.71168923 -0.4496957   0.48065174]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 455 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 455 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 455 is 1
Human Feedback received at timestep 455 of 1
Current timestep = 456. State = [[-0.22290638 -0.07855412  0.22455059  1.        ]]. Action = [[-0.6656038  -0.9962639   0.7627293   0.66294014]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 456 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 456 is tensor(0.0134, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 456 is 1
Human Feedback received at timestep 456 of 1
Current timestep = 457. State = [[-0.23663776 -0.11900909  0.25205547  1.        ]]. Action = [[-0.17405975 -0.4936155   0.93864226  0.49499118]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 457 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 457 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 457 is 1
Human Feedback received at timestep 457 of 1
Current timestep = 458. State = [[-0.24426684 -0.14302829  0.273299    1.        ]]. Action = [[ 0.21500874 -0.44444847 -0.54656243  0.66984653]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 458 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 458 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 458 is -1
Human Feedback received at timestep 458 of -1
Current timestep = 459. State = [[-0.24237323 -0.15475614  0.27057138  1.        ]]. Action = [[-0.70694524  0.17180777 -0.9274216   0.36160433]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 459 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 459 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 459 is -1
Human Feedback received at timestep 459 of -1
Current timestep = 460. State = [[-0.24104954 -0.15647464  0.2722026   1.        ]]. Action = [[-0.72387296  0.3427235  -0.756585    0.61741734]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 460 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 460 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 460 is -1
Human Feedback received at timestep 460 of -1
Current timestep = 461. State = [[-0.23038937 -0.1567728   0.26765007  1.        ]]. Action = [[ 0.87162304 -0.00705463 -0.37570417  0.9560052 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 461 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 461 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 461 is -1
Human Feedback received at timestep 461 of -1
Current timestep = 462. State = [[-0.20754683 -0.16225955  0.25381044  1.        ]]. Action = [[-0.94119036  0.4362496  -0.3036828   0.4588021 ]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 462 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 462 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 462 is -1
Human Feedback received at timestep 462 of -1
Current timestep = 463. State = [[-0.1917219  -0.17562667  0.25152138  1.        ]]. Action = [[ 0.73981583 -0.773819   -0.11588669  0.9191904 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 463 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 463 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 463 is -1
Human Feedback received at timestep 463 of -1
Current timestep = 464. State = [[-0.16354035 -0.18717152  0.24472456  1.        ]]. Action = [[ 0.36266792  0.35704803 -0.02402335  0.82648253]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 464 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 464 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 464 is -1
Human Feedback received at timestep 464 of -1
Current timestep = 465. State = [[-0.14775816 -0.19940688  0.23749192  1.        ]]. Action = [[ 0.48650968 -0.8387707  -0.45204794  0.83698416]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 465 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 465 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 465 is -1
Human Feedback received at timestep 465 of -1
Current timestep = 466. State = [[-0.12239801 -0.21303922  0.22245936  1.        ]]. Action = [[ 0.4709648   0.2325083  -0.09852052  0.9012735 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 466 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 466 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 466 is -1
Human Feedback received at timestep 466 of -1
Current timestep = 467. State = [[-0.25082195  0.00258666  0.23257805  1.        ]]. Action = [[-0.9515985  -0.8270557   0.82732344 -0.31360984]]. Reward = [-1.]
Curr episode timestep = 15
Scene graph at timestep 467 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 467 is tensor(0.0192, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 467 is 1
Human Feedback received at timestep 467 of 1
Current timestep = 468. State = [[-0.250306   -0.0036521   0.22905955  1.        ]]. Action = [[ 0.34829712 -0.36140805 -0.06757182  0.68500185]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 468 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 468 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 468 is 1
Human Feedback received at timestep 468 of 1
Current timestep = 469. State = [[-0.23700076 -0.02253736  0.23272952  1.        ]]. Action = [[ 0.6441511  -0.6019305   0.39427817  0.6730093 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 469 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 469 is tensor(0.0116, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 469 is 1
Human Feedback received at timestep 469 of 1
Current timestep = 470. State = [[-0.21788646 -0.05402835  0.24105695  1.        ]]. Action = [[ 0.3312707  -0.8395574   0.10028982  0.622946  ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 470 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 470 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 470 is 1
Human Feedback received at timestep 470 of 1
Current timestep = 471. State = [[-0.21186467 -0.09518915  0.24503627  1.        ]]. Action = [[-0.3526904  -0.9032781   0.05388594  0.63933444]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 471 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 471 is tensor(0.0119, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 471 is 1
Human Feedback received at timestep 471 of 1
Current timestep = 472. State = [[-0.21728086 -0.12241706  0.24383932  1.        ]]. Action = [[ 0.05721331 -0.10946518 -0.37590796  0.49469066]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 472 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 472 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 472 is 1
Human Feedback received at timestep 472 of 1
Current timestep = 473. State = [[-0.21582393 -0.1288356   0.23522237  1.        ]]. Action = [[-0.8915664  -0.681649   -0.4249211   0.62426925]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 473 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 473 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 473 is 1
Human Feedback received at timestep 473 of 1
Current timestep = 474. State = [[-0.21632877 -0.13156272  0.2360359   1.        ]]. Action = [[ 0.9002608  -0.17915612 -0.30914396  0.3846656 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: No entry zone
Scene graph at timestep 474 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 474 is tensor(0.0136, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 474 is -1
Human Feedback received at timestep 474 of -1
Current timestep = 475. State = [[-0.2261552  -0.1472928   0.24009031  1.        ]]. Action = [[-0.8425425  -0.70567155  0.37731266  0.15196371]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 475 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 475 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 475 is 1
Human Feedback received at timestep 475 of 1
Current timestep = 476. State = [[-0.22959493 -0.1825429   0.2431342   1.        ]]. Action = [[ 0.9084363  -0.8228323  -0.3806138   0.78385687]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 476 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 476 is tensor(0.0156, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 476 is -1
Human Feedback received at timestep 476 of -1
Current timestep = 477. State = [[-0.21475059 -0.20364113  0.23682149  1.        ]]. Action = [[-0.87677926  0.43079734  0.9176738   0.7671106 ]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 477 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 477 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 477 is -1
Human Feedback received at timestep 477 of -1
Current timestep = 478. State = [[-0.21538962 -0.22276814  0.24346316  1.        ]]. Action = [[-0.09018558 -0.7862665   0.3992355   0.35953212]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 478 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 478 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 478 is -1
Human Feedback received at timestep 478 of -1
Current timestep = 479. State = [[-0.20276356 -0.23377593  0.26214302  1.        ]]. Action = [[0.7664399  0.4210466  0.82935476 0.7699431 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 479 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 479 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 479 is -1
Human Feedback received at timestep 479 of -1
Current timestep = 480. State = [[-0.17487918 -0.24803466  0.28708404  1.        ]]. Action = [[ 0.51709247 -0.86900836  0.19395232  0.49942744]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 480 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 480 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 480 is -1
Human Feedback received at timestep 480 of -1
Current timestep = 481. State = [[-0.1505988  -0.27962914  0.30934188  1.        ]]. Action = [[ 0.62895596 -0.8792297   0.60731065  0.5064914 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 481 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 481 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 481 is -1
Human Feedback received at timestep 481 of -1
Current timestep = 482. State = [[-0.12905386 -0.30454156  0.32723758  1.        ]]. Action = [[-0.22598577 -0.8771857   0.10427737  0.5044807 ]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 482 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 482 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 482 is 1
Human Feedback received at timestep 482 of 1
Current timestep = 483. State = [[-0.12419537 -0.31143093  0.33055627  1.        ]]. Action = [[ 0.6976068  -0.37852198  0.79890573  0.6262809 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 483 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 483 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 483 is -1
Human Feedback received at timestep 483 of -1
Current timestep = 484. State = [[-0.12442591 -0.31166098  0.3306595   1.        ]]. Action = [[-0.9895461  -0.16180974 -0.17169243  0.15298164]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 484 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 484 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 484 is -1
Human Feedback received at timestep 484 of -1
Current timestep = 485. State = [[-0.12444432 -0.31164023  0.3306578   1.        ]]. Action = [[-0.7808173  -0.9430739  -0.01653647  0.7076826 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 485 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 485 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 485 is -1
Human Feedback received at timestep 485 of -1
Current timestep = 486. State = [[-0.127828   -0.31247896  0.3293849   1.        ]]. Action = [[-0.4547994   0.16567111 -0.15699893  0.5574484 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 486 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 486 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 486 is -1
Human Feedback received at timestep 486 of -1
Current timestep = 487. State = [[-0.13018912 -0.3135443   0.3290043   1.        ]]. Action = [[-0.12237674 -0.6324412   0.6993258   0.80321956]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 487 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 487 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 487 is 1
Human Feedback received at timestep 487 of 1
Current timestep = 488. State = [[-0.2508548   0.00246222  0.23256706  1.        ]]. Action = [[ 0.09286165  0.50877845  0.1369772  -0.09393907]]. Reward = [-1.]
Curr episode timestep = 20
Scene graph at timestep 488 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 488 is tensor(0.0322, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 488 is -1
Human Feedback received at timestep 488 of -1
Current timestep = 489. State = [[-0.24622034 -0.01390763  0.21997534  1.        ]]. Action = [[ 0.5467416  -0.9629292  -0.57859206  0.70091426]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 489 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 489 is tensor(0.0170, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 489 is 1
Human Feedback received at timestep 489 of 1
Current timestep = 490. State = [[-0.25026277  0.00267442  0.2325975   1.        ]]. Action = [[ 0.77360487 -0.82230765  0.6557293  -0.45107144]]. Reward = [-1.]
Curr episode timestep = 1
Scene graph at timestep 490 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 490 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 490 is 1
Human Feedback received at timestep 490 of 1
Current timestep = 491. State = [[-0.2510569  -0.01030617  0.23708385  1.        ]]. Action = [[-0.13046205 -0.47425175  0.24366605  0.94229424]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 491 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 491 is tensor(0.0157, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 491 is 1
Human Feedback received at timestep 491 of 1
Current timestep = 492. State = [[-0.25256193 -0.02308847  0.24241552  1.        ]]. Action = [[-0.562001    0.45325089  0.5433624   0.75753903]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 492 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 492 is tensor(0.0141, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 492 is 1
Human Feedback received at timestep 492 of 1
Current timestep = 493. State = [[-0.25699845 -0.01309927  0.25511798  1.        ]]. Action = [[-0.1479879   0.6148212   0.85366166  0.8079951 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 493 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 493 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 493 is -1
Human Feedback received at timestep 493 of -1
Current timestep = 494. State = [[-0.26608065 -0.01733949  0.289115    1.        ]]. Action = [[-0.19161195 -0.94076777  0.59334195  0.8507217 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 494 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 494 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 494 is 1
Human Feedback received at timestep 494 of 1
Current timestep = 495. State = [[-0.27459645 -0.03695727  0.3095632   1.        ]]. Action = [[-0.56013906 -0.5307624   0.8774121   0.4749595 ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 495 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 495 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 495 is 1
Human Feedback received at timestep 495 of 1
Current timestep = 496. State = [[-0.27601236 -0.03973798  0.31194305  1.        ]]. Action = [[-0.4121982  -0.17707539  0.26918626  0.63343525]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 496 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 496 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 496 is 1
Human Feedback received at timestep 496 of 1
Current timestep = 497. State = [[-0.27586633 -0.04039868  0.31284273  1.        ]]. Action = [[-0.53990823  0.31647658 -0.62888086  0.6105689 ]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 497 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 497 is tensor(0.0109, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 497 is -1
Human Feedback received at timestep 497 of -1
Current timestep = 498. State = [[-0.27604786 -0.04060843  0.31282693  1.        ]]. Action = [[-0.76409453 -0.04159772 -0.87825745  0.8447237 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 498 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 498 is tensor(0.0117, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 498 is 1
Human Feedback received at timestep 498 of 1
Current timestep = 499. State = [[-0.27618816 -0.04085163  0.31283763  1.        ]]. Action = [[-0.44248593  0.22070229 -0.5088112   0.8665588 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 499 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 499 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 499 is 1
Human Feedback received at timestep 499 of 1
Current timestep = 500. State = [[-0.27621138 -0.04095336  0.3128432   1.        ]]. Action = [[-0.35680276 -0.927723    0.21983624  0.35153115]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 500 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 500 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 500 is 1
Human Feedback received at timestep 500 of 1
Current timestep = 501. State = [[-0.27621138 -0.04095336  0.3128432   1.        ]]. Action = [[-0.14833212 -0.7641292  -0.98934454  0.71857464]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 501 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 501 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 501 is 1
Human Feedback received at timestep 501 of 1
Current timestep = 502. State = [[-0.27621138 -0.04095336  0.3128432   1.        ]]. Action = [[-0.22251701 -0.7603058  -0.57343507  0.630702  ]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 502 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 502 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 502 is 1
Human Feedback received at timestep 502 of 1
Current timestep = 503. State = [[-0.27110547 -0.04137704  0.3174849   1.        ]]. Action = [[ 0.2633257  -0.00680715  0.2050811   0.832428  ]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 503 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 503 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 503 is -1
Human Feedback received at timestep 503 of -1
Current timestep = 504. State = [[-0.2513063  -0.04684549  0.33673203  1.        ]]. Action = [[ 0.7839563 -0.3099531  0.5597062  0.5112295]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 504 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 504 is tensor(0.0121, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 504 is 1
Human Feedback received at timestep 504 of 1
Current timestep = 505. State = [[-0.23876426 -0.07113916  0.34429646  1.        ]]. Action = [[-0.26215184 -0.8931217  -0.92768884  0.7710979 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 505 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 505 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 505 is 1
Human Feedback received at timestep 505 of 1
Current timestep = 506. State = [[-0.23020948 -0.11005127  0.33933482  1.        ]]. Action = [[ 0.5988438  -0.7898245   0.7318227   0.74847364]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 506 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 506 is tensor(0.0122, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 506 is 1
Human Feedback received at timestep 506 of 1
Current timestep = 507. State = [[-0.21695371 -0.132154    0.34930712  1.        ]]. Action = [[-8.7105054e-01  3.4134626e-02  7.3814392e-04  2.2309816e-01]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 507 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 507 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 507 is -1
Human Feedback received at timestep 507 of -1
Current timestep = 508. State = [[-0.21520248 -0.1364615   0.3513908   1.        ]]. Action = [[-0.89456856 -0.8876233   0.45697713  0.5728133 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 508 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 508 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 508 is -1
Human Feedback received at timestep 508 of -1
Current timestep = 509. State = [[-0.22698763 -0.15564056  0.34175476  1.        ]]. Action = [[-0.73063064 -0.8794371  -0.89019877  0.8050387 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 509 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 509 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 509 is 1
Human Feedback received at timestep 509 of 1
Current timestep = 510. State = [[-0.24405867 -0.18286951  0.32582554  1.        ]]. Action = [[-0.3382936  -0.14949799 -0.06321025  0.48633397]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 510 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 510 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 510 is 1
Human Feedback received at timestep 510 of 1
Current timestep = 511. State = [[-0.25046563  0.00265387  0.23252673  1.        ]]. Action = [[-0.19332898 -0.64673966  0.8206303  -0.659006  ]]. Reward = [-1.]
Curr episode timestep = 20
Scene graph at timestep 511 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 511 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 511 is 1
Human Feedback received at timestep 511 of 1
Current timestep = 512. State = [[-0.25108007  0.00291965  0.22861317  1.        ]]. Action = [[-0.8987255  -0.36821306  0.8252139   0.8011025 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 512 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 512 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 512 is 1
Human Feedback received at timestep 512 of 1
Current timestep = 513. State = [[-0.25081655  0.00287488  0.22851275  1.        ]]. Action = [[-0.54517424 -0.12789601  0.5249524   0.77317333]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 513 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 513 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 513 is 1
Human Feedback received at timestep 513 of 1
Current timestep = 514. State = [[-0.25081655  0.00287488  0.22851275  1.        ]]. Action = [[-0.8373559  -0.92811364 -0.7951357   0.6298132 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 514 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 514 is tensor(0.0118, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 514 is 1
Human Feedback received at timestep 514 of 1
Current timestep = 515. State = [[-0.25081655  0.00287488  0.22851275  1.        ]]. Action = [[-0.90257496 -0.8060781  -0.80340433  0.60228276]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 515 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 515 is tensor(0.0132, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 515 is 1
Human Feedback received at timestep 515 of 1
Current timestep = 516. State = [[-0.25081655  0.00287488  0.22851275  1.        ]]. Action = [[-0.6098565  -0.7381705  -0.09154451  0.34591568]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 516 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 516 is tensor(0.0126, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 516 is 1
Human Feedback received at timestep 516 of 1
Current timestep = 517. State = [[-0.24564359 -0.01573595  0.23679473  1.        ]]. Action = [[ 0.26611257 -0.9438923   0.76143265  0.952386  ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 517 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 517 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 517 is 1
Human Feedback received at timestep 517 of 1
Current timestep = 518. State = [[-0.24636042 -0.03659068  0.24642731  1.        ]]. Action = [[-0.5340848  -0.4518041   0.30438972  0.37351036]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 518 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 518 is tensor(0.0140, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 518 is 1
Human Feedback received at timestep 518 of 1
Current timestep = 519. State = [[-0.24864525 -0.05922838  0.25570226  1.        ]]. Action = [[-0.2701257  -0.9309545   0.44066513  0.71207476]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 519 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 519 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 519 is 1
Human Feedback received at timestep 519 of 1
Current timestep = 520. State = [[-0.25831643 -0.09754511  0.26019076  1.        ]]. Action = [[-0.08888167 -0.70550233 -0.9701224   0.84272885]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 520 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 520 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 520 is 1
Human Feedback received at timestep 520 of 1
Current timestep = 521. State = [[-0.25923097 -0.11636511  0.24380639  1.        ]]. Action = [[-0.55288094 -0.58663654  0.82608044  0.79292107]]. Reward = [0.]
Curr episode timestep = 9
Action ignored: Workspace boundary
Scene graph at timestep 521 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 521 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 521 is 1
Human Feedback received at timestep 521 of 1
Current timestep = 522. State = [[-0.2596     -0.12112672  0.2427934   1.        ]]. Action = [[-0.2215904  -0.87827665  0.35544956  0.90215015]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 522 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 522 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 522 is -1
Human Feedback received at timestep 522 of -1
Current timestep = 523. State = [[-0.25999504 -0.12211522  0.24295211  1.        ]]. Action = [[-0.22014695  0.8089218   0.7228054   0.25280976]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 523 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 523 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 523 is -1
Human Feedback received at timestep 523 of -1
Current timestep = 524. State = [[-0.25999504 -0.12211522  0.24295211  1.        ]]. Action = [[-0.8140341  -0.35961092  0.01615691  0.6791792 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 524 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 524 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 524 is -1
Human Feedback received at timestep 524 of -1
Current timestep = 525. State = [[-0.26208854 -0.12069783  0.23587322  1.        ]]. Action = [[-0.14054686  0.23409808 -0.399343    0.6960026 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 525 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 525 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 525 is -1
Human Feedback received at timestep 525 of -1
Current timestep = 526. State = [[-0.26247936 -0.11975022  0.22574474  1.        ]]. Action = [[-0.75592625  0.21715128 -0.6638399   0.60676885]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 526 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 526 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 526 is -1
Human Feedback received at timestep 526 of -1
Current timestep = 527. State = [[-0.26340532 -0.11880291  0.22496025  1.        ]]. Action = [[-0.9814653   0.12222004  0.10456514  0.45580614]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 527 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 527 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 527 is -1
Human Feedback received at timestep 527 of -1
Current timestep = 528. State = [[-0.2635646  -0.11841226  0.2247698   1.        ]]. Action = [[-0.86494184 -0.6812093   0.9197675   0.63469505]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 528 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 528 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 528 is 1
Human Feedback received at timestep 528 of 1
Current timestep = 529. State = [[-0.26058534 -0.1334165   0.22237174  1.        ]]. Action = [[ 0.309242   -0.9146344  -0.16692293  0.14898264]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 529 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 529 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 529 is -1
Human Feedback received at timestep 529 of -1
Current timestep = 530. State = [[-0.24229342 -0.1523825   0.21164203  1.        ]]. Action = [[ 0.8580394  -0.04190505 -0.20301664  0.5186379 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 530 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 530 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 530 is -1
Human Feedback received at timestep 530 of -1
Current timestep = 531. State = [[-0.2126263  -0.17491919  0.19629471  1.        ]]. Action = [[ 0.6113248 -0.9950675 -0.7219366  0.71718  ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 531 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 531 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 531 is 1
Human Feedback received at timestep 531 of 1
Current timestep = 532. State = [[-0.1985457  -0.2170115   0.16091695  1.        ]]. Action = [[-0.2837075  -0.7087776  -0.6778493   0.83600044]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 532 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 532 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 532 is 1
Human Feedback received at timestep 532 of 1
Current timestep = 533. State = [[-0.20602731 -0.25760132  0.13144304  1.        ]]. Action = [[-0.3587085  -0.9341721  -0.6507159   0.63973093]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 533 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 533 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 533 is -1
Human Feedback received at timestep 533 of -1
Current timestep = 534. State = [[-0.22473875 -0.3022595   0.10354695  1.        ]]. Action = [[-0.68393135 -0.74615425 -0.41495103  0.522315  ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 534 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 534 is tensor(0.0084, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 534 is 1
Human Feedback received at timestep 534 of 1
Current timestep = 535. State = [[-0.23489898 -0.33170974  0.09039931  1.        ]]. Action = [[-0.81624144 -0.2953862   0.72284794  0.7638018 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 535 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 535 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 535 is -1
Human Feedback received at timestep 535 of -1
Current timestep = 536. State = [[-0.23561871 -0.33762822  0.08868195  1.        ]]. Action = [[-0.67787194  0.3063513   0.45488453  0.5454025 ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 536 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 536 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 536 is 1
Human Feedback received at timestep 536 of 1
Current timestep = 537. State = [[-0.23574734 -0.33800521  0.08866272  1.        ]]. Action = [[-0.19116277 -0.8114239  -0.7702091   0.9706371 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 537 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 537 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 537 is 1
Human Feedback received at timestep 537 of 1
Current timestep = 538. State = [[-0.23574734 -0.33800521  0.08866272  1.        ]]. Action = [[ 0.8237195  -0.7093746   0.00582838  0.4798901 ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 538 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 538 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 538 is 1
Human Feedback received at timestep 538 of 1
Current timestep = 539. State = [[-0.23574734 -0.33800521  0.08866272  1.        ]]. Action = [[-0.5044411  -0.57284117 -0.2568587   0.74565387]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 539 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 539 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 539 is 1
Human Feedback received at timestep 539 of 1
Current timestep = 540. State = [[-0.23574734 -0.33800521  0.08866272  1.        ]]. Action = [[-0.92388546 -0.47228062 -0.0033735   0.8596952 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 540 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 540 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 540 is 1
Human Feedback received at timestep 540 of 1
Current timestep = 541. State = [[-0.23574734 -0.33800521  0.08866272  1.        ]]. Action = [[ 0.8339689  -0.34768528  0.18688416  0.05419862]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 541 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 541 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 541 is 1
Human Feedback received at timestep 541 of 1
Current timestep = 542. State = [[-0.23574734 -0.33800521  0.08866272  1.        ]]. Action = [[-0.64115816 -0.82320535  0.9863734   0.5486653 ]]. Reward = [0.]
Curr episode timestep = 30
Action ignored: Workspace boundary
Scene graph at timestep 542 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 542 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 542 is -1
Human Feedback received at timestep 542 of -1
Current timestep = 543. State = [[-0.23566672 -0.33789846  0.08862356  1.        ]]. Action = [[-0.40542233 -0.912764    0.19880581  0.74805117]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 543 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 543 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 543 is 1
Human Feedback received at timestep 543 of 1
Current timestep = 544. State = [[-0.23566672 -0.33789846  0.08862356  1.        ]]. Action = [[-0.4015273  -0.0578199  -0.38343185  0.36769438]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Scene graph at timestep 544 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 544 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 544 is 1
Human Feedback received at timestep 544 of 1
Current timestep = 545. State = [[-0.23566672 -0.33789846  0.08862356  1.        ]]. Action = [[-0.444085    0.3308246   0.5071548   0.94648266]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 545 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 545 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 545 is 1
Human Feedback received at timestep 545 of 1
Current timestep = 546. State = [[-0.23566672 -0.33789846  0.08862356  1.        ]]. Action = [[-0.5643186   0.11867237  0.51463234  0.7842488 ]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Scene graph at timestep 546 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 546 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 546 is 1
Human Feedback received at timestep 546 of 1
Current timestep = 547. State = [[-0.23566672 -0.33789846  0.08862356  1.        ]]. Action = [[-0.78028625 -0.51265615 -0.14770979  0.58500254]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 547 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 547 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 547 is 1
Human Feedback received at timestep 547 of 1
Current timestep = 548. State = [[-0.23566672 -0.33789846  0.08862356  1.        ]]. Action = [[-0.02840382 -0.349442    0.859782    0.64747155]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 548 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 548 is tensor(0.0073, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 548 is 1
Human Feedback received at timestep 548 of 1
Current timestep = 549. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.23633939 -0.43867415 -0.57005435  0.564608  ]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 549 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 549 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 549 is 1
Human Feedback received at timestep 549 of 1
Current timestep = 550. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.5019123   0.16489792 -0.39160186  0.9793682 ]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 550 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 550 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 550 is 1
Human Feedback received at timestep 550 of 1
Current timestep = 551. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.9298441  -0.8714008  -0.1023078  -0.51515096]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 551 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 551 is tensor(0.0068, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 551 is 1
Human Feedback received at timestep 551 of 1
Current timestep = 552. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[ 0.33675253 -0.9916919   0.8294095   0.90921617]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 552 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 552 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 552 is 1
Human Feedback received at timestep 552 of 1
Current timestep = 553. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.55047226 -0.31899673  0.39920974  0.8951905 ]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 553 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 553 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 553 is 1
Human Feedback received at timestep 553 of 1
Current timestep = 554. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.16239816 -0.529417   -0.6361437   0.7242372 ]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 554 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 554 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 554 is 1
Human Feedback received at timestep 554 of 1
Current timestep = 555. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.9735387  -0.73123455 -0.8899326   0.589386  ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 555 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 555 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 555 is 1
Human Feedback received at timestep 555 of 1
Current timestep = 556. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[ 0.463791   -0.7344503  -0.04948258  0.8367014 ]]. Reward = [0.]
Curr episode timestep = 44
Action ignored: Workspace boundary
Scene graph at timestep 556 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 556 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 556 is 1
Human Feedback received at timestep 556 of 1
Current timestep = 557. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.7555932  -0.58198225  0.21872115  0.5238707 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 557 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 557 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 557 is 1
Human Feedback received at timestep 557 of 1
Current timestep = 558. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.7742952   0.17210686 -0.8794936   0.5233395 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 558 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 558 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 558 is 1
Human Feedback received at timestep 558 of 1
Current timestep = 559. State = [[-0.23565412 -0.33792114  0.08862393  1.        ]]. Action = [[-0.4387486   0.22551787 -0.8393678   0.8260393 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 559 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 559 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 559 is 1
Human Feedback received at timestep 559 of 1
Current timestep = 560. State = [[-0.23567736 -0.3379556   0.08860636  1.        ]]. Action = [[ 0.09467328 -0.98313737  0.7357688   0.9328952 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 560 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 560 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 560 is 1
Human Feedback received at timestep 560 of 1
Current timestep = 561. State = [[-0.22478679 -0.32625067  0.09766184  1.        ]]. Action = [[0.75414956 0.60641074 0.8452811  0.47385263]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 561 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 561 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 561 is -1
Human Feedback received at timestep 561 of -1
Current timestep = 562. State = [[-0.21272855 -0.3114082   0.11033093  1.        ]]. Action = [[ 0.7726538  -0.7574497   0.84165466  0.8017497 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Scene graph at timestep 562 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 562 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 562 is 1
Human Feedback received at timestep 562 of 1
Current timestep = 563. State = [[-0.21029876 -0.3080926   0.11101916  1.        ]]. Action = [[ 0.07762945 -0.5904984   0.30490112  0.69047844]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 563 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 563 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 563 is 1
Human Feedback received at timestep 563 of 1
Current timestep = 564. State = [[-0.20943926 -0.30765134  0.11176241  1.        ]]. Action = [[-0.27662265 -0.5158745   0.30311882  0.92086375]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 564 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 564 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 564 is 1
Human Feedback received at timestep 564 of 1
Current timestep = 565. State = [[-0.20876051 -0.30728927  0.11238065  1.        ]]. Action = [[-0.26904035 -0.67477894 -0.43151224  0.50297475]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 565 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 565 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 565 is 1
Human Feedback received at timestep 565 of 1
Current timestep = 566. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[-0.9348959  -0.47580332 -0.55579764  0.7662668 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 566 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 566 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 566 is 1
Human Feedback received at timestep 566 of 1
Current timestep = 567. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[-0.76346546 -0.8182479  -0.7533624   0.86571884]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 567 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 567 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 567 is -1
Human Feedback received at timestep 567 of -1
Current timestep = 568. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[-0.01593524 -0.7652239   0.8690846   0.57612324]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 568 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 568 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 568 is -1
Human Feedback received at timestep 568 of -1
Current timestep = 569. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[ 0.46024156 -0.27853954 -0.5207107   0.751143  ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 569 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 569 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 569 is 1
Human Feedback received at timestep 569 of 1
Current timestep = 570. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[-0.40068972 -0.34563845  0.57152534  0.81882334]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Scene graph at timestep 570 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 570 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 570 is 1
Human Feedback received at timestep 570 of 1
Current timestep = 571. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[ 0.9666753  -0.59115446  0.9487276   0.77723634]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 571 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 571 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 571 is -1
Human Feedback received at timestep 571 of -1
Current timestep = 572. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[-0.6748291  -0.08921814 -0.62203455  0.20398319]]. Reward = [0.]
Curr episode timestep = 60
Action ignored: Workspace boundary
Scene graph at timestep 572 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 572 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 572 is 1
Human Feedback received at timestep 572 of 1
Current timestep = 573. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[ 0.08965611 -0.78003454  0.69624543  0.6960807 ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 573 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 573 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 573 is -1
Human Feedback received at timestep 573 of -1
Current timestep = 574. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[ 0.42904294 -0.5445789  -0.9236702   0.70746493]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 574 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 574 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 574 is 1
Human Feedback received at timestep 574 of 1
Current timestep = 575. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[ 0.12327278 -0.56303036  0.49873602  0.6181047 ]]. Reward = [0.]
Curr episode timestep = 63
Action ignored: Workspace boundary
Scene graph at timestep 575 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 575 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 575 is 1
Human Feedback received at timestep 575 of 1
Current timestep = 576. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[ 0.5775864  -0.07153881  0.5021906   0.78492475]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Scene graph at timestep 576 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 576 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 576 is 1
Human Feedback received at timestep 576 of 1
Current timestep = 577. State = [[-0.20849992 -0.30717853  0.11264075  1.        ]]. Action = [[-0.13379359 -0.9213484   0.7020979   0.8891909 ]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Scene graph at timestep 577 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 577 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 577 is 1
Human Feedback received at timestep 577 of 1
Current timestep = 578. State = [[-0.25025666  0.00256083  0.23256533  1.        ]]. Action = [[ 0.54315996 -0.00340426  0.5250366  -0.12825137]]. Reward = [-1.]
Curr episode timestep = 66
Scene graph at timestep 578 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 578 is tensor(0.0301, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 578 is 1
Human Feedback received at timestep 578 of 1
Current timestep = 579. State = [[-0.24561213 -0.01724907  0.24800812  1.        ]]. Action = [[ 0.07794225 -0.90232784  0.8990536   0.6890955 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 579 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 579 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 579 is 1
Human Feedback received at timestep 579 of 1
Current timestep = 580. State = [[-0.2469662  -0.04205204  0.2635365   1.        ]]. Action = [[ 0.29073894 -0.20399976 -0.6735499   0.52590775]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 580 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 580 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 580 is 1
Human Feedback received at timestep 580 of 1
Current timestep = 581. State = [[-0.2390649  -0.04908507  0.25608012  1.        ]]. Action = [[-0.5982882  -0.39187658 -0.4973867   0.22417164]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 581 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 581 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 581 is 1
Human Feedback received at timestep 581 of 1
Current timestep = 582. State = [[-0.23734641 -0.04805384  0.257216    1.        ]]. Action = [[-0.05919701  0.21744967  0.28193903  0.8075738 ]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 582 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 582 is tensor(0.0092, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 582 is 1
Human Feedback received at timestep 582 of 1
Current timestep = 583. State = [[-0.22601983 -0.06097063  0.26145032  1.        ]]. Action = [[ 0.649699   -0.76193887 -0.03712517  0.83341455]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 583 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 583 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 583 is -1
Human Feedback received at timestep 583 of -1
Current timestep = 584. State = [[-0.20088361 -0.07290696  0.26384786  1.        ]]. Action = [[ 0.7628417   0.14463127 -0.002684    0.8741437 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 584 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 584 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 584 is 1
Human Feedback received at timestep 584 of 1
Current timestep = 585. State = [[-0.17446698 -0.09052587  0.26690567  1.        ]]. Action = [[ 0.25390506 -0.8295252   0.16909945  0.6803591 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 585 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 585 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 585 is -1
Human Feedback received at timestep 585 of -1
Current timestep = 586. State = [[-0.16099735 -0.10735865  0.2732815   1.        ]]. Action = [[ 0.12163365 -0.00231683  0.07642412  0.56533873]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 586 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 586 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 586 is -1
Human Feedback received at timestep 586 of -1
Current timestep = 587. State = [[-0.16530626 -0.102405    0.26358116  1.        ]]. Action = [[-0.64385724  0.5793344  -0.8143083   0.9159657 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 587 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 587 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 587 is -1
Human Feedback received at timestep 587 of -1
Current timestep = 588. State = [[-0.17997138 -0.10481105  0.2545645   1.        ]]. Action = [[-0.7257253  -0.48997903  0.3796115   0.6653428 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 588 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 588 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 588 is -1
Human Feedback received at timestep 588 of -1
Current timestep = 589. State = [[-0.20945476 -0.12284745  0.24974406  1.        ]]. Action = [[-0.697875   -0.42791802 -0.4940673   0.18147552]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 589 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 589 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 589 is 1
Human Feedback received at timestep 589 of 1
Current timestep = 590. State = [[-0.23547527 -0.12437171  0.23588274  1.        ]]. Action = [[-0.17281789  0.40534925  0.01420248  0.44699   ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 590 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 590 is tensor(0.0071, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 590 is -1
Human Feedback received at timestep 590 of -1
Current timestep = 591. State = [[-0.24211536 -0.12035584  0.23249164  1.        ]]. Action = [[-0.77030534 -0.2979765   0.26007104  0.8014405 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 591 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 591 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 591 is 1
Human Feedback received at timestep 591 of 1
Current timestep = 592. State = [[-0.2537809  -0.13757977  0.2204691   1.        ]]. Action = [[-0.41062248 -0.9091762  -0.87570786  0.6773064 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 592 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 592 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 592 is -1
Human Feedback received at timestep 592 of -1
Current timestep = 593. State = [[-0.2676008 -0.1545144  0.2022061  1.       ]]. Action = [[-0.80836976  0.22105527 -0.80249566  0.84823966]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 593 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 593 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 593 is -1
Human Feedback received at timestep 593 of -1
Current timestep = 594. State = [[-0.25677755 -0.17287345  0.19673674  1.        ]]. Action = [[ 0.8840318  -0.8192325  -0.4835708   0.06226504]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 594 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 594 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 594 is 1
Human Feedback received at timestep 594 of 1
Current timestep = 595. State = [[-0.24445182 -0.19322395  0.17613912  1.        ]]. Action = [[-0.7269479   0.23225224  0.23816371  0.77310646]]. Reward = [0.]
Curr episode timestep = 16
Action ignored: Workspace boundary
Scene graph at timestep 595 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 595 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 595 is 1
Human Feedback received at timestep 595 of 1
Current timestep = 596. State = [[-0.24518795 -0.18778825  0.16174212  1.        ]]. Action = [[-0.27673483  0.50973654 -0.9253323   0.64290833]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 596 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 596 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 596 is -1
Human Feedback received at timestep 596 of -1
Current timestep = 597. State = [[-0.24527201 -0.17927963  0.13429777  1.        ]]. Action = [[-0.7337776  -0.7088883  -0.39595526  0.5313095 ]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 597 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 597 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 597 is 1
Human Feedback received at timestep 597 of 1
Current timestep = 598. State = [[-0.24412556 -0.17811365  0.13257374  1.        ]]. Action = [[-0.8059718   0.03187311  0.5059166   0.8467907 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 598 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 598 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 598 is 1
Human Feedback received at timestep 598 of 1
Current timestep = 599. State = [[-0.24167803 -0.16120014  0.12149757  1.        ]]. Action = [[ 0.2683704   0.9401729  -0.90618616  0.7922578 ]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 599 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 599 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 599 is -1
Human Feedback received at timestep 599 of -1
Current timestep = 600. State = [[-0.23226166 -0.14392659  0.09413285  1.        ]]. Action = [[ 0.3182845  -0.4097774   0.3698156   0.90321493]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 600 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 600 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 600 is 1
Human Feedback received at timestep 600 of 1
Current timestep = 601. State = [[-0.21658082 -0.14591733  0.10581113  1.        ]]. Action = [[0.5314491  0.02350748 0.76743996 0.5454807 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 601 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 601 is tensor(0.0105, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 601 is -1
Human Feedback received at timestep 601 of -1
Current timestep = 602. State = [[-0.2051781  -0.14609553  0.12075327  1.        ]]. Action = [[-0.9523122  -0.3755784  -0.62373614  0.5276799 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 602 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 602 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 602 is 1
Human Feedback received at timestep 602 of 1
Current timestep = 603. State = [[-0.20072484 -0.15752308  0.12725955  1.        ]]. Action = [[ 0.14719546 -0.66607845  0.20930433  0.65751624]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 603 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 603 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 603 is 1
Human Feedback received at timestep 603 of 1
Current timestep = 604. State = [[-0.20062833 -0.17913693  0.1332477   1.        ]]. Action = [[-0.38896942 -0.30740577 -0.12417769  0.8349    ]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 604 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 604 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 604 is 1
Human Feedback received at timestep 604 of 1
Current timestep = 605. State = [[-0.21331395 -0.17581812  0.12940426  1.        ]]. Action = [[-0.5541293   0.8501656  -0.15199298  0.8645034 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 605 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 605 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 605 is 1
Human Feedback received at timestep 605 of 1
Current timestep = 606. State = [[-0.23228566 -0.17976348  0.13568906  1.        ]]. Action = [[-0.566093  -0.8446283  0.6502211  0.8281729]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 606 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 606 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 606 is 1
Human Feedback received at timestep 606 of 1
Current timestep = 607. State = [[-0.2514377  -0.18821384  0.1437764   1.        ]]. Action = [[-0.1945669   0.27248418 -0.65389866  0.5077164 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 607 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 607 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 607 is -1
Human Feedback received at timestep 607 of -1
Current timestep = 608. State = [[-0.26011148 -0.18508679  0.13843007  1.        ]]. Action = [[-0.30258483 -0.31695324 -0.6948263   0.6880388 ]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 608 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 608 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 608 is 1
Human Feedback received at timestep 608 of 1
Current timestep = 609. State = [[-0.25975525 -0.20112     0.14492957  1.        ]]. Action = [[ 0.2245313  -0.90479743  0.58795214  0.9074409 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 609 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 609 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 609 is 1
Human Feedback received at timestep 609 of 1
Current timestep = 610. State = [[-0.2599042  -0.21902825  0.15043458  1.        ]]. Action = [[-0.77122235 -0.9682835   0.17690241  0.7486447 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 610 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 610 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 610 is 1
Human Feedback received at timestep 610 of 1
Current timestep = 611. State = [[-0.2526583  -0.23513491  0.14562778  1.        ]]. Action = [[ 0.50516415 -0.8118659  -0.7477409   0.64698017]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 611 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 611 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 611 is -1
Human Feedback received at timestep 611 of -1
Current timestep = 612. State = [[-0.24249129 -0.2603209   0.13080367  1.        ]]. Action = [[-0.9385317  -0.18270743 -0.15972096  0.40968573]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 612 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 612 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 612 is -1
Human Feedback received at timestep 612 of -1
Current timestep = 613. State = [[-0.23174419 -0.2681156   0.12465654  1.        ]]. Action = [[ 0.5714083  -0.30757624 -0.34406316  0.7281537 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 613 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 613 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 613 is 1
Human Feedback received at timestep 613 of 1
Current timestep = 614. State = [[-0.21728437 -0.27721047  0.11295602  1.        ]]. Action = [[-0.35137892 -0.93081236  0.9421611   0.7900964 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 614 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 614 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 614 is -1
Human Feedback received at timestep 614 of -1
Current timestep = 615. State = [[-0.21468289 -0.27850598  0.11288835  1.        ]]. Action = [[-0.9471987  -0.39879495 -0.23011166  0.66370964]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 615 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 615 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 615 is -1
Human Feedback received at timestep 615 of -1
Current timestep = 616. State = [[-0.2261844  -0.27387378  0.10704394  1.        ]]. Action = [[-0.8546242   0.41793728 -0.3748362   0.87828386]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 616 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 616 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 616 is -1
Human Feedback received at timestep 616 of -1
Current timestep = 617. State = [[-0.24288273 -0.27310163  0.1091447   1.        ]]. Action = [[-0.43320096 -0.1524458   0.94032705  0.63135886]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 617 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 617 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 617 is -1
Human Feedback received at timestep 617 of -1
Current timestep = 618. State = [[-0.2549053  -0.27173558  0.1245518   1.        ]]. Action = [[-0.9168393  -0.91863805  0.00676703  0.65915203]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 618 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 618 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 618 is 1
Human Feedback received at timestep 618 of 1
Current timestep = 619. State = [[-0.25480348 -0.27278668  0.12622198  1.        ]]. Action = [[-0.28911418 -0.76025885  0.80467045  0.8310158 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 619 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 619 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 619 is 1
Human Feedback received at timestep 619 of 1
Current timestep = 620. State = [[-0.25449723 -0.27303314  0.12696181  1.        ]]. Action = [[-0.83955467 -0.41325688  0.59460783  0.80899096]]. Reward = [0.]
Curr episode timestep = 41
Action ignored: Workspace boundary
Scene graph at timestep 620 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 620 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 620 is 1
Human Feedback received at timestep 620 of 1
Current timestep = 621. State = [[-0.25435627 -0.27325097  0.12752259  1.        ]]. Action = [[-0.8898025  0.920321   0.7649069  0.8187218]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 621 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 621 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 621 is -1
Human Feedback received at timestep 621 of -1
Current timestep = 622. State = [[-0.2542949  -0.27335095  0.12769727  1.        ]]. Action = [[ 0.4897083  -0.93660086 -0.08639055  0.8527391 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 622 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 622 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 622 is 1
Human Feedback received at timestep 622 of 1
Current timestep = 623. State = [[-0.25805733 -0.28253695  0.14213575  1.        ]]. Action = [[-0.139049   -0.39916152  0.8434777   0.94278955]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 623 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 623 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 623 is 1
Human Feedback received at timestep 623 of 1
Current timestep = 624. State = [[-0.2640714  -0.291938    0.16421346  1.        ]]. Action = [[-0.66198516 -0.64543086  0.02206779  0.7109561 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 624 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 624 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 624 is 1
Human Feedback received at timestep 624 of 1
Current timestep = 625. State = [[-0.26618403 -0.2940779   0.16548176  1.        ]]. Action = [[-0.37990308 -0.01774186  0.59983444  0.74781394]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 625 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 625 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 625 is 1
Human Feedback received at timestep 625 of 1
Current timestep = 626. State = [[-0.26668647 -0.29457125  0.1657817   1.        ]]. Action = [[-0.30014068 -0.03763944 -0.9317268   0.35648167]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 626 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 626 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 626 is 1
Human Feedback received at timestep 626 of 1
Current timestep = 627. State = [[-0.2673028  -0.29480046  0.16594921  1.        ]]. Action = [[-0.04281217 -0.46447527  0.43833506  0.563051  ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 627 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 627 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 627 is 1
Human Feedback received at timestep 627 of 1
Current timestep = 628. State = [[-0.26743296 -0.29500902  0.16602923  1.        ]]. Action = [[-0.5422823  -0.63935584  0.830698    0.7250663 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 628 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 628 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 628 is -1
Human Feedback received at timestep 628 of -1
Current timestep = 629. State = [[-0.25416672 -0.28739887  0.16752005  1.        ]]. Action = [[ 0.9767984   0.2788235  -0.03268617  0.75850916]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 629 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 629 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 629 is -1
Human Feedback received at timestep 629 of -1
Current timestep = 630. State = [[-0.24113974 -0.28016624  0.16721855  1.        ]]. Action = [[-0.68353915  0.3165623   0.71008766  0.3497703 ]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 630 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 630 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 630 is -1
Human Feedback received at timestep 630 of -1
Current timestep = 631. State = [[-0.23780504 -0.2779264   0.16677338  1.        ]]. Action = [[ 0.28896165 -0.6277972   0.44904613  0.9417894 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 631 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 631 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 631 is 1
Human Feedback received at timestep 631 of 1
Current timestep = 632. State = [[-0.23644684 -0.27630413  0.1663107   1.        ]]. Action = [[-0.9203026  -0.37104863  0.6373272   0.7383342 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 632 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 632 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 632 is -1
Human Feedback received at timestep 632 of -1
Current timestep = 633. State = [[-0.23617283 -0.27595845  0.16620913  1.        ]]. Action = [[-0.79887736 -0.5174786  -0.36137193  0.55367684]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 633 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 633 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 633 is -1
Human Feedback received at timestep 633 of -1
Current timestep = 634. State = [[-0.22660276 -0.27506414  0.15929128  1.        ]]. Action = [[ 0.5441947  -0.07023501 -0.5931336   0.93065953]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 634 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 634 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 634 is -1
Human Feedback received at timestep 634 of -1
Current timestep = 635. State = [[-0.21401343 -0.27598426  0.1493567   1.        ]]. Action = [[-0.908357   -0.6107312  -0.32622695  0.72795606]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 635 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 635 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 635 is 1
Human Feedback received at timestep 635 of 1
Current timestep = 636. State = [[-0.21971263 -0.2773875   0.14837947  1.        ]]. Action = [[-0.58605963 -0.01214159  0.08931077  0.5714452 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 636 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 636 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 636 is 1
Human Feedback received at timestep 636 of 1
Current timestep = 637. State = [[-0.2288462  -0.26854506  0.15927303  1.        ]]. Action = [[-0.48547792  0.7212937   0.88114357  0.39672303]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 637 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 637 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 637 is -1
Human Feedback received at timestep 637 of -1
Current timestep = 638. State = [[-0.24426062 -0.2549677   0.17866318  1.        ]]. Action = [[-0.78769517 -0.49966693 -0.59777117  0.60781384]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 638 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 638 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 638 is -1
Human Feedback received at timestep 638 of -1
Current timestep = 639. State = [[-0.24989466  0.00283775  0.23290339  1.        ]]. Action = [[ 0.37098837 -0.60421824 -0.8025064  -0.19975793]]. Reward = [-1.]
Curr episode timestep = 60
Scene graph at timestep 639 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 639 is tensor(0.0162, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 639 is -1
Human Feedback received at timestep 639 of -1
Current timestep = 640. State = [[-2.4831280e-01 -3.5961632e-05  2.3430048e-01  1.0000000e+00]]. Action = [[-0.6313478  -0.88913524  0.6826134   0.87927675]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 640 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 640 is tensor(0.0123, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 640 is 1
Human Feedback received at timestep 640 of 1
Current timestep = 641. State = [[-0.24819155 -0.0014736   0.23450504  1.        ]]. Action = [[-0.898105   -0.38196504 -0.06550455  0.8020966 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 641 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 641 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 641 is 1
Human Feedback received at timestep 641 of 1
Current timestep = 642. State = [[-0.24802864 -0.00227433  0.23468988  1.        ]]. Action = [[-0.66894764  0.32302248 -0.7254316   0.43707323]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 642 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 642 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 642 is -1
Human Feedback received at timestep 642 of -1
Current timestep = 643. State = [[-0.24792016 -0.0027638   0.23477204  1.        ]]. Action = [[-0.7019002  -0.90210754  0.41706705  0.36670518]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 643 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 643 is tensor(0.0076, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 643 is 1
Human Feedback received at timestep 643 of 1
Current timestep = 644. State = [[-0.24821867 -0.00306253  0.22650334  1.        ]]. Action = [[ 0.14713383 -0.03934562 -0.80432475  0.93316245]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 644 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 644 is tensor(0.0098, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 644 is 1
Human Feedback received at timestep 644 of 1
Current timestep = 645. State = [[-0.2470431  -0.00337669  0.21031672  1.        ]]. Action = [[-0.83621186  0.5492822   0.5969465   0.44389725]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 645 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 645 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 645 is 1
Human Feedback received at timestep 645 of 1
Current timestep = 646. State = [[-0.24694382 -0.02256876  0.20515856  1.        ]]. Action = [[-0.11203706 -0.9335086  -0.28446794  0.7925103 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 646 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 646 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 646 is 1
Human Feedback received at timestep 646 of 1
Current timestep = 647. State = [[-0.23413706 -0.05585941  0.19897951  1.        ]]. Action = [[ 0.85503256 -0.65514266  0.12007725  0.41096377]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 647 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 647 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 647 is 1
Human Feedback received at timestep 647 of 1
Current timestep = 648. State = [[-0.22092396 -0.07701667  0.19810577  1.        ]]. Action = [[-0.35267162 -0.06745207  0.19235826  0.2923491 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 648 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 648 is tensor(0.0099, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 648 is -1
Human Feedback received at timestep 648 of -1
Current timestep = 649. State = [[-0.23193333 -0.07910369  0.19031543  1.        ]]. Action = [[-0.59784216  0.2578603  -0.7101122   0.5981369 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 649 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 649 is tensor(0.0120, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 649 is -1
Human Feedback received at timestep 649 of -1
Current timestep = 650. State = [[-0.23308699 -0.09032806  0.17927301  1.        ]]. Action = [[ 0.8436177  -0.7688809  -0.06269044  0.91033554]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 650 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 650 is tensor(0.0080, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 650 is -1
Human Feedback received at timestep 650 of -1
Current timestep = 651. State = [[-0.22224729 -0.10560623  0.16624565  1.        ]]. Action = [[-0.27806228  0.0034461  -0.47099578  0.7761111 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 651 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 651 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 651 is -1
Human Feedback received at timestep 651 of -1
Current timestep = 652. State = [[-0.2317877  -0.10821755  0.1481959   1.        ]]. Action = [[-0.6606706  -0.02375096 -0.24222934  0.37599146]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 652 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 652 is tensor(0.0127, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 652 is -1
Human Feedback received at timestep 652 of -1
Current timestep = 653. State = [[-0.24188258 -0.12173291  0.13067327  1.        ]]. Action = [[ 0.45835185 -0.7279623  -0.93925524  0.7233523 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 653 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 653 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 653 is 1
Human Feedback received at timestep 653 of 1
Current timestep = 654. State = [[-0.24281165 -0.14284201  0.09789971  1.        ]]. Action = [[-0.7139593  -0.8338184  -0.01562101  0.27988052]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 654 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 654 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 654 is 1
Human Feedback received at timestep 654 of 1
Current timestep = 655. State = [[-0.23666455 -0.14398116  0.10324974  1.        ]]. Action = [[0.23798835 0.10557139 0.82458115 0.7803476 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 655 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 655 is tensor(0.0089, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 655 is -1
Human Feedback received at timestep 655 of -1
Current timestep = 656. State = [[-0.23794295 -0.16096574  0.1175576   1.        ]]. Action = [[-0.5682394  -0.87572336  0.17569542  0.5991676 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 656 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 656 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 656 is 1
Human Feedback received at timestep 656 of 1
Current timestep = 657. State = [[-0.24353129 -0.17994769  0.12350161  1.        ]]. Action = [[-0.5749036  -0.8354918   0.50307536  0.6956842 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 657 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 657 is tensor(0.0083, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 657 is 1
Human Feedback received at timestep 657 of 1
Current timestep = 658. State = [[-0.24863108 -0.18177201  0.1251357   1.        ]]. Action = [[-0.24314594  0.23201334  0.082546    0.69482064]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 658 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 658 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 658 is 1
Human Feedback received at timestep 658 of 1
Current timestep = 659. State = [[-0.25393927 -0.17934279  0.12789138  1.        ]]. Action = [[-0.8008614   0.5553359  -0.71429586  0.6573887 ]]. Reward = [0.]
Curr episode timestep = 19
Action ignored: Workspace boundary
Scene graph at timestep 659 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 659 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 659 is -1
Human Feedback received at timestep 659 of -1
Current timestep = 660. State = [[-0.25417593 -0.17908353  0.12964201  1.        ]]. Action = [[-0.29910886  0.7346971  -0.8906648   0.72096014]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 660 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 660 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 660 is -1
Human Feedback received at timestep 660 of -1
Current timestep = 661. State = [[-0.2546825  -0.18554358  0.12635294  1.        ]]. Action = [[ 0.26127863 -0.43383503 -0.47612095  0.702585  ]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 661 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 661 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 661 is 1
Human Feedback received at timestep 661 of 1
Current timestep = 662. State = [[-0.2549341  -0.1925195   0.12155367  1.        ]]. Action = [[-0.77360225 -0.01935101 -0.77311945  0.7219918 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 662 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 662 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 662 is -1
Human Feedback received at timestep 662 of -1
Current timestep = 663. State = [[-0.25491154 -0.19277747  0.12152892  1.        ]]. Action = [[-0.34842712  0.06894827 -0.03269625  0.51372695]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 663 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 663 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 663 is -1
Human Feedback received at timestep 663 of -1
Current timestep = 664. State = [[-0.2549193  -0.19286929  0.12152897  1.        ]]. Action = [[-0.4153338   0.18338883  0.5430591   0.708647  ]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 664 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 664 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 664 is -1
Human Feedback received at timestep 664 of -1
Current timestep = 665. State = [[-0.2549193  -0.19286929  0.12152897  1.        ]]. Action = [[-0.93912077 -0.41932595  0.20598447  0.76407456]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 665 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 665 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 665 is 1
Human Feedback received at timestep 665 of 1
Current timestep = 666. State = [[-0.25121167 -0.18098384  0.1271468   1.        ]]. Action = [[0.06423211 0.789026   0.6321267  0.633466  ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 666 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 666 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 666 is -1
Human Feedback received at timestep 666 of -1
Current timestep = 667. State = [[-0.2375637  -0.15170537  0.13529177  1.        ]]. Action = [[ 0.70937634  0.6829717  -0.02113575  0.49750578]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 667 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 667 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 667 is -1
Human Feedback received at timestep 667 of -1
Current timestep = 668. State = [[-0.2244839  -0.1330596   0.13707718  1.        ]]. Action = [[-0.61610043 -0.9199819   0.7555423   0.8346082 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 668 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 668 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 668 is 1
Human Feedback received at timestep 668 of 1
Current timestep = 669. State = [[-0.22432818 -0.14618042  0.1385907   1.        ]]. Action = [[-0.18717533 -0.9405792  -0.05494756  0.4467528 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 669 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 669 is tensor(0.0082, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 669 is 1
Human Feedback received at timestep 669 of 1
Current timestep = 670. State = [[-0.2331822  -0.1726868   0.13136962  1.        ]]. Action = [[-0.5365527  -0.38657916 -0.6324352   0.7344662 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 670 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 670 is tensor(0.0091, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 670 is 1
Human Feedback received at timestep 670 of 1
Current timestep = 671. State = [[-0.23974596 -0.18660642  0.12079579  1.        ]]. Action = [[-0.90683895 -0.08430135  0.47068822  0.8355303 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 671 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 671 is tensor(0.0070, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 671 is 1
Human Feedback received at timestep 671 of 1
Current timestep = 672. State = [[-0.23972164 -0.18844824  0.1207779   1.        ]]. Action = [[-0.7189834  -0.88958555 -0.5748749   0.6532612 ]]. Reward = [0.]
Curr episode timestep = 32
Action ignored: Workspace boundary
Scene graph at timestep 672 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 672 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 672 is -1
Human Feedback received at timestep 672 of -1
Current timestep = 673. State = [[-0.23975916 -0.18858351  0.1207712   1.        ]]. Action = [[-0.66403437 -0.9418988   0.08780205  0.78724885]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 673 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 673 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 673 is 1
Human Feedback received at timestep 673 of 1
Current timestep = 674. State = [[-0.23982908 -0.18855748  0.12075409  1.        ]]. Action = [[-0.5523786  -0.78162825  0.6049266   0.90562785]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Scene graph at timestep 674 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 674 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 674 is 1
Human Feedback received at timestep 674 of 1
Current timestep = 675. State = [[-0.23982908 -0.18855748  0.12075409  1.        ]]. Action = [[-0.97533107  0.7096944  -0.15234941  0.82044303]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 675 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 675 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 675 is -1
Human Feedback received at timestep 675 of -1
Current timestep = 676. State = [[-0.23719966 -0.19348507  0.13094847  1.        ]]. Action = [[ 0.18428886 -0.1989271   0.88862205  0.3025726 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 676 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 676 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 676 is -1
Human Feedback received at timestep 676 of -1
Current timestep = 677. State = [[-0.23514771 -0.19754338  0.14314418  1.        ]]. Action = [[-0.7637606   0.18591094 -0.05893201  0.74072075]]. Reward = [0.]
Curr episode timestep = 37
Action ignored: Workspace boundary
Scene graph at timestep 677 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 677 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 677 is -1
Human Feedback received at timestep 677 of -1
Current timestep = 678. State = [[-0.23533669 -0.19771685  0.14318745  1.        ]]. Action = [[-0.6274204  0.6024444  0.5495771  0.6435137]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 678 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 678 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 678 is -1
Human Feedback received at timestep 678 of -1
Current timestep = 679. State = [[-0.23544605 -0.18540347  0.15629412  1.        ]]. Action = [[-0.16199762  0.7832463   0.87197125  0.69772947]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 679 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 679 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 679 is -1
Human Feedback received at timestep 679 of -1
Current timestep = 680. State = [[-0.23815542 -0.17597649  0.18198411  1.        ]]. Action = [[ 0.04967773 -0.3491199  -0.13055372  0.7863517 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 680 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 680 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 680 is -1
Human Feedback received at timestep 680 of -1
Current timestep = 681. State = [[-0.22572586 -0.1938138   0.19112659  1.        ]]. Action = [[ 0.9840102  -0.88561225  0.3822416   0.55608857]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 681 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 681 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 681 is 1
Human Feedback received at timestep 681 of 1
Current timestep = 682. State = [[-0.21051633 -0.21202146  0.19364914  1.        ]]. Action = [[-0.07096678  0.04026747 -0.79146266  0.7613771 ]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 682 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 682 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 682 is -1
Human Feedback received at timestep 682 of -1
Current timestep = 683. State = [[-0.19393238 -0.22065799  0.18574399  1.        ]]. Action = [[ 0.59948945 -0.2543534   0.39132142  0.4262445 ]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 683 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 683 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 683 is -1
Human Feedback received at timestep 683 of -1
Current timestep = 684. State = [[-0.1792068  -0.20879826  0.18044986  1.        ]]. Action = [[ 0.26099634  0.8978573  -0.84063876  0.8735393 ]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 684 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 684 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 684 is -1
Human Feedback received at timestep 684 of -1
Current timestep = 685. State = [[-0.17001684 -0.21184632  0.17009471  1.        ]]. Action = [[-0.24042279 -0.9245058   0.71701527  0.60916483]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 685 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 685 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 685 is -1
Human Feedback received at timestep 685 of -1
Current timestep = 686. State = [[-0.1717733  -0.22293523  0.17669004  1.        ]]. Action = [[-0.05560982  0.3131156   0.06106925  0.77676296]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 686 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 686 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 686 is -1
Human Feedback received at timestep 686 of -1
Current timestep = 687. State = [[-0.17642526 -0.23422943  0.19143112  1.        ]]. Action = [[-0.29954958 -0.58186024  0.7318373   0.8336947 ]]. Reward = [0.]
Curr episode timestep = 47
Scene graph at timestep 687 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 687 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 687 is -1
Human Feedback received at timestep 687 of -1
Current timestep = 688. State = [[-0.19036944 -0.24485996  0.20547026  1.        ]]. Action = [[-0.507897    0.05269861 -0.5720428   0.699739  ]]. Reward = [0.]
Curr episode timestep = 48
Scene graph at timestep 688 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 688 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 688 is -1
Human Feedback received at timestep 688 of -1
Current timestep = 689. State = [[-0.20533793 -0.24570513  0.20052911  1.        ]]. Action = [[-0.31459552  0.05525494 -0.07159281  0.60956454]]. Reward = [0.]
Curr episode timestep = 49
Scene graph at timestep 689 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 689 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 689 is -1
Human Feedback received at timestep 689 of -1
Current timestep = 690. State = [[-0.20347413 -0.26106244  0.20784235  1.        ]]. Action = [[ 0.84419465 -0.9645545   0.62582946  0.5374266 ]]. Reward = [0.]
Curr episode timestep = 50
Scene graph at timestep 690 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 690 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 690 is -1
Human Feedback received at timestep 690 of -1
Current timestep = 691. State = [[-0.19719845 -0.2930003   0.21724108  1.        ]]. Action = [[-0.24690801 -0.5669612  -0.22569704  0.5086607 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 691 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 691 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 691 is 1
Human Feedback received at timestep 691 of 1
Current timestep = 692. State = [[-0.20087034 -0.30991867  0.21516167  1.        ]]. Action = [[ 0.02890158 -0.87896687 -0.24861789  0.8087232 ]]. Reward = [0.]
Curr episode timestep = 52
Action ignored: Workspace boundary
Scene graph at timestep 692 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 692 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 692 is 1
Human Feedback received at timestep 692 of 1
Current timestep = 693. State = [[-0.20125371 -0.31115198  0.21516258  1.        ]]. Action = [[ 0.54965186 -0.69395185 -0.31707406  0.90763867]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 693 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 693 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 693 is -1
Human Feedback received at timestep 693 of -1
Current timestep = 694. State = [[-0.20125371 -0.31115198  0.21516258  1.        ]]. Action = [[-0.6034599  -0.24356157  0.16881454  0.13315105]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 694 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 694 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 694 is -1
Human Feedback received at timestep 694 of -1
Current timestep = 695. State = [[-0.19799253 -0.311905    0.22661515  1.        ]]. Action = [[0.11071384 0.06210876 0.88014317 0.14830148]]. Reward = [0.]
Curr episode timestep = 55
Scene graph at timestep 695 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 695 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 695 is -1
Human Feedback received at timestep 695 of -1
Current timestep = 696. State = [[-0.2065564  -0.30761018  0.25427932  1.        ]]. Action = [[-0.9569475   0.53936505  0.6271157   0.80911624]]. Reward = [0.]
Curr episode timestep = 56
Scene graph at timestep 696 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 696 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 696 is -1
Human Feedback received at timestep 696 of -1
Current timestep = 697. State = [[-0.23698811 -0.30074313  0.278328    1.        ]]. Action = [[-0.70367867  0.06244814 -0.03390634  0.6030563 ]]. Reward = [0.]
Curr episode timestep = 57
Scene graph at timestep 697 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 697 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 697 is -1
Human Feedback received at timestep 697 of -1
Current timestep = 698. State = [[-0.25176534 -0.28653923  0.2754667   1.        ]]. Action = [[ 0.4770224   0.4236499  -0.5244789   0.06717992]]. Reward = [0.]
Curr episode timestep = 58
Scene graph at timestep 698 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 698 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 698 is -1
Human Feedback received at timestep 698 of -1
Current timestep = 699. State = [[-0.24874116 -0.27758044  0.2700855   1.        ]]. Action = [[-0.56274086  0.39168823  0.80821776  0.4851495 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 699 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 699 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 699 is -1
Human Feedback received at timestep 699 of -1
Current timestep = 700. State = [[-0.24690484 -0.26158065  0.27611166  1.        ]]. Action = [[-0.2099272   0.7540231   0.65288174  0.18340921]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 700 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 700 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 700 is -1
Human Feedback received at timestep 700 of -1
Current timestep = 701. State = [[-0.2502982  -0.24124715  0.2858634   1.        ]]. Action = [[-0.6450782   0.8951924  -0.2578721  -0.03939736]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 701 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 701 is tensor(0.0046, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 701 is -1
Human Feedback received at timestep 701 of -1
Current timestep = 702. State = [[-0.2525284  -0.22247918  0.29367146  1.        ]]. Action = [[-0.18966436  0.7804599   0.38798356  0.8208592 ]]. Reward = [0.]
Curr episode timestep = 62
Scene graph at timestep 702 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 702 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 702 is -1
Human Feedback received at timestep 702 of -1
Current timestep = 703. State = [[-0.24884813 -0.20741856  0.30207506  1.        ]]. Action = [[ 0.9546801  -0.41576135 -0.42531067  0.6621585 ]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 703 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 703 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 703 is -1
Human Feedback received at timestep 703 of -1
Current timestep = 704. State = [[-0.23139523 -0.21096815  0.2958608   1.        ]]. Action = [[-0.84640604  0.3735757   0.05098796  0.8429661 ]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Scene graph at timestep 704 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 704 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 704 is -1
Human Feedback received at timestep 704 of -1
Current timestep = 705. State = [[-0.2244098  -0.225723    0.28666112  1.        ]]. Action = [[ 0.35948372 -0.7397272  -0.7881047   0.8150332 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 705 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 705 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 705 is -1
Human Feedback received at timestep 705 of -1
Current timestep = 706. State = [[-0.20025887 -0.25239736  0.26167637  1.        ]]. Action = [[ 0.74435985 -0.50783414 -0.12892371  0.8768029 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 706 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 706 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 706 is -1
Human Feedback received at timestep 706 of -1
Current timestep = 707. State = [[-0.16992982 -0.273831    0.24447274  1.        ]]. Action = [[ 0.7843723  -0.50737476 -0.76822656  0.6315943 ]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 707 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 707 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 707 is -1
Human Feedback received at timestep 707 of -1
Current timestep = 708. State = [[-0.1429858  -0.29054952  0.21932533  1.        ]]. Action = [[-0.1576305  -0.89032865 -0.5793283   0.5004935 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 708 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 708 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 708 is -1
Human Feedback received at timestep 708 of -1
Current timestep = 709. State = [[-0.14165759 -0.30120575  0.21329023  1.        ]]. Action = [[-0.38849026 -0.29968804 -0.21676648  0.51531315]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 709 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 709 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 709 is -1
Human Feedback received at timestep 709 of -1
Current timestep = 710. State = [[-0.14480543 -0.30955815  0.21033697  1.        ]]. Action = [[-0.85247576 -0.6544828  -0.10781497  0.7223407 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 710 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 710 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 710 is 1
Human Feedback received at timestep 710 of 1
Current timestep = 711. State = [[-0.1450132  -0.31030464  0.21033749  1.        ]]. Action = [[-0.24678731 -0.30812204  0.8367505  -0.06222415]]. Reward = [0.]
Curr episode timestep = 71
Action ignored: Workspace boundary
Scene graph at timestep 711 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 711 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 711 is -1
Human Feedback received at timestep 711 of -1
Current timestep = 712. State = [[-0.1450132  -0.31030464  0.21033749  1.        ]]. Action = [[-0.82031506 -0.18802667  0.9373839   0.78119004]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Scene graph at timestep 712 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 712 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 712 is -1
Human Feedback received at timestep 712 of -1
Current timestep = 713. State = [[-0.1450132  -0.31030464  0.21033749  1.        ]]. Action = [[ 0.06777322 -0.64209545  0.78414416  0.80764794]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Scene graph at timestep 713 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 713 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 713 is 1
Human Feedback received at timestep 713 of 1
Current timestep = 714. State = [[-0.1355079  -0.30028394  0.20296927  1.        ]]. Action = [[ 0.78946114  0.47046125 -0.3484943   0.4399829 ]]. Reward = [0.]
Curr episode timestep = 74
Scene graph at timestep 714 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 714 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 714 is -1
Human Feedback received at timestep 714 of -1
Current timestep = 715. State = [[-0.11225556 -0.27710393  0.19022101  1.        ]]. Action = [[ 0.28241503  0.85867    -0.03821898  0.7906227 ]]. Reward = [0.]
Curr episode timestep = 75
Scene graph at timestep 715 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 715 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 715 is -1
Human Feedback received at timestep 715 of -1
Current timestep = 716. State = [[-0.10738259 -0.25085557  0.18366408  1.        ]]. Action = [[-0.41380847  0.34633946 -0.35904264  0.17255378]]. Reward = [0.]
Curr episode timestep = 76
Scene graph at timestep 716 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 716 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 716 is 1
Human Feedback received at timestep 716 of 1
Current timestep = 717. State = [[-0.11669689 -0.23456663  0.16864006  1.        ]]. Action = [[-0.54783016  0.3748815  -0.24519217  0.54392064]]. Reward = [0.]
Curr episode timestep = 77
Scene graph at timestep 717 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 717 is tensor(0.0101, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 717 is 1
Human Feedback received at timestep 717 of 1
Current timestep = 718. State = [[-0.13743976 -0.21696131  0.14918052  1.        ]]. Action = [[-0.29458523  0.29779208 -0.60980594  0.7253635 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 718 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 718 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 718 is -1
Human Feedback received at timestep 718 of -1
Current timestep = 719. State = [[-0.15633002 -0.21099152  0.12723711  1.        ]]. Action = [[-0.3613906  -0.25668752 -0.37101316  0.44988585]]. Reward = [0.]
Curr episode timestep = 79
Scene graph at timestep 719 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 719 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 719 is 1
Human Feedback received at timestep 719 of 1
Current timestep = 720. State = [[-0.17679009 -0.22289404  0.12196852  1.        ]]. Action = [[-0.76451206 -0.41996467  0.55435765  0.69760716]]. Reward = [0.]
Curr episode timestep = 80
Scene graph at timestep 720 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 720 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 720 is 1
Human Feedback received at timestep 720 of 1
Current timestep = 721. State = [[-0.18422687 -0.22253859  0.14094807  1.        ]]. Action = [[0.6138549  0.49104357 0.7828566  0.5348443 ]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 721 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 721 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 721 is -1
Human Feedback received at timestep 721 of -1
Current timestep = 722. State = [[-0.17815953 -0.22720785  0.15370002  1.        ]]. Action = [[ 0.07618964 -0.7697174  -0.48934603  0.76385427]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 722 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 722 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 722 is -1
Human Feedback received at timestep 722 of -1
Current timestep = 723. State = [[-0.18027003 -0.25530374  0.16091977  1.        ]]. Action = [[-0.03772169 -0.7512942   0.7661722   0.7454171 ]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 723 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 723 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 723 is 1
Human Feedback received at timestep 723 of 1
Current timestep = 724. State = [[-0.19029866 -0.26584774  0.1768876   1.        ]]. Action = [[-0.6983942   0.5487652   0.2572559   0.67786133]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 724 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 724 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 724 is 1
Human Feedback received at timestep 724 of 1
Current timestep = 725. State = [[-0.21537125 -0.269995    0.17679138  1.        ]]. Action = [[-0.51582694 -0.42452025 -0.8444584   0.7655544 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 725 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 725 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 725 is 1
Human Feedback received at timestep 725 of 1
Current timestep = 726. State = [[-0.2255977  -0.28166628  0.16787362  1.        ]]. Action = [[-0.8916437  -0.7812092  -0.79113716  0.9425447 ]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Scene graph at timestep 726 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 726 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 726 is 1
Human Feedback received at timestep 726 of 1
Current timestep = 727. State = [[-0.23424582 -0.28116083  0.1775224   1.        ]]. Action = [[-0.4538331   0.16144514  0.958825    0.97608256]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 727 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 727 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 727 is -1
Human Feedback received at timestep 727 of -1
Current timestep = 728. State = [[-0.24717347 -0.27717537  0.19334319  1.        ]]. Action = [[-0.6382331  -0.26142502  0.29126143  0.35451043]]. Reward = [0.]
Curr episode timestep = 88
Action ignored: Workspace boundary
Scene graph at timestep 728 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 728 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 728 is -1
Human Feedback received at timestep 728 of -1
Current timestep = 729. State = [[-0.25034994 -0.27761552  0.1934844   1.        ]]. Action = [[ 0.7150537  -0.5838479   0.37322235  0.8300638 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Scene graph at timestep 729 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 729 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 729 is -1
Human Feedback received at timestep 729 of -1
Current timestep = 730. State = [[-0.2525126  -0.2770931   0.19310322  1.        ]]. Action = [[-0.35427874 -0.4085281  -0.7017648   0.6097969 ]]. Reward = [0.]
Curr episode timestep = 90
Action ignored: Workspace boundary
Scene graph at timestep 730 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 730 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 730 is 1
Human Feedback received at timestep 730 of 1
Current timestep = 731. State = [[-0.2534774  -0.27501565  0.19181232  1.        ]]. Action = [[ 0.10912573  0.09863317 -0.29145026  0.82692456]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 731 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 731 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 731 is -1
Human Feedback received at timestep 731 of -1
Current timestep = 732. State = [[-0.25422722 -0.26639265  0.19294551  1.        ]]. Action = [[-0.14573759  0.33149445  0.29841065  0.6463133 ]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 732 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 732 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 732 is 1
Human Feedback received at timestep 732 of 1
Current timestep = 733. State = [[-0.2549884  -0.25806487  0.19590527  1.        ]]. Action = [[-0.04431975 -0.932118    0.68197465  0.8401942 ]]. Reward = [0.]
Curr episode timestep = 93
Action ignored: Workspace boundary
Scene graph at timestep 733 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 733 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 733 is 1
Human Feedback received at timestep 733 of 1
Current timestep = 734. State = [[-0.25479114 -0.25741255  0.19591708  1.        ]]. Action = [[-0.02835023 -0.88705784  0.03621697  0.80373454]]. Reward = [0.]
Curr episode timestep = 94
Action ignored: Workspace boundary
Scene graph at timestep 734 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 734 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 734 is -1
Human Feedback received at timestep 734 of -1
Current timestep = 735. State = [[-0.24972469  0.00276511  0.23293597  1.        ]]. Action = [[ 0.6791464  -0.8574891  -0.12148863 -0.1223253 ]]. Reward = [-1.]
Curr episode timestep = 95
Scene graph at timestep 735 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 735 is tensor(0.0173, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 735 is -1
Human Feedback received at timestep 735 of -1
Current timestep = 736. State = [[-0.24896394  0.00181707  0.23397686  1.        ]]. Action = [[-0.6278788  -0.49734032 -0.70107293  0.8428031 ]]. Reward = [0.]
Curr episode timestep = 0
Action ignored: Workspace boundary
Scene graph at timestep 736 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 736 is tensor(0.0143, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 736 is 1
Human Feedback received at timestep 736 of 1
Current timestep = 737. State = [[-0.24057765 -0.0034623   0.22816713  1.        ]]. Action = [[ 0.67101717 -0.23721075 -0.61598426  0.7643244 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 737 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 737 is tensor(0.0160, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 737 is -1
Human Feedback received at timestep 737 of -1
Current timestep = 738. State = [[-0.22505075 -0.007667    0.21672961  1.        ]]. Action = [[-0.8057327  -0.848389    0.5693393   0.49713445]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 738 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 738 is tensor(0.0125, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 738 is 1
Human Feedback received at timestep 738 of 1
Current timestep = 739. State = [[-0.21121494 -0.02688587  0.21856518  1.        ]]. Action = [[ 0.71999073 -0.8182655   0.13547337  0.77587306]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 739 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 739 is tensor(0.0137, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 739 is 1
Human Feedback received at timestep 739 of 1
Current timestep = 740. State = [[-0.20183077 -0.05718924  0.22715834  1.        ]]. Action = [[-0.7657395  -0.5124659   0.65045667  0.8693378 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 740 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 740 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 740 is 1
Human Feedback received at timestep 740 of 1
Current timestep = 741. State = [[-0.22363122 -0.08117425  0.24366254  1.        ]]. Action = [[-0.9814464  -0.26140702  0.19696856  0.5349624 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 741 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 741 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 741 is 1
Human Feedback received at timestep 741 of 1
Current timestep = 742. State = [[-0.25068006 -0.09108585  0.25297618  1.        ]]. Action = [[-0.7854509   0.0870831  -0.53644663  0.48588538]]. Reward = [0.]
Curr episode timestep = 6
Action ignored: Workspace boundary
Scene graph at timestep 742 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 742 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 742 is -1
Human Feedback received at timestep 742 of -1
Current timestep = 743. State = [[-0.25559166 -0.09309759  0.25305834  1.        ]]. Action = [[-0.7471682  -0.8355325   0.8860489   0.70045614]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 743 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 743 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 743 is 1
Human Feedback received at timestep 743 of 1
Current timestep = 744. State = [[-0.25644878 -0.09322308  0.25269488  1.        ]]. Action = [[-0.7144015  -0.758297   -0.34245294  0.81666493]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 744 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 744 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 744 is 1
Human Feedback received at timestep 744 of 1
Current timestep = 745. State = [[-0.25059357 -0.07945596  0.25039953  1.        ]]. Action = [[ 0.76493025  0.76086426 -0.29834336  0.5722635 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 745 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 745 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 745 is -1
Human Feedback received at timestep 745 of -1
Current timestep = 746. State = [[-0.23414789 -0.07618     0.25356257  1.        ]]. Action = [[ 0.13912702 -0.773704    0.5045476   0.8000884 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 746 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 746 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 746 is 1
Human Feedback received at timestep 746 of 1
Current timestep = 747. State = [[-0.23111354 -0.10286208  0.2610296   1.        ]]. Action = [[-0.11503124 -0.76116633 -0.14968562  0.6360483 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 747 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 747 is tensor(0.0065, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 747 is 1
Human Feedback received at timestep 747 of 1
Current timestep = 748. State = [[-0.2327687  -0.12132534  0.2651898   1.        ]]. Action = [[-0.7747326  -0.95979667  0.85458684  0.01924455]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 748 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 748 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 748 is 1
Human Feedback received at timestep 748 of 1
Current timestep = 749. State = [[-0.2384908  -0.13883787  0.2783477   1.        ]]. Action = [[-0.39553738 -0.6257246   0.9016026   0.6924343 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 749 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 749 is tensor(0.0075, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 749 is 1
Human Feedback received at timestep 749 of 1
Current timestep = 750. State = [[-0.2511087  -0.157826    0.30782464  1.        ]]. Action = [[-0.26405197 -0.08441848  0.7762308   0.1430037 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 750 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 750 is tensor(0.0066, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 750 is 1
Human Feedback received at timestep 750 of 1
Current timestep = 751. State = [[-0.25713772 -0.16481555  0.3317609   1.        ]]. Action = [[-0.68182     0.21593153  0.09726977  0.6038611 ]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 751 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 751 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 751 is -1
Human Feedback received at timestep 751 of -1
Current timestep = 752. State = [[-0.24616507 -0.16765761  0.33224827  1.        ]]. Action = [[ 0.8820255  -0.09167355 -0.4149431   0.7131016 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 752 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 752 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 752 is -1
Human Feedback received at timestep 752 of -1
Current timestep = 753. State = [[-0.22936082 -0.17884108  0.3232992   1.        ]]. Action = [[-0.05680907 -0.47279948 -0.32781088  0.49859858]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 753 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 753 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 753 is 1
Human Feedback received at timestep 753 of 1
Current timestep = 754. State = [[-0.23558982 -0.20252691  0.30557844  1.        ]]. Action = [[-0.57288176 -0.61075693 -0.8305579   0.73748124]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 754 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 754 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 754 is -1
Human Feedback received at timestep 754 of -1
Current timestep = 755. State = [[-0.2312713  -0.21264987  0.2897633   1.        ]]. Action = [[0.67819285 0.35801888 0.24173343 0.8972722 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 755 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 755 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 755 is -1
Human Feedback received at timestep 755 of -1
Current timestep = 756. State = [[-0.22409032 -0.20904864  0.2886985   1.        ]]. Action = [[-0.988218   -0.36149907  0.17965353  0.73206806]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 756 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 756 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 756 is -1
Human Feedback received at timestep 756 of -1
Current timestep = 757. State = [[-0.21079576 -0.22164728  0.28769138  1.        ]]. Action = [[ 0.8819066  -0.80697817 -0.16603762  0.55301785]]. Reward = [0.]
Curr episode timestep = 21
Scene graph at timestep 757 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 757 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 757 is -1
Human Feedback received at timestep 757 of -1
Current timestep = 758. State = [[-0.17399769 -0.22919312  0.28619194  1.        ]]. Action = [[0.52655077 0.58839893 0.4678948  0.9128063 ]]. Reward = [0.]
Curr episode timestep = 22
Scene graph at timestep 758 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 758 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 758 is -1
Human Feedback received at timestep 758 of -1
Current timestep = 759. State = [[-0.16278668 -0.22515713  0.288215    1.        ]]. Action = [[-0.23331046 -0.20047855 -0.46506047  0.6887262 ]]. Reward = [0.]
Curr episode timestep = 23
Scene graph at timestep 759 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 759 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 759 is -1
Human Feedback received at timestep 759 of -1
Current timestep = 760. State = [[-0.16299962 -0.23376594  0.28027984  1.        ]]. Action = [[ 0.11589301 -0.43322152  0.00492442  0.6624974 ]]. Reward = [0.]
Curr episode timestep = 24
Scene graph at timestep 760 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 760 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 760 is -1
Human Feedback received at timestep 760 of -1
Current timestep = 761. State = [[-0.15326348 -0.23729643  0.26918614  1.        ]]. Action = [[ 0.4923315   0.16123664 -0.57744956  0.62930083]]. Reward = [0.]
Curr episode timestep = 25
Scene graph at timestep 761 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 761 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 761 is -1
Human Feedback received at timestep 761 of -1
Current timestep = 762. State = [[-0.14075793 -0.2295839   0.26627484  1.        ]]. Action = [[-0.6244933   0.66845155  0.75830173  0.5475466 ]]. Reward = [0.]
Curr episode timestep = 26
Scene graph at timestep 762 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 762 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 762 is -1
Human Feedback received at timestep 762 of -1
Current timestep = 763. State = [[-0.14662142 -0.22900654  0.2816561   1.        ]]. Action = [[ 0.2038151  -0.65650535  0.14999282  0.77800035]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 763 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 763 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 763 is -1
Human Feedback received at timestep 763 of -1
Current timestep = 764. State = [[-0.15592058 -0.22375162  0.29558176  1.        ]]. Action = [[-0.7563978   0.8029851   0.4694749   0.90647364]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 764 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 764 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 764 is -1
Human Feedback received at timestep 764 of -1
Current timestep = 765. State = [[-0.16021504 -0.21918383  0.31999013  1.        ]]. Action = [[ 0.8108437  -0.601957    0.54045963  0.8479717 ]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 765 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 765 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 765 is -1
Human Feedback received at timestep 765 of -1
Current timestep = 766. State = [[-0.15331458 -0.23868418  0.33871195  1.        ]]. Action = [[ 0.03684783 -0.66044635  0.13656569  0.8027419 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 766 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 766 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 766 is -1
Human Feedback received at timestep 766 of -1
Current timestep = 767. State = [[-0.14011288 -0.2588612   0.34494177  1.        ]]. Action = [[ 0.69298077 -0.24957585 -0.02345008  0.9334518 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 767 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 767 is tensor(0.0064, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 767 is -1
Human Feedback received at timestep 767 of -1
Current timestep = 768. State = [[-0.11899068 -0.2661527   0.3556733   1.        ]]. Action = [[0.05715239 0.1252296  0.57256126 0.7850225 ]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 768 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 768 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 768 is -1
Human Feedback received at timestep 768 of -1
Current timestep = 769. State = [[-0.1145669  -0.27529415  0.35881415  1.        ]]. Action = [[ 0.12965047 -0.45922017 -0.78193355  0.8308128 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 769 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 769 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 769 is -1
Human Feedback received at timestep 769 of -1
Current timestep = 770. State = [[-0.11633629 -0.2839243   0.3389057   1.        ]]. Action = [[-0.769343    0.17698538 -0.50703776  0.74873173]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 770 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 770 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 770 is -1
Human Feedback received at timestep 770 of -1
Current timestep = 771. State = [[-0.12911873 -0.27741465  0.3163066   1.        ]]. Action = [[-0.11267227  0.46735597 -0.7916995   0.79359794]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 771 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 771 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 771 is 1
Human Feedback received at timestep 771 of 1
Current timestep = 772. State = [[-0.13292195 -0.25562933  0.28860193  1.        ]]. Action = [[ 0.05944777  0.75094247 -0.26663625  0.19587255]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 772 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 772 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 772 is -1
Human Feedback received at timestep 772 of -1
Current timestep = 773. State = [[-0.1476389  -0.25112742  0.27482712  1.        ]]. Action = [[-0.7134772  -0.8001875  -0.09205025  0.39890945]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 773 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 773 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 773 is -1
Human Feedback received at timestep 773 of -1
Current timestep = 774. State = [[-0.16719663 -0.25492498  0.27161518  1.        ]]. Action = [[-0.41214466  0.4657327   0.2694186   0.93577766]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 774 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 774 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 774 is 1
Human Feedback received at timestep 774 of 1
Current timestep = 775. State = [[-0.17928112 -0.2374435   0.2819808   1.        ]]. Action = [[-0.29284275  0.7522619   0.4375291   0.87625146]]. Reward = [0.]
Curr episode timestep = 39
Scene graph at timestep 775 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 775 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 775 is -1
Human Feedback received at timestep 775 of -1
Current timestep = 776. State = [[-0.19100063 -0.22452672  0.2886243   1.        ]]. Action = [[ 0.20435071 -0.50563097 -0.42276847  0.8414439 ]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 776 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 776 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 776 is -1
Human Feedback received at timestep 776 of -1
Current timestep = 777. State = [[-0.20847037 -0.2468148   0.27745804  1.        ]]. Action = [[-0.93757755 -0.77057457 -0.68801755  0.05251431]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 777 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 777 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 777 is -1
Human Feedback received at timestep 777 of -1
Current timestep = 778. State = [[-0.24007402 -0.26567954  0.26925227  1.        ]]. Action = [[-0.47404063  0.06328404  0.934129    0.35100007]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 778 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 778 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 778 is -1
Human Feedback received at timestep 778 of -1
Current timestep = 779. State = [[-0.25576168 -0.26912284  0.28278098  1.        ]]. Action = [[-0.69261503 -0.77114576  0.8259176   0.5956745 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 779 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 779 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 779 is -1
Human Feedback received at timestep 779 of -1
Current timestep = 780. State = [[-0.2650718  -0.28139737  0.28984973  1.        ]]. Action = [[-0.10641468 -0.66132355  0.25853825  0.76574326]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 780 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 780 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 780 is -1
Human Feedback received at timestep 780 of -1
Current timestep = 781. State = [[-0.27030393 -0.2933414   0.29733256  1.        ]]. Action = [[ 0.3428893  -0.49172133 -0.17371851  0.33787012]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 781 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 781 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 781 is -1
Human Feedback received at timestep 781 of -1
Current timestep = 782. State = [[-0.2637059  -0.28576663  0.30094463  1.        ]]. Action = [[0.5115503  0.521168   0.32506514 0.93267536]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 782 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 782 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 782 is 1
Human Feedback received at timestep 782 of 1
Current timestep = 783. State = [[-0.2557818  -0.27668557  0.30540946  1.        ]]. Action = [[-0.7993653  -0.39734447  0.00624752  0.6551881 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 783 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 783 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 783 is -1
Human Feedback received at timestep 783 of -1
Current timestep = 784. State = [[-0.25404766 -0.27588922  0.3065068   1.        ]]. Action = [[-0.49994588 -0.8343204   0.12771344  0.7757174 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 784 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 784 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 784 is -1
Human Feedback received at timestep 784 of -1
Current timestep = 785. State = [[-0.2501382   0.00261508  0.23267762  1.        ]]. Action = [[ 0.00497496 -0.5672173  -0.5819103  -0.30298233]]. Reward = [-1.]
Curr episode timestep = 49
Scene graph at timestep 785 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 785 is tensor(0.0198, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 785 is 1
Human Feedback received at timestep 785 of 1
Current timestep = 786. State = [[-0.24911994 -0.01057761  0.21401219  1.        ]]. Action = [[ 0.48488665 -0.6915523  -0.85233283  0.6055801 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 786 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 786 is tensor(0.0103, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 786 is 1
Human Feedback received at timestep 786 of 1
Current timestep = 787. State = [[-0.24489379 -0.02344401  0.18159114  1.        ]]. Action = [[-0.5742275   0.1922915  -0.32494318  0.6039604 ]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 787 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 787 is tensor(0.0112, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 787 is -1
Human Feedback received at timestep 787 of -1
Current timestep = 788. State = [[-0.2447063  -0.04514386  0.16867864  1.        ]]. Action = [[-0.04104728 -0.88062644 -0.8767081   0.31607008]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 788 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 788 is tensor(0.0058, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 788 is 1
Human Feedback received at timestep 788 of 1
Current timestep = 789. State = [[-0.24522714 -0.06514587  0.13922489  1.        ]]. Action = [[-0.6007078  -0.78424513 -0.2247991   0.77649736]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 789 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 789 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 789 is 1
Human Feedback received at timestep 789 of 1
Current timestep = 790. State = [[-0.24546725 -0.06748625  0.13563749  1.        ]]. Action = [[-0.8514092 -0.8283255 -0.7625502  0.6918349]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 790 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 790 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 790 is 1
Human Feedback received at timestep 790 of 1
Current timestep = 791. State = [[-0.24544916 -0.06758001  0.1356375   1.        ]]. Action = [[-0.7614206  -0.54049724 -0.1287176   0.76604223]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 791 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 791 is tensor(0.0039, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 791 is 1
Human Feedback received at timestep 791 of 1
Current timestep = 792. State = [[-0.24707519 -0.07965674  0.13996214  1.        ]]. Action = [[-0.30944395 -0.4552346   0.485983    0.459167  ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 792 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 792 is tensor(0.0049, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 792 is 1
Human Feedback received at timestep 792 of 1
Current timestep = 793. State = [[-0.24930455 -0.09177833  0.14482822  1.        ]]. Action = [[-0.878086   -0.5415647  -0.05322152  0.1976788 ]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 793 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 793 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 793 is 1
Human Feedback received at timestep 793 of 1
Current timestep = 794. State = [[-0.25069708 -0.09415025  0.14545348  1.        ]]. Action = [[-0.753996   -0.53079766 -0.1327663   0.8726938 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 794 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 794 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 794 is 1
Human Feedback received at timestep 794 of 1
Current timestep = 795. State = [[-0.25114948 -0.10672691  0.15939228  1.        ]]. Action = [[-0.12049443 -0.5203064   0.9648287   0.5509466 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 795 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 795 is tensor(0.0062, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 795 is 1
Human Feedback received at timestep 795 of 1
Current timestep = 796. State = [[-0.25514656 -0.11892214  0.18600787  1.        ]]. Action = [[-0.48642075 -0.25651044 -0.32999098  0.54784775]]. Reward = [0.]
Curr episode timestep = 10
Action ignored: Workspace boundary
Scene graph at timestep 796 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 796 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 796 is -1
Human Feedback received at timestep 796 of -1
Current timestep = 797. State = [[-0.25910673 -0.13816376  0.19162327  1.        ]]. Action = [[-0.209373   -0.9069078  -0.18010908  0.60305524]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 797 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 797 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 797 is 1
Human Feedback received at timestep 797 of 1
Current timestep = 798. State = [[-0.2633614  -0.16081184  0.1925421   1.        ]]. Action = [[-0.750984    0.09177065 -0.53958327  0.87916577]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 798 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 798 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 798 is -1
Human Feedback received at timestep 798 of -1
Current timestep = 799. State = [[-0.26419842 -0.1644283   0.19232927  1.        ]]. Action = [[-0.6517626  -0.636601    0.12061334  0.6272359 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 799 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 799 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 799 is 1
Human Feedback received at timestep 799 of 1
Current timestep = 800. State = [[-0.26256976 -0.15303996  0.18375504  1.        ]]. Action = [[ 0.27647793  0.68368936 -0.65363115  0.42083144]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 800 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 800 is tensor(0.0041, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 800 is -1
Human Feedback received at timestep 800 of -1
Current timestep = 801. State = [[-0.253201   -0.15492596  0.16037501  1.        ]]. Action = [[ 0.40596414 -0.81843793 -0.61716664  0.58350205]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 801 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 801 is tensor(0.0027, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 801 is 1
Human Feedback received at timestep 801 of 1
Current timestep = 802. State = [[-0.24728447 -0.16234635  0.1422239   1.        ]]. Action = [[-0.18155807  0.4045937   0.31787145  0.6577532 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 802 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 802 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 802 is -1
Human Feedback received at timestep 802 of -1
Current timestep = 803. State = [[-0.24659476 -0.15921101  0.14170264  1.        ]]. Action = [[-0.74838716 -0.67277205 -0.84029675  0.12694836]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 803 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 803 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 803 is -1
Human Feedback received at timestep 803 of -1
Current timestep = 804. State = [[-0.2501608   0.0026125   0.23258282  1.        ]]. Action = [[-0.33346617  0.62937856  0.03830457 -0.03511822]]. Reward = [-1.]
Curr episode timestep = 18
Scene graph at timestep 804 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 804 is tensor(0.0133, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 804 is 1
Human Feedback received at timestep 804 of 1
Current timestep = 805. State = [[-0.24334294 -0.01452194  0.2465198   1.        ]]. Action = [[ 0.2977357  -0.7557546   0.82695055  0.66160417]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 805 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 805 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 805 is 1
Human Feedback received at timestep 805 of 1
Current timestep = 806. State = [[-0.24278077 -0.04261571  0.26875392  1.        ]]. Action = [[-0.2613244  -0.51528734  0.12294066  0.7279887 ]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 806 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 806 is tensor(0.0093, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 806 is 1
Human Feedback received at timestep 806 of 1
Current timestep = 807. State = [[-0.24016058 -0.04965186  0.26993003  1.        ]]. Action = [[ 0.6781924   0.46531034 -0.5895844   0.4335096 ]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 807 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 807 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 807 is 1
Human Feedback received at timestep 807 of 1
Current timestep = 808. State = [[-0.22133218 -0.0455118   0.26969397  1.        ]]. Action = [[ 0.08331251 -0.07630014  0.77186656  0.62977266]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 808 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 808 is tensor(0.0085, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 808 is -1
Human Feedback received at timestep 808 of -1
Current timestep = 809. State = [[-0.21469894 -0.04484808  0.28167263  1.        ]]. Action = [[-0.17008168  0.04378629 -0.19013393  0.72541475]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 809 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 809 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 809 is -1
Human Feedback received at timestep 809 of -1
Current timestep = 810. State = [[-0.20374784 -0.0608377   0.2908514   1.        ]]. Action = [[ 0.8358189  -0.9169801   0.49602056  0.5698612 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 810 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 810 is tensor(0.0094, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 810 is 1
Human Feedback received at timestep 810 of 1
Current timestep = 811. State = [[-0.19774498 -0.09175575  0.31222975  1.        ]]. Action = [[-0.7090048  -0.53619164  0.36843276  0.80283546]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 811 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 811 is tensor(0.0096, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 811 is -1
Human Feedback received at timestep 811 of -1
Current timestep = 812. State = [[-0.20932017 -0.12208375  0.32476506  1.        ]]. Action = [[ 0.04224694 -0.6100057   0.09937882  0.5653367 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 812 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 812 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 812 is -1
Human Feedback received at timestep 812 of -1
Current timestep = 813. State = [[-0.21551414 -0.14850387  0.33542085  1.        ]]. Action = [[-0.147717   -0.50066096  0.42516088  0.7701647 ]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 813 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 813 is tensor(0.0037, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 813 is 1
Human Feedback received at timestep 813 of 1
Current timestep = 814. State = [[-0.22690235 -0.17131959  0.34380046  1.        ]]. Action = [[-0.76404494 -0.2299025  -0.35254025  0.89583194]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 814 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 814 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 814 is -1
Human Feedback received at timestep 814 of -1
Current timestep = 815. State = [[-0.25217175 -0.197691    0.35057232  1.        ]]. Action = [[-0.6860476 -0.7109533  0.4485438  0.8435788]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 815 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 815 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 815 is 1
Human Feedback received at timestep 815 of 1
Current timestep = 816. State = [[-0.27268657 -0.21663965  0.36032683  1.        ]]. Action = [[-0.73558086 -0.9481248   0.2879988   0.37137008]]. Reward = [0.]
Curr episode timestep = 11
Action ignored: Workspace boundary
Scene graph at timestep 816 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 816 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 816 is -1
Human Feedback received at timestep 816 of -1
Current timestep = 817. State = [[-0.2756707  -0.21968162  0.36138436  1.        ]]. Action = [[-0.81633633 -0.2251358  -0.6588858   0.5322294 ]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 817 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 817 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 817 is -1
Human Feedback received at timestep 817 of -1
Current timestep = 818. State = [[-0.27690414 -0.2207839   0.3616563   1.        ]]. Action = [[-0.23754174  0.07679093 -0.31838953  0.7473633 ]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 818 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 818 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 818 is -1
Human Feedback received at timestep 818 of -1
Current timestep = 819. State = [[-0.27741873 -0.22119421  0.36180824  1.        ]]. Action = [[-0.27282733 -0.76889277  0.7620878   0.92042685]]. Reward = [0.]
Curr episode timestep = 14
Action ignored: Workspace boundary
Scene graph at timestep 819 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 819 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 819 is -1
Human Feedback received at timestep 819 of -1
Current timestep = 820. State = [[-0.2764615  -0.22266135  0.35747546  1.        ]]. Action = [[ 0.28579438 -0.1690495  -0.4420337   0.8090427 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 820 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 820 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 820 is -1
Human Feedback received at timestep 820 of -1
Current timestep = 821. State = [[-0.26420027 -0.23416698  0.34968135  1.        ]]. Action = [[ 0.90250015 -0.58170885 -0.21206897  0.6998944 ]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 821 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 821 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 821 is -1
Human Feedback received at timestep 821 of -1
Current timestep = 822. State = [[-0.24203198 -0.24708447  0.33924884  1.        ]]. Action = [[-0.8992093   0.10939229  0.4586742   0.9186735 ]]. Reward = [0.]
Curr episode timestep = 17
Action ignored: Workspace boundary
Scene graph at timestep 822 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 822 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 822 is -1
Human Feedback received at timestep 822 of -1
Current timestep = 823. State = [[-0.23774225 -0.24494609  0.32514802  1.        ]]. Action = [[-0.2140367   0.48224616 -0.91397005  0.8623674 ]]. Reward = [0.]
Curr episode timestep = 18
Scene graph at timestep 823 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 823 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 823 is -1
Human Feedback received at timestep 823 of -1
Current timestep = 824. State = [[-0.24039833 -0.24227361  0.30878824  1.        ]]. Action = [[-0.24397826 -0.2019676   0.35940337  0.7425027 ]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 824 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 824 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 824 is -1
Human Feedback received at timestep 824 of -1
Current timestep = 825. State = [[-0.24568652 -0.25609863  0.32020348  1.        ]]. Action = [[ 0.06360412 -0.6233791   0.74805284  0.87763834]]. Reward = [0.]
Curr episode timestep = 20
Scene graph at timestep 825 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 825 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 825 is -1
Human Feedback received at timestep 825 of -1
Current timestep = 826. State = [[-0.2505082  -0.26740903  0.33275187  1.        ]]. Action = [[-0.72320557 -0.9338681   0.6085229   0.78517115]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 826 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 826 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 826 is 1
Human Feedback received at timestep 826 of 1
Current timestep = 827. State = [[-0.2527966  -0.27013025  0.33428597  1.        ]]. Action = [[-0.43006927  0.14026308  0.60871696  0.6284045 ]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 827 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 827 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 827 is -1
Human Feedback received at timestep 827 of -1
Current timestep = 828. State = [[-0.25334814 -0.27051562  0.33470428  1.        ]]. Action = [[-0.4499247  -0.5620856  -0.74525803  0.8402591 ]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 828 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 828 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 828 is 1
Human Feedback received at timestep 828 of 1
Current timestep = 829. State = [[-0.25340086 -0.2704514   0.33469984  1.        ]]. Action = [[-0.51353097 -0.73221976  0.01474309  0.42349172]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 829 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 829 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 829 is -1
Human Feedback received at timestep 829 of -1
Current timestep = 830. State = [[-0.25340086 -0.2704514   0.33469984  1.        ]]. Action = [[-0.7806865   0.01232624 -0.49279487  0.5358684 ]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 830 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 830 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 830 is -1
Human Feedback received at timestep 830 of -1
Current timestep = 831. State = [[-0.25340086 -0.2704514   0.33469984  1.        ]]. Action = [[-0.88861394  0.9026029  -0.17826003  0.75032425]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 831 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 831 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 831 is 1
Human Feedback received at timestep 831 of 1
Current timestep = 832. State = [[-0.25340086 -0.2704514   0.33469984  1.        ]]. Action = [[-0.41759658 -0.18061012 -0.9364029   0.8508568 ]]. Reward = [0.]
Curr episode timestep = 27
Action ignored: Workspace boundary
Scene graph at timestep 832 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 832 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 832 is -1
Human Feedback received at timestep 832 of -1
Current timestep = 833. State = [[-0.25340086 -0.2704514   0.33469984  1.        ]]. Action = [[-0.69933873 -0.91693324 -0.23681188  0.6470741 ]]. Reward = [0.]
Curr episode timestep = 28
Action ignored: Workspace boundary
Scene graph at timestep 833 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 833 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 833 is 1
Human Feedback received at timestep 833 of 1
Current timestep = 834. State = [[-0.25525764 -0.2807605   0.34388077  1.        ]]. Action = [[-0.0752272  -0.54037064  0.48513222  0.92446434]]. Reward = [0.]
Curr episode timestep = 29
Scene graph at timestep 834 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 834 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 834 is 1
Human Feedback received at timestep 834 of 1
Current timestep = 835. State = [[-0.25079682 -0.28577748  0.35138613  1.        ]]. Action = [[ 0.4413265   0.3542354  -0.45889318  0.84183574]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 835 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 835 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 835 is 1
Human Feedback received at timestep 835 of 1
Current timestep = 836. State = [[-0.23123363 -0.26626435  0.3451674   1.        ]]. Action = [[0.5958258  0.8160684  0.03504062 0.8756081 ]]. Reward = [0.]
Curr episode timestep = 31
Scene graph at timestep 836 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 836 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 836 is 1
Human Feedback received at timestep 836 of 1
Current timestep = 837. State = [[-0.20964421 -0.23897189  0.33305478  1.        ]]. Action = [[ 0.40377247  0.37955177 -0.78361595  0.87139213]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 837 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 837 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 837 is -1
Human Feedback received at timestep 837 of -1
Current timestep = 838. State = [[-0.19775471 -0.21512066  0.3098923   1.        ]]. Action = [[-0.5347039   0.77885485 -0.38627613  0.7745743 ]]. Reward = [0.]
Curr episode timestep = 33
Scene graph at timestep 838 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 838 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 838 is -1
Human Feedback received at timestep 838 of -1
Current timestep = 839. State = [[-0.21288218 -0.20980267  0.2953419   1.        ]]. Action = [[-0.37869257 -0.7280225  -0.3181333   0.4206698 ]]. Reward = [0.]
Curr episode timestep = 34
Scene graph at timestep 839 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 839 is tensor(0.0054, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 839 is -1
Human Feedback received at timestep 839 of -1
Current timestep = 840. State = [[-0.21821263 -0.22838256  0.27526507  1.        ]]. Action = [[ 0.53240323 -0.55966145 -0.92296296  0.681988  ]]. Reward = [0.]
Curr episode timestep = 35
Scene graph at timestep 840 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 840 is tensor(0.0074, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 840 is -1
Human Feedback received at timestep 840 of -1
Current timestep = 841. State = [[-0.20100322 -0.25749665  0.25060707  1.        ]]. Action = [[ 0.44783592 -0.8094457   0.5237148   0.6587459 ]]. Reward = [0.]
Curr episode timestep = 36
Scene graph at timestep 841 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 841 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 841 is -1
Human Feedback received at timestep 841 of -1
Current timestep = 842. State = [[-0.19116405 -0.27468476  0.2491612   1.        ]]. Action = [[-0.04783398  0.19381046 -0.29489863  0.6660477 ]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 842 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 842 is tensor(0.0047, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 842 is -1
Human Feedback received at timestep 842 of -1
Current timestep = 843. State = [[-0.19822341 -0.27641463  0.25005102  1.        ]]. Action = [[-0.8054106   0.17644382  0.4546113   0.5588337 ]]. Reward = [0.]
Curr episode timestep = 38
Scene graph at timestep 843 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 843 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 843 is -1
Human Feedback received at timestep 843 of -1
Current timestep = 844. State = [[-0.21157427 -0.27684224  0.25687397  1.        ]]. Action = [[-0.49397433 -0.6815033   0.69259536  0.6839161 ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 844 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 844 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 844 is -1
Human Feedback received at timestep 844 of -1
Current timestep = 845. State = [[-0.21477057 -0.27601752  0.25671688  1.        ]]. Action = [[ 0.2718253  -0.8630491   0.53873277  0.9158795 ]]. Reward = [0.]
Curr episode timestep = 40
Action ignored: Workspace boundary
Scene graph at timestep 845 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 845 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 845 is -1
Human Feedback received at timestep 845 of -1
Current timestep = 846. State = [[-0.21904163 -0.28353897  0.26231977  1.        ]]. Action = [[-0.05694693 -0.4839986   0.39653397  0.40073788]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 846 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 846 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 846 is -1
Human Feedback received at timestep 846 of -1
Current timestep = 847. State = [[-0.22901627 -0.2976885   0.26486298  1.        ]]. Action = [[-0.22001797 -0.3216135  -0.5637862   0.68034506]]. Reward = [0.]
Curr episode timestep = 42
Scene graph at timestep 847 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 847 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 847 is -1
Human Feedback received at timestep 847 of -1
Current timestep = 848. State = [[-0.2332365  -0.30694342  0.2615509   1.        ]]. Action = [[ 0.05911565 -0.6808961  -0.25767958  0.8038006 ]]. Reward = [0.]
Curr episode timestep = 43
Action ignored: Workspace boundary
Scene graph at timestep 848 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 848 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 848 is 1
Human Feedback received at timestep 848 of 1
Current timestep = 849. State = [[-0.22968258 -0.29579884  0.24914001  1.        ]]. Action = [[ 0.30542505  0.6964524  -0.8111972   0.54608524]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 849 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 849 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 849 is 1
Human Feedback received at timestep 849 of 1
Current timestep = 850. State = [[-0.22132105 -0.28478625  0.22941779  1.        ]]. Action = [[-0.8097687  -0.8323807  -0.22485471  0.8277037 ]]. Reward = [0.]
Curr episode timestep = 45
Action ignored: Workspace boundary
Scene graph at timestep 850 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 850 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 850 is 1
Human Feedback received at timestep 850 of 1
Current timestep = 851. State = [[-0.21900447 -0.28182596  0.22732405  1.        ]]. Action = [[ 0.4881618  -0.6520764   0.75635624  0.8213837 ]]. Reward = [0.]
Curr episode timestep = 46
Action ignored: Workspace boundary
Scene graph at timestep 851 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 851 is tensor(7.2612e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 851 is -1
Human Feedback received at timestep 851 of -1
Current timestep = 852. State = [[-0.21865252 -0.28133294  0.22703026  1.        ]]. Action = [[-0.88654524  0.26762867  0.19452477  0.5511594 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 852 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 852 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 852 is -1
Human Feedback received at timestep 852 of -1
Current timestep = 853. State = [[-0.21854623 -0.28117535  0.2269535   1.        ]]. Action = [[-0.46094054 -0.6659112   0.37797678  0.5746745 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 853 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 853 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 853 is -1
Human Feedback received at timestep 853 of -1
Current timestep = 854. State = [[-0.21843976 -0.2810173   0.22687663  1.        ]]. Action = [[ 0.18181682 -0.63350123 -0.21928757  0.5425134 ]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 854 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 854 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 854 is 1
Human Feedback received at timestep 854 of 1
Current timestep = 855. State = [[-0.21843976 -0.2810173   0.22687663  1.        ]]. Action = [[ 0.84008956 -0.9114882  -0.6568986   0.6538278 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Scene graph at timestep 855 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 855 is tensor(0.0017, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 855 is -1
Human Feedback received at timestep 855 of -1
Current timestep = 856. State = [[-0.22415097 -0.274529    0.21437019  1.        ]]. Action = [[-0.28192878  0.37117088 -0.77317375  0.7953856 ]]. Reward = [0.]
Curr episode timestep = 51
Scene graph at timestep 856 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 856 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 856 is 1
Human Feedback received at timestep 856 of 1
Current timestep = 857. State = [[-0.2205081  -0.273457    0.18137917  1.        ]]. Action = [[ 0.63855386 -0.4966842  -0.88498855  0.56867385]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 857 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 857 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 857 is -1
Human Feedback received at timestep 857 of -1
Current timestep = 858. State = [[-0.20976956 -0.2805575   0.1448524   1.        ]]. Action = [[ 0.07422185 -0.69664323  0.46309412  0.7890327 ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 858 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 858 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 858 is -1
Human Feedback received at timestep 858 of -1
Current timestep = 859. State = [[-0.20961165 -0.28108194  0.14242002  1.        ]]. Action = [[ 0.59978664 -0.9654446   0.5494828   0.67579114]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 859 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 859 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 859 is 1
Human Feedback received at timestep 859 of 1
Current timestep = 860. State = [[-0.2094074  -0.2812318   0.14251226  1.        ]]. Action = [[-0.8819084  -0.5697256  -0.7856684   0.84999275]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 860 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 860 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 860 is 1
Human Feedback received at timestep 860 of 1
Current timestep = 861. State = [[-0.2093191  -0.28113613  0.14247516  1.        ]]. Action = [[-0.10469306 -0.6873874   0.01309836  0.7845447 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 861 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 861 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 861 is 1
Human Feedback received at timestep 861 of 1
Current timestep = 862. State = [[-0.2092862  -0.28107977  0.14245139  1.        ]]. Action = [[-0.07507628 -0.90153885  0.00342369  0.9496094 ]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 862 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 862 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 862 is -1
Human Feedback received at timestep 862 of -1
Current timestep = 863. State = [[-0.2092862  -0.28107977  0.14245139  1.        ]]. Action = [[-0.44532335 -0.9586909  -0.12851155  0.6607363 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Scene graph at timestep 863 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 863 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 863 is 1
Human Feedback received at timestep 863 of 1
Current timestep = 864. State = [[-0.20925331 -0.28102338  0.14242764  1.        ]]. Action = [[ 0.16458786 -0.8712785   0.258947    0.8328583 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 864 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 864 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 864 is -1
Human Feedback received at timestep 864 of -1
Current timestep = 865. State = [[-0.20807372 -0.29033273  0.13443173  1.        ]]. Action = [[ 0.15841055 -0.46681964 -0.51958525  0.85322094]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 865 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 865 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 865 is 1
Human Feedback received at timestep 865 of 1
Current timestep = 866. State = [[-0.1984374  -0.29195508  0.12685747  1.        ]]. Action = [[0.12313783 0.48841572 0.83538663 0.5620456 ]]. Reward = [0.]
Curr episode timestep = 61
Scene graph at timestep 866 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 866 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 866 is -1
Human Feedback received at timestep 866 of -1
Current timestep = 867. State = [[-0.1922423  -0.28741127  0.13679536  1.        ]]. Action = [[-0.11291045 -0.89701307  0.6196785   0.6765199 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 867 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 867 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 867 is 1
Human Feedback received at timestep 867 of 1
Current timestep = 868. State = [[-0.19988732 -0.28220028  0.13567306  1.        ]]. Action = [[-0.56041557  0.29063964 -0.08176744  0.88437486]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 868 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 868 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 868 is -1
Human Feedback received at timestep 868 of -1
Current timestep = 869. State = [[-0.20649737 -0.27907786  0.13667412  1.        ]]. Action = [[-0.14229739 -0.7892804  -0.5216822   0.30156755]]. Reward = [0.]
Curr episode timestep = 64
Action ignored: Workspace boundary
Scene graph at timestep 869 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 869 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 869 is 1
Human Feedback received at timestep 869 of 1
Current timestep = 870. State = [[-0.2240657  -0.28628677  0.1332848   1.        ]]. Action = [[-0.8122983  -0.42747664 -0.32072127  0.8581419 ]]. Reward = [0.]
Curr episode timestep = 65
Scene graph at timestep 870 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 870 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 870 is 1
Human Feedback received at timestep 870 of 1
Current timestep = 871. State = [[-0.23454472 -0.28701645  0.134196    1.        ]]. Action = [[ 0.23319662 -0.04192728  0.29238725  0.5703018 ]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 871 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 871 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 871 is 1
Human Feedback received at timestep 871 of 1
Current timestep = 872. State = [[-0.23360962 -0.2889068   0.14173149  1.        ]]. Action = [[-0.8934614   0.18454015 -0.28197223  0.9149389 ]]. Reward = [0.]
Curr episode timestep = 67
Action ignored: Workspace boundary
Scene graph at timestep 872 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 872 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 872 is -1
Human Feedback received at timestep 872 of -1
Current timestep = 873. State = [[-0.23218282 -0.28344294  0.14453351  1.        ]]. Action = [[-0.30454862 -0.7844771  -0.10618335  0.8795258 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 873 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 873 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 873 is -1
Human Feedback received at timestep 873 of -1
Current timestep = 874. State = [[-0.23414724 -0.2850901   0.14712958  1.        ]]. Action = [[-0.21776444 -0.32777017  0.0934217   0.82969487]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 874 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 874 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 874 is 1
Human Feedback received at timestep 874 of 1
Current timestep = 875. State = [[-0.23813918 -0.291145    0.14996004  1.        ]]. Action = [[-0.79493403 -0.86170256  0.7756076   0.70896196]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 875 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 875 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 875 is 1
Human Feedback received at timestep 875 of 1
Current timestep = 876. State = [[-0.24543768 -0.30443317  0.1642243   1.        ]]. Action = [[-0.20167613 -0.26203072  0.8463216   0.91276526]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 876 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 876 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 876 is -1
Human Feedback received at timestep 876 of -1
Current timestep = 877. State = [[-0.25553802 -0.32029712  0.1817312   1.        ]]. Action = [[-0.832271    0.18446434  0.84729826  0.53907347]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Scene graph at timestep 877 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 877 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 877 is -1
Human Feedback received at timestep 877 of -1
Current timestep = 878. State = [[-0.25768095 -0.322015    0.18301514  1.        ]]. Action = [[ 0.12623084 -0.8027103   0.6052729   0.68175197]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Scene graph at timestep 878 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 878 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 878 is 1
Human Feedback received at timestep 878 of 1
Current timestep = 879. State = [[-0.25828442 -0.32248828  0.18340185  1.        ]]. Action = [[-0.93397504  0.8526013  -0.6225816   0.5096419 ]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Scene graph at timestep 879 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 879 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 879 is 1
Human Feedback received at timestep 879 of 1
Current timestep = 880. State = [[-0.25856647 -0.32249916  0.18351667  1.        ]]. Action = [[-0.4304164  -0.2004382   0.21003437  0.35650647]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 880 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 880 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 880 is 1
Human Feedback received at timestep 880 of 1
Current timestep = 881. State = [[-0.2586827  -0.32264414  0.18359429  1.        ]]. Action = [[-0.74411935  0.22974825 -0.8057644   0.39880526]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Scene graph at timestep 881 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 881 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 881 is 1
Human Feedback received at timestep 881 of 1
Current timestep = 882. State = [[-0.2586827  -0.32264414  0.18359429  1.        ]]. Action = [[-0.45962143  0.47139657 -0.44272768  0.88622594]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Scene graph at timestep 882 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 882 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 882 is 1
Human Feedback received at timestep 882 of 1
Current timestep = 883. State = [[-0.25637868 -0.31421012  0.19076617  1.        ]]. Action = [[-0.01407939  0.5391269   0.42414963  0.7976639 ]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 883 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 883 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 883 is 1
Human Feedback received at timestep 883 of 1
Current timestep = 884. State = [[-0.25653017 -0.30738786  0.20177485  1.        ]]. Action = [[ 0.1845007  -0.5190345  -0.02427447  0.7358053 ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Scene graph at timestep 884 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 884 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 884 is 1
Human Feedback received at timestep 884 of 1
Current timestep = 885. State = [[-0.25823328 -0.30503514  0.20111917  1.        ]]. Action = [[ 0.09987843 -0.8217947  -0.584594    0.86796904]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Scene graph at timestep 885 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 885 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 885 is 1
Human Feedback received at timestep 885 of 1
Current timestep = 886. State = [[-0.2606724  -0.3031939   0.20106769  1.        ]]. Action = [[-0.67206836  0.66204333  0.9533744   0.33397555]]. Reward = [0.]
Curr episode timestep = 81
Action ignored: Workspace boundary
Scene graph at timestep 886 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 886 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 886 is -1
Human Feedback received at timestep 886 of -1
Current timestep = 887. State = [[-0.2631128  -0.28894597  0.19911437  1.        ]]. Action = [[-0.06134033  0.8097571  -0.20056391  0.6801684 ]]. Reward = [0.]
Curr episode timestep = 82
Scene graph at timestep 887 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 887 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 887 is 1
Human Feedback received at timestep 887 of 1
Current timestep = 888. State = [[-0.26314846 -0.26536724  0.19729303  1.        ]]. Action = [[-0.77299505 -0.42199695 -0.15525925  0.07215703]]. Reward = [0.]
Curr episode timestep = 83
Action ignored: Workspace boundary
Scene graph at timestep 888 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 888 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 888 is 1
Human Feedback received at timestep 888 of 1
Current timestep = 889. State = [[-0.26172233 -0.25415644  0.20580646  1.        ]]. Action = [[0.0829345  0.46941638 0.75079954 0.691928  ]]. Reward = [0.]
Curr episode timestep = 84
Scene graph at timestep 889 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 889 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 889 is -1
Human Feedback received at timestep 889 of -1
Current timestep = 890. State = [[-0.25945088 -0.2429549   0.2222544   1.        ]]. Action = [[-0.63561225  0.6319363  -0.0161882   0.5078387 ]]. Reward = [0.]
Curr episode timestep = 85
Action ignored: Workspace boundary
Scene graph at timestep 890 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 890 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 890 is 1
Human Feedback received at timestep 890 of 1
Current timestep = 891. State = [[-0.24842723 -0.2357381   0.22038274  1.        ]]. Action = [[ 0.7984812  -0.24212867 -0.6460471   0.7394061 ]]. Reward = [0.]
Curr episode timestep = 86
Scene graph at timestep 891 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 891 is tensor(0.0014, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 891 is -1
Human Feedback received at timestep 891 of -1
Current timestep = 892. State = [[-0.2403994  -0.24043211  0.20896558  1.        ]]. Action = [[-0.9662161  -0.34436202  0.5648602   0.5754347 ]]. Reward = [0.]
Curr episode timestep = 87
Action ignored: Workspace boundary
Scene graph at timestep 892 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 892 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 892 is 1
Human Feedback received at timestep 892 of 1
Current timestep = 893. State = [[-0.23744829 -0.24548416  0.19466898  1.        ]]. Action = [[ 0.0274713  -0.10498083 -0.97199905  0.8406919 ]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 893 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 893 is tensor(0.0013, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 893 is -1
Human Feedback received at timestep 893 of -1
Current timestep = 894. State = [[-0.2224382  -0.23351622  0.16433963  1.        ]]. Action = [[ 0.51343656  0.30341148 -0.6818832   0.78143334]]. Reward = [0.]
Curr episode timestep = 89
Scene graph at timestep 894 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 894 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 894 is -1
Human Feedback received at timestep 894 of -1
Current timestep = 895. State = [[-0.19874406 -0.22602303  0.13243793  1.        ]]. Action = [[ 0.4864416  -0.33387864 -0.7167234   0.8465152 ]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 895 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 895 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 895 is -1
Human Feedback received at timestep 895 of -1
Current timestep = 896. State = [[-0.18606527 -0.22157884  0.10295974  1.        ]]. Action = [[-0.41818035  0.78430474 -0.15029436  0.5810895 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 896 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 896 is tensor(0.0020, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 896 is -1
Human Feedback received at timestep 896 of -1
Current timestep = 897. State = [[-0.18735869 -0.19844863  0.10120986  1.        ]]. Action = [[-0.42174685 -0.29641628 -0.05305231  0.89409864]]. Reward = [0.]
Curr episode timestep = 92
Scene graph at timestep 897 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 897 is tensor(0.0019, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 897 is 1
Human Feedback received at timestep 897 of 1
Current timestep = 898. State = [[-0.19767389 -0.19095533  0.09522385  1.        ]]. Action = [[-0.29261756 -0.00993061 -0.52251893  0.26649606]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 898 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 898 is tensor(0.0033, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 898 is 1
Human Feedback received at timestep 898 of 1
Current timestep = 899. State = [[-0.20778422 -0.18670551  0.08678996  1.        ]]. Action = [[-0.3664313   0.3450693   0.38259447  0.45735002]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 899 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 899 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 899 is 1
Human Feedback received at timestep 899 of 1
Current timestep = 900. State = [[-0.21020149 -0.18340258  0.08097878  1.        ]]. Action = [[ 0.638703   -0.6305829  -0.92328364  0.65081346]]. Reward = [0.]
Curr episode timestep = 95
Scene graph at timestep 900 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 900 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 900 is 1
Human Feedback received at timestep 900 of 1
Current timestep = 901. State = [[-0.20789996 -0.19596338  0.06677805  1.        ]]. Action = [[-0.77565885 -0.22463119  0.31420708  0.20363545]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 901 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 901 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 901 is 1
Human Feedback received at timestep 901 of 1
Current timestep = 902. State = [[-0.21245714 -0.20024541  0.06865938  1.        ]]. Action = [[ 0.23066163 -0.98876816 -0.84713787  0.85069704]]. Reward = [0.]
Curr episode timestep = 97
Action ignored: Workspace boundary
Scene graph at timestep 902 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 902 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 902 is 1
Human Feedback received at timestep 902 of 1
Current timestep = 903. State = [[-0.21474294 -0.20214422  0.06941633  1.        ]]. Action = [[ 0.8307773   0.36183023 -0.8522155   0.80993867]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Scene graph at timestep 903 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 903 is tensor(0.0025, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 903 is 1
Human Feedback received at timestep 903 of 1
Current timestep = 904. State = [[-0.22504957 -0.20060003  0.06547283  1.        ]]. Action = [[-0.6479992  -0.2909428  -0.57684124  0.58941567]]. Reward = [0.]
Curr episode timestep = 99
Scene graph at timestep 904 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 904 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 904 is 1
Human Feedback received at timestep 904 of 1
Current timestep = 905. State = [[-0.23295096 -0.19992113  0.05762892  1.        ]]. Action = [[-0.892477    0.2852522   0.0961858   0.81878793]]. Reward = [0.]
Curr episode timestep = 100
Action ignored: Workspace boundary
Scene graph at timestep 905 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 905 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 905 is 1
Human Feedback received at timestep 905 of 1
Current timestep = 906. State = [[-0.23322757 -0.19982007  0.05757479  1.        ]]. Action = [[-0.63431543 -0.22054106  0.48220372  0.5774939 ]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Scene graph at timestep 906 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 906 is tensor(0.0026, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 906 is 1
Human Feedback received at timestep 906 of 1
Current timestep = 907. State = [[-0.23322757 -0.19982007  0.05757479  1.        ]]. Action = [[-0.7162021  -0.53088546  0.48514533  0.6732352 ]]. Reward = [0.]
Curr episode timestep = 102
Action ignored: Workspace boundary
Scene graph at timestep 907 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 907 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 907 is 1
Human Feedback received at timestep 907 of 1
Current timestep = 908. State = [[-0.23322757 -0.19982007  0.05757479  1.        ]]. Action = [[ 0.3640964  -0.949339   -0.88327634  0.3416263 ]]. Reward = [0.]
Curr episode timestep = 103
Action ignored: Workspace boundary
Scene graph at timestep 908 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 908 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 908 is 1
Human Feedback received at timestep 908 of 1
Current timestep = 909. State = [[-0.23909023 -0.20651847  0.05395323  1.        ]]. Action = [[-0.27604252 -0.9403734  -0.44402522  0.6818476 ]]. Reward = [0.]
Curr episode timestep = 104
Scene graph at timestep 909 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 909 is tensor(0.0030, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 909 is 1
Human Feedback received at timestep 909 of 1
Current timestep = 910. State = [[-0.24497637 -0.21928826  0.03889243  1.        ]]. Action = [[ 0.2316668  -0.11425567 -0.13843995  0.8631339 ]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 910 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 910 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 910 is 1
Human Feedback received at timestep 910 of 1
Current timestep = 911. State = [[-0.24189536 -0.22933751  0.02752169  1.        ]]. Action = [[-0.7500774  0.1718837  0.8530849  0.6847886]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Scene graph at timestep 911 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 911 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 911 is 1
Human Feedback received at timestep 911 of 1
Current timestep = 912. State = [[-0.2389718  -0.22994432  0.0271174   1.        ]]. Action = [[-0.30580974 -0.22267538 -0.26953113  0.8246026 ]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Scene graph at timestep 912 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 912 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 912 is 1
Human Feedback received at timestep 912 of 1
Current timestep = 913. State = [[-0.23953266 -0.23747954  0.03199967  1.        ]]. Action = [[-0.19236076 -0.41002423  0.4081359   0.5216789 ]]. Reward = [0.]
Curr episode timestep = 108
Scene graph at timestep 913 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 913 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 913 is 1
Human Feedback received at timestep 913 of 1
Current timestep = 914. State = [[-0.24120595 -0.24453923  0.036669    1.        ]]. Action = [[-0.7169642  -0.43654692  0.67230296  0.86008286]]. Reward = [0.]
Curr episode timestep = 109
Action ignored: Workspace boundary
Scene graph at timestep 914 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 914 is tensor(7.0175e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 914 is 1
Human Feedback received at timestep 914 of 1
Current timestep = 915. State = [[-0.23675135 -0.26274228  0.04573728  1.        ]]. Action = [[ 0.40193033 -0.7726833   0.23922384  0.6109853 ]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 915 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 915 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 915 is 1
Human Feedback received at timestep 915 of 1
Current timestep = 916. State = [[-0.23154864 -0.2837872   0.0505169   1.        ]]. Action = [[ 0.3430549 -0.9418331  0.027632   0.4254607]]. Reward = [0.]
Curr episode timestep = 111
Action ignored: Workspace boundary
Scene graph at timestep 916 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 916 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 916 is 1
Human Feedback received at timestep 916 of 1
Current timestep = 917. State = [[-0.23214963 -0.30345398  0.06129621  1.        ]]. Action = [[-0.19459802 -0.3678419   0.59580255  0.8814728 ]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 917 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 917 is tensor(7.1980e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 917 is 1
Human Feedback received at timestep 917 of 1
Current timestep = 918. State = [[-0.23335862 -0.31479198  0.07217469  1.        ]]. Action = [[ 0.43482637 -0.6418812   0.14204967  0.9236313 ]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Scene graph at timestep 918 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 918 is tensor(7.2611e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 918 is 1
Human Feedback received at timestep 918 of 1
Current timestep = 919. State = [[-0.23362426 -0.32065892  0.07653038  1.        ]]. Action = [[-0.1255731  -0.9425743   0.01775301  0.80579615]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Scene graph at timestep 919 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 919 is tensor(1.4809e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 919 is 1
Human Feedback received at timestep 919 of 1
Current timestep = 920. State = [[-0.23076247 -0.32281512  0.07703271  1.        ]]. Action = [[-0.92838115  0.12130082  0.16652071  0.8402586 ]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: Workspace boundary
Scene graph at timestep 920 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 920 is tensor(3.4213e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 920 is 1
Human Feedback received at timestep 920 of 1
Current timestep = 921. State = [[-0.23025416 -0.32886514  0.07872817  1.        ]]. Action = [[-0.75514156  0.48560536  0.04367578  0.6664916 ]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: Workspace boundary
Scene graph at timestep 921 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 921 is tensor(7.4596e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 921 is 1
Human Feedback received at timestep 921 of 1
Current timestep = 922. State = [[-0.2290849  -0.32903132  0.07739594  1.        ]]. Action = [[-0.32276124 -0.59359473 -0.7910301   0.7884947 ]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Scene graph at timestep 922 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 922 is tensor(6.3791e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 922 is 1
Human Feedback received at timestep 922 of 1
Current timestep = 923. State = [[-0.22948779 -0.32908073  0.0750564   1.        ]]. Action = [[ 0.5034549  -0.7548834  -0.31431055  0.27776122]]. Reward = [0.]
Curr episode timestep = 118
Action ignored: Workspace boundary
Scene graph at timestep 923 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 923 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 923 is -1
Human Feedback received at timestep 923 of -1
Current timestep = 924. State = [[-0.22953236 -0.32916912  0.07456264  1.        ]]. Action = [[0.7509129  0.06615722 0.22235072 0.7406689 ]]. Reward = [0.]
Curr episode timestep = 119
Action ignored: Workspace boundary
Scene graph at timestep 924 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 924 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 924 is 1
Human Feedback received at timestep 924 of 1
Current timestep = 925. State = [[-0.23023888 -0.3310497   0.07515783  1.        ]]. Action = [[-0.11646354  0.60673046 -0.90792066  0.44699728]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Scene graph at timestep 925 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 925 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 925 is 1
Human Feedback received at timestep 925 of 1
Current timestep = 926. State = [[-0.23118982 -0.33290175  0.07616711  1.        ]]. Action = [[-0.07413471 -0.5104736   0.00880468  0.9115175 ]]. Reward = [0.]
Curr episode timestep = 121
Action ignored: Workspace boundary
Scene graph at timestep 926 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 926 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 926 is 1
Human Feedback received at timestep 926 of 1
Current timestep = 927. State = [[-0.23170596 -0.3338059   0.07621073  1.        ]]. Action = [[-0.27872705  0.32012665  0.67656684  0.70599115]]. Reward = [0.]
Curr episode timestep = 122
Action ignored: Workspace boundary
Scene graph at timestep 927 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 927 is tensor(3.3922e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 927 is -1
Human Feedback received at timestep 927 of -1
Current timestep = 928. State = [[-0.22829197 -0.32801598  0.0690232   1.        ]]. Action = [[ 0.6171894   0.8831054  -0.70671725  0.93931663]]. Reward = [0.]
Curr episode timestep = 123
Scene graph at timestep 928 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 928 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 928 is 1
Human Feedback received at timestep 928 of 1
Current timestep = 929. State = [[-0.20918934 -0.30709955  0.04714137  1.        ]]. Action = [[0.28509676 0.2797184  0.25240338 0.48160172]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 929 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 929 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 929 is -1
Human Feedback received at timestep 929 of -1
Current timestep = 930. State = [[-0.20176592 -0.29300126  0.04894729  1.        ]]. Action = [[-0.3425982   0.6972598   0.30571842  0.7387128 ]]. Reward = [0.]
Curr episode timestep = 125
Scene graph at timestep 930 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 930 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 930 is -1
Human Feedback received at timestep 930 of -1
Current timestep = 931. State = [[-0.25093338  0.00259682  0.23261952  1.        ]]. Action = [[ 0.72929347  0.14596081 -0.21001798  0.5473716 ]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 931 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 931 is tensor(0.0294, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 931 is 1
Human Feedback received at timestep 931 of 1
Current timestep = 932. State = [[-0.25346693  0.02965781  0.24374361  1.        ]]. Action = [[-0.02647573  0.5108478   0.61466527  0.82410526]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 932 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 932 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 932 is 1
Human Feedback received at timestep 932 of 1
Current timestep = 933. State = [[-0.2467715   0.0280033   0.24771418  1.        ]]. Action = [[ 0.7127795  -0.94109833 -0.9407924   0.52746546]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 933 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 933 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 933 is -1
Human Feedback received at timestep 933 of -1
Current timestep = 934. State = [[-0.22844678  0.01184843  0.22988096  1.        ]]. Action = [[-0.49680507  0.10368121 -0.79278487  0.3213029 ]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 934 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 934 is tensor(0.0108, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 934 is 1
Human Feedback received at timestep 934 of 1
Current timestep = 935. State = [[-0.22507502  0.00841403  0.22710948  1.        ]]. Action = [[-0.9664069   0.45687866  0.9726255   0.19809055]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 935 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 935 is tensor(0.0115, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 935 is -1
Human Feedback received at timestep 935 of -1
Current timestep = 936. State = [[-0.21474954  0.00533539  0.22189124  1.        ]]. Action = [[ 0.66188836 -0.08920199 -0.38808274  0.8080306 ]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 936 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 936 is tensor(0.0111, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 936 is 1
Human Feedback received at timestep 936 of 1
Current timestep = 937. State = [[-0.19455037 -0.01272584  0.22014922  1.        ]]. Action = [[-0.05559301 -0.7824835   0.7116504   0.67817307]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 937 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 937 is tensor(0.0100, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 937 is 1
Human Feedback received at timestep 937 of 1
Current timestep = 938. State = [[-0.1957558  -0.04011129  0.22425061  1.        ]]. Action = [[-0.33077848 -0.41291595 -0.5972657   0.7131307 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 938 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 938 is tensor(0.0102, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 938 is 1
Human Feedback received at timestep 938 of 1
Current timestep = 939. State = [[-0.20028517 -0.05817119  0.21065927  1.        ]]. Action = [[ 0.04603064 -0.3431477  -0.6639204   0.4807402 ]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 939 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 939 is tensor(0.0086, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 939 is 1
Human Feedback received at timestep 939 of 1
Current timestep = 940. State = [[-0.21036227 -0.05880816  0.18971135  1.        ]]. Action = [[-0.7608344   0.5930728   0.20999014  0.54326403]]. Reward = [0.]
Curr episode timestep = 8
Scene graph at timestep 940 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 940 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 940 is 1
Human Feedback received at timestep 940 of 1
Current timestep = 941. State = [[-0.22061963 -0.05930175  0.18463197  1.        ]]. Action = [[ 0.44882572 -0.51269054 -0.3345753   0.77084565]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 941 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 941 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 941 is 1
Human Feedback received at timestep 941 of 1
Current timestep = 942. State = [[-0.22538535 -0.08255337  0.18083903  1.        ]]. Action = [[-0.62899226 -0.7578487   0.13082528  0.53342915]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 942 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 942 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 942 is 1
Human Feedback received at timestep 942 of 1
Current timestep = 943. State = [[-0.23245838 -0.10823406  0.17926854  1.        ]]. Action = [[ 0.34518266 -0.34650493 -0.20255184  0.4729905 ]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 943 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 943 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 943 is 1
Human Feedback received at timestep 943 of 1
Current timestep = 944. State = [[-0.22971508 -0.12104008  0.17560597  1.        ]]. Action = [[-0.9957096  -0.19148266  0.26331556  0.75551736]]. Reward = [0.]
Curr episode timestep = 12
Action ignored: Workspace boundary
Scene graph at timestep 944 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 944 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 944 is 1
Human Feedback received at timestep 944 of 1
Current timestep = 945. State = [[-0.22946204 -0.12218212  0.18517792  1.        ]]. Action = [[-0.21246517  0.08933294  0.83597636  0.5710714 ]]. Reward = [0.]
Curr episode timestep = 13
Scene graph at timestep 945 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 945 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 945 is -1
Human Feedback received at timestep 945 of -1
Current timestep = 946. State = [[-0.23099789 -0.13332067  0.2020569   1.        ]]. Action = [[ 0.37335777 -0.6768439   0.07480621  0.6616453 ]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 946 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 946 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 946 is -1
Human Feedback received at timestep 946 of -1
Current timestep = 947. State = [[-0.22663362 -0.14680818  0.20691916  1.        ]]. Action = [[-0.75749195 -0.28847206  0.3085308   0.03606033]]. Reward = [0.]
Curr episode timestep = 15
Action ignored: Workspace boundary
Scene graph at timestep 947 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 947 is tensor(0.0055, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 947 is 1
Human Feedback received at timestep 947 of 1
Current timestep = 948. State = [[-0.25008854  0.00248833  0.23259784  1.        ]]. Action = [[ 0.8682151  -0.62274015 -0.3556261  -0.04936874]]. Reward = [-1.]
Curr episode timestep = 16
Scene graph at timestep 948 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 948 is tensor(0.0104, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 948 is 1
Human Feedback received at timestep 948 of 1
Current timestep = 949. State = [[-0.25234208 -0.01004473  0.24078803  1.        ]]. Action = [[-0.26802266 -0.5252288   0.5557183   0.7842356 ]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 949 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 949 is tensor(0.0114, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 949 is 1
Human Feedback received at timestep 949 of 1
Current timestep = 950. State = [[-0.25787374 -0.02371658  0.2552621   1.        ]]. Action = [[-0.38379633 -0.5494965  -0.09347469  0.63486767]]. Reward = [0.]
Curr episode timestep = 1
Action ignored: Workspace boundary
Scene graph at timestep 950 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 950 is tensor(0.0107, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 950 is 1
Human Feedback received at timestep 950 of 1
Current timestep = 951. State = [[-0.25888434 -0.0261816   0.2555417   1.        ]]. Action = [[-0.72384405  0.5814383   0.4012959  -0.00213897]]. Reward = [0.]
Curr episode timestep = 2
Action ignored: Workspace boundary
Scene graph at timestep 951 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 951 is tensor(0.0110, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 951 is 1
Human Feedback received at timestep 951 of 1
Current timestep = 952. State = [[-0.24950553 -0.03955643  0.25078723  1.        ]]. Action = [[ 0.919868   -0.6509587  -0.571858    0.45133364]]. Reward = [0.]
Curr episode timestep = 3
Scene graph at timestep 952 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 952 is tensor(0.0069, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 952 is 1
Human Feedback received at timestep 952 of 1
Current timestep = 953. State = [[-0.23334035 -0.05182277  0.24148066  1.        ]]. Action = [[-0.37717265 -0.8757213  -0.7061706   0.634856  ]]. Reward = [0.]
Curr episode timestep = 4
Action ignored: Workspace boundary
Scene graph at timestep 953 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 953 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 953 is 1
Human Feedback received at timestep 953 of 1
Current timestep = 954. State = [[-0.22963782 -0.07356977  0.24741338  1.        ]]. Action = [[-0.02549028 -0.9594716   0.56424403  0.7964337 ]]. Reward = [0.]
Curr episode timestep = 5
Scene graph at timestep 954 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 954 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 954 is 1
Human Feedback received at timestep 954 of 1
Current timestep = 955. State = [[-0.23960933 -0.10200431  0.24914525  1.        ]]. Action = [[-0.57324064 -0.14475733 -0.58160025  0.8417027 ]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 955 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 955 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 955 is -1
Human Feedback received at timestep 955 of -1
Current timestep = 956. State = [[-0.23440586 -0.09816152  0.24703312  1.        ]]. Action = [[0.91460514 0.65688276 0.20017207 0.78716254]]. Reward = [0.]
Curr episode timestep = 7
Scene graph at timestep 956 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 956 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 956 is -1
Human Feedback received at timestep 956 of -1
Current timestep = 957. State = [[-0.22033906 -0.08789493  0.24595821  1.        ]]. Action = [[-0.92895734 -0.7235034   0.49953198  0.5813012 ]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 957 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 957 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 957 is 1
Human Feedback received at timestep 957 of 1
Current timestep = 958. State = [[-0.22330579 -0.08291686  0.24322037  1.        ]]. Action = [[-0.5682099   0.12127864 -0.17930901  0.8264773 ]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 958 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 958 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 958 is -1
Human Feedback received at timestep 958 of -1
Current timestep = 959. State = [[-0.2391081  -0.07843377  0.22898827  1.        ]]. Action = [[-0.5382233   0.16264486 -0.8077179   0.5898565 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 959 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 959 is tensor(0.0063, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 959 is -1
Human Feedback received at timestep 959 of -1
Current timestep = 960. State = [[-0.25350514 -0.07430635  0.20676778  1.        ]]. Action = [[ 0.12684274  0.02304077 -0.08809251  0.22027683]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 960 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 960 is tensor(0.0077, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 960 is 1
Human Feedback received at timestep 960 of 1
Current timestep = 961. State = [[-0.24272133 -0.08890585  0.20866233  1.        ]]. Action = [[ 0.6966214  -0.9234395   0.39748454  0.66840315]]. Reward = [0.]
Curr episode timestep = 12
Scene graph at timestep 961 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 961 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 961 is 1
Human Feedback received at timestep 961 of 1
Current timestep = 962. State = [[-0.23429565 -0.10417513  0.2120507   1.        ]]. Action = [[-0.84186006 -0.0287711   0.7990483   0.71998024]]. Reward = [0.]
Curr episode timestep = 13
Action ignored: Workspace boundary
Scene graph at timestep 962 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 962 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 962 is -1
Human Feedback received at timestep 962 of -1
Current timestep = 963. State = [[-0.22382763 -0.10540541  0.20816003  1.        ]]. Action = [[ 0.6037519   0.06545675 -0.4148594   0.70207167]]. Reward = [0.]
Curr episode timestep = 14
Scene graph at timestep 963 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 963 is tensor(0.0052, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 963 is -1
Human Feedback received at timestep 963 of -1
Current timestep = 964. State = [[-0.2120958  -0.10686397  0.18953902  1.        ]]. Action = [[-0.36778438 -0.00077677 -0.4162042   0.6921332 ]]. Reward = [0.]
Curr episode timestep = 15
Scene graph at timestep 964 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 964 is tensor(0.0053, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 964 is -1
Human Feedback received at timestep 964 of -1
Current timestep = 965. State = [[-0.22555113 -0.10455234  0.16878462  1.        ]]. Action = [[-0.87307954  0.28969598 -0.79734594  0.75035167]]. Reward = [0.]
Curr episode timestep = 16
Scene graph at timestep 965 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 965 is tensor(0.0059, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 965 is -1
Human Feedback received at timestep 965 of -1
Current timestep = 966. State = [[-0.25347188 -0.1022976   0.13827617  1.        ]]. Action = [[-0.41806197 -0.21619165 -0.2990024   0.8465518 ]]. Reward = [0.]
Curr episode timestep = 17
Scene graph at timestep 966 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 966 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 966 is 1
Human Feedback received at timestep 966 of 1
Current timestep = 967. State = [[-0.26300573 -0.10283732  0.12586446  1.        ]]. Action = [[-0.9278142  -0.01308191  0.04663754  0.57084703]]. Reward = [0.]
Curr episode timestep = 18
Action ignored: Workspace boundary
Scene graph at timestep 967 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 967 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 967 is -1
Human Feedback received at timestep 967 of -1
Current timestep = 968. State = [[-0.2668574  -0.10601407  0.12661967  1.        ]]. Action = [[-0.02398074 -0.1716485   0.27656162  0.37798917]]. Reward = [0.]
Curr episode timestep = 19
Scene graph at timestep 968 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 968 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 968 is 1
Human Feedback received at timestep 968 of 1
Current timestep = 969. State = [[-0.26845723 -0.10869755  0.1273491   1.        ]]. Action = [[-0.8779581  -0.737977    0.23748636  0.8289943 ]]. Reward = [0.]
Curr episode timestep = 20
Action ignored: Workspace boundary
Scene graph at timestep 969 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 969 is tensor(0.0036, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 969 is 1
Human Feedback received at timestep 969 of 1
Current timestep = 970. State = [[-0.26855198 -0.10908155  0.12741688  1.        ]]. Action = [[-0.86295354 -0.27104926 -0.19492722  0.338773  ]]. Reward = [0.]
Curr episode timestep = 21
Action ignored: Workspace boundary
Scene graph at timestep 970 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 970 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 970 is 1
Human Feedback received at timestep 970 of 1
Current timestep = 971. State = [[-0.2686754  -0.10923616  0.12743562  1.        ]]. Action = [[-0.8433581  -0.4630394  -0.3761825   0.36951196]]. Reward = [0.]
Curr episode timestep = 22
Action ignored: Workspace boundary
Scene graph at timestep 971 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 971 is tensor(0.0040, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 971 is 1
Human Feedback received at timestep 971 of 1
Current timestep = 972. State = [[-0.2686754  -0.10923616  0.12743562  1.        ]]. Action = [[-0.08308727 -0.62249476 -0.23422325  0.77386856]]. Reward = [0.]
Curr episode timestep = 23
Action ignored: Workspace boundary
Scene graph at timestep 972 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 972 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 972 is 1
Human Feedback received at timestep 972 of 1
Current timestep = 973. State = [[-0.2686754  -0.10923616  0.12743562  1.        ]]. Action = [[-0.04641491 -0.61662054 -0.7683389   0.67926717]]. Reward = [0.]
Curr episode timestep = 24
Action ignored: Workspace boundary
Scene graph at timestep 973 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 973 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 973 is 1
Human Feedback received at timestep 973 of 1
Current timestep = 974. State = [[-0.2686754  -0.10923616  0.12743562  1.        ]]. Action = [[-0.84228355 -0.5900012   0.2969129   0.65502906]]. Reward = [0.]
Curr episode timestep = 25
Action ignored: Workspace boundary
Scene graph at timestep 974 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 974 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 974 is 1
Human Feedback received at timestep 974 of 1
Current timestep = 975. State = [[-0.2686754  -0.10923616  0.12743562  1.        ]]. Action = [[-0.8583251  -0.565824   -0.37491548  0.730893  ]]. Reward = [0.]
Curr episode timestep = 26
Action ignored: Workspace boundary
Scene graph at timestep 975 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 975 is tensor(0.0028, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 975 is 1
Human Feedback received at timestep 975 of 1
Current timestep = 976. State = [[-0.26998824 -0.12307868  0.11909569  1.        ]]. Action = [[-0.01656747 -0.7114576  -0.860451    0.5938339 ]]. Reward = [0.]
Curr episode timestep = 27
Scene graph at timestep 976 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 976 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 976 is 1
Human Feedback received at timestep 976 of 1
Current timestep = 977. State = [[-0.26580465 -0.13098806  0.09652866  1.        ]]. Action = [[ 0.6468468   0.61512804 -0.09734404  0.7890463 ]]. Reward = [0.]
Curr episode timestep = 28
Scene graph at timestep 977 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 977 is tensor(0.0031, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 977 is 1
Human Feedback received at timestep 977 of 1
Current timestep = 978. State = [[-0.25482562 -0.12103947  0.09145194  1.        ]]. Action = [[-0.26048672 -0.29013044  0.73593473  0.32909167]]. Reward = [0.]
Curr episode timestep = 29
Action ignored: Workspace boundary
Scene graph at timestep 978 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 978 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 978 is 1
Human Feedback received at timestep 978 of 1
Current timestep = 979. State = [[-0.2550665  -0.11584933  0.0823615   1.        ]]. Action = [[-0.12403202  0.06444049 -0.85610634  0.6881001 ]]. Reward = [0.]
Curr episode timestep = 30
Scene graph at timestep 979 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 979 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 979 is 1
Human Feedback received at timestep 979 of 1
Current timestep = 980. State = [[-0.25989014 -0.1127453   0.05321417  1.        ]]. Action = [[ 0.01637042 -0.90877426 -0.90802604  0.6259208 ]]. Reward = [0.]
Curr episode timestep = 31
Action ignored: Workspace boundary
Scene graph at timestep 980 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 980 is tensor(0.0050, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 980 is 1
Human Feedback received at timestep 980 of 1
Current timestep = 981. State = [[-0.2565735  -0.10553582  0.05091069  1.        ]]. Action = [[0.23543417 0.40233517 0.33012724 0.75574994]]. Reward = [0.]
Curr episode timestep = 32
Scene graph at timestep 981 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 981 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 981 is 1
Human Feedback received at timestep 981 of 1
Current timestep = 982. State = [[-0.25296745 -0.10153015  0.05341824  1.        ]]. Action = [[-0.46497214 -0.30035806  0.5976839   0.51109624]]. Reward = [0.]
Curr episode timestep = 33
Action ignored: Workspace boundary
Scene graph at timestep 982 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 982 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 982 is 1
Human Feedback received at timestep 982 of 1
Current timestep = 983. State = [[-0.25224647 -0.09971375  0.05316847  1.        ]]. Action = [[ 0.6149204  -0.8187329  -0.9471365   0.50176716]]. Reward = [0.]
Curr episode timestep = 34
Action ignored: Workspace boundary
Scene graph at timestep 983 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 983 is tensor(0.0061, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 983 is 1
Human Feedback received at timestep 983 of 1
Current timestep = 984. State = [[-0.2518352  -0.09803655  0.05293141  1.        ]]. Action = [[-0.50974023 -0.6278839  -0.878129    0.8961983 ]]. Reward = [0.]
Curr episode timestep = 35
Action ignored: Workspace boundary
Scene graph at timestep 984 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 984 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 984 is 1
Human Feedback received at timestep 984 of 1
Current timestep = 985. State = [[-0.2514871  -0.09660295  0.05273314  1.        ]]. Action = [[-0.53677285 -0.06912756 -0.5261342   0.8320236 ]]. Reward = [0.]
Curr episode timestep = 36
Action ignored: Workspace boundary
Scene graph at timestep 985 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 985 is tensor(0.0045, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 985 is 1
Human Feedback received at timestep 985 of 1
Current timestep = 986. State = [[-0.24241866 -0.09255668  0.0545802   1.        ]]. Action = [[0.51516557 0.22624111 0.11864352 0.49026513]]. Reward = [0.]
Curr episode timestep = 37
Scene graph at timestep 986 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 986 is tensor(0.0044, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 986 is 1
Human Feedback received at timestep 986 of 1
Current timestep = 987. State = [[-0.23251905 -0.08932784  0.05585055  1.        ]]. Action = [[-0.5236507   0.1894362  -0.2521807   0.38978875]]. Reward = [0.]
Curr episode timestep = 38
Action ignored: Workspace boundary
Scene graph at timestep 987 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 987 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 987 is 1
Human Feedback received at timestep 987 of 1
Current timestep = 988. State = [[-0.23059294 -0.08762276  0.05607967  1.        ]]. Action = [[-0.19675857  0.4212618  -0.5041239   0.752167  ]]. Reward = [0.]
Curr episode timestep = 39
Action ignored: Workspace boundary
Scene graph at timestep 988 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 988 is tensor(0.0042, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 988 is 1
Human Feedback received at timestep 988 of 1
Current timestep = 989. State = [[-0.23054981 -0.10301708  0.05688518  1.        ]]. Action = [[-0.0731678  -0.9217688  -0.0616492   0.30771708]]. Reward = [0.]
Curr episode timestep = 40
Scene graph at timestep 989 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 989 is tensor(0.0048, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 989 is 1
Human Feedback received at timestep 989 of 1
Current timestep = 990. State = [[-0.22366042 -0.12214933  0.0640945   1.        ]]. Action = [[ 0.33235168 -0.12309408  0.47237086  0.5855694 ]]. Reward = [0.]
Curr episode timestep = 41
Scene graph at timestep 990 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 990 is tensor(0.0034, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 990 is 1
Human Feedback received at timestep 990 of 1
Current timestep = 991. State = [[-0.2157043  -0.12719208  0.07508016  1.        ]]. Action = [[-0.5995518 -0.9426137 -0.8259284  0.5342035]]. Reward = [0.]
Curr episode timestep = 42
Action ignored: Workspace boundary
Scene graph at timestep 991 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 991 is tensor(0.0038, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 991 is 1
Human Feedback received at timestep 991 of 1
Current timestep = 992. State = [[-0.22557832 -0.14844851  0.07781864  1.        ]]. Action = [[-0.828283   -0.9316602   0.1840682   0.69129753]]. Reward = [0.]
Curr episode timestep = 43
Scene graph at timestep 992 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 992 is tensor(0.0032, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 992 is 1
Human Feedback received at timestep 992 of 1
Current timestep = 993. State = [[-0.24076596 -0.18683672  0.08192746  1.        ]]. Action = [[-0.3259427  -0.5962681  -0.1347217   0.78801346]]. Reward = [0.]
Curr episode timestep = 44
Scene graph at timestep 993 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 993 is tensor(0.0029, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 993 is 1
Human Feedback received at timestep 993 of 1
Current timestep = 994. State = [[-0.25673744 -0.2250203   0.07549389  1.        ]]. Action = [[-0.25482875 -0.98037434 -0.50403607  0.43685424]]. Reward = [0.]
Curr episode timestep = 45
Scene graph at timestep 994 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 994 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 994 is 1
Human Feedback received at timestep 994 of 1
Current timestep = 995. State = [[-0.25662807 -0.24352814  0.06753314  1.        ]]. Action = [[ 0.61832094  0.38218546 -0.05691475  0.95901144]]. Reward = [0.]
Curr episode timestep = 46
Scene graph at timestep 995 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 995 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 995 is -1
Human Feedback received at timestep 995 of -1
Current timestep = 996. State = [[-0.25110188 -0.23854645  0.06606369  1.        ]]. Action = [[-0.641626   -0.59114534 -0.6928333   0.8863847 ]]. Reward = [0.]
Curr episode timestep = 47
Action ignored: Workspace boundary
Scene graph at timestep 996 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 996 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 996 is -1
Human Feedback received at timestep 996 of -1
Current timestep = 997. State = [[-0.24906182 -0.23578174  0.06544728  1.        ]]. Action = [[-0.8559336   0.9292104  -0.15223628  0.5649798 ]]. Reward = [0.]
Curr episode timestep = 48
Action ignored: Workspace boundary
Scene graph at timestep 997 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 997 is tensor(0.0008, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 997 is 1
Human Feedback received at timestep 997 of 1
Current timestep = 998. State = [[-0.24842885 -0.23484114  0.06521362  1.        ]]. Action = [[-0.97148746 -0.3197024  -0.7691092   0.88922787]]. Reward = [0.]
Curr episode timestep = 49
Action ignored: Workspace boundary
Scene graph at timestep 998 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 998 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 998 is -1
Human Feedback received at timestep 998 of -1
Current timestep = 999. State = [[-0.24838957 -0.23486948  0.06519172  1.        ]]. Action = [[-0.83827776 -0.9233427   0.6230705   0.5131892 ]]. Reward = [0.]
Curr episode timestep = 50
Action ignored: Workspace boundary
Scene graph at timestep 999 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 999 is tensor(0.0016, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 999 is 1
Human Feedback received at timestep 999 of 1
Current timestep = 1000. State = [[-0.24838957 -0.23486948  0.06519172  1.        ]]. Action = [[-0.9647963 -0.8543575  0.8413366  0.4440105]]. Reward = [0.]
Curr episode timestep = 51
Action ignored: Workspace boundary
Scene graph at timestep 1000 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1000 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1000 is 1
Human Feedback received at timestep 1000 of 1
Current timestep = 1001. State = [[-0.25012797 -0.23231132  0.0589273   1.        ]]. Action = [[-0.08409899  0.10711944 -0.46176243  0.7346591 ]]. Reward = [0.]
Curr episode timestep = 52
Scene graph at timestep 1001 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1001 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1001 is 1
Human Feedback received at timestep 1001 of 1
Current timestep = 1002. State = [[-0.25270265 -0.2303356   0.04576283  1.        ]]. Action = [[-0.76966804 -0.70680416  0.15904915  0.576802  ]]. Reward = [0.]
Curr episode timestep = 53
Action ignored: Workspace boundary
Scene graph at timestep 1002 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1002 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1002 is 1
Human Feedback received at timestep 1002 of 1
Current timestep = 1003. State = [[-0.2526126  -0.22999683  0.04502196  1.        ]]. Action = [[ 0.00871432 -0.60107666 -0.3505298   0.6135087 ]]. Reward = [0.]
Curr episode timestep = 54
Action ignored: Workspace boundary
Scene graph at timestep 1003 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1003 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1003 is 1
Human Feedback received at timestep 1003 of 1
Current timestep = 1004. State = [[-0.2526126  -0.22999683  0.04502196  1.        ]]. Action = [[-0.34566224  0.0939877   0.61347806  0.56466293]]. Reward = [0.]
Curr episode timestep = 55
Action ignored: Workspace boundary
Scene graph at timestep 1004 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1004 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1004 is 1
Human Feedback received at timestep 1004 of 1
Current timestep = 1005. State = [[-0.25258106 -0.22993994  0.04501085  1.        ]]. Action = [[-0.6744404  -0.14361846 -0.6298842   0.6568005 ]]. Reward = [0.]
Curr episode timestep = 56
Action ignored: Workspace boundary
Scene graph at timestep 1005 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1005 is tensor(4.3916e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1005 is 1
Human Feedback received at timestep 1005 of 1
Current timestep = 1006. State = [[-0.25258106 -0.22993994  0.04501085  1.        ]]. Action = [[-0.51150244 -0.9828197  -0.33633113  0.39476323]]. Reward = [0.]
Curr episode timestep = 57
Action ignored: Workspace boundary
Scene graph at timestep 1006 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1006 is tensor(7.8643e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1006 is 1
Human Feedback received at timestep 1006 of 1
Current timestep = 1007. State = [[-0.25258106 -0.22993994  0.04501085  1.        ]]. Action = [[-0.727448   -0.867717   -0.00894886  0.7234788 ]]. Reward = [0.]
Curr episode timestep = 58
Action ignored: Workspace boundary
Scene graph at timestep 1007 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1007 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1007 is 1
Human Feedback received at timestep 1007 of 1
Current timestep = 1008. State = [[-0.25251797 -0.22982614  0.04498865  1.        ]]. Action = [[-0.5407395  -0.01589191  0.26529348  0.7547586 ]]. Reward = [0.]
Curr episode timestep = 59
Action ignored: Workspace boundary
Scene graph at timestep 1008 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1008 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1008 is 1
Human Feedback received at timestep 1008 of 1
Current timestep = 1009. State = [[-0.25196224 -0.21768887  0.03936767  1.        ]]. Action = [[-0.12158304  0.7652867  -0.2676934   0.9492841 ]]. Reward = [0.]
Curr episode timestep = 60
Scene graph at timestep 1009 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1009 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1009 is -1
Human Feedback received at timestep 1009 of -1
Current timestep = 1010. State = [[-0.25172296 -0.2076081   0.02913547  1.        ]]. Action = [[ 0.67376995 -0.02332425 -0.71574     0.734923  ]]. Reward = [0.]
Curr episode timestep = 61
Action ignored: Workspace boundary
Scene graph at timestep 1010 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1010 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1010 is 1
Human Feedback received at timestep 1010 of 1
Current timestep = 1011. State = [[-0.24974065 -0.20276974  0.02815613  1.        ]]. Action = [[-0.19107807  0.05049062 -0.21270347  0.4340546 ]]. Reward = [0.]
Curr episode timestep = 62
Action ignored: Workspace boundary
Scene graph at timestep 1011 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1011 is tensor(0.0021, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1011 is 1
Human Feedback received at timestep 1011 of 1
Current timestep = 1012. State = [[-0.2421733  -0.20736039  0.0332778   1.        ]]. Action = [[ 0.60952806 -0.44047332  0.46607113  0.48584938]]. Reward = [0.]
Curr episode timestep = 63
Scene graph at timestep 1012 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1012 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1012 is 1
Human Feedback received at timestep 1012 of 1
Current timestep = 1013. State = [[-0.22448017 -0.22826245  0.04998996  1.        ]]. Action = [[ 0.6047789  -0.8682129   0.71442735  0.11689079]]. Reward = [0.]
Curr episode timestep = 64
Scene graph at timestep 1013 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1013 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1013 is 1
Human Feedback received at timestep 1013 of 1
Current timestep = 1014. State = [[-0.2060186  -0.24995609  0.07090144  1.        ]]. Action = [[-0.9147327  -0.5188848  -0.8719518   0.85433733]]. Reward = [0.]
Curr episode timestep = 65
Action ignored: Workspace boundary
Scene graph at timestep 1014 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1014 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1014 is 1
Human Feedback received at timestep 1014 of 1
Current timestep = 1015. State = [[-0.2124183  -0.2684461   0.07563873  1.        ]]. Action = [[-0.7787761  -0.5736266   0.1662606   0.78057086]]. Reward = [0.]
Curr episode timestep = 66
Scene graph at timestep 1015 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1015 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1015 is 1
Human Feedback received at timestep 1015 of 1
Current timestep = 1016. State = [[-0.23577163 -0.27844647  0.07368093  1.        ]]. Action = [[-0.6949704   0.38532174 -0.40095067  0.74656606]]. Reward = [0.]
Curr episode timestep = 67
Scene graph at timestep 1016 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1016 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1016 is 1
Human Feedback received at timestep 1016 of 1
Current timestep = 1017. State = [[-0.25201815 -0.27307814  0.07026038  1.        ]]. Action = [[-0.23280859 -0.78402543  0.27510452  0.8838513 ]]. Reward = [0.]
Curr episode timestep = 68
Action ignored: Workspace boundary
Scene graph at timestep 1017 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1017 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1017 is 1
Human Feedback received at timestep 1017 of 1
Current timestep = 1018. State = [[-0.25135133 -0.27413547  0.07575312  1.        ]]. Action = [[ 0.15642571 -0.03053826  0.48668516  0.74824023]]. Reward = [0.]
Curr episode timestep = 69
Scene graph at timestep 1018 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1018 is tensor(7.8392e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1018 is 1
Human Feedback received at timestep 1018 of 1
Current timestep = 1019. State = [[-0.25082982 -0.27415097  0.08330291  1.        ]]. Action = [[-0.88057244 -0.9231851   0.60952675  0.6472025 ]]. Reward = [0.]
Curr episode timestep = 70
Action ignored: Workspace boundary
Scene graph at timestep 1019 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1019 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1019 is 1
Human Feedback received at timestep 1019 of 1
Current timestep = 1020. State = [[-0.24490222 -0.27853048  0.07988841  1.        ]]. Action = [[ 0.63057566 -0.38375175 -0.509422    0.88221836]]. Reward = [0.]
Curr episode timestep = 71
Scene graph at timestep 1020 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1020 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1020 is 1
Human Feedback received at timestep 1020 of 1
Current timestep = 1021. State = [[-0.23536777 -0.28494292  0.07312852  1.        ]]. Action = [[ 0.14804792 -0.87400395  0.5981684   0.8615799 ]]. Reward = [0.]
Curr episode timestep = 72
Action ignored: Workspace boundary
Scene graph at timestep 1021 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1021 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1021 is 1
Human Feedback received at timestep 1021 of 1
Current timestep = 1022. State = [[-0.23476271 -0.28540856  0.07326384  1.        ]]. Action = [[-0.69989145 -0.70081913  0.3428347   0.680161  ]]. Reward = [0.]
Curr episode timestep = 73
Action ignored: Workspace boundary
Scene graph at timestep 1022 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1022 is tensor(5.8687e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1022 is 1
Human Feedback received at timestep 1022 of 1
Current timestep = 1023. State = [[-0.23460644 -0.28549552  0.07329746  1.        ]]. Action = [[-0.07832956 -0.9393927   0.02513111  0.88611877]]. Reward = [0.]
Curr episode timestep = 74
Action ignored: Workspace boundary
Scene graph at timestep 1023 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1023 is tensor(8.9625e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1023 is 1
Human Feedback received at timestep 1023 of 1
Current timestep = 1024. State = [[-0.23461978 -0.28554332  0.07329749  1.        ]]. Action = [[ 0.22349226 -0.9554001  -0.39152837  0.6774049 ]]. Reward = [0.]
Curr episode timestep = 75
Action ignored: Workspace boundary
Scene graph at timestep 1024 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1024 is tensor(2.2254e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1024 is 1
Human Feedback received at timestep 1024 of 1
Current timestep = 1025. State = [[-0.23461978 -0.28554332  0.07329749  1.        ]]. Action = [[-0.96538955  0.4707477   0.18065548  0.69448197]]. Reward = [0.]
Curr episode timestep = 76
Action ignored: Workspace boundary
Scene graph at timestep 1025 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1025 is tensor(2.4411e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1025 is 1
Human Feedback received at timestep 1025 of 1
Current timestep = 1026. State = [[-0.23461978 -0.28554332  0.07329749  1.        ]]. Action = [[-0.87516755 -0.67729115  0.48824716  0.676319  ]]. Reward = [0.]
Curr episode timestep = 77
Action ignored: Workspace boundary
Scene graph at timestep 1026 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1026 is tensor(6.8990e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1026 is -1
Human Feedback received at timestep 1026 of -1
Current timestep = 1027. State = [[-0.22922269 -0.28053698  0.0753242   1.        ]]. Action = [[0.32712507 0.3497939  0.29211307 0.87095284]]. Reward = [0.]
Curr episode timestep = 78
Scene graph at timestep 1027 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1027 is tensor(6.9035e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1027 is 1
Human Feedback received at timestep 1027 of 1
Current timestep = 1028. State = [[-0.22337863 -0.27450958  0.07524461  1.        ]]. Action = [[-0.6655152  -0.66911584 -0.7888884   0.506632  ]]. Reward = [0.]
Curr episode timestep = 79
Action ignored: Workspace boundary
Scene graph at timestep 1028 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1028 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1028 is 1
Human Feedback received at timestep 1028 of 1
Current timestep = 1029. State = [[-0.22200641 -0.27320984  0.07505186  1.        ]]. Action = [[-0.8283599  -0.619755   -0.4229282   0.49849284]]. Reward = [0.]
Curr episode timestep = 80
Action ignored: Workspace boundary
Scene graph at timestep 1029 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1029 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1029 is 1
Human Feedback received at timestep 1029 of 1
Current timestep = 1030. State = [[-0.21300098 -0.28237066  0.07300082  1.        ]]. Action = [[ 0.48170447 -0.5075923  -0.32445097  0.24885404]]. Reward = [0.]
Curr episode timestep = 81
Scene graph at timestep 1030 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1030 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1030 is 1
Human Feedback received at timestep 1030 of 1
Current timestep = 1031. State = [[-0.20077592 -0.2910344   0.07058225  1.        ]]. Action = [[ 0.0819732 -0.4706223  0.8464885  0.8417729]]. Reward = [0.]
Curr episode timestep = 82
Action ignored: Workspace boundary
Scene graph at timestep 1031 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1031 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1031 is -1
Human Feedback received at timestep 1031 of -1
Current timestep = 1032. State = [[-0.20181303 -0.29278535  0.07177199  1.        ]]. Action = [[-0.28747928  0.01752985  0.30744302  0.70096874]]. Reward = [0.]
Curr episode timestep = 83
Scene graph at timestep 1032 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1032 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1032 is 1
Human Feedback received at timestep 1032 of 1
Current timestep = 1033. State = [[-0.20327526 -0.29412162  0.07295045  1.        ]]. Action = [[ 0.04928958 -0.744829   -0.7387562   0.35241032]]. Reward = [0.]
Curr episode timestep = 84
Action ignored: Workspace boundary
Scene graph at timestep 1033 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1033 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1033 is 1
Human Feedback received at timestep 1033 of 1
Current timestep = 1034. State = [[-0.2090938  -0.29632336  0.07786322  1.        ]]. Action = [[-0.456262    0.02521002  0.27758944  0.8577306 ]]. Reward = [0.]
Curr episode timestep = 85
Scene graph at timestep 1034 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1034 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1034 is 1
Human Feedback received at timestep 1034 of 1
Current timestep = 1035. State = [[-0.21214786 -0.29891858  0.08480711  1.        ]]. Action = [[-0.35491908 -0.96052885 -0.6119224   0.80201054]]. Reward = [0.]
Curr episode timestep = 86
Action ignored: Workspace boundary
Scene graph at timestep 1035 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1035 is tensor(5.4738e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1035 is 1
Human Feedback received at timestep 1035 of 1
Current timestep = 1036. State = [[-0.20985085 -0.2982576   0.09379184  1.        ]]. Action = [[0.28238535 0.16475523 0.3743645  0.92227757]]. Reward = [0.]
Curr episode timestep = 87
Scene graph at timestep 1036 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1036 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1036 is 1
Human Feedback received at timestep 1036 of 1
Current timestep = 1037. State = [[-0.20687248 -0.29935905  0.11497358  1.        ]]. Action = [[-0.13280338 -0.06310987  0.67794836  0.20361805]]. Reward = [0.]
Curr episode timestep = 88
Scene graph at timestep 1037 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1037 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1037 is 1
Human Feedback received at timestep 1037 of 1
Current timestep = 1038. State = [[-0.20962626 -0.29710743  0.13502201  1.        ]]. Action = [[-0.3011757  -0.21747583 -0.37154496  0.8594717 ]]. Reward = [0.]
Curr episode timestep = 89
Action ignored: Workspace boundary
Scene graph at timestep 1038 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1038 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1038 is 1
Human Feedback received at timestep 1038 of 1
Current timestep = 1039. State = [[-0.20503066 -0.293882    0.14076103  1.        ]]. Action = [[0.53265214 0.12573159 0.27094162 0.46524024]]. Reward = [0.]
Curr episode timestep = 90
Scene graph at timestep 1039 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1039 is tensor(0.0010, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1039 is 1
Human Feedback received at timestep 1039 of 1
Current timestep = 1040. State = [[-0.19221278 -0.28179824  0.16237001  1.        ]]. Action = [[0.2843933  0.47827685 0.7640443  0.5878444 ]]. Reward = [0.]
Curr episode timestep = 91
Scene graph at timestep 1040 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1040 is tensor(0.0015, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1040 is -1
Human Feedback received at timestep 1040 of -1
Current timestep = 1041. State = [[-0.18306111 -0.27001455  0.18388668  1.        ]]. Action = [[-0.61106247 -0.5095819   0.19201744  0.8567263 ]]. Reward = [0.]
Curr episode timestep = 92
Action ignored: Workspace boundary
Scene graph at timestep 1041 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1041 is tensor(0.0022, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1041 is -1
Human Feedback received at timestep 1041 of -1
Current timestep = 1042. State = [[-0.18632689 -0.28016227  0.19181062  1.        ]]. Action = [[-0.3917315  -0.6051376   0.3241912   0.69599545]]. Reward = [0.]
Curr episode timestep = 93
Scene graph at timestep 1042 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1042 is tensor(0.0024, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1042 is 1
Human Feedback received at timestep 1042 of 1
Current timestep = 1043. State = [[-0.18556646 -0.29527077  0.19522238  1.        ]]. Action = [[ 0.7396188  -0.42273647 -0.8107122   0.8145604 ]]. Reward = [0.]
Curr episode timestep = 94
Scene graph at timestep 1043 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1043 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1043 is -1
Human Feedback received at timestep 1043 of -1
Current timestep = 1044. State = [[-0.16897236 -0.30724722  0.17910732  1.        ]]. Action = [[ 0.7409444  -0.66601944  0.00157344  0.6627121 ]]. Reward = [0.]
Curr episode timestep = 95
Action ignored: Workspace boundary
Scene graph at timestep 1044 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1044 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1044 is 1
Human Feedback received at timestep 1044 of 1
Current timestep = 1045. State = [[-0.17525426 -0.29988638  0.18500431  1.        ]]. Action = [[-0.8461792   0.7204664   0.57736254  0.6952566 ]]. Reward = [0.]
Curr episode timestep = 96
Scene graph at timestep 1045 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1045 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1045 is -1
Human Feedback received at timestep 1045 of -1
Current timestep = 1046. State = [[-0.19260526 -0.28572667  0.18983673  1.        ]]. Action = [[-0.1040948  0.1828332 -0.4026779  0.6590462]]. Reward = [0.]
Curr episode timestep = 97
Scene graph at timestep 1046 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1046 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1046 is -1
Human Feedback received at timestep 1046 of -1
Current timestep = 1047. State = [[-0.19518657 -0.2823298   0.18752077  1.        ]]. Action = [[-0.16969979 -0.87902117 -0.95362234  0.81833684]]. Reward = [0.]
Curr episode timestep = 98
Action ignored: Workspace boundary
Scene graph at timestep 1047 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1047 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1047 is -1
Human Feedback received at timestep 1047 of -1
Current timestep = 1048. State = [[-0.19551757 -0.28183612  0.18729956  1.        ]]. Action = [[ 0.40464365 -0.8954128  -0.6880891   0.5662787 ]]. Reward = [0.]
Curr episode timestep = 99
Action ignored: Workspace boundary
Scene graph at timestep 1048 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1048 is tensor(9.6982e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1048 is 1
Human Feedback received at timestep 1048 of 1
Current timestep = 1049. State = [[-0.20036599 -0.27147692  0.18126965  1.        ]]. Action = [[-0.2446906   0.51937985 -0.3599761   0.3497951 ]]. Reward = [0.]
Curr episode timestep = 100
Scene graph at timestep 1049 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1049 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1049 is -1
Human Feedback received at timestep 1049 of -1
Current timestep = 1050. State = [[-0.21013437 -0.2576532   0.17253417  1.        ]]. Action = [[ 0.33859336 -0.721108   -0.63572305  0.74716187]]. Reward = [0.]
Curr episode timestep = 101
Action ignored: Workspace boundary
Scene graph at timestep 1050 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1050 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1050 is 1
Human Feedback received at timestep 1050 of 1
Current timestep = 1051. State = [[-0.22157839 -0.26564863  0.17931862  1.        ]]. Action = [[-0.7266901 -0.4472456  0.6274371  0.6529288]]. Reward = [0.]
Curr episode timestep = 102
Scene graph at timestep 1051 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1051 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1051 is -1
Human Feedback received at timestep 1051 of -1
Current timestep = 1052. State = [[-0.23971696 -0.26401755  0.18310489  1.        ]]. Action = [[-0.03857791  0.3909092  -0.4572544   0.7548268 ]]. Reward = [0.]
Curr episode timestep = 103
Scene graph at timestep 1052 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1052 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1052 is 1
Human Feedback received at timestep 1052 of 1
Current timestep = 1053. State = [[-0.2441121  -0.25864244  0.17910019  1.        ]]. Action = [[-0.57545483 -0.2148006   0.5399202   0.8917227 ]]. Reward = [0.]
Curr episode timestep = 104
Action ignored: Workspace boundary
Scene graph at timestep 1053 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1053 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1053 is 1
Human Feedback received at timestep 1053 of 1
Current timestep = 1054. State = [[-0.24419105 -0.25803867  0.17854217  1.        ]]. Action = [[ 0.12136972 -0.13662976 -0.04581094  0.68703854]]. Reward = [0.]
Curr episode timestep = 105
Scene graph at timestep 1054 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1054 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1054 is 1
Human Feedback received at timestep 1054 of 1
Current timestep = 1055. State = [[-0.24408442 -0.2581386   0.17834648  1.        ]]. Action = [[-0.73479843 -0.9495751   0.47072363  0.71140695]]. Reward = [0.]
Curr episode timestep = 106
Action ignored: Workspace boundary
Scene graph at timestep 1055 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1055 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1055 is -1
Human Feedback received at timestep 1055 of -1
Current timestep = 1056. State = [[-0.24408442 -0.2581386   0.17834648  1.        ]]. Action = [[-0.92653203  0.27158928  0.25672865  0.3512609 ]]. Reward = [0.]
Curr episode timestep = 107
Action ignored: Workspace boundary
Scene graph at timestep 1056 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1056 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1056 is -1
Human Feedback received at timestep 1056 of -1
Current timestep = 1057. State = [[-0.24404968 -0.25808987  0.17833686  1.        ]]. Action = [[-0.51143557 -0.06488949  0.27484715  0.3318621 ]]. Reward = [0.]
Curr episode timestep = 108
Action ignored: Workspace boundary
Scene graph at timestep 1057 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1057 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1057 is 1
Human Feedback received at timestep 1057 of 1
Current timestep = 1058. State = [[-0.2345201  -0.25708914  0.17906581  1.        ]]. Action = [[ 0.67071867 -0.00077564  0.04129124  0.45607638]]. Reward = [0.]
Curr episode timestep = 109
Scene graph at timestep 1058 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1058 is tensor(0.0006, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1058 is 1
Human Feedback received at timestep 1058 of 1
Current timestep = 1059. State = [[-0.22557813 -0.2548514   0.16982059  1.        ]]. Action = [[ 0.1187315  -0.13161111 -0.7762687   0.10960019]]. Reward = [0.]
Curr episode timestep = 110
Scene graph at timestep 1059 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1059 is tensor(0.0018, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1059 is -1
Human Feedback received at timestep 1059 of -1
Current timestep = 1060. State = [[-0.22717981 -0.26956415  0.13582379  1.        ]]. Action = [[-0.459705   -0.61683667 -0.6768364   0.43236136]]. Reward = [0.]
Curr episode timestep = 111
Scene graph at timestep 1060 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1060 is tensor(0.0009, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1060 is -1
Human Feedback received at timestep 1060 of -1
Current timestep = 1061. State = [[-0.22821918 -0.2871099   0.11277839  1.        ]]. Action = [[ 0.44213986 -0.15695226  0.01176512  0.63581204]]. Reward = [0.]
Curr episode timestep = 112
Scene graph at timestep 1061 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1061 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1061 is -1
Human Feedback received at timestep 1061 of -1
Current timestep = 1062. State = [[-0.22032379 -0.29233646  0.10873529  1.        ]]. Action = [[ 0.11610532 -0.5431803  -0.6288967   0.7837229 ]]. Reward = [0.]
Curr episode timestep = 113
Action ignored: Workspace boundary
Scene graph at timestep 1062 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1062 is tensor(5.0010e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1062 is -1
Human Feedback received at timestep 1062 of -1
Current timestep = 1063. State = [[-0.2201123  -0.29246944  0.1086987   1.        ]]. Action = [[-0.45994115 -0.5953958   0.589836    0.6393368 ]]. Reward = [0.]
Curr episode timestep = 114
Action ignored: Workspace boundary
Scene graph at timestep 1063 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1063 is tensor(8.1942e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1063 is 1
Human Feedback received at timestep 1063 of 1
Current timestep = 1064. State = [[-0.22004864 -0.2925069   0.10871833  1.        ]]. Action = [[ 0.7144122 -0.9002395  0.1778599  0.6437702]]. Reward = [0.]
Curr episode timestep = 115
Action ignored: Workspace boundary
Scene graph at timestep 1064 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1064 is tensor(1.5034e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1064 is 1
Human Feedback received at timestep 1064 of 1
Current timestep = 1065. State = [[-0.22004864 -0.2925069   0.10871833  1.        ]]. Action = [[-0.86305904 -0.20958138  0.4805746   0.62329555]]. Reward = [0.]
Curr episode timestep = 116
Action ignored: Workspace boundary
Scene graph at timestep 1065 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1065 is tensor(5.4479e-05, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1065 is 1
Human Feedback received at timestep 1065 of 1
Current timestep = 1066. State = [[-0.22004864 -0.2925069   0.10871833  1.        ]]. Action = [[-0.62471306 -0.47168732 -0.5821611   0.32984698]]. Reward = [0.]
Curr episode timestep = 117
Action ignored: Workspace boundary
Scene graph at timestep 1066 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1066 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1066 is 1
Human Feedback received at timestep 1066 of 1
Current timestep = 1067. State = [[-0.21752758 -0.28880823  0.11529176  1.        ]]. Action = [[-0.08111548  0.4201833   0.64611197  0.5924139 ]]. Reward = [0.]
Curr episode timestep = 118
Scene graph at timestep 1067 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1067 is tensor(0.0002, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1067 is -1
Human Feedback received at timestep 1067 of -1
Current timestep = 1068. State = [[-0.21478646 -0.2877848   0.13647199  1.        ]]. Action = [[ 0.0024569  -0.12367791  0.65924144  0.8097532 ]]. Reward = [0.]
Curr episode timestep = 119
Scene graph at timestep 1068 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1068 is tensor(0.0004, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1068 is -1
Human Feedback received at timestep 1068 of -1
Current timestep = 1069. State = [[-0.21304102 -0.287537    0.15444511  1.        ]]. Action = [[-0.5098614  -0.5981933   0.3137561   0.03223515]]. Reward = [0.]
Curr episode timestep = 120
Action ignored: Workspace boundary
Scene graph at timestep 1069 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1069 is tensor(0.0012, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1069 is -1
Human Feedback received at timestep 1069 of -1
Current timestep = 1070. State = [[-0.20936008 -0.27868992  0.16052283  1.        ]]. Action = [[0.20507956 0.5209801  0.24887645 0.855592  ]]. Reward = [0.]
Curr episode timestep = 121
Scene graph at timestep 1070 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1070 is tensor(0.0003, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1070 is -1
Human Feedback received at timestep 1070 of -1
Current timestep = 1071. State = [[-0.20727274 -0.2738219   0.17105396  1.        ]]. Action = [[ 0.0037123  -0.4453084   0.14524245  0.6030741 ]]. Reward = [0.]
Curr episode timestep = 122
Scene graph at timestep 1071 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1071 is tensor(0.0011, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1071 is -1
Human Feedback received at timestep 1071 of -1
Current timestep = 1072. State = [[-0.20786983 -0.27646786  0.1740225   1.        ]]. Action = [[-0.46822023 -0.6490888  -0.83140177  0.89837575]]. Reward = [0.]
Curr episode timestep = 123
Action ignored: Workspace boundary
Scene graph at timestep 1072 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1072 is tensor(0.0007, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1072 is -1
Human Feedback received at timestep 1072 of -1
Current timestep = 1073. State = [[-0.21455924 -0.26757297  0.18398248  1.        ]]. Action = [[-0.714969   0.7129333  0.5629995  0.8986249]]. Reward = [0.]
Curr episode timestep = 124
Scene graph at timestep 1073 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1073 is tensor(0.0001, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1073 is -1
Human Feedback received at timestep 1073 of -1
Current timestep = 1074. State = [[-0.22947434 -0.2558648   0.20176284  1.        ]]. Action = [[ 0.21616888 -0.8522935   0.61420155  0.801291  ]]. Reward = [0.]
Curr episode timestep = 125
Action ignored: Workspace boundary
Scene graph at timestep 1074 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1074 is tensor(0.0005, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1074 is 1
Human Feedback received at timestep 1074 of 1
Current timestep = 1075. State = [[-0.25013322  0.0027538   0.23267783  1.        ]]. Action = [[ 0.15325499 -0.3933584  -0.6127859   0.64300513]]. Reward = [0.]
Curr episode timestep = 126
Scene graph at timestep 1075 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1075 is tensor(0.0183, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1075 is 1
Human Feedback received at timestep 1075 of 1
Current timestep = 1076. State = [[-0.25028113  0.0010652   0.22572508  1.        ]]. Action = [[-0.0696016  -0.07676667 -0.9194354   0.78005433]]. Reward = [0.]
Curr episode timestep = 0
Scene graph at timestep 1076 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1076 is tensor(0.0056, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1076 is 1
Human Feedback received at timestep 1076 of 1
Current timestep = 1077. State = [[-0.24642314  0.00784711  0.21049225  1.        ]]. Action = [[0.41412413 0.49693    0.13127172 0.84766996]]. Reward = [0.]
Curr episode timestep = 1
Scene graph at timestep 1077 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1077 is tensor(0.0067, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1077 is 1
Human Feedback received at timestep 1077 of 1
Current timestep = 1078. State = [[-0.24298443  0.00143395  0.1991841   1.        ]]. Action = [[-0.11072272 -0.8011729  -0.8465736   0.71663713]]. Reward = [0.]
Curr episode timestep = 2
Scene graph at timestep 1078 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1078 is tensor(0.0078, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1078 is 1
Human Feedback received at timestep 1078 of 1
Current timestep = 1079. State = [[-0.24352466 -0.01209294  0.17344885  1.        ]]. Action = [[-0.5315705  -0.33083487 -0.77267534  0.677644  ]]. Reward = [0.]
Curr episode timestep = 3
Action ignored: Workspace boundary
Scene graph at timestep 1079 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1079 is tensor(0.0087, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1079 is 1
Human Feedback received at timestep 1079 of 1
Current timestep = 1080. State = [[-0.24803379 -0.03283245  0.16268359  1.        ]]. Action = [[-0.3810767 -0.920556  -0.6422643  0.4756745]]. Reward = [0.]
Curr episode timestep = 4
Scene graph at timestep 1080 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1080 is tensor(0.0079, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1080 is 1
Human Feedback received at timestep 1080 of 1
Current timestep = 1081. State = [[-0.25289243 -0.05488691  0.14104772  1.        ]]. Action = [[-0.4754097  -0.48466158  0.35495782  0.6901146 ]]. Reward = [0.]
Curr episode timestep = 5
Action ignored: Workspace boundary
Scene graph at timestep 1081 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1081 is tensor(0.0072, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1081 is 1
Human Feedback received at timestep 1081 of 1
Current timestep = 1082. State = [[-0.2539982  -0.07247768  0.13706012  1.        ]]. Action = [[ 0.09931028 -0.7020959  -0.09795403  0.58438516]]. Reward = [0.]
Curr episode timestep = 6
Scene graph at timestep 1082 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1082 is tensor(0.0060, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1082 is 1
Human Feedback received at timestep 1082 of 1
Current timestep = 1083. State = [[-0.25188822 -0.09203811  0.13596486  1.        ]]. Action = [[-0.50924385 -0.57931304 -0.1523205   0.67257035]]. Reward = [0.]
Curr episode timestep = 7
Action ignored: Workspace boundary
Scene graph at timestep 1083 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1083 is tensor(0.0057, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1083 is 1
Human Feedback received at timestep 1083 of 1
Current timestep = 1084. State = [[-0.2517598  -0.09434193  0.13590547  1.        ]]. Action = [[-0.33858246 -0.4036864  -0.9686475   0.26634037]]. Reward = [0.]
Curr episode timestep = 8
Action ignored: Workspace boundary
Scene graph at timestep 1084 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1084 is tensor(0.0051, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1084 is 1
Human Feedback received at timestep 1084 of 1
Current timestep = 1085. State = [[-0.24376522 -0.10800026  0.12829813  1.        ]]. Action = [[ 0.6894971  -0.79706764 -0.94611734  0.48865283]]. Reward = [0.]
Curr episode timestep = 9
Scene graph at timestep 1085 is [True, False, False, False, True, False, False, True, True, False]
State prediction error at timestep 1085 is tensor(0.0035, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1085 is 1
Human Feedback received at timestep 1085 of 1
Current timestep = 1086. State = [[-0.22250098 -0.13419071  0.10191067  1.        ]]. Action = [[ 0.4771675  -0.13717598  0.55611014  0.5862694 ]]. Reward = [0.]
Curr episode timestep = 10
Scene graph at timestep 1086 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1086 is tensor(0.0043, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1086 is 1
Human Feedback received at timestep 1086 of 1
Current timestep = 1087. State = [[-0.20065016 -0.14499353  0.09823203  1.        ]]. Action = [[ 0.6834874  -0.31142277 -0.829886    0.51346755]]. Reward = [0.]
Curr episode timestep = 11
Scene graph at timestep 1087 is [True, False, False, True, False, False, False, True, True, False]
State prediction error at timestep 1087 is tensor(0.0023, grad_fn=<MseLossBackward0>)
Recommended Feedback at timestep 1087 is -1
Human Feedback received at timestep 1087 of -1
