{<class 'stable_baselines3.common.policies.ActorCriticPolicy'>: {'MlpPolicy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'CnnPolicy': <class 'stable_baselines3.common.policies.ActorCriticCnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.common.policies.MultiInputActorCriticPolicy'>}, <class 'stable_baselines3.td3.policies.TD3Policy'>: {'MlpPolicy': <class 'stable_baselines3.td3.policies.TD3Policy'>, 'CnnPolicy': <class 'stable_baselines3.td3.policies.CnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.td3.policies.MultiInputPolicy'>}, <class 'stable_baselines3.dqn.policies.DQNPolicy'>: {'MlpPolicy': <class 'stable_baselines3.dqn.policies.DQNPolicy'>, 'CnnPolicy': <class 'stable_baselines3.dqn.policies.CnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.dqn.policies.MultiInputPolicy'>}, <class 'stable_baselines3.sac.policies.SACPolicy'>: {'MlpPolicy': <class 'stable_baselines3.sac.policies.SACPolicy'>, 'CnnPolicy': <class 'stable_baselines3.sac.policies.CnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.sac.policies.MultiInputPolicy'>}, <class 'stable_baselines3.active_tamer.policies.SACPolicy'>: {'MlpPolicy': <class 'stable_baselines3.active_tamer.policies.SACPolicy'>, 'CnnPolicy': <class 'stable_baselines3.active_tamer.policies.CnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.active_tamer.policies.MultiInputPolicy'>}, <class 'stable_baselines3.active_tamer.policies.SACHPolicy'>: {'HumanMlpPolicy': <class 'stable_baselines3.active_tamer.policies.SACHPolicy'>}, <class 'stable_baselines3.active_tamer.policies.ActiveSACHPolicy'>: {'ActiveMlpPolicy': <class 'stable_baselines3.active_tamer.policies.ActiveSACHPolicy'>}}
<class 'stable_baselines3.active_tamer.policies.SACPolicy'>
{'MlpPolicy': <class 'stable_baselines3.active_tamer.policies.SACPolicy'>, 'CnnPolicy': <class 'stable_baselines3.active_tamer.policies.CnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.active_tamer.policies.MultiInputPolicy'>}
MlpPolicy
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Model Policy = SACPolicy(
  (actor): Actor(
    (features_extractor): FlattenExtractor(
      (flatten): Flatten(start_dim=1, end_dim=-1)
    )
    (latent_pi): Sequential(
      (0): Linear(in_features=8, out_features=400, bias=True)
      (1): ReLU()
      (2): Linear(in_features=400, out_features=300, bias=True)
      (3): ReLU()
    )
    (mu): Linear(in_features=300, out_features=2, bias=True)
    (log_std): Linear(in_features=300, out_features=2, bias=True)
  )
  (critic): ContinuousCritic(
    (features_extractor): FlattenExtractor(
      (flatten): Flatten(start_dim=1, end_dim=-1)
    )
    (qf0): Sequential(
      (0): Linear(in_features=10, out_features=400, bias=True)
      (1): ReLU()
      (2): Linear(in_features=400, out_features=300, bias=True)
      (3): ReLU()
      (4): Linear(in_features=300, out_features=1, bias=True)
    )
    (qf1): Sequential(
      (0): Linear(in_features=10, out_features=400, bias=True)
      (1): ReLU()
      (2): Linear(in_features=400, out_features=300, bias=True)
      (3): ReLU()
      (4): Linear(in_features=300, out_features=1, bias=True)
    )
  )
  (critic_target): ContinuousCritic(
    (features_extractor): FlattenExtractor(
      (flatten): Flatten(start_dim=1, end_dim=-1)
    )
    (qf0): Sequential(
      (0): Linear(in_features=10, out_features=400, bias=True)
      (1): ReLU()
      (2): Linear(in_features=400, out_features=300, bias=True)
      (3): ReLU()
      (4): Linear(in_features=300, out_features=1, bias=True)
    )
    (qf1): Sequential(
      (0): Linear(in_features=10, out_features=400, bias=True)
      (1): ReLU()
      (2): Linear(in_features=400, out_features=300, bias=True)
      (3): ReLU()
      (4): Linear(in_features=300, out_features=1, bias=True)
    )
  )
)
Logging to log_dir/SAC_5
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | -207     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 57       |
|    time_elapsed    | 10       |
|    total_timesteps | 613      |
| train/             |          |
|    actor_loss      | -0.15    |
|    critic_loss     | 9.37     |
|    ent_coef        | 0.666    |
|    ent_coef_loss   | -0.856   |
|    learning_rate   | 0.00073  |
|    n_updates       | 612      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 274      |
|    ep_rew_mean     | -220     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 69       |
|    time_elapsed    | 31       |
|    total_timesteps | 2196     |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 50.5     |
|    ent_coef        | 0.271    |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.00073  |
|    n_updates       | 2195     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -178     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 70       |
|    time_elapsed    | 86       |
|    total_timesteps | 6050     |
| train/             |          |
|    actor_loss      | -18.9    |
|    critic_loss     | 2.91     |
|    ent_coef        | 0.136    |
|    ent_coef_loss   | -0.289   |
|    learning_rate   | 0.00073  |
|    n_updates       | 6049     |
---------------------------------
